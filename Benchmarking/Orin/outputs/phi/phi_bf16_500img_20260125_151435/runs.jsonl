{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11164.898, "latencies_ms": [11164.898], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a woman standing in a spacious living room with yellow walls. She is positioned near a dining table, which is surrounded by several chairs. The room is well-lit, with a television placed on the left side of the room. A potted plant can be seen on the right side of the room, adding a touch of greenery to the space.\n", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 20232.4, "ram_available_mb": 42608.5, "ram_percent": 32.2}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 20256.8, "ram_available_mb": 42584.1, "ram_percent": 32.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 12926.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 12926.0, "gpu_utilization_percent": 69.362}, "power_stats": {"power_gpu_soc_mean_watts": 20.538, "power_cpu_cv_mean_watts": 1.714, "power_sys_5v0_mean_watts": 8.925, "gpu_utilization_percent_mean": 69.362, "power_watts_avg": 20.538, "energy_joules_est": 229.32, "duration_seconds": 11.165, "sample_count": 94}, "timestamp": "2026-01-25T15:16:40.981739"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8249.27, "latencies_ms": [8249.27], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "- Television: 1\n- Chair: 6\n- Table: 1\n- Sofa: 1\n- Cabinet: 2\n- Window: 3\n- Plant: 2\n- Rug: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20256.8, "ram_available_mb": 42584.1, "ram_percent": 32.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20251.7, "ram_available_mb": 42589.2, "ram_percent": 32.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 12926.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 12926.0, "gpu_utilization_percent": 72.129}, "power_stats": {"power_gpu_soc_mean_watts": 21.917, "power_cpu_cv_mean_watts": 1.46, "power_sys_5v0_mean_watts": 8.717, "gpu_utilization_percent_mean": 72.129, "power_watts_avg": 21.917, "energy_joules_est": 180.81, "duration_seconds": 8.25, "sample_count": 70}, "timestamp": "2026-01-25T15:16:51.256517"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11241.658, "latencies_ms": [11241.658], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden floor with a rug and a yellow vase on a pink tray. The main objects in the room, such as the television, the chairs, and the dining table, are positioned in the background, with the television on the left side of the room and the chairs and table on the right side. The person is standing near", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20251.7, "ram_available_mb": 42589.2, "ram_percent": 32.2}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 20237.7, "ram_available_mb": 42603.2, "ram_percent": 32.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 12926.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 12926.0, "gpu_utilization_percent": 69.167}, "power_stats": {"power_gpu_soc_mean_watts": 20.582, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.755, "gpu_utilization_percent_mean": 69.167, "power_watts_avg": 20.582, "energy_joules_est": 231.39, "duration_seconds": 11.242, "sample_count": 96}, "timestamp": "2026-01-25T15:17:04.560015"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7815.152, "latencies_ms": [7815.152], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image depicts a warm and inviting living room with yellow walls and hardwood flooring. A person is standing near the kitchen area, which is visible in the background with a refrigerator and cabinets.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 20237.7, "ram_available_mb": 42603.2, "ram_percent": 32.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20236.2, "ram_available_mb": 42604.7, "ram_percent": 32.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 12926.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 12926.0, "gpu_utilization_percent": 71.657}, "power_stats": {"power_gpu_soc_mean_watts": 22.001, "power_cpu_cv_mean_watts": 1.441, "power_sys_5v0_mean_watts": 8.745, "gpu_utilization_percent_mean": 71.657, "power_watts_avg": 22.001, "energy_joules_est": 171.96, "duration_seconds": 7.816, "sample_count": 67}, "timestamp": "2026-01-25T15:17:14.420004"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6708.233, "latencies_ms": [6708.233], "images_per_second": 0.149, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The room has a warm and inviting atmosphere with yellow walls and wooden flooring. The lighting is soft and natural, coming from the large windows that let in plenty of sunlight.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20236.2, "ram_available_mb": 42604.7, "ram_percent": 32.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20255.2, "ram_available_mb": 42585.7, "ram_percent": 32.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 12926.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 12926.0, "gpu_utilization_percent": 73.776}, "power_stats": {"power_gpu_soc_mean_watts": 22.208, "power_cpu_cv_mean_watts": 1.492, "power_sys_5v0_mean_watts": 8.873, "gpu_utilization_percent_mean": 73.776, "power_watts_avg": 22.208, "energy_joules_est": 148.99, "duration_seconds": 6.709, "sample_count": 58}, "timestamp": "2026-01-25T15:17:23.167832"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12870.65, "latencies_ms": [12870.65], "images_per_second": 0.078, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility featuring a brown bear in its natural habitat. The bear, with its fur glistening in the sunlight, is sitting on a bed of lush green grass. Its head is slightly tilted to the left, and it gazes directly into the camera, giving the impression of a curious or attentive demeanor. The bear", "error": null, "sys_before": {"cpu_percent": 8.6, "ram_used_mb": 20255.2, "ram_available_mb": 42585.7, "ram_percent": 32.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21834.2, "ram_available_mb": 41006.7, "ram_percent": 34.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14490.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 14490.0, "gpu_utilization_percent": 72.518}, "power_stats": {"power_gpu_soc_mean_watts": 21.634, "power_cpu_cv_mean_watts": 1.577, "power_sys_5v0_mean_watts": 8.846, "gpu_utilization_percent_mean": 72.518, "power_watts_avg": 21.634, "energy_joules_est": 278.46, "duration_seconds": 12.871, "sample_count": 110}, "timestamp": "2026-01-25T15:17:38.075312"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5432.104, "latencies_ms": [5432.104], "images_per_second": 0.184, "prompt_tokens": 39, "response_tokens_est": 11, "n_tiles": 16, "output_text": "grass: numerous\nbear: 1\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21834.2, "ram_available_mb": 41006.7, "ram_percent": 34.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 23427.1, "ram_available_mb": 39413.8, "ram_percent": 37.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 16070.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 16070.0, "gpu_utilization_percent": 82.244}, "power_stats": {"power_gpu_soc_mean_watts": 25.71, "power_cpu_cv_mean_watts": 0.774, "power_sys_5v0_mean_watts": 8.751, "gpu_utilization_percent_mean": 82.244, "power_watts_avg": 25.71, "energy_joules_est": 139.68, "duration_seconds": 5.433, "sample_count": 45}, "timestamp": "2026-01-25T15:17:45.555920"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12918.468, "latencies_ms": [12918.468], "images_per_second": 0.077, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The bear is positioned in the foreground of the image, appearing large and prominent. The background is a natural setting with grass, which is less detailed and smaller in size, indicating that the bear is closer to the viewer. The bear's head is turned slightly to the left, and its ears are perked up, suggesting it is attentive to its surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 23427.1, "ram_available_mb": 39413.8, "ram_percent": 37.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25024.4, "ram_available_mb": 37816.5, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.339}, "power_stats": {"power_gpu_soc_mean_watts": 22.22, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.891, "gpu_utilization_percent_mean": 71.339, "power_watts_avg": 22.22, "energy_joules_est": 287.06, "duration_seconds": 12.919, "sample_count": 109}, "timestamp": "2026-01-25T15:18:00.522804"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9460.546, "latencies_ms": [9460.546], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a close-up of a bear's face, with its fur appearing soft and well-groomed. The bear is sitting in a grassy area, with the background consisting of green grass and some brown patches.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25024.4, "ram_available_mb": 37816.5, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25018.4, "ram_available_mb": 37822.5, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.287}, "power_stats": {"power_gpu_soc_mean_watts": 23.926, "power_cpu_cv_mean_watts": 1.507, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 77.287, "power_watts_avg": 23.926, "energy_joules_est": 226.37, "duration_seconds": 9.461, "sample_count": 80}, "timestamp": "2026-01-25T15:18:12.041759"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9636.126, "latencies_ms": [9636.126], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The bear in the image has a rich golden-brown fur, and the lighting is soft and natural, suggesting it might be a sunny day. The bear is sitting in a grassy area, which adds a contrasting green background to its fur.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25018.4, "ram_available_mb": 37822.5, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25017.6, "ram_available_mb": 37823.3, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.025}, "power_stats": {"power_gpu_soc_mean_watts": 23.675, "power_cpu_cv_mean_watts": 1.578, "power_sys_5v0_mean_watts": 9.084, "gpu_utilization_percent_mean": 76.025, "power_watts_avg": 23.675, "energy_joules_est": 228.15, "duration_seconds": 9.637, "sample_count": 81}, "timestamp": "2026-01-25T15:18:23.720239"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12354.791, "latencies_ms": [12354.791], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a cozy bedroom with a large bed taking up most of the space. The bed is covered with a blue comforter, and there is a window above it, allowing natural light to fill the room. A wooden bookshelf is situated next to the bed, filled with numerous books of various sizes. \n\nIn addition to the books, there are two potted", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25017.6, "ram_available_mb": 37823.3, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24993.9, "ram_available_mb": 37847.0, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.538}, "power_stats": {"power_gpu_soc_mean_watts": 22.927, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 9.073, "gpu_utilization_percent_mean": 73.538, "power_watts_avg": 22.927, "energy_joules_est": 283.27, "duration_seconds": 12.355, "sample_count": 104}, "timestamp": "2026-01-25T15:18:38.141325"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9230.896, "latencies_ms": [9230.896], "images_per_second": 0.108, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "Bed: 1\nBookshelf: 1\nPlants: 2\nBooks: numerous\nBasket: 1\nMirror: 1\nDresser: 1\nWindow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24993.9, "ram_available_mb": 37847.0, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24985.8, "ram_available_mb": 37855.1, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.756}, "power_stats": {"power_gpu_soc_mean_watts": 24.034, "power_cpu_cv_mean_watts": 1.479, "power_sys_5v0_mean_watts": 9.005, "gpu_utilization_percent_mean": 76.756, "power_watts_avg": 24.034, "energy_joules_est": 221.87, "duration_seconds": 9.232, "sample_count": 78}, "timestamp": "2026-01-25T15:18:49.430026"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10347.135, "latencies_ms": [10347.135], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The bed is in the foreground on the left side of the image, with a wooden dresser and mirror behind it. The bookshelf is in the background on the right side, filled with books and plants. The window is in the middle ground, providing natural light to the room.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24985.8, "ram_available_mb": 37855.1, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24991.5, "ram_available_mb": 37849.4, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.182}, "power_stats": {"power_gpu_soc_mean_watts": 23.436, "power_cpu_cv_mean_watts": 1.634, "power_sys_5v0_mean_watts": 9.097, "gpu_utilization_percent_mean": 75.182, "power_watts_avg": 23.436, "energy_joules_est": 242.51, "duration_seconds": 10.348, "sample_count": 88}, "timestamp": "2026-01-25T15:19:01.829664"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11503.639, "latencies_ms": [11503.639], "images_per_second": 0.087, "prompt_tokens": 37, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image depicts a cozy bedroom with a large bed covered in a blue comforter, a wooden dresser with a mirror, and a bookshelf filled with books. The room has a window with a view of greenery outside, and there are potted plants adding a touch of nature to the space.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24991.5, "ram_available_mb": 37849.4, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24997.9, "ram_available_mb": 37843.0, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.571}, "power_stats": {"power_gpu_soc_mean_watts": 23.252, "power_cpu_cv_mean_watts": 1.668, "power_sys_5v0_mean_watts": 9.032, "gpu_utilization_percent_mean": 75.571, "power_watts_avg": 23.252, "energy_joules_est": 267.5, "duration_seconds": 11.504, "sample_count": 98}, "timestamp": "2026-01-25T15:19:15.362438"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9413.16, "latencies_ms": [9413.16], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The room is filled with natural light from the large window, which shows a view of lush green trees outside. The bed is covered with a blue comforter, and there is a wooden bookshelf filled with various books and decorative items.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24997.9, "ram_available_mb": 37843.0, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24997.6, "ram_available_mb": 37843.3, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.049}, "power_stats": {"power_gpu_soc_mean_watts": 23.812, "power_cpu_cv_mean_watts": 1.573, "power_sys_5v0_mean_watts": 9.147, "gpu_utilization_percent_mean": 76.049, "power_watts_avg": 23.812, "energy_joules_est": 224.16, "duration_seconds": 9.414, "sample_count": 81}, "timestamp": "2026-01-25T15:19:26.831096"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11207.653, "latencies_ms": [11207.653], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene day at a suburban intersection. Dominating the scene is a stop sign, its octagonal shape and bold red color standing out against the backdrop of a clear blue sky dotted with fluffy white clouds. The sign, mounted on a sturdy black pole, is positioned on the right side of the frame, commanding attention.\n", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24997.6, "ram_available_mb": 37843.3, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25089.4, "ram_available_mb": 37751.5, "ram_percent": 39.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.938}, "power_stats": {"power_gpu_soc_mean_watts": 20.772, "power_cpu_cv_mean_watts": 1.932, "power_sys_5v0_mean_watts": 8.853, "gpu_utilization_percent_mean": 68.938, "power_watts_avg": 20.772, "energy_joules_est": 232.82, "duration_seconds": 11.209, "sample_count": 96}, "timestamp": "2026-01-25T15:19:40.076160"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7229.88, "latencies_ms": [7229.88], "images_per_second": 0.138, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "stop sign: 1, tree: 5, car: 1, building: 1, bush: 3, bench: 1, street: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25035.7, "ram_available_mb": 37805.2, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25023.8, "ram_available_mb": 37817.1, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.934}, "power_stats": {"power_gpu_soc_mean_watts": 22.619, "power_cpu_cv_mean_watts": 1.616, "power_sys_5v0_mean_watts": 8.914, "gpu_utilization_percent_mean": 71.934, "power_watts_avg": 22.619, "energy_joules_est": 163.55, "duration_seconds": 7.231, "sample_count": 61}, "timestamp": "2026-01-25T15:19:49.340294"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8345.753, "latencies_ms": [8345.753], "images_per_second": 0.12, "prompt_tokens": 44, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The stop sign is in the foreground on the right side of the image, near the center. In the background, there is a road that curves to the left with a vehicle visible on it, and beyond the road, there are trees and a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25023.8, "ram_available_mb": 37817.1, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25120.4, "ram_available_mb": 37720.5, "ram_percent": 40.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.592}, "power_stats": {"power_gpu_soc_mean_watts": 21.827, "power_cpu_cv_mean_watts": 1.744, "power_sys_5v0_mean_watts": 8.894, "gpu_utilization_percent_mean": 70.592, "power_watts_avg": 21.827, "energy_joules_est": 182.18, "duration_seconds": 8.346, "sample_count": 71}, "timestamp": "2026-01-25T15:19:59.740411"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8966.219, "latencies_ms": [8966.219], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image shows a red stop sign with the word \"STOP\" in white letters, mounted on a metal pole at an intersection. The sign is partially obscured by a tree, and the background features a clear sky, some greenery, and a building in the distance.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 25120.4, "ram_available_mb": 37720.5, "ram_percent": 40.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25125.9, "ram_available_mb": 37715.0, "ram_percent": 40.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.539}, "power_stats": {"power_gpu_soc_mean_watts": 21.203, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 8.85, "gpu_utilization_percent_mean": 71.539, "power_watts_avg": 21.203, "energy_joules_est": 190.12, "duration_seconds": 8.967, "sample_count": 76}, "timestamp": "2026-01-25T15:20:10.728971"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8350.037, "latencies_ms": [8350.037], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The stop sign in the image is octagonal and red with white lettering, indicating a command to stop. The sign is mounted on a single metal post and is positioned at an intersection with a clear sky in the background and some greenery around.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25125.9, "ram_available_mb": 37715.0, "ram_percent": 40.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25130.5, "ram_available_mb": 37710.4, "ram_percent": 40.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.771}, "power_stats": {"power_gpu_soc_mean_watts": 21.807, "power_cpu_cv_mean_watts": 1.734, "power_sys_5v0_mean_watts": 8.91, "gpu_utilization_percent_mean": 70.771, "power_watts_avg": 21.807, "energy_joules_est": 182.1, "duration_seconds": 8.351, "sample_count": 70}, "timestamp": "2026-01-25T15:20:21.101494"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11205.314, "latencies_ms": [11205.314], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are three teddy bears of different sizes and colors, all sitting together on a couch. The largest teddy bear is positioned in the center, with the other two smaller ones on either side of it. The teddy bears are arranged in a way that they appear to be hugging each other, creating a sense of warmth and togethern", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 25051.9, "ram_available_mb": 37789.0, "ram_percent": 39.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25084.5, "ram_available_mb": 37756.4, "ram_percent": 39.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.01}, "power_stats": {"power_gpu_soc_mean_watts": 20.73, "power_cpu_cv_mean_watts": 1.924, "power_sys_5v0_mean_watts": 8.859, "gpu_utilization_percent_mean": 69.01, "power_watts_avg": 20.73, "energy_joules_est": 232.3, "duration_seconds": 11.206, "sample_count": 96}, "timestamp": "2026-01-25T15:20:34.356519"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10499.862, "latencies_ms": [10499.862], "images_per_second": 0.095, "prompt_tokens": 39, "response_tokens_est": 71, "n_tiles": 16, "output_text": "- Teddy bear: 3\n- Stuffed animal: 3\n- Stuffed animal: 3\n- Stuffed animal: 3\n- Stuffed animal: 3\n- Stuffed animal: 3\n- Stuffed animal: 3\n- Stuffed animal: 3", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25084.5, "ram_available_mb": 37756.4, "ram_percent": 39.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25028.5, "ram_available_mb": 37812.4, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.865}, "power_stats": {"power_gpu_soc_mean_watts": 21.211, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 8.852, "gpu_utilization_percent_mean": 69.865, "power_watts_avg": 21.211, "energy_joules_est": 222.73, "duration_seconds": 10.501, "sample_count": 89}, "timestamp": "2026-01-25T15:20:46.906977"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11281.869, "latencies_ms": [11281.869], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, there is a large brown teddy bear with a smaller one in front of it, both are positioned close to each other, suggesting a sense of closeness or affection. In the background, there is a blue blanket, which is partially visible and appears to be underneath the teddy bears, indicating that they are placed on top of it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25028.5, "ram_available_mb": 37812.4, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25123.9, "ram_available_mb": 37717.0, "ram_percent": 40.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.198}, "power_stats": {"power_gpu_soc_mean_watts": 20.818, "power_cpu_cv_mean_watts": 1.911, "power_sys_5v0_mean_watts": 8.865, "gpu_utilization_percent_mean": 69.198, "power_watts_avg": 20.818, "energy_joules_est": 234.88, "duration_seconds": 11.282, "sample_count": 96}, "timestamp": "2026-01-25T15:21:00.232540"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7751.431, "latencies_ms": [7751.431], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "Three teddy bears are huddled together on a couch, creating a cozy and comforting scene. The bears appear to be snuggled up close, possibly for warmth or companionship.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25033.6, "ram_available_mb": 37807.2, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25038.2, "ram_available_mb": 37802.7, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.697}, "power_stats": {"power_gpu_soc_mean_watts": 21.71, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 8.873, "gpu_utilization_percent_mean": 73.697, "power_watts_avg": 21.71, "energy_joules_est": 168.3, "duration_seconds": 7.752, "sample_count": 66}, "timestamp": "2026-01-25T15:21:10.037376"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7405.185, "latencies_ms": [7405.185], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image features three teddy bears with a warm, golden-brown color palette. The lighting appears to be soft and diffused, highlighting the plush texture of the bears' fur.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25038.2, "ram_available_mb": 37802.7, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24995.9, "ram_available_mb": 37845.0, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.111}, "power_stats": {"power_gpu_soc_mean_watts": 22.479, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 71.111, "power_watts_avg": 22.479, "energy_joules_est": 166.47, "duration_seconds": 7.406, "sample_count": 63}, "timestamp": "2026-01-25T15:21:19.468657"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11236.003, "latencies_ms": [11236.003], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is skiing down a snowy hill on a sunny day. She is wearing a red and black ski jacket, and her skis are visible as she glides through the snow. The woman is holding ski poles, which help her maintain balance and control as she descends the slope. The snowy hill provides a picturesque backdrop for her", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24995.9, "ram_available_mb": 37845.0, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24985.0, "ram_available_mb": 37855.9, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.887}, "power_stats": {"power_gpu_soc_mean_watts": 19.972, "power_cpu_cv_mean_watts": 1.941, "power_sys_5v0_mean_watts": 8.878, "gpu_utilization_percent_mean": 68.887, "power_watts_avg": 19.972, "energy_joules_est": 224.42, "duration_seconds": 11.237, "sample_count": 97}, "timestamp": "2026-01-25T15:21:32.757957"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8579.205, "latencies_ms": [8579.205], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "hill: 1, red jacket: 1, black pants: 1, ski poles: 2, snow: 1, blue boots: 1, white snow: 1, red and white striped hat: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24985.0, "ram_available_mb": 37855.9, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25001.9, "ram_available_mb": 37839.0, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.676}, "power_stats": {"power_gpu_soc_mean_watts": 21.825, "power_cpu_cv_mean_watts": 1.732, "power_sys_5v0_mean_watts": 8.877, "gpu_utilization_percent_mean": 71.676, "power_watts_avg": 21.825, "energy_joules_est": 187.26, "duration_seconds": 8.58, "sample_count": 74}, "timestamp": "2026-01-25T15:21:43.382225"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10244.948, "latencies_ms": [10244.948], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The skier is positioned in the foreground of the image, actively skiing down the slope. The red poles are held in the skier's hands, aiding in balance and direction. In the background, there are red poles lined up on the snow, possibly indicating a boundary or path for skiers.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25001.9, "ram_available_mb": 37839.0, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24993.7, "ram_available_mb": 37847.2, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.402}, "power_stats": {"power_gpu_soc_mean_watts": 20.377, "power_cpu_cv_mean_watts": 1.874, "power_sys_5v0_mean_watts": 8.905, "gpu_utilization_percent_mean": 69.402, "power_watts_avg": 20.377, "energy_joules_est": 208.77, "duration_seconds": 10.246, "sample_count": 87}, "timestamp": "2026-01-25T15:21:55.679052"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8842.72, "latencies_ms": [8842.72], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "A person is skiing on a snowy slope with red poles and wearing a red and black jacket, black pants, and a striped hat. The background shows a snowy mountain with red poles marking the boundaries of the skiing area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24993.7, "ram_available_mb": 37847.2, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24988.5, "ram_available_mb": 37852.4, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.16}, "power_stats": {"power_gpu_soc_mean_watts": 21.234, "power_cpu_cv_mean_watts": 1.741, "power_sys_5v0_mean_watts": 8.87, "gpu_utilization_percent_mean": 72.16, "power_watts_avg": 21.234, "energy_joules_est": 187.78, "duration_seconds": 8.843, "sample_count": 75}, "timestamp": "2026-01-25T15:22:06.554154"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6592.517, "latencies_ms": [6592.517], "images_per_second": 0.152, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The skier is wearing a red and black jacket, black pants, and blue skis. The sky is overcast and the snow appears to be freshly fallen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24988.5, "ram_available_mb": 37852.4, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25000.3, "ram_available_mb": 37840.6, "ram_percent": 39.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.71, "power_cpu_cv_mean_watts": 1.624, "power_sys_5v0_mean_watts": 9.011, "gpu_utilization_percent_mean": 72.0, "power_watts_avg": 22.71, "energy_joules_est": 149.73, "duration_seconds": 6.593, "sample_count": 56}, "timestamp": "2026-01-25T15:22:15.162774"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11298.204, "latencies_ms": [11298.204], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a cozy kitchen scene. Dominating the space is a white refrigerator, standing tall on the right side of the frame. Adjacent to it, a white oven and a white dishwasher are neatly arranged, their pristine surfaces reflecting the light. A white microwave is tucked away on the left side,", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 25000.3, "ram_available_mb": 37840.6, "ram_percent": 39.8}, "sys_after": {"cpu_percent": 9.0, "ram_used_mb": 24438.3, "ram_available_mb": 38402.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.515}, "power_stats": {"power_gpu_soc_mean_watts": 19.733, "power_cpu_cv_mean_watts": 2.255, "power_sys_5v0_mean_watts": 8.913, "gpu_utilization_percent_mean": 69.515, "power_watts_avg": 19.733, "energy_joules_est": 222.96, "duration_seconds": 11.299, "sample_count": 97}, "timestamp": "2026-01-25T15:22:28.504697"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8479.301, "latencies_ms": [8479.301], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "refrigerator: 1, oven: 1, stove: 1, dishwasher: 1, microwave: 1, cabinet: 4, drawer: 1, countertop: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24334.8, "ram_available_mb": 38506.1, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24425.1, "ram_available_mb": 38415.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.027}, "power_stats": {"power_gpu_soc_mean_watts": 21.844, "power_cpu_cv_mean_watts": 1.734, "power_sys_5v0_mean_watts": 8.882, "gpu_utilization_percent_mean": 72.027, "power_watts_avg": 21.844, "energy_joules_est": 185.24, "duration_seconds": 8.48, "sample_count": 73}, "timestamp": "2026-01-25T15:22:39.015502"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8676.676, "latencies_ms": [8676.676], "images_per_second": 0.115, "prompt_tokens": 44, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The refrigerator is located in the background, towards the right side of the image, while the oven is in the foreground on the left side. The drawer is situated between the oven and the refrigerator, closer to the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24328.4, "ram_available_mb": 38512.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.824}, "power_stats": {"power_gpu_soc_mean_watts": 21.439, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 70.824, "power_watts_avg": 21.439, "energy_joules_est": 186.03, "duration_seconds": 8.677, "sample_count": 74}, "timestamp": "2026-01-25T15:22:49.748235"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8905.299, "latencies_ms": [8905.299], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image shows a small, well-lit kitchen with wooden cabinets and white appliances. There is a white refrigerator on the right side, a white oven and dishwasher on the left, and a wooden drawer in the center.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24356.4, "ram_available_mb": 38484.5, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.132}, "power_stats": {"power_gpu_soc_mean_watts": 21.676, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 8.884, "gpu_utilization_percent_mean": 71.132, "power_watts_avg": 21.676, "energy_joules_est": 193.04, "duration_seconds": 8.906, "sample_count": 76}, "timestamp": "2026-01-25T15:23:00.698202"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7418.591, "latencies_ms": [7418.591], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The kitchen has wooden cabinets and white appliances, with a light brownish-beige wall and beige tiled floor. The lighting appears to be artificial, coming from ceiling fixtures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24356.4, "ram_available_mb": 38484.5, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24417.5, "ram_available_mb": 38423.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.571}, "power_stats": {"power_gpu_soc_mean_watts": 22.289, "power_cpu_cv_mean_watts": 1.678, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 71.571, "power_watts_avg": 22.289, "energy_joules_est": 165.37, "duration_seconds": 7.419, "sample_count": 63}, "timestamp": "2026-01-25T15:23:10.173898"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12431.688, "latencies_ms": [12431.688], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two men playing baseball on a field. One man is running towards the base, while the other is throwing the ball. The baseball player running is wearing a blue helmet and a baseball glove, and he is in motion, likely trying to reach the base before the ball is caught. The other player is standing and holding a baseball glove, ready to catch", "error": null, "sys_before": {"cpu_percent": 8.6, "ram_used_mb": 24322.4, "ram_available_mb": 38518.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24320.7, "ram_available_mb": 38520.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.623}, "power_stats": {"power_gpu_soc_mean_watts": 22.495, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 9.135, "gpu_utilization_percent_mean": 72.623, "power_watts_avg": 22.495, "energy_joules_est": 279.67, "duration_seconds": 12.433, "sample_count": 106}, "timestamp": "2026-01-25T15:23:24.669630"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 12751.485, "latencies_ms": [12751.485], "images_per_second": 0.078, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "1. Player: 2\n2. Pitcher: 1\n3. Baseball glove: 1\n4. Baseball: 1\n5. Pitcher's mound: 1\n6. Pitcher's arm: 1\n7. Pitcher's hand: 1\n8. Pitcher's wristband", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24320.7, "ram_available_mb": 38520.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24350.4, "ram_available_mb": 38490.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.349}, "power_stats": {"power_gpu_soc_mean_watts": 22.855, "power_cpu_cv_mean_watts": 1.745, "power_sys_5v0_mean_watts": 9.081, "gpu_utilization_percent_mean": 74.349, "power_watts_avg": 22.855, "energy_joules_est": 291.45, "duration_seconds": 12.752, "sample_count": 109}, "timestamp": "2026-01-25T15:23:39.433511"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11817.859, "latencies_ms": [11817.859], "images_per_second": 0.085, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, there is a baseball player in a white shirt and gray pants running towards the left side of the image, while another player in a green shirt and gray pants is standing and holding a baseball glove to the right side of the image. The background consists of a grassy field with trees and a wooden fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24350.4, "ram_available_mb": 38490.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.079}, "power_stats": {"power_gpu_soc_mean_watts": 22.924, "power_cpu_cv_mean_watts": 1.745, "power_sys_5v0_mean_watts": 9.139, "gpu_utilization_percent_mean": 73.079, "power_watts_avg": 22.924, "energy_joules_est": 270.93, "duration_seconds": 11.819, "sample_count": 101}, "timestamp": "2026-01-25T15:23:53.270598"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7512.297, "latencies_ms": [7512.297], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 33, "n_tiles": 16, "output_text": "Two men are playing baseball on a field with trees in the background. One man is running towards the base while the other is preparing to throw the ball.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24349.0, "ram_available_mb": 38491.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24449.6, "ram_available_mb": 38391.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 80.109}, "power_stats": {"power_gpu_soc_mean_watts": 24.827, "power_cpu_cv_mean_watts": 1.302, "power_sys_5v0_mean_watts": 9.131, "gpu_utilization_percent_mean": 80.109, "power_watts_avg": 24.827, "energy_joules_est": 186.52, "duration_seconds": 7.513, "sample_count": 64}, "timestamp": "2026-01-25T15:24:02.831250"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9602.573, "latencies_ms": [9602.573], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows two individuals playing baseball on a field with a clear sky above. The person in the foreground is wearing a white shirt and grey pants, while the person in the background is wearing a green cap and a green shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24449.6, "ram_available_mb": 38391.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24384.9, "ram_available_mb": 38456.0, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.333}, "power_stats": {"power_gpu_soc_mean_watts": 23.177, "power_cpu_cv_mean_watts": 1.587, "power_sys_5v0_mean_watts": 9.167, "gpu_utilization_percent_mean": 76.333, "power_watts_avg": 23.177, "energy_joules_est": 222.57, "duration_seconds": 9.603, "sample_count": 81}, "timestamp": "2026-01-25T15:24:14.461783"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11242.528, "latencies_ms": [11242.528], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a tennis player in the midst of a powerful swing, holding a tennis racket and preparing to hit the ball. The player is positioned on a tennis court, with a blue wall in the background featuring a J.P. Morgan logo. There are several other people in the scene, likely spectators or fellow players, watching the game. The tennis player is focused on", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24384.8, "ram_available_mb": 38456.1, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24339.7, "ram_available_mb": 38501.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.474}, "power_stats": {"power_gpu_soc_mean_watts": 19.761, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.908, "gpu_utilization_percent_mean": 69.474, "power_watts_avg": 19.761, "energy_joules_est": 222.18, "duration_seconds": 11.243, "sample_count": 95}, "timestamp": "2026-01-25T15:24:27.766120"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9043.933, "latencies_ms": [9043.933], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 58, "n_tiles": 16, "output_text": "- J.P. Morgan: 1\n- Wall: 1\n- Man: 1\n- Tennis racket: 1\n- Tennis ball: 1\n- Tennis court: 1\n- Fan: 1\n- Spectator: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24339.7, "ram_available_mb": 38501.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24316.7, "ram_available_mb": 38524.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.286}, "power_stats": {"power_gpu_soc_mean_watts": 21.566, "power_cpu_cv_mean_watts": 1.774, "power_sys_5v0_mean_watts": 8.91, "gpu_utilization_percent_mean": 71.286, "power_watts_avg": 21.566, "energy_joules_est": 195.06, "duration_seconds": 9.045, "sample_count": 77}, "timestamp": "2026-01-25T15:24:38.852501"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11326.12, "latencies_ms": [11326.12], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a tennis player is positioned on the court, holding a tennis racket and preparing to hit a ball. The player is near the baseline, which is the line closest to the net. In the background, there is a wall with the J.P. Morgan logo, and a few spectators are seated behind it. To the right of the player,", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24316.7, "ram_available_mb": 38524.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24311.9, "ram_available_mb": 38529.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_gpu_soc_mean_watts": 19.66, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.886, "gpu_utilization_percent_mean": 70.0, "power_watts_avg": 19.66, "energy_joules_est": 222.68, "duration_seconds": 11.327, "sample_count": 95}, "timestamp": "2026-01-25T15:24:52.205733"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6803.723, "latencies_ms": [6803.723], "images_per_second": 0.147, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A tennis player is in the midst of a backhand swing on a tennis court. The J.P. Morgan logo is prominently displayed on the back wall of the court.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24311.9, "ram_available_mb": 38529.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24312.5, "ram_available_mb": 38528.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.632}, "power_stats": {"power_gpu_soc_mean_watts": 22.884, "power_cpu_cv_mean_watts": 1.546, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 72.632, "power_watts_avg": 22.884, "energy_joules_est": 155.71, "duration_seconds": 6.804, "sample_count": 57}, "timestamp": "2026-01-25T15:25:01.056899"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5587.889, "latencies_ms": [5587.889], "images_per_second": 0.179, "prompt_tokens": 36, "response_tokens_est": 29, "n_tiles": 16, "output_text": "The tennis player is wearing a white cap and a white and blue striped shirt. The court is blue with a green surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24312.5, "ram_available_mb": 38528.4, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24316.0, "ram_available_mb": 38524.9, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.021}, "power_stats": {"power_gpu_soc_mean_watts": 23.79, "power_cpu_cv_mean_watts": 1.423, "power_sys_5v0_mean_watts": 9.079, "gpu_utilization_percent_mean": 74.021, "power_watts_avg": 23.79, "energy_joules_est": 132.95, "duration_seconds": 5.589, "sample_count": 47}, "timestamp": "2026-01-25T15:25:08.685015"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11305.027, "latencies_ms": [11305.027], "images_per_second": 0.088, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of young children gathered together on a tennis court. They are posing for a picture, with some of them holding tennis rackets. The children are standing in front of a tennis net, which divides the court into two halves. The scene captures a moment of camaraderie and shared interest among the kids as they enjoy their time on the court.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 24316.0, "ram_available_mb": 38524.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24351.4, "ram_available_mb": 38489.5, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.896}, "power_stats": {"power_gpu_soc_mean_watts": 19.747, "power_cpu_cv_mean_watts": 1.915, "power_sys_5v0_mean_watts": 8.87, "gpu_utilization_percent_mean": 69.896, "power_watts_avg": 19.747, "energy_joules_est": 223.25, "duration_seconds": 11.306, "sample_count": 96}, "timestamp": "2026-01-25T15:25:22.031067"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8284.468, "latencies_ms": [8284.468], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "children: 11\nbags: 2\nrackets: 2\nsneakers: 5\nhats: 7\nt-shirts: 3\nnets: 2\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24351.4, "ram_available_mb": 38489.5, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24361.0, "ram_available_mb": 38479.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.7}, "power_stats": {"power_gpu_soc_mean_watts": 21.612, "power_cpu_cv_mean_watts": 1.688, "power_sys_5v0_mean_watts": 8.927, "gpu_utilization_percent_mean": 72.7, "power_watts_avg": 21.612, "energy_joules_est": 179.06, "duration_seconds": 8.285, "sample_count": 70}, "timestamp": "2026-01-25T15:25:32.381598"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11247.992, "latencies_ms": [11247.992], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a group of children is gathered near a tennis court, with some standing closer to the camera and others further away, creating a sense of depth. The children are positioned in front of a green fence that serves as the background, indicating that the tennis court is enclosed. The children are standing on a blue surface, likely the tennis court itself, which is the central", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24299.3, "ram_available_mb": 38541.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24335.6, "ram_available_mb": 38505.3, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.312}, "power_stats": {"power_gpu_soc_mean_watts": 20.022, "power_cpu_cv_mean_watts": 1.932, "power_sys_5v0_mean_watts": 8.88, "gpu_utilization_percent_mean": 69.312, "power_watts_avg": 20.022, "energy_joules_est": 225.22, "duration_seconds": 11.249, "sample_count": 96}, "timestamp": "2026-01-25T15:25:45.688176"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8361.909, "latencies_ms": [8361.909], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A group of children and one adult are gathered on a tennis court, with the adult holding a trophy, suggesting a tennis-related event or competition. The setting appears to be outdoors, with trees and a fence visible in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24335.6, "ram_available_mb": 38505.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24309.6, "ram_available_mb": 38531.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.239}, "power_stats": {"power_gpu_soc_mean_watts": 21.965, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 8.928, "gpu_utilization_percent_mean": 72.239, "power_watts_avg": 21.965, "energy_joules_est": 183.68, "duration_seconds": 8.363, "sample_count": 71}, "timestamp": "2026-01-25T15:25:56.070520"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8203.25, "latencies_ms": [8203.25], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a group of children on a tennis court with a bright and sunny day, casting shadows on the ground. They are wearing various sports attire, including white, red, and black, and some are wearing hats.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24309.6, "ram_available_mb": 38531.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24317.8, "ram_available_mb": 38523.1, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.884}, "power_stats": {"power_gpu_soc_mean_watts": 21.441, "power_cpu_cv_mean_watts": 1.724, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 71.884, "power_watts_avg": 21.441, "energy_joules_est": 175.9, "duration_seconds": 8.204, "sample_count": 69}, "timestamp": "2026-01-25T15:26:06.332658"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11248.19, "latencies_ms": [11248.19], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene by a body of water, possibly a river or a lake. A woman is standing on the left side of the frame, holding a camera and taking a picture of a bird on the shore. There are several other people in the scene, with some sitting on the ground and others standing. \n\nIn the background, a boat can be seen on the", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24317.8, "ram_available_mb": 38523.1, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24300.4, "ram_available_mb": 38540.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.698}, "power_stats": {"power_gpu_soc_mean_watts": 19.975, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.903, "gpu_utilization_percent_mean": 69.698, "power_watts_avg": 19.975, "energy_joules_est": 224.7, "duration_seconds": 11.249, "sample_count": 96}, "timestamp": "2026-01-25T15:26:19.630766"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7141.62, "latencies_ms": [7141.62], "images_per_second": 0.14, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "people: 3, birds: 1, camera: 1, backpack: 1, water: 1, sun: 1, buildings: 1, trees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24300.4, "ram_available_mb": 38540.4, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24343.2, "ram_available_mb": 38497.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.183}, "power_stats": {"power_gpu_soc_mean_watts": 22.689, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 73.183, "power_watts_avg": 22.689, "energy_joules_est": 162.05, "duration_seconds": 7.142, "sample_count": 60}, "timestamp": "2026-01-25T15:26:28.821993"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11271.791, "latencies_ms": [11271.791], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a paved walkway where a person is taking a photo of a bird on the ground. To the left, two individuals are seated on the edge of the walkway, observing the scene. In the background, there is a body of water with a boat and buildings along the shore. The bird is positioned near the center of the image, closer", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24343.2, "ram_available_mb": 38497.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24365.8, "ram_available_mb": 38475.1, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.144}, "power_stats": {"power_gpu_soc_mean_watts": 19.823, "power_cpu_cv_mean_watts": 1.928, "power_sys_5v0_mean_watts": 8.906, "gpu_utilization_percent_mean": 70.144, "power_watts_avg": 19.823, "energy_joules_est": 223.45, "duration_seconds": 11.272, "sample_count": 97}, "timestamp": "2026-01-25T15:26:42.136783"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6784.388, "latencies_ms": [6784.388], "images_per_second": 0.147, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image depicts a serene scene by a river with a bridge overhead. People are sitting on the edge of the riverbank, observing a heron in the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24312.2, "ram_available_mb": 38528.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24346.2, "ram_available_mb": 38494.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.603}, "power_stats": {"power_gpu_soc_mean_watts": 22.769, "power_cpu_cv_mean_watts": 1.567, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 73.603, "power_watts_avg": 22.769, "energy_joules_est": 154.49, "duration_seconds": 6.785, "sample_count": 58}, "timestamp": "2026-01-25T15:26:50.951176"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11089.653, "latencies_ms": [11089.653], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The image features a serene waterfront scene with a curved metal structure overhead, likely a bridge or an awning, casting shadows on the ground. The lighting suggests it's either early morning or late afternoon, with the sun low in the sky, creating a warm glow and highlighting the calm water and the presence of a single white bird on the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24346.2, "ram_available_mb": 38494.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24334.3, "ram_available_mb": 38506.6, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.309}, "power_stats": {"power_gpu_soc_mean_watts": 20.741, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.922, "gpu_utilization_percent_mean": 69.309, "power_watts_avg": 20.741, "energy_joules_est": 230.02, "duration_seconds": 11.09, "sample_count": 94}, "timestamp": "2026-01-25T15:27:04.062050"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11241.58, "latencies_ms": [11241.58], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the image, a woman is standing and holding a cell phone in her hands. She is wearing a white shirt and has a bracelet on her wrist. The woman appears to be looking at the phone screen, possibly reading a message or browsing the internet. The cell phone is positioned in her right hand, and she seems to be focused on the device.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24334.3, "ram_available_mb": 38506.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 24408.4, "ram_available_mb": 38432.5, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.063}, "power_stats": {"power_gpu_soc_mean_watts": 19.903, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 70.063, "power_watts_avg": 19.903, "energy_joules_est": 223.76, "duration_seconds": 11.242, "sample_count": 95}, "timestamp": "2026-01-25T15:27:17.335717"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11416.778, "latencies_ms": [11416.778], "images_per_second": 0.088, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "1. Hello Kitty phone case: 1\n2. Woman's face: 1\n3. Woman's hand: 1\n4. Woman's bracelet: 1\n5. Woman's ring: 1\n6. Woman's watch: 1\n7. Woman's ear: 1\n8. Woman's", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24304.9, "ram_available_mb": 38536.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 24313.4, "ram_available_mb": 38527.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.337}, "power_stats": {"power_gpu_soc_mean_watts": 20.744, "power_cpu_cv_mean_watts": 1.929, "power_sys_5v0_mean_watts": 8.889, "gpu_utilization_percent_mean": 69.337, "power_watts_avg": 20.744, "energy_joules_est": 236.84, "duration_seconds": 11.417, "sample_count": 98}, "timestamp": "2026-01-25T15:27:30.772963"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10930.185, "latencies_ms": [10930.185], "images_per_second": 0.091, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The woman is holding a phone in her right hand, which is positioned in the foreground of the image. The phone screen displays the text \"HELLO KITTY,\" indicating it is the main object of focus. In the background, there are blurred figures of other people, suggesting that the woman is not alone and is possibly in a public space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24313.4, "ram_available_mb": 38527.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24301.7, "ram_available_mb": 38539.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.376}, "power_stats": {"power_gpu_soc_mean_watts": 20.271, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 70.376, "power_watts_avg": 20.271, "energy_joules_est": 221.58, "duration_seconds": 10.931, "sample_count": 93}, "timestamp": "2026-01-25T15:27:43.719992"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8042.1, "latencies_ms": [8042.1], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A woman is holding a phone case with the text \"HELLO KITTY\" on it, while wearing a bracelet and a ring. She is wearing a sleeveless top with a graphic design.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24301.7, "ram_available_mb": 38539.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24252.7, "ram_available_mb": 38588.2, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.217}, "power_stats": {"power_gpu_soc_mean_watts": 21.909, "power_cpu_cv_mean_watts": 1.695, "power_sys_5v0_mean_watts": 8.916, "gpu_utilization_percent_mean": 72.217, "power_watts_avg": 21.909, "energy_joules_est": 176.21, "duration_seconds": 8.043, "sample_count": 69}, "timestamp": "2026-01-25T15:27:53.820633"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8340.092, "latencies_ms": [8340.092], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image features a person holding a phone with a case that has a Hello Kitty design on it. The person is wearing a white sleeveless top with a black and white pattern, and a green bracelet on their wrist.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24197.4, "ram_available_mb": 38643.5, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24294.2, "ram_available_mb": 38546.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.944}, "power_stats": {"power_gpu_soc_mean_watts": 21.722, "power_cpu_cv_mean_watts": 1.76, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 70.944, "power_watts_avg": 21.722, "energy_joules_est": 181.18, "duration_seconds": 8.341, "sample_count": 71}, "timestamp": "2026-01-25T15:28:04.175726"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11277.933, "latencies_ms": [11277.933], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a group of children is gathered around a red and white striped carousel horse, which is positioned on a wooden floor. The children are sitting on the horse, with some of them hugging each other, creating a sense of camaraderie and excitement. The carousel horse is the central focus of the scene, and the children are enjoying their time together", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24294.2, "ram_available_mb": 38546.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24305.5, "ram_available_mb": 38535.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.745}, "power_stats": {"power_gpu_soc_mean_watts": 19.609, "power_cpu_cv_mean_watts": 1.933, "power_sys_5v0_mean_watts": 8.889, "gpu_utilization_percent_mean": 70.745, "power_watts_avg": 19.609, "energy_joules_est": 221.16, "duration_seconds": 11.279, "sample_count": 98}, "timestamp": "2026-01-25T15:28:17.487464"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9943.216, "latencies_ms": [9943.216], "images_per_second": 0.101, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "- children: 5\n\n- red and white striped barrier: 1\n\n- stage: 1\n\n- microphone stand: 1\n\n- floor: 1\n\n- wall: 1\n\n- ceiling: 1\n\n- light fixture: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24191.8, "ram_available_mb": 38649.1, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24192.7, "ram_available_mb": 38648.2, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.376}, "power_stats": {"power_gpu_soc_mean_watts": 21.304, "power_cpu_cv_mean_watts": 1.823, "power_sys_5v0_mean_watts": 8.901, "gpu_utilization_percent_mean": 71.376, "power_watts_avg": 21.304, "energy_joules_est": 211.84, "duration_seconds": 9.944, "sample_count": 85}, "timestamp": "2026-01-25T15:28:29.468649"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11267.504, "latencies_ms": [11267.504], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a red and white striped barrel with a group of children sitting inside it, positioned towards the left side of the image. The children are seated closely together, with one child in the front wearing a black jacket and another in the back wearing a white hoodie. In the background, there is a yellow and red striped object", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24192.7, "ram_available_mb": 38648.2, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24194.9, "ram_available_mb": 38646.0, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.739, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.0, "power_watts_avg": 20.739, "energy_joules_est": 233.69, "duration_seconds": 11.268, "sample_count": 95}, "timestamp": "2026-01-25T15:28:42.769036"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8368.026, "latencies_ms": [8368.026], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A group of children is seated on a red and white striped carousel ride inside a building with a wooden floor and a brick wall in the background. The children appear to be enjoying the ride, with some looking ahead and others looking around.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24194.9, "ram_available_mb": 38646.0, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24192.3, "ram_available_mb": 38648.6, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.789}, "power_stats": {"power_gpu_soc_mean_watts": 21.929, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 72.789, "power_watts_avg": 21.929, "energy_joules_est": 183.52, "duration_seconds": 8.369, "sample_count": 71}, "timestamp": "2026-01-25T15:28:53.150923"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7114.815, "latencies_ms": [7114.815], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image shows a group of children sitting on a red and white striped carousel ride inside a building with wooden flooring and brick walls. The lighting is dim, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24192.3, "ram_available_mb": 38648.6, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24194.1, "ram_available_mb": 38646.8, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.633}, "power_stats": {"power_gpu_soc_mean_watts": 22.513, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.01, "gpu_utilization_percent_mean": 72.633, "power_watts_avg": 22.513, "energy_joules_est": 160.19, "duration_seconds": 7.115, "sample_count": 60}, "timestamp": "2026-01-25T15:29:02.322466"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12460.534, "latencies_ms": [12460.534], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility, featuring a black and white photograph of a sandwich on a white plate. The sandwich, which is the main subject of the image, is composed of two slices of bread, one of which is slightly larger than the other. The larger slice is filled with a dark substance, possibly a type of meat or vegetable, and the", "error": null, "sys_before": {"cpu_percent": 16.0, "ram_used_mb": 24196.1, "ram_available_mb": 38644.8, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24194.6, "ram_available_mb": 38646.3, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.925}, "power_stats": {"power_gpu_soc_mean_watts": 22.169, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 9.149, "gpu_utilization_percent_mean": 73.925, "power_watts_avg": 22.169, "energy_joules_est": 276.25, "duration_seconds": 12.461, "sample_count": 106}, "timestamp": "2026-01-25T15:29:16.830426"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8771.503, "latencies_ms": [8771.503], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "plate: 1\nbread: 1\nfilling: 1\nknife: 1\nperson: 1\nbackground: 1\nlighting: 1\ncolor: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24194.6, "ram_available_mb": 38646.3, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24190.6, "ram_available_mb": 38650.3, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.56}, "power_stats": {"power_gpu_soc_mean_watts": 24.27, "power_cpu_cv_mean_watts": 1.458, "power_sys_5v0_mean_watts": 9.127, "gpu_utilization_percent_mean": 78.56, "power_watts_avg": 24.27, "energy_joules_est": 212.9, "duration_seconds": 8.772, "sample_count": 75}, "timestamp": "2026-01-25T15:29:27.632321"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11684.717, "latencies_ms": [11684.717], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The sandwich is positioned in the foreground, occupying the central space of the plate. It is placed near the edge of the plate, with a clear space around it. In the background, there is a blurred object that appears to be a cup or container, suggesting it is further away from the viewer than the sandwich.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24190.6, "ram_available_mb": 38650.3, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24193.8, "ram_available_mb": 38647.1, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.848}, "power_stats": {"power_gpu_soc_mean_watts": 23.065, "power_cpu_cv_mean_watts": 1.743, "power_sys_5v0_mean_watts": 9.242, "gpu_utilization_percent_mean": 73.848, "power_watts_avg": 23.065, "energy_joules_est": 269.52, "duration_seconds": 11.685, "sample_count": 99}, "timestamp": "2026-01-25T15:29:41.362464"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10251.276, "latencies_ms": [10251.276], "images_per_second": 0.098, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image shows a close-up of a sandwich on a white plate, with a blurred background that suggests a dining setting. The sandwich appears to be a burger with a bun, and there is a small container of sauce on the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24193.8, "ram_available_mb": 38647.1, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24186.1, "ram_available_mb": 38654.8, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_gpu_soc_mean_watts": 23.592, "power_cpu_cv_mean_watts": 1.579, "power_sys_5v0_mean_watts": 9.103, "gpu_utilization_percent_mean": 76.0, "power_watts_avg": 23.592, "energy_joules_est": 241.86, "duration_seconds": 10.252, "sample_count": 87}, "timestamp": "2026-01-25T15:29:53.657348"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10771.465, "latencies_ms": [10771.465], "images_per_second": 0.093, "prompt_tokens": 36, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image is a black and white photograph, focusing on a sandwich with a visible filling that appears to be a creamy substance, possibly cheese or a spread. The lighting is soft and diffused, casting gentle shadows and highlighting the textures of the sandwich and the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24186.1, "ram_available_mb": 38654.8, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24194.5, "ram_available_mb": 38646.4, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.56}, "power_stats": {"power_gpu_soc_mean_watts": 23.282, "power_cpu_cv_mean_watts": 1.668, "power_sys_5v0_mean_watts": 9.189, "gpu_utilization_percent_mean": 74.56, "power_watts_avg": 23.282, "energy_joules_est": 250.8, "duration_seconds": 10.772, "sample_count": 91}, "timestamp": "2026-01-25T15:30:06.471365"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12224.042, "latencies_ms": [12224.042], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility and balance, featuring a person paddleboarding on a vast body of water. The individual, clad in a wetsuit, stands confidently on a paddleboard, holding a paddle in their hands. The water, a deep shade of blue, stretches out to meet a distant shoreline, where a few", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24194.5, "ram_available_mb": 38646.4, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24188.4, "ram_available_mb": 38652.5, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.346}, "power_stats": {"power_gpu_soc_mean_watts": 21.734, "power_cpu_cv_mean_watts": 1.798, "power_sys_5v0_mean_watts": 9.117, "gpu_utilization_percent_mean": 73.346, "power_watts_avg": 21.734, "energy_joules_est": 265.69, "duration_seconds": 12.225, "sample_count": 104}, "timestamp": "2026-01-25T15:30:20.734979"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8388.635, "latencies_ms": [8388.635], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "water: numerous\npaddle: 1\nsurfboard: 1\nperson: 1\nwetsuit: 1\nsand: numerous\ntrees: numerous\nhouses: numerous", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24188.4, "ram_available_mb": 38652.5, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24304.7, "ram_available_mb": 38536.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.915}, "power_stats": {"power_gpu_soc_mean_watts": 23.539, "power_cpu_cv_mean_watts": 1.461, "power_sys_5v0_mean_watts": 9.101, "gpu_utilization_percent_mean": 76.915, "power_watts_avg": 23.539, "energy_joules_est": 197.47, "duration_seconds": 8.389, "sample_count": 71}, "timestamp": "2026-01-25T15:30:31.178307"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10390.894, "latencies_ms": [10390.894], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The person is standing on a paddleboard in the foreground, closer to the viewer, while the waves and the horizon line are in the background, indicating they are further away. The paddleboarder is positioned near the center of the image, creating a sense of balance in the composition.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24221.3, "ram_available_mb": 38619.6, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24218.4, "ram_available_mb": 38622.5, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.932}, "power_stats": {"power_gpu_soc_mean_watts": 22.342, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 9.133, "gpu_utilization_percent_mean": 73.932, "power_watts_avg": 22.342, "energy_joules_est": 232.17, "duration_seconds": 10.392, "sample_count": 88}, "timestamp": "2026-01-25T15:30:43.595696"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7481.294, "latencies_ms": [7481.294], "images_per_second": 0.134, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A person is paddleboarding in the ocean, with a coastline in the background. The image is in black and white, giving it a timeless feel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24218.4, "ram_available_mb": 38622.5, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24224.2, "ram_available_mb": 38616.7, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.841}, "power_stats": {"power_gpu_soc_mean_watts": 24.21, "power_cpu_cv_mean_watts": 1.341, "power_sys_5v0_mean_watts": 9.122, "gpu_utilization_percent_mean": 78.841, "power_watts_avg": 24.21, "energy_joules_est": 181.14, "duration_seconds": 7.482, "sample_count": 63}, "timestamp": "2026-01-25T15:30:53.105942"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7991.103, "latencies_ms": [7991.103], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image is in black and white, featuring a person paddleboarding on a calm body of water. The sky is overcast, and the weather appears to be clear with no visible precipitation.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24224.2, "ram_available_mb": 38616.7, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24187.2, "ram_available_mb": 38653.7, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.985}, "power_stats": {"power_gpu_soc_mean_watts": 23.579, "power_cpu_cv_mean_watts": 1.47, "power_sys_5v0_mean_watts": 9.198, "gpu_utilization_percent_mean": 76.985, "power_watts_avg": 23.579, "energy_joules_est": 188.44, "duration_seconds": 7.992, "sample_count": 67}, "timestamp": "2026-01-25T15:31:03.132350"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11296.63, "latencies_ms": [11296.63], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a workspace, bathed in soft light. Dominating the scene is a white desk, its surface a testament to a busy mind at work. On the left, a laptop sits open, its screen glowing with unseen information. To its right, a desktop computer stands tall, its monitor displaying a window filled with text. A white", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24187.2, "ram_available_mb": 38653.7, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24242.3, "ram_available_mb": 38598.6, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.75}, "power_stats": {"power_gpu_soc_mean_watts": 19.907, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.915, "gpu_utilization_percent_mean": 69.75, "power_watts_avg": 19.907, "energy_joules_est": 224.89, "duration_seconds": 11.297, "sample_count": 96}, "timestamp": "2026-01-25T15:31:16.478996"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7239.739, "latencies_ms": [7239.739], "images_per_second": 0.138, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "computer: 2, mouse: 1, keyboard: 1, monitor: 1, lamp: 1, figurine: 1, speaker: 2, desk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24242.3, "ram_available_mb": 38598.6, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24223.3, "ram_available_mb": 38617.6, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.197}, "power_stats": {"power_gpu_soc_mean_watts": 22.531, "power_cpu_cv_mean_watts": 1.602, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 72.197, "power_watts_avg": 22.531, "energy_joules_est": 163.13, "duration_seconds": 7.24, "sample_count": 61}, "timestamp": "2026-01-25T15:31:25.765836"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11223.182, "latencies_ms": [11223.182], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a laptop on the left side of the desk, which is positioned near the center of the image. A desktop computer with a monitor is in the background, slightly to the right of the center. A keyboard and a mouse are placed in front of the desktop, with the mouse to the left of the keyboard. Two speakers are located on the right side", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24223.3, "ram_available_mb": 38617.6, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24187.1, "ram_available_mb": 38653.8, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.189}, "power_stats": {"power_gpu_soc_mean_watts": 20.726, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.92, "gpu_utilization_percent_mean": 69.189, "power_watts_avg": 20.726, "energy_joules_est": 232.62, "duration_seconds": 11.224, "sample_count": 95}, "timestamp": "2026-01-25T15:31:39.034083"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7800.759, "latencies_ms": [7800.759], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image depicts a home office setup with a desktop computer, a laptop, and a pair of speakers on a desk. There is a window with blinds partially open, allowing natural light to enter the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24187.1, "ram_available_mb": 38653.8, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24240.8, "ram_available_mb": 38600.1, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.212}, "power_stats": {"power_gpu_soc_mean_watts": 22.033, "power_cpu_cv_mean_watts": 1.663, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 72.212, "power_watts_avg": 22.033, "energy_joules_est": 171.89, "duration_seconds": 7.801, "sample_count": 66}, "timestamp": "2026-01-25T15:31:48.859833"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7984.165, "latencies_ms": [7984.165], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a workspace with a white color scheme, including a white desk, a white computer monitor, and a white keyboard. The lighting appears to be artificial, coming from a lamp on the right side of the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24240.8, "ram_available_mb": 38600.1, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24224.8, "ram_available_mb": 38616.1, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.25}, "power_stats": {"power_gpu_soc_mean_watts": 21.335, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 72.25, "power_watts_avg": 21.335, "energy_joules_est": 170.36, "duration_seconds": 7.985, "sample_count": 68}, "timestamp": "2026-01-25T15:31:58.864918"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11275.501, "latencies_ms": [11275.501], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a bustling scene on a highway. Dominating the frame is a green highway sign, standing tall on a concrete overpass. The sign is a guide for travelers, pointing towards three major destinations: North 101, Hollywood Blvd, and Sunset Blvd. Each destination is clearly marked with white text and arrows, indicating the direction to take", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24224.8, "ram_available_mb": 38616.1, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24277.7, "ram_available_mb": 38563.2, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.103}, "power_stats": {"power_gpu_soc_mean_watts": 19.968, "power_cpu_cv_mean_watts": 1.924, "power_sys_5v0_mean_watts": 8.914, "gpu_utilization_percent_mean": 69.103, "power_watts_avg": 19.968, "energy_joules_est": 225.16, "duration_seconds": 11.276, "sample_count": 97}, "timestamp": "2026-01-25T15:32:12.175962"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9250.486, "latencies_ms": [9250.486], "images_per_second": 0.108, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- Taxi: 1\n\n- Van: 2\n\n- Suv: 3\n\n- Truck: 1\n\n- Car: 2\n\n- Overpass: 1\n\n- Exit sign: 1\n\n- Tree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24220.9, "ram_available_mb": 38620.0, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24233.5, "ram_available_mb": 38607.4, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.179}, "power_stats": {"power_gpu_soc_mean_watts": 21.478, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 8.92, "gpu_utilization_percent_mean": 70.179, "power_watts_avg": 21.478, "energy_joules_est": 198.7, "duration_seconds": 9.251, "sample_count": 78}, "timestamp": "2026-01-25T15:32:23.483351"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11341.503, "latencies_ms": [11341.503], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a black SUV with the license plate LC10055, positioned on the right side of the road. In the background, there is a white van and a silver taxi, both on the left side of the road. The main objects, which are the vehicles, are spatially arranged in a way that the black SUV is closest", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24233.5, "ram_available_mb": 38607.4, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24284.1, "ram_available_mb": 38556.8, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.104}, "power_stats": {"power_gpu_soc_mean_watts": 19.616, "power_cpu_cv_mean_watts": 1.915, "power_sys_5v0_mean_watts": 8.88, "gpu_utilization_percent_mean": 70.104, "power_watts_avg": 19.616, "energy_joules_est": 222.49, "duration_seconds": 11.342, "sample_count": 96}, "timestamp": "2026-01-25T15:32:36.849328"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10430.765, "latencies_ms": [10430.765], "images_per_second": 0.096, "prompt_tokens": 37, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image shows a busy highway scene with multiple vehicles, including a black SUV and a white van, traveling on a multi-lane road. Above the road, there is an overpass with green directional signs indicating the lanes for North 101 towards Ventura, Hollywood Blvd, and Sunset Blvd.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24187.3, "ram_available_mb": 38653.6, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24217.9, "ram_available_mb": 38623.0, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.337}, "power_stats": {"power_gpu_soc_mean_watts": 20.527, "power_cpu_cv_mean_watts": 1.84, "power_sys_5v0_mean_watts": 8.888, "gpu_utilization_percent_mean": 71.337, "power_watts_avg": 20.527, "energy_joules_est": 214.13, "duration_seconds": 10.431, "sample_count": 89}, "timestamp": "2026-01-25T15:32:49.303111"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6605.972, "latencies_ms": [6605.972], "images_per_second": 0.151, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image shows a clear day with bright sunlight casting shadows under the overpass. The overpass is constructed with concrete and metal, and the signs are green with white text.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24217.9, "ram_available_mb": 38623.0, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24244.8, "ram_available_mb": 38596.1, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.554}, "power_stats": {"power_gpu_soc_mean_watts": 22.786, "power_cpu_cv_mean_watts": 1.58, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 72.554, "power_watts_avg": 22.786, "energy_joules_est": 150.54, "duration_seconds": 6.607, "sample_count": 56}, "timestamp": "2026-01-25T15:32:57.925449"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12386.679, "latencies_ms": [12386.679], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene on a city street. Dominating the frame is a red double-decker bus, a common sight in many cities around the world. The bus is in motion, driving on the right side of the road, as indicated by the white lines on the road. The license plate of the bus reads \"ALM 898\", a detail that", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 24244.8, "ram_available_mb": 38596.1, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24235.0, "ram_available_mb": 38605.9, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.726}, "power_stats": {"power_gpu_soc_mean_watts": 22.804, "power_cpu_cv_mean_watts": 1.791, "power_sys_5v0_mean_watts": 9.149, "gpu_utilization_percent_mean": 73.726, "power_watts_avg": 22.804, "energy_joules_est": 282.48, "duration_seconds": 12.387, "sample_count": 106}, "timestamp": "2026-01-25T15:33:12.371715"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8642.694, "latencies_ms": [8642.694], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "bus: 1, window: many, license plate: 1, advertisement: 1, pedestrian: 1, building: 1, tree: many, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24235.0, "ram_available_mb": 38605.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24181.1, "ram_available_mb": 38659.8, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.26}, "power_stats": {"power_gpu_soc_mean_watts": 24.332, "power_cpu_cv_mean_watts": 1.443, "power_sys_5v0_mean_watts": 9.131, "gpu_utilization_percent_mean": 78.26, "power_watts_avg": 24.332, "energy_joules_est": 210.31, "duration_seconds": 8.643, "sample_count": 73}, "timestamp": "2026-01-25T15:33:23.049772"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10049.076, "latencies_ms": [10049.076], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The red double-decker bus is in the foreground of the image, driving on the road. In the background, there are other vehicles, including a white van on the right side of the image. The bus is closer to the camera than the buildings in the distance.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24181.1, "ram_available_mb": 38659.8, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24188.9, "ram_available_mb": 38652.0, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.57}, "power_stats": {"power_gpu_soc_mean_watts": 23.11, "power_cpu_cv_mean_watts": 1.62, "power_sys_5v0_mean_watts": 9.201, "gpu_utilization_percent_mean": 76.57, "power_watts_avg": 23.11, "energy_joules_est": 232.25, "duration_seconds": 10.05, "sample_count": 86}, "timestamp": "2026-01-25T15:33:35.130116"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9437.649, "latencies_ms": [9437.649], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A red double-decker bus is driving on a city street with buildings in the background. The bus has a sign that reads \"St. Paul's Cathedral Aldwych\" and a number \"15\" on the front.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24188.9, "ram_available_mb": 38652.0, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24229.6, "ram_available_mb": 38611.3, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.138}, "power_stats": {"power_gpu_soc_mean_watts": 23.95, "power_cpu_cv_mean_watts": 1.521, "power_sys_5v0_mean_watts": 9.111, "gpu_utilization_percent_mean": 77.138, "power_watts_avg": 23.95, "energy_joules_est": 226.05, "duration_seconds": 9.438, "sample_count": 80}, "timestamp": "2026-01-25T15:33:46.598413"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9542.525, "latencies_ms": [9542.525], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image features a vibrant red double-decker bus with a clear sky in the background, suggesting a sunny day. The bus is adorned with advertisements and the number 15 is prominently displayed on the front.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24229.6, "ram_available_mb": 38611.3, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24199.2, "ram_available_mb": 38641.7, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.812}, "power_stats": {"power_gpu_soc_mean_watts": 23.749, "power_cpu_cv_mean_watts": 1.576, "power_sys_5v0_mean_watts": 9.204, "gpu_utilization_percent_mean": 75.812, "power_watts_avg": 23.749, "energy_joules_est": 226.64, "duration_seconds": 9.543, "sample_count": 80}, "timestamp": "2026-01-25T15:33:58.154856"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11320.844, "latencies_ms": [11320.844], "images_per_second": 0.088, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a black and white cat is the main subject, lying on top of an open laptop. The cat's fur is a mix of black and white, with its eyes looking directly at the camera, giving it a somewhat serious expression. The laptop, which is silver in color, is placed on a white surface. The keyboard of the laptop is visible, and the word \"WOR", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24199.2, "ram_available_mb": 38641.7, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24231.4, "ram_available_mb": 38609.5, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.206}, "power_stats": {"power_gpu_soc_mean_watts": 19.613, "power_cpu_cv_mean_watts": 1.932, "power_sys_5v0_mean_watts": 8.913, "gpu_utilization_percent_mean": 69.206, "power_watts_avg": 19.613, "energy_joules_est": 222.05, "duration_seconds": 11.321, "sample_count": 97}, "timestamp": "2026-01-25T15:34:11.534840"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7220.364, "latencies_ms": [7220.364], "images_per_second": 0.138, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "cat: 1, keyboard: 1, laptop: 1, paper: 1, pen: 1, screwdriver: 1, cloth: 1, box: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24231.4, "ram_available_mb": 38609.5, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24230.6, "ram_available_mb": 38610.3, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.279}, "power_stats": {"power_gpu_soc_mean_watts": 22.626, "power_cpu_cv_mean_watts": 1.602, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 72.279, "power_watts_avg": 22.626, "energy_joules_est": 163.38, "duration_seconds": 7.221, "sample_count": 61}, "timestamp": "2026-01-25T15:34:20.783158"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9663.244, "latencies_ms": [9663.244], "images_per_second": 0.103, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The cat is positioned in the foreground on the left side of the image, partially obscuring the laptop keyboard which is in the foreground on the right side. The laptop is placed on a surface that appears to be a desk or table, and the background is a plain, light-colored wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24230.6, "ram_available_mb": 38610.3, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24309.2, "ram_available_mb": 38531.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.756}, "power_stats": {"power_gpu_soc_mean_watts": 21.211, "power_cpu_cv_mean_watts": 1.836, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.756, "power_watts_avg": 21.211, "energy_joules_est": 204.98, "duration_seconds": 9.664, "sample_count": 82}, "timestamp": "2026-01-25T15:34:32.497646"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7999.658, "latencies_ms": [7999.658], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A black and white cat with striking yellow eyes is lying down on top of an open laptop, partially covering the keyboard. The cat appears to be resting or sleeping, with its body stretched out across the laptop's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24309.2, "ram_available_mb": 38531.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24314.8, "ram_available_mb": 38526.1, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.103}, "power_stats": {"power_gpu_soc_mean_watts": 22.187, "power_cpu_cv_mean_watts": 1.673, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 72.103, "power_watts_avg": 22.187, "energy_joules_est": 177.5, "duration_seconds": 8.0, "sample_count": 68}, "timestamp": "2026-01-25T15:34:42.528751"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8164.749, "latencies_ms": [8164.749], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image features a black cat with striking yellow eyes lying on a white surface, possibly a desk or table. The lighting in the image is bright, illuminating the cat's fur and the keys of the laptop in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24194.7, "ram_available_mb": 38646.2, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24184.9, "ram_available_mb": 38656.0, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.522}, "power_stats": {"power_gpu_soc_mean_watts": 21.791, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 70.522, "power_watts_avg": 21.791, "energy_joules_est": 177.93, "duration_seconds": 8.165, "sample_count": 69}, "timestamp": "2026-01-25T15:34:52.727004"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11218.694, "latencies_ms": [11218.694], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a breathtaking view of the Sydney Harbour Bridge in Australia. The bridge, a marvel of engineering, is a large steel arch structure painted in a sleek black color. It spans across the water, connecting the two sides of the harbor. \n\nIn the sky above, two airplanes are captured mid-flight. They are flying", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24184.9, "ram_available_mb": 38656.0, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24215.9, "ram_available_mb": 38625.0, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.895}, "power_stats": {"power_gpu_soc_mean_watts": 20.734, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.923, "gpu_utilization_percent_mean": 68.895, "power_watts_avg": 20.734, "energy_joules_est": 232.62, "duration_seconds": 11.219, "sample_count": 95}, "timestamp": "2026-01-25T15:35:05.971211"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8683.224, "latencies_ms": [8683.224], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Airplanes: 2\n- Clouds: 1\n- Bridge: 1\n- Cars: 1\n- Bus: 1\n- Water: 1\n- Cityscape: 1\n- Trees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24215.9, "ram_available_mb": 38625.0, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24196.4, "ram_available_mb": 38644.5, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.548}, "power_stats": {"power_gpu_soc_mean_watts": 21.971, "power_cpu_cv_mean_watts": 1.706, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 72.548, "power_watts_avg": 21.971, "energy_joules_est": 190.79, "duration_seconds": 8.684, "sample_count": 73}, "timestamp": "2026-01-25T15:35:16.677328"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11246.525, "latencies_ms": [11246.525], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large, curved bridge spanning across the image, with two airplanes flying in the background. The airplanes are positioned in the sky, with one flying higher and to the right of the bridge, and the other flying lower and to the left of the bridge. The bridge appears to be in the middle ground of the image, with the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24196.4, "ram_available_mb": 38644.5, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24219.8, "ram_available_mb": 38621.1, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.253}, "power_stats": {"power_gpu_soc_mean_watts": 20.821, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 69.253, "power_watts_avg": 20.821, "energy_joules_est": 234.18, "duration_seconds": 11.247, "sample_count": 95}, "timestamp": "2026-01-25T15:35:29.984769"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6413.441, "latencies_ms": [6413.441], "images_per_second": 0.156, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "Two airplanes are flying in formation over the Sydney Harbour Bridge in Australia. The sky is cloudy and the city skyline can be seen in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24219.8, "ram_available_mb": 38621.1, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24291.8, "ram_available_mb": 38549.1, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.611}, "power_stats": {"power_gpu_soc_mean_watts": 23.412, "power_cpu_cv_mean_watts": 1.513, "power_sys_5v0_mean_watts": 9.011, "gpu_utilization_percent_mean": 73.611, "power_watts_avg": 23.412, "energy_joules_est": 150.17, "duration_seconds": 6.414, "sample_count": 54}, "timestamp": "2026-01-25T15:35:38.451448"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9417.573, "latencies_ms": [9417.573], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The sky is overcast with a mix of white and gray clouds, and the lighting is diffused, with no harsh shadows visible. Two airplanes, one with a red and white color scheme and the other with a darker color scheme, are flying in the sky above the bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24195.2, "ram_available_mb": 38645.7, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24225.3, "ram_available_mb": 38615.6, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.975}, "power_stats": {"power_gpu_soc_mean_watts": 21.495, "power_cpu_cv_mean_watts": 1.817, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 70.975, "power_watts_avg": 21.495, "energy_joules_est": 202.44, "duration_seconds": 9.418, "sample_count": 80}, "timestamp": "2026-01-25T15:35:49.892715"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11173.508, "latencies_ms": [11173.508], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in the wild, featuring two zebras in a grassy field. The zebra on the left, with its distinctive black and white stripes, is facing the camera, its head turned to the right. Its mane is a lighter shade of black, contrasting with the darker stripes on its body. The zebra on", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24225.3, "ram_available_mb": 38615.6, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24221.0, "ram_available_mb": 38619.9, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.337}, "power_stats": {"power_gpu_soc_mean_watts": 20.799, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.918, "gpu_utilization_percent_mean": 69.337, "power_watts_avg": 20.799, "energy_joules_est": 232.41, "duration_seconds": 11.174, "sample_count": 95}, "timestamp": "2026-01-25T15:36:03.128649"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9695.364, "latencies_ms": [9695.364], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "zebra: 2\n\nhair: numerous\n\nstripes: numerous\n\ngrass: sparse\n\nground: visible\n\nzebra's head: 1\n\nzebra's neck: 1\n\nzebra's body: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24221.0, "ram_available_mb": 38619.9, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24229.5, "ram_available_mb": 38611.4, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.614}, "power_stats": {"power_gpu_soc_mean_watts": 21.076, "power_cpu_cv_mean_watts": 1.819, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 70.614, "power_watts_avg": 21.076, "energy_joules_est": 204.35, "duration_seconds": 9.696, "sample_count": 83}, "timestamp": "2026-01-25T15:36:14.867421"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11135.318, "latencies_ms": [11135.318], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a zebra's head and neck prominently displayed, with its head turned towards the left side of the image, suggesting it is looking or facing in that direction. The background is filled with the full body of another zebra, which is positioned to the right of the foreground zebra, creating a sense of depth. The grass", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24229.5, "ram_available_mb": 38611.4, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24230.1, "ram_available_mb": 38610.8, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.411}, "power_stats": {"power_gpu_soc_mean_watts": 20.933, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 69.411, "power_watts_avg": 20.933, "energy_joules_est": 233.11, "duration_seconds": 11.136, "sample_count": 95}, "timestamp": "2026-01-25T15:36:28.031509"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6846.412, "latencies_ms": [6846.412], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "In the image, two zebras are standing close to each other in a grassy area. One zebra is nuzzling the other, showing affection or social bonding.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24230.1, "ram_available_mb": 38610.8, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24233.6, "ram_available_mb": 38607.3, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.259}, "power_stats": {"power_gpu_soc_mean_watts": 22.876, "power_cpu_cv_mean_watts": 1.574, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 74.259, "power_watts_avg": 22.876, "energy_joules_est": 156.63, "duration_seconds": 6.847, "sample_count": 58}, "timestamp": "2026-01-25T15:36:36.928014"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7780.498, "latencies_ms": [7780.498], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image is a black and white photograph capturing a moment between two zebras. The lighting is soft and diffused, highlighting the intricate patterns of the zebras' stripes and the texture of their fur.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24233.6, "ram_available_mb": 38607.3, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24228.0, "ram_available_mb": 38612.9, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.364}, "power_stats": {"power_gpu_soc_mean_watts": 22.182, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 9.055, "gpu_utilization_percent_mean": 72.364, "power_watts_avg": 22.182, "energy_joules_est": 172.6, "duration_seconds": 7.781, "sample_count": 66}, "timestamp": "2026-01-25T15:36:46.740775"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11152.654, "latencies_ms": [11152.654], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a cozy bedroom scene. Dominating the space is a bed, adorned with a vibrant comforter that boasts a geometric pattern in hues of pink, blue, and green. The bed is positioned against a wall, which is characterized by a window dressed in white blinds, allowing a soft light to filter into the room.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24228.0, "ram_available_mb": 38612.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24210.9, "ram_available_mb": 38630.0, "ram_percent": 38.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.337}, "power_stats": {"power_gpu_soc_mean_watts": 20.895, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 69.337, "power_watts_avg": 20.895, "energy_joules_est": 233.05, "duration_seconds": 11.153, "sample_count": 95}, "timestamp": "2026-01-25T15:36:59.935932"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8002.366, "latencies_ms": [8002.366], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "- bed: 1\n- chair: 1\n- table: 1\n- window: 2\n- wall: 1\n- floor: 1\n- curtain: 1\n- door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24210.9, "ram_available_mb": 38630.0, "ram_percent": 38.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24226.4, "ram_available_mb": 38614.5, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.088}, "power_stats": {"power_gpu_soc_mean_watts": 22.268, "power_cpu_cv_mean_watts": 1.655, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 73.088, "power_watts_avg": 22.268, "energy_joules_est": 178.21, "duration_seconds": 8.003, "sample_count": 68}, "timestamp": "2026-01-25T15:37:09.970725"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11254.237, "latencies_ms": [11254.237], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a bed with a colorful patterned cover situated near the right side of the frame. To the left of the bed, there is a small round table with a single black object on top, positioned closer to the viewer than the bed. In the background, there is a window with a view of a building outside, and a chair is", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24226.4, "ram_available_mb": 38614.5, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24230.5, "ram_available_mb": 38610.4, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.726}, "power_stats": {"power_gpu_soc_mean_watts": 20.1, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.908, "gpu_utilization_percent_mean": 69.726, "power_watts_avg": 20.1, "energy_joules_est": 226.22, "duration_seconds": 11.255, "sample_count": 95}, "timestamp": "2026-01-25T15:37:23.282769"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8429.296, "latencies_ms": [8429.296], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a cozy bedroom with a large window that offers a view of a stone wall outside. The room features a bed with a colorful patterned bedspread, a round wooden table with a black chair, and a purple wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24230.5, "ram_available_mb": 38610.4, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24245.4, "ram_available_mb": 38595.5, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.901}, "power_stats": {"power_gpu_soc_mean_watts": 22.019, "power_cpu_cv_mean_watts": 1.697, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 72.901, "power_watts_avg": 22.019, "energy_joules_est": 185.62, "duration_seconds": 8.43, "sample_count": 71}, "timestamp": "2026-01-25T15:37:33.732803"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5071.2, "latencies_ms": [5071.2], "images_per_second": 0.197, "prompt_tokens": 36, "response_tokens_est": 24, "n_tiles": 16, "output_text": "The room has a purple wall and a stone wall outside. The bed has a colorful pattern on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24245.4, "ram_available_mb": 38595.5, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24247.7, "ram_available_mb": 38593.2, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.146}, "power_stats": {"power_gpu_soc_mean_watts": 24.305, "power_cpu_cv_mean_watts": 1.279, "power_sys_5v0_mean_watts": 9.127, "gpu_utilization_percent_mean": 77.146, "power_watts_avg": 24.305, "energy_joules_est": 123.27, "duration_seconds": 5.072, "sample_count": 41}, "timestamp": "2026-01-25T15:37:40.817844"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11173.774, "latencies_ms": [11173.774], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a purple bus driving down a street. The bus is a double-decker bus, and it is traveling on a road with a crosswalk in front of it. There are several people around the bus, with one person walking on the sidewalk and another person standing near the bus. \n\nIn addition to the bus, there are two traffic lights visible in the", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24247.7, "ram_available_mb": 38593.2, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24366.3, "ram_available_mb": 38474.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.084}, "power_stats": {"power_gpu_soc_mean_watts": 20.863, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 69.084, "power_watts_avg": 20.863, "energy_joules_est": 233.13, "duration_seconds": 11.174, "sample_count": 95}, "timestamp": "2026-01-25T15:37:54.071120"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7881.634, "latencies_ms": [7881.634], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "bus: 1, window: 12, wheelchair symbol: 1, traffic light: 1, pedestrian: 1, bus stop: 1, tree: 1, building: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24242.7, "ram_available_mb": 38598.2, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24392.3, "ram_available_mb": 38448.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.299}, "power_stats": {"power_gpu_soc_mean_watts": 22.171, "power_cpu_cv_mean_watts": 1.668, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 73.299, "power_watts_avg": 22.171, "energy_joules_est": 174.76, "duration_seconds": 7.882, "sample_count": 67}, "timestamp": "2026-01-25T15:38:04.018299"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10491.853, "latencies_ms": [10491.853], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The bus is in the foreground of the image, positioned on the left side of the frame, and appears to be moving towards the right. There is a pedestrian walking on the sidewalk in the background, behind the bus. The bus is also closer to the camera than the buildings in the background, which are further away and appear smaller.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24262.1, "ram_available_mb": 38578.8, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24401.9, "ram_available_mb": 38439.0, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.222}, "power_stats": {"power_gpu_soc_mean_watts": 21.048, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 69.222, "power_watts_avg": 21.048, "energy_joules_est": 220.85, "duration_seconds": 10.492, "sample_count": 90}, "timestamp": "2026-01-25T15:38:16.526925"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7725.962, "latencies_ms": [7725.962], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A purple South Tyne Metrocentre bus is driving on the road, with a pedestrian walking on the sidewalk. The bus is marked with the route number 96 and is headed towards Bens.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24323.2, "ram_available_mb": 38517.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24327.4, "ram_available_mb": 38513.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.231}, "power_stats": {"power_gpu_soc_mean_watts": 22.408, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 71.231, "power_watts_avg": 22.408, "energy_joules_est": 173.14, "duration_seconds": 7.727, "sample_count": 65}, "timestamp": "2026-01-25T15:38:26.304792"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7456.89, "latencies_ms": [7456.89], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The bus is purple with a colorful design and the words \"Metrocentre via Bens\" on the front. It is a sunny day with clear skies and the bus is driving on a city street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24261.5, "ram_available_mb": 38579.4, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24382.0, "ram_available_mb": 38458.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.403}, "power_stats": {"power_gpu_soc_mean_watts": 22.393, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 9.052, "gpu_utilization_percent_mean": 72.403, "power_watts_avg": 22.393, "energy_joules_est": 167.0, "duration_seconds": 7.458, "sample_count": 62}, "timestamp": "2026-01-25T15:38:35.775755"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11171.128, "latencies_ms": [11171.128], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a close-up view of a white plate holding a group of green apples. There are six apples in total, with one prominently displayed in the foreground and the others slightly blurred in the background. The apples are fresh and appear to be of the same variety, with a vibrant green color and a shiny surface. The background is", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 24382.0, "ram_available_mb": 38458.9, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24301.1, "ram_available_mb": 38539.8, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.876}, "power_stats": {"power_gpu_soc_mean_watts": 20.74, "power_cpu_cv_mean_watts": 1.92, "power_sys_5v0_mean_watts": 8.925, "gpu_utilization_percent_mean": 68.876, "power_watts_avg": 20.74, "energy_joules_est": 231.7, "duration_seconds": 11.172, "sample_count": 97}, "timestamp": "2026-01-25T15:38:48.974207"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3174.631, "latencies_ms": [3174.631], "images_per_second": 0.315, "prompt_tokens": 39, "response_tokens_est": 6, "n_tiles": 16, "output_text": "apple: 5\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24301.1, "ram_available_mb": 38539.8, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24294.2, "ram_available_mb": 38546.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 86.615}, "power_stats": {"power_gpu_soc_mean_watts": 28.805, "power_cpu_cv_mean_watts": 0.57, "power_sys_5v0_mean_watts": 9.241, "gpu_utilization_percent_mean": 86.615, "power_watts_avg": 28.805, "energy_joules_est": 91.46, "duration_seconds": 3.175, "sample_count": 26}, "timestamp": "2026-01-25T15:38:54.178992"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10353.55, "latencies_ms": [10353.55], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground, there are several green apples closely packed together, with one prominently in the center. The apples in the background are slightly out of focus, indicating they are further away from the viewer. The lighting suggests that the apples are positioned near a light source, possibly on a table or countertop.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24294.2, "ram_available_mb": 38546.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24310.5, "ram_available_mb": 38530.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.888}, "power_stats": {"power_gpu_soc_mean_watts": 21.11, "power_cpu_cv_mean_watts": 1.894, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 69.888, "power_watts_avg": 21.11, "energy_joules_est": 218.58, "duration_seconds": 10.354, "sample_count": 89}, "timestamp": "2026-01-25T15:39:06.563350"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7092.844, "latencies_ms": [7092.844], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image shows a close-up of a group of green apples on a white plate. The apples are fresh and shiny, indicating that they are likely ripe and ready to eat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24310.5, "ram_available_mb": 38530.4, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24310.7, "ram_available_mb": 38530.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.933}, "power_stats": {"power_gpu_soc_mean_watts": 22.059, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 74.933, "power_watts_avg": 22.059, "energy_joules_est": 156.48, "duration_seconds": 7.093, "sample_count": 60}, "timestamp": "2026-01-25T15:39:15.686975"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8667.478, "latencies_ms": [8667.478], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image features a group of green apples with a shiny surface, indicating they might be fresh and possibly wet from washing. The lighting in the image is soft and diffused, with no harsh shadows, suggesting an indoor setting with ambient lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24310.7, "ram_available_mb": 38530.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24316.3, "ram_available_mb": 38524.6, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.562}, "power_stats": {"power_gpu_soc_mean_watts": 21.759, "power_cpu_cv_mean_watts": 1.772, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 70.562, "power_watts_avg": 21.759, "energy_joules_est": 188.61, "duration_seconds": 8.668, "sample_count": 73}, "timestamp": "2026-01-25T15:39:26.405393"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11181.78, "latencies_ms": [11181.78], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a baseball game. The central figures are the batter, the catcher, and the umpire. The batter, dressed in a white uniform with red accents, is in the midst of a powerful swing, his body coiled with the force of the hit. His black bat is caught mid-swing, poised to connect with the incoming ball. ", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24316.3, "ram_available_mb": 38524.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24294.9, "ram_available_mb": 38546.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.116}, "power_stats": {"power_gpu_soc_mean_watts": 20.805, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.921, "gpu_utilization_percent_mean": 69.116, "power_watts_avg": 20.805, "energy_joules_est": 232.65, "duration_seconds": 11.182, "sample_count": 95}, "timestamp": "2026-01-25T15:39:39.615119"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7497.908, "latencies_ms": [7497.908], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "pitcher: 1, catcher: 1, umpire: 1, batter: 1, runner: 1, base: 1, ball: 1, glove: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24294.9, "ram_available_mb": 38546.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24291.5, "ram_available_mb": 38549.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.333}, "power_stats": {"power_gpu_soc_mean_watts": 22.604, "power_cpu_cv_mean_watts": 1.621, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 72.333, "power_watts_avg": 22.604, "energy_joules_est": 169.5, "duration_seconds": 7.498, "sample_count": 63}, "timestamp": "2026-01-25T15:39:49.133114"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11154.387, "latencies_ms": [11154.387], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a baseball player is swinging a bat, positioned near the center of the image, with the catcher and umpire behind him, closer to the background. The pitcher, who is further back in the image, has just thrown the ball towards the batter. The batter is standing in the batter's box, which is located on the left side of the", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24291.5, "ram_available_mb": 38549.4, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24289.0, "ram_available_mb": 38551.9, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.653}, "power_stats": {"power_gpu_soc_mean_watts": 20.893, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 69.653, "power_watts_avg": 20.893, "energy_joules_est": 233.06, "duration_seconds": 11.155, "sample_count": 95}, "timestamp": "2026-01-25T15:40:02.330042"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7668.813, "latencies_ms": [7668.813], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image captures a moment from a baseball game, with a batter in the midst of a swing at the pitch. The catcher and umpire are in position behind the batter, ready to react to the play.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24289.0, "ram_available_mb": 38551.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24287.8, "ram_available_mb": 38553.1, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.785}, "power_stats": {"power_gpu_soc_mean_watts": 22.331, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 8.929, "gpu_utilization_percent_mean": 71.785, "power_watts_avg": 22.331, "energy_joules_est": 171.27, "duration_seconds": 7.669, "sample_count": 65}, "timestamp": "2026-01-25T15:40:12.035422"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7795.881, "latencies_ms": [7795.881], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image captures a baseball game in progress with a clear view of the field and players. The lighting appears to be natural daylight, and the weather seems fair, as there are no signs of rain or overcast skies.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24287.8, "ram_available_mb": 38553.1, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24290.1, "ram_available_mb": 38550.8, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.833}, "power_stats": {"power_gpu_soc_mean_watts": 22.131, "power_cpu_cv_mean_watts": 1.705, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 71.833, "power_watts_avg": 22.131, "energy_joules_est": 172.55, "duration_seconds": 7.797, "sample_count": 66}, "timestamp": "2026-01-25T15:40:21.844690"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11178.337, "latencies_ms": [11178.337], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table covered with a red tablecloth, set for a meal. On the table, there is a large white cake topped with red and blue berries, accompanied by a plate of cheese and crackers. A bowl of grapes is also present on the table. \n\nIn addition to the main dishes, there are several", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24290.1, "ram_available_mb": 38550.8, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24302.0, "ram_available_mb": 38538.9, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.768}, "power_stats": {"power_gpu_soc_mean_watts": 20.807, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.93, "gpu_utilization_percent_mean": 68.768, "power_watts_avg": 20.807, "energy_joules_est": 232.6, "duration_seconds": 11.179, "sample_count": 95}, "timestamp": "2026-01-25T15:40:35.048325"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9977.451, "latencies_ms": [9977.451], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "cheese: 10\ngrapes: 1 bunch\nblueberries: 1 cup\nstrawberries: 1 cup\nwine glasses: 6\nplates: 10\ncakes: 1\nknife: 2\ngrapes: 1 bunch", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24302.0, "ram_available_mb": 38538.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24299.9, "ram_available_mb": 38541.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.047}, "power_stats": {"power_gpu_soc_mean_watts": 21.429, "power_cpu_cv_mean_watts": 1.828, "power_sys_5v0_mean_watts": 8.919, "gpu_utilization_percent_mean": 71.047, "power_watts_avg": 21.429, "energy_joules_est": 213.82, "duration_seconds": 9.978, "sample_count": 85}, "timestamp": "2026-01-25T15:40:47.082659"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11164.981, "latencies_ms": [11164.981], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large white cake with red and blue berries on top, placed on the left side of the table. To the right of the cake, there is a plate with an assortment of cheeses and crackers, and further to the right, there are stacks of white plates. In the background, there are several wine glasses and", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24299.9, "ram_available_mb": 38541.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24306.2, "ram_available_mb": 38534.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.206}, "power_stats": {"power_gpu_soc_mean_watts": 20.811, "power_cpu_cv_mean_watts": 1.924, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 69.206, "power_watts_avg": 20.811, "energy_joules_est": 232.37, "duration_seconds": 11.166, "sample_count": 97}, "timestamp": "2026-01-25T15:41:00.275017"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9421.687, "latencies_ms": [9421.687], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image depicts a table set for a meal with a variety of foods and drinks. There is a large white cake with red and blue berries on top, a plate of cheese and crackers, a bowl of grapes, and several wine glasses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24306.2, "ram_available_mb": 38534.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24377.0, "ram_available_mb": 38463.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.475}, "power_stats": {"power_gpu_soc_mean_watts": 21.657, "power_cpu_cv_mean_watts": 1.782, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 71.475, "power_watts_avg": 21.657, "energy_joules_est": 204.06, "duration_seconds": 9.422, "sample_count": 80}, "timestamp": "2026-01-25T15:41:11.743927"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8799.34, "latencies_ms": [8799.34], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image features a vibrant red tablecloth that contrasts with the white cheesecake topped with red and blue berries. The table is set with clear glassware and plates, and the lighting appears to be natural, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24273.8, "ram_available_mb": 38567.1, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24302.2, "ram_available_mb": 38538.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.632}, "power_stats": {"power_gpu_soc_mean_watts": 21.602, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 70.632, "power_watts_avg": 21.602, "energy_joules_est": 190.1, "duration_seconds": 8.8, "sample_count": 76}, "timestamp": "2026-01-25T15:41:22.574865"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11138.592, "latencies_ms": [11138.592], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seen surfing on a wave in the ocean. He is wearing a black wetsuit and is riding a blue surfboard. The wave he is on is a beautiful shade of green, and it's curling over to the right side of the image. The man is in the process of paddling, his arms extended above", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24302.2, "ram_available_mb": 38538.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24276.5, "ram_available_mb": 38564.4, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.074}, "power_stats": {"power_gpu_soc_mean_watts": 20.909, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 69.074, "power_watts_avg": 20.909, "energy_joules_est": 232.91, "duration_seconds": 11.139, "sample_count": 94}, "timestamp": "2026-01-25T15:41:35.759320"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7555.558, "latencies_ms": [7555.558], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "wave: 1\nsurfboard: 1\nman: 1\nwater: 1\nsurf: 1\nsand: 1\nsun: 1\nsea: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24276.5, "ram_available_mb": 38564.4, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24276.9, "ram_available_mb": 38564.0, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.73}, "power_stats": {"power_gpu_soc_mean_watts": 22.456, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 72.73, "power_watts_avg": 22.456, "energy_joules_est": 169.68, "duration_seconds": 7.556, "sample_count": 63}, "timestamp": "2026-01-25T15:41:45.347393"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10704.891, "latencies_ms": [10704.891], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is breaking to the right side of the image. The wave originates in the background and extends towards the left, creating a dynamic spatial relationship between the surfer and the wave. The surfer is closer to the viewer than the wave, emphasizing the action of surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24276.9, "ram_available_mb": 38564.0, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24273.3, "ram_available_mb": 38567.6, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.637}, "power_stats": {"power_gpu_soc_mean_watts": 21.029, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.637, "power_watts_avg": 21.029, "energy_joules_est": 225.13, "duration_seconds": 10.705, "sample_count": 91}, "timestamp": "2026-01-25T15:41:58.095174"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5867.359, "latencies_ms": [5867.359], "images_per_second": 0.17, "prompt_tokens": 37, "response_tokens_est": 30, "n_tiles": 16, "output_text": "A person is surfing a wave in the ocean. The wave is green and the surfer is wearing a black wetsuit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24273.3, "ram_available_mb": 38567.6, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24276.9, "ram_available_mb": 38564.0, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.673}, "power_stats": {"power_gpu_soc_mean_watts": 23.877, "power_cpu_cv_mean_watts": 1.406, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 76.673, "power_watts_avg": 23.877, "energy_joules_est": 140.11, "duration_seconds": 5.868, "sample_count": 49}, "timestamp": "2026-01-25T15:42:05.972648"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7109.372, "latencies_ms": [7109.372], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The surfer is wearing a black wetsuit and is riding a wave in the ocean. The wave is a vibrant green color and the water is splashing around the surfer.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24276.9, "ram_available_mb": 38564.0, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24279.7, "ram_available_mb": 38561.2, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.617}, "power_stats": {"power_gpu_soc_mean_watts": 22.545, "power_cpu_cv_mean_watts": 1.642, "power_sys_5v0_mean_watts": 9.069, "gpu_utilization_percent_mean": 72.617, "power_watts_avg": 22.545, "energy_joules_est": 160.3, "duration_seconds": 7.11, "sample_count": 60}, "timestamp": "2026-01-25T15:42:15.125188"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10602.03, "latencies_ms": [10602.03], "images_per_second": 0.094, "prompt_tokens": 24, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image is a black and white photograph of a group of children, likely from a school, posing for a group picture. They are all dressed in formal attire, with some wearing ties. The children are arranged in rows, with some sitting on the ground and others standing. The photograph appears to be from the early 20th century.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 24279.7, "ram_available_mb": 38561.2, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24273.2, "ram_available_mb": 38567.7, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.633}, "power_stats": {"power_gpu_soc_mean_watts": 20.882, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 69.633, "power_watts_avg": 20.882, "energy_joules_est": 221.4, "duration_seconds": 10.603, "sample_count": 90}, "timestamp": "2026-01-25T15:42:27.750366"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7506.962, "latencies_ms": [7506.962], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "children: 30, boys: 12, girls: 18, adults: 2, benches: 2, trees: 1, building: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24273.2, "ram_available_mb": 38567.7, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24312.1, "ram_available_mb": 38528.8, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.922}, "power_stats": {"power_gpu_soc_mean_watts": 22.597, "power_cpu_cv_mean_watts": 1.646, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 73.922, "power_watts_avg": 22.597, "energy_joules_est": 169.66, "duration_seconds": 7.508, "sample_count": 64}, "timestamp": "2026-01-25T15:42:37.279418"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11146.044, "latencies_ms": [11146.044], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a group of children is seated on the ground, with some sitting closer to the front and others further back, creating a sense of depth. The children in the background are standing, with a few positioned in the middle ground and others towards the back, adding to the layered effect of the group. The children in the front appear to be the main focus, with", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24312.1, "ram_available_mb": 38528.8, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24279.6, "ram_available_mb": 38561.2, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.316}, "power_stats": {"power_gpu_soc_mean_watts": 20.927, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 69.316, "power_watts_avg": 20.927, "energy_joules_est": 233.27, "duration_seconds": 11.147, "sample_count": 95}, "timestamp": "2026-01-25T15:42:50.437060"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7076.839, "latencies_ms": [7076.839], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image depicts a large group of children gathered together, likely for a group photo. They are dressed in a mix of formal and casual attire, suggesting a special occasion or event.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24279.6, "ram_available_mb": 38561.2, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24271.8, "ram_available_mb": 38569.1, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.783}, "power_stats": {"power_gpu_soc_mean_watts": 22.835, "power_cpu_cv_mean_watts": 1.561, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 74.783, "power_watts_avg": 22.835, "energy_joules_est": 161.61, "duration_seconds": 7.077, "sample_count": 60}, "timestamp": "2026-01-25T15:42:59.542647"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8044.447, "latencies_ms": [8044.447], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image is a black and white photograph, indicating it was taken in an era before color photography was common. The lighting is even, with no harsh shadows, suggesting it was taken in a controlled environment, possibly indoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24271.8, "ram_available_mb": 38569.1, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24298.3, "ram_available_mb": 38542.6, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.203}, "power_stats": {"power_gpu_soc_mean_watts": 21.893, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 9.015, "gpu_utilization_percent_mean": 71.203, "power_watts_avg": 21.893, "energy_joules_est": 176.13, "duration_seconds": 8.045, "sample_count": 69}, "timestamp": "2026-01-25T15:43:09.643976"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11127.355, "latencies_ms": [11127.355], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with a plate of food, including a sandwich and a bowl of soup. The sandwich is placed on the left side of the plate, while the soup is in a bowl on the right side. A wine glass is also present on the table, positioned towards the top right corner. \n\nA knife can be seen resting on", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24298.3, "ram_available_mb": 38542.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24318.4, "ram_available_mb": 38522.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.442}, "power_stats": {"power_gpu_soc_mean_watts": 20.822, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.442, "power_watts_avg": 20.822, "energy_joules_est": 231.71, "duration_seconds": 11.128, "sample_count": 95}, "timestamp": "2026-01-25T15:43:22.811788"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7659.104, "latencies_ms": [7659.104], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "plate: 1\nbread: 2\nknife: 1\nbowl: 1\nplate: 1\nwine glass: 1\nplate: 1\nknife: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24318.4, "ram_available_mb": 38522.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24325.7, "ram_available_mb": 38515.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.924}, "power_stats": {"power_gpu_soc_mean_watts": 22.436, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 72.924, "power_watts_avg": 22.436, "energy_joules_est": 171.85, "duration_seconds": 7.66, "sample_count": 66}, "timestamp": "2026-01-25T15:43:32.525642"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10061.394, "latencies_ms": [10061.394], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a white plate with a piece of bread on it, positioned to the left of a mortar and pestle. The mortar and pestle are placed on the right side of the plate. In the background, there is a glass of red wine and a person sitting at the table.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24325.7, "ram_available_mb": 38515.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24297.3, "ram_available_mb": 38543.6, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.448}, "power_stats": {"power_gpu_soc_mean_watts": 21.258, "power_cpu_cv_mean_watts": 1.873, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 70.448, "power_watts_avg": 21.258, "energy_joules_est": 213.9, "duration_seconds": 10.062, "sample_count": 87}, "timestamp": "2026-01-25T15:43:44.626739"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6417.472, "latencies_ms": [6417.472], "images_per_second": 0.156, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "The image shows a wooden table with a plate of grilled bread and a glass of red wine. There is also a knife and a napkin on the table.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24297.3, "ram_available_mb": 38543.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24316.9, "ram_available_mb": 38524.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.074}, "power_stats": {"power_gpu_soc_mean_watts": 23.494, "power_cpu_cv_mean_watts": 1.505, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 76.074, "power_watts_avg": 23.494, "energy_joules_est": 150.79, "duration_seconds": 6.418, "sample_count": 54}, "timestamp": "2026-01-25T15:43:53.080002"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8377.257, "latencies_ms": [8377.257], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image features a wooden table with a warm, natural lighting that highlights the textures of the food and tableware. A glass of red wine and a piece of crusty bread are visible, suggesting a cozy, indoor dining setting.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24316.9, "ram_available_mb": 38524.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24368.6, "ram_available_mb": 38472.3, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.535}, "power_stats": {"power_gpu_soc_mean_watts": 21.94, "power_cpu_cv_mean_watts": 1.748, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 71.535, "power_watts_avg": 21.94, "energy_joules_est": 183.81, "duration_seconds": 8.378, "sample_count": 71}, "timestamp": "2026-01-25T15:44:03.505795"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11122.747, "latencies_ms": [11122.747], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a skier is performing a jump in the air, showcasing their skills and athleticism. The skier is wearing a colorful outfit and is in the middle of a jump, with their skis and poles visible. \n\nThere are several other people in the scene, including a man standing on the left side of the image, a woman", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24315.0, "ram_available_mb": 38525.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24267.6, "ram_available_mb": 38573.3, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.156}, "power_stats": {"power_gpu_soc_mean_watts": 20.916, "power_cpu_cv_mean_watts": 1.935, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.156, "power_watts_avg": 20.916, "energy_joules_est": 232.66, "duration_seconds": 11.124, "sample_count": 96}, "timestamp": "2026-01-25T15:44:16.678505"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7426.578, "latencies_ms": [7426.578], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "person: 3, snowboarder: 1, snowboard: 1, trees: 10, snow: 1, mountain: 1, sky: 1, flags: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24267.6, "ram_available_mb": 38573.3, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24283.5, "ram_available_mb": 38557.4, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.762}, "power_stats": {"power_gpu_soc_mean_watts": 22.616, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 73.762, "power_watts_avg": 22.616, "energy_joules_est": 167.98, "duration_seconds": 7.428, "sample_count": 63}, "timestamp": "2026-01-25T15:44:26.120229"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11061.353, "latencies_ms": [11061.353], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "In the foreground, there is a snowboarder performing a trick in the air, with their snowboard parallel to the ground. In the background, there are two spectators watching the performance, one standing to the left and the other to the right of the snowboarder. The sky is clear and blue, indicating that the event is taking place on a sunny day.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24283.5, "ram_available_mb": 38557.4, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24320.5, "ram_available_mb": 38520.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.865}, "power_stats": {"power_gpu_soc_mean_watts": 20.812, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 69.865, "power_watts_avg": 20.812, "energy_joules_est": 230.22, "duration_seconds": 11.062, "sample_count": 96}, "timestamp": "2026-01-25T15:44:39.238469"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8086.657, "latencies_ms": [8086.657], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A skier is performing a mid-air trick on a snowy mountain slope, with a clear blue sky in the background. Spectators, including a child, are watching the skier's impressive jump from the sidelines.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24266.9, "ram_available_mb": 38574.0, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24279.2, "ram_available_mb": 38561.7, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.13}, "power_stats": {"power_gpu_soc_mean_watts": 22.212, "power_cpu_cv_mean_watts": 1.718, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 72.13, "power_watts_avg": 22.212, "energy_joules_est": 179.64, "duration_seconds": 8.087, "sample_count": 69}, "timestamp": "2026-01-25T15:44:49.351710"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7562.165, "latencies_ms": [7562.165], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The skier is wearing a colorful outfit with a mix of green, red, and white, and is performing a trick in the air. The sky is clear and blue, indicating good weather conditions for skiing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24279.2, "ram_available_mb": 38561.7, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24278.4, "ram_available_mb": 38562.5, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.891}, "power_stats": {"power_gpu_soc_mean_watts": 22.23, "power_cpu_cv_mean_watts": 1.702, "power_sys_5v0_mean_watts": 9.021, "gpu_utilization_percent_mean": 70.891, "power_watts_avg": 22.23, "energy_joules_est": 168.12, "duration_seconds": 7.563, "sample_count": 64}, "timestamp": "2026-01-25T15:44:58.940491"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11237.26, "latencies_ms": [11237.26], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is seen cross-country skiing on a snowy mountain. The skier, dressed in a vibrant green jacket and black pants, is in motion, gliding over the snow-covered terrain. The skier is equipped with ski poles, aiding in their navigation through the snow. The mountain is blanketed in a thick", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24278.4, "ram_available_mb": 38562.5, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24292.5, "ram_available_mb": 38548.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.844}, "power_stats": {"power_gpu_soc_mean_watts": 19.741, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.92, "gpu_utilization_percent_mean": 69.844, "power_watts_avg": 19.741, "energy_joules_est": 221.85, "duration_seconds": 11.238, "sample_count": 96}, "timestamp": "2026-01-25T15:45:12.219340"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9081.258, "latencies_ms": [9081.258], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "sky: 1\nclouds: 3\nperson: 1\nski poles: 2\ngreen shirt: 1\nblack pants: 1\nsnow: 1\nmountain: 1\ntree: 1\nrock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24292.5, "ram_available_mb": 38548.4, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24302.1, "ram_available_mb": 38538.8, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.667}, "power_stats": {"power_gpu_soc_mean_watts": 21.779, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 71.667, "power_watts_avg": 21.779, "energy_joules_est": 197.8, "duration_seconds": 9.082, "sample_count": 78}, "timestamp": "2026-01-25T15:45:23.330259"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11176.676, "latencies_ms": [11176.676], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is standing on skis, facing away from the camera, with a clear view of the snowy landscape behind them. The person is positioned on the right side of the image, with a vast expanse of snow-covered ground extending to the left. The background features a mountain range under a blue sky with scattered clouds, suggesting the person is at a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24302.1, "ram_available_mb": 38538.8, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24300.6, "ram_available_mb": 38540.3, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.093}, "power_stats": {"power_gpu_soc_mean_watts": 20.989, "power_cpu_cv_mean_watts": 1.915, "power_sys_5v0_mean_watts": 8.921, "gpu_utilization_percent_mean": 70.093, "power_watts_avg": 20.989, "energy_joules_est": 234.6, "duration_seconds": 11.177, "sample_count": 97}, "timestamp": "2026-01-25T15:45:36.544932"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8447.257, "latencies_ms": [8447.257], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "A person is cross-country skiing on a snowy mountain trail with a clear blue sky above and a few clouds scattered across it. The skier is wearing a green top and black pants, and is holding ski poles in their hands.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24300.6, "ram_available_mb": 38540.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24298.1, "ram_available_mb": 38542.8, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.577}, "power_stats": {"power_gpu_soc_mean_watts": 22.126, "power_cpu_cv_mean_watts": 1.692, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 72.577, "power_watts_avg": 22.126, "energy_joules_est": 186.92, "duration_seconds": 8.448, "sample_count": 71}, "timestamp": "2026-01-25T15:45:47.022366"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8053.497, "latencies_ms": [8053.497], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image features a clear blue sky with scattered white clouds, indicating fair weather. The snow-covered landscape is bathed in sunlight, casting shadows and highlighting the white of the snow against the blue of the sky and the earth.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24298.1, "ram_available_mb": 38542.8, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24296.6, "ram_available_mb": 38544.3, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.783}, "power_stats": {"power_gpu_soc_mean_watts": 22.027, "power_cpu_cv_mean_watts": 1.73, "power_sys_5v0_mean_watts": 9.01, "gpu_utilization_percent_mean": 70.783, "power_watts_avg": 22.027, "energy_joules_est": 177.41, "duration_seconds": 8.054, "sample_count": 69}, "timestamp": "2026-01-25T15:45:57.110906"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11217.523, "latencies_ms": [11217.523], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a close-up view of a banana and a chocolate-covered donut, both of which are encased in a plastic bag. The banana, with its characteristic yellow color, is positioned to the left of the donut. The donut, with its brown color, is on the right side of the image. The background is dark, which", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24296.6, "ram_available_mb": 38544.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24267.3, "ram_available_mb": 38573.6, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.155}, "power_stats": {"power_gpu_soc_mean_watts": 19.879, "power_cpu_cv_mean_watts": 1.928, "power_sys_5v0_mean_watts": 8.934, "gpu_utilization_percent_mean": 70.155, "power_watts_avg": 19.879, "energy_joules_est": 223.01, "duration_seconds": 11.218, "sample_count": 97}, "timestamp": "2026-01-25T15:46:10.371834"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3825.901, "latencies_ms": [3825.901], "images_per_second": 0.261, "prompt_tokens": 39, "response_tokens_est": 12, "n_tiles": 16, "output_text": "banana: 1, donut: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24267.3, "ram_available_mb": 38573.6, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24370.8, "ram_available_mb": 38470.1, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 83.812}, "power_stats": {"power_gpu_soc_mean_watts": 27.194, "power_cpu_cv_mean_watts": 0.851, "power_sys_5v0_mean_watts": 9.162, "gpu_utilization_percent_mean": 83.812, "power_watts_avg": 27.194, "energy_joules_est": 104.06, "duration_seconds": 3.827, "sample_count": 32}, "timestamp": "2026-01-25T15:46:16.257309"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9675.876, "latencies_ms": [9675.876], "images_per_second": 0.103, "prompt_tokens": 44, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The banana is positioned in the foreground on the left side of the image, appearing larger and more detailed. The donut is in the background, partially obscured by the banana, and appears smaller due to the perspective. The donut is to the right of the banana when viewing the image.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24370.8, "ram_available_mb": 38470.1, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24369.5, "ram_available_mb": 38471.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.145}, "power_stats": {"power_gpu_soc_mean_watts": 21.403, "power_cpu_cv_mean_watts": 1.848, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 70.145, "power_watts_avg": 21.403, "energy_joules_est": 207.11, "duration_seconds": 9.676, "sample_count": 83}, "timestamp": "2026-01-25T15:46:27.972574"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8205.575, "latencies_ms": [8205.575], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a chocolate glazed donut and a banana, both placed inside a plastic bag, likely for storage or transport. The setting appears to be a kitchen or a place where food items are being prepared or packaged.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24272.6, "ram_available_mb": 38568.3, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24366.8, "ram_available_mb": 38474.1, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.986}, "power_stats": {"power_gpu_soc_mean_watts": 22.106, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 72.986, "power_watts_avg": 22.106, "energy_joules_est": 181.41, "duration_seconds": 8.206, "sample_count": 70}, "timestamp": "2026-01-25T15:46:38.240187"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8236.932, "latencies_ms": [8236.932], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a banana and a chocolate-covered donut placed closely together. The banana is yellow, and the donut is brown with a glossy finish, indicating it might be coated with a sugary glaze.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24295.1, "ram_available_mb": 38545.8, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24340.3, "ram_available_mb": 38500.6, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.871}, "power_stats": {"power_gpu_soc_mean_watts": 21.89, "power_cpu_cv_mean_watts": 1.75, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 70.871, "power_watts_avg": 21.89, "energy_joules_est": 180.32, "duration_seconds": 8.238, "sample_count": 70}, "timestamp": "2026-01-25T15:46:48.496210"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10938.635, "latencies_ms": [10938.635], "images_per_second": 0.091, "prompt_tokens": 24, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image presents a scene featuring a white mug with a skull and crossbones design, accompanied by a knife with a black handle. The mug is placed on a surface with a striped pattern, and the knife is positioned next to it. The overall setting appears to be a table or countertop.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24268.6, "ram_available_mb": 38572.3, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24430.0, "ram_available_mb": 38410.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.72}, "power_stats": {"power_gpu_soc_mean_watts": 22.323, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 9.14, "gpu_utilization_percent_mean": 73.72, "power_watts_avg": 22.323, "energy_joules_est": 244.2, "duration_seconds": 10.939, "sample_count": 93}, "timestamp": "2026-01-25T15:47:01.497236"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4867.009, "latencies_ms": [4867.009], "images_per_second": 0.205, "prompt_tokens": 39, "response_tokens_est": 12, "n_tiles": 16, "output_text": "mug: 1, knife: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24271.4, "ram_available_mb": 38569.5, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24304.0, "ram_available_mb": 38536.9, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 85.488}, "power_stats": {"power_gpu_soc_mean_watts": 27.055, "power_cpu_cv_mean_watts": 0.8, "power_sys_5v0_mean_watts": 9.188, "gpu_utilization_percent_mean": 85.488, "power_watts_avg": 27.055, "energy_joules_est": 131.69, "duration_seconds": 4.868, "sample_count": 41}, "timestamp": "2026-01-25T15:47:08.419327"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8827.759, "latencies_ms": [8827.759], "images_per_second": 0.113, "prompt_tokens": 44, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The skull and crossbones mug is positioned to the left of the knife, which is lying flat on the surface in the foreground. The background is a textured surface that appears to be a table or countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24304.0, "ram_available_mb": 38536.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24427.7, "ram_available_mb": 38413.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.427}, "power_stats": {"power_gpu_soc_mean_watts": 23.589, "power_cpu_cv_mean_watts": 1.575, "power_sys_5v0_mean_watts": 9.226, "gpu_utilization_percent_mean": 75.427, "power_watts_avg": 23.589, "energy_joules_est": 208.25, "duration_seconds": 8.828, "sample_count": 75}, "timestamp": "2026-01-25T15:47:19.309733"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8843.055, "latencies_ms": [8843.055], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A white mug with a skull and crossbones design is placed on a textured surface, accompanied by a knife with a black handle lying next to it. The setting appears to be a table or countertop.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24267.4, "ram_available_mb": 38573.5, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24265.8, "ram_available_mb": 38575.1, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.6}, "power_stats": {"power_gpu_soc_mean_watts": 23.301, "power_cpu_cv_mean_watts": 1.494, "power_sys_5v0_mean_watts": 9.093, "gpu_utilization_percent_mean": 76.6, "power_watts_avg": 23.301, "energy_joules_est": 206.07, "duration_seconds": 8.844, "sample_count": 75}, "timestamp": "2026-01-25T15:47:30.194153"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12068.491, "latencies_ms": [12068.491], "images_per_second": 0.083, "prompt_tokens": 36, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The image features a white ceramic mug with a black skull and crossbones symbol, accompanied by a black-handled knife with a silver blade, all placed on a surface with a textured grey and white pattern. The lighting in the image is soft and diffused, casting gentle shadows and highlighting the details of the mug and knife.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24265.8, "ram_available_mb": 38575.1, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24388.8, "ram_available_mb": 38452.1, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.762}, "power_stats": {"power_gpu_soc_mean_watts": 22.563, "power_cpu_cv_mean_watts": 1.815, "power_sys_5v0_mean_watts": 9.124, "gpu_utilization_percent_mean": 72.762, "power_watts_avg": 22.563, "energy_joules_est": 272.32, "duration_seconds": 12.069, "sample_count": 105}, "timestamp": "2026-01-25T15:47:44.288019"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12324.049, "latencies_ms": [12324.049], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people gathered around a counter in a room. A man is standing at the counter, possibly a bartender, while several other people are standing or sitting nearby. There are at least five people in the scene, with some of them closer to the counter and others standing further back.\n\nThe room has a dining table in the background, and there are multiple", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 24327.0, "ram_available_mb": 38513.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24411.6, "ram_available_mb": 38429.3, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.181}, "power_stats": {"power_gpu_soc_mean_watts": 22.795, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 9.204, "gpu_utilization_percent_mean": 73.181, "power_watts_avg": 22.795, "energy_joules_est": 280.94, "duration_seconds": 12.325, "sample_count": 105}, "timestamp": "2026-01-25T15:47:58.647170"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11113.917, "latencies_ms": [11113.917], "images_per_second": 0.09, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. People: 5\n2. Bottles: 10\n3. Glasses: 4\n4. Barrel: 1\n5. Counter: 1\n6. Window: 1\n7. Chair: 1\n8. Shoe: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24351.6, "ram_available_mb": 38489.3, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24286.3, "ram_available_mb": 38554.6, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.245}, "power_stats": {"power_gpu_soc_mean_watts": 23.383, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 9.164, "gpu_utilization_percent_mean": 76.245, "power_watts_avg": 23.383, "energy_joules_est": 259.89, "duration_seconds": 11.115, "sample_count": 94}, "timestamp": "2026-01-25T15:48:11.794778"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12429.476, "latencies_ms": [12429.476], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person in a blue shirt is standing close to a counter with a person in a white shirt behind it. In the background, there are three other individuals standing near a window. The person in the white shirt is positioned between the person in the blue shirt and the person in the black shirt, who is standing to the right of the person in", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24286.3, "ram_available_mb": 38554.6, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24333.3, "ram_available_mb": 38507.6, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.66}, "power_stats": {"power_gpu_soc_mean_watts": 22.921, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 9.188, "gpu_utilization_percent_mean": 74.66, "power_watts_avg": 22.921, "energy_joules_est": 284.91, "duration_seconds": 12.43, "sample_count": 106}, "timestamp": "2026-01-25T15:48:26.277936"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8265.16, "latencies_ms": [8265.16], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A group of people are gathered around a counter in a restaurant or cafe, with one person standing at the counter and others standing nearby. It appears to be a casual and social atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24333.3, "ram_available_mb": 38507.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24408.3, "ram_available_mb": 38432.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.676}, "power_stats": {"power_gpu_soc_mean_watts": 24.507, "power_cpu_cv_mean_watts": 1.398, "power_sys_5v0_mean_watts": 9.161, "gpu_utilization_percent_mean": 79.676, "power_watts_avg": 24.507, "energy_joules_est": 202.57, "duration_seconds": 8.266, "sample_count": 71}, "timestamp": "2026-01-25T15:48:36.603462"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8128.773, "latencies_ms": [8128.773], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image shows a group of people in an indoor setting with warm lighting. The walls are painted in a teal color, and there is a wooden bar counter with a person behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24344.8, "ram_available_mb": 38496.1, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24272.9, "ram_available_mb": 38568.0, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.478}, "power_stats": {"power_gpu_soc_mean_watts": 24.44, "power_cpu_cv_mean_watts": 1.439, "power_sys_5v0_mean_watts": 9.261, "gpu_utilization_percent_mean": 77.478, "power_watts_avg": 24.44, "energy_joules_est": 198.68, "duration_seconds": 8.129, "sample_count": 69}, "timestamp": "2026-01-25T15:48:46.753405"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11174.482, "latencies_ms": [11174.482], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a marshy area with a large body of water in the foreground. Two white birds, possibly egrets, are standing in the water, adding a touch of life to the scene. In the background, there is a harbor filled with various boats and ships, showcasing a bustling maritime environment. The sky above is", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24272.9, "ram_available_mb": 38568.0, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24374.7, "ram_available_mb": 38466.2, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.726}, "power_stats": {"power_gpu_soc_mean_watts": 20.832, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 68.726, "power_watts_avg": 20.832, "energy_joules_est": 232.8, "duration_seconds": 11.175, "sample_count": 95}, "timestamp": "2026-01-25T15:48:59.992469"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7185.981, "latencies_ms": [7185.981], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "sky: 1, clouds: 1, airplane: 1, boats: 5, buildings: 2, poles: 4, grass: 1, birds: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24304.6, "ram_available_mb": 38536.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24419.8, "ram_available_mb": 38421.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.672}, "power_stats": {"power_gpu_soc_mean_watts": 22.734, "power_cpu_cv_mean_watts": 1.602, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 72.672, "power_watts_avg": 22.734, "energy_joules_est": 163.38, "duration_seconds": 7.187, "sample_count": 61}, "timestamp": "2026-01-25T15:49:09.227492"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8015.836, "latencies_ms": [8015.836], "images_per_second": 0.125, "prompt_tokens": 44, "response_tokens_est": 51, "n_tiles": 16, "output_text": "In the foreground, there is a grassy area with two birds standing near the center. The background features a marina with several boats and docks extending into the water. The sky above is filled with clouds, suggesting an overcast day.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24419.8, "ram_available_mb": 38421.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24421.5, "ram_available_mb": 38419.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.353}, "power_stats": {"power_gpu_soc_mean_watts": 22.113, "power_cpu_cv_mean_watts": 1.725, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 71.353, "power_watts_avg": 22.113, "energy_joules_est": 177.27, "duration_seconds": 8.017, "sample_count": 68}, "timestamp": "2026-01-25T15:49:19.303321"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7876.304, "latencies_ms": [7876.304], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image depicts a marshy area with two birds standing in the foreground, and a large industrial area with multiple cranes and buildings in the background. The sky is cloudy, suggesting an overcast day.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24421.5, "ram_available_mb": 38419.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24267.2, "ram_available_mb": 38573.7, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.448}, "power_stats": {"power_gpu_soc_mean_watts": 22.306, "power_cpu_cv_mean_watts": 1.673, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 72.448, "power_watts_avg": 22.306, "energy_joules_est": 175.7, "duration_seconds": 7.877, "sample_count": 67}, "timestamp": "2026-01-25T15:49:29.234866"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7583.427, "latencies_ms": [7583.427], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The sky is overcast with a mix of blue and gray clouds, suggesting a gloomy or cloudy day. The grass in the foreground is a vibrant green, contrasting with the industrial structures in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24267.2, "ram_available_mb": 38573.7, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24263.1, "ram_available_mb": 38577.8, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.547}, "power_stats": {"power_gpu_soc_mean_watts": 22.319, "power_cpu_cv_mean_watts": 1.683, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 71.547, "power_watts_avg": 22.319, "energy_joules_est": 169.27, "duration_seconds": 7.584, "sample_count": 64}, "timestamp": "2026-01-25T15:49:38.853215"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11150.74, "latencies_ms": [11150.74], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photo, a man is kneeling on the floor in a bathroom, working on a toilet. He is wearing a black shirt and jeans, and has a tool belt around his waist. The bathroom is equipped with a white toilet, a sink, and a mirror. The walls of the bathroom are adorned", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24263.6, "ram_available_mb": 38577.3, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24291.6, "ram_available_mb": 38549.3, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.379}, "power_stats": {"power_gpu_soc_mean_watts": 20.911, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.379, "power_watts_avg": 20.911, "energy_joules_est": 233.19, "duration_seconds": 11.151, "sample_count": 95}, "timestamp": "2026-01-25T15:49:52.029536"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10819.18, "latencies_ms": [10819.18], "images_per_second": 0.092, "prompt_tokens": 39, "response_tokens_est": 74, "n_tiles": 16, "output_text": "1. Toilet: 1\n2. Glove: 1\n3. Tool belt: 1\n4. Tiles: 1\n5. Tile adhesive: 1\n6. Tile cutter: 1\n7. Hammer: 1\n8. Screwdriver: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24291.6, "ram_available_mb": 38549.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24306.7, "ram_available_mb": 38534.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.859}, "power_stats": {"power_gpu_soc_mean_watts": 21.146, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.916, "gpu_utilization_percent_mean": 69.859, "power_watts_avg": 21.146, "energy_joules_est": 228.8, "duration_seconds": 10.82, "sample_count": 92}, "timestamp": "2026-01-25T15:50:04.862723"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11202.789, "latencies_ms": [11202.789], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The person is seated on a toilet in the foreground, with their body oriented towards the right side of the image. The background features a wall with a checkered pattern, and there is a shelf with various items on it to the right of the person. The person's left hand is placed on the toilet, suggesting they are in the process of using it", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24306.7, "ram_available_mb": 38534.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24292.8, "ram_available_mb": 38548.1, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.959, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.932, "gpu_utilization_percent_mean": 70.0, "power_watts_avg": 20.959, "energy_joules_est": 234.81, "duration_seconds": 11.203, "sample_count": 95}, "timestamp": "2026-01-25T15:50:18.099699"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7865.594, "latencies_ms": [7865.594], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A man is sitting on a toilet in a bathroom, wearing a black shirt and jeans, with a tool belt around his waist. He appears to be fixing or maintaining the toilet.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24292.8, "ram_available_mb": 38548.1, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24289.9, "ram_available_mb": 38551.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.149}, "power_stats": {"power_gpu_soc_mean_watts": 22.389, "power_cpu_cv_mean_watts": 1.661, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 73.149, "power_watts_avg": 22.389, "energy_joules_est": 176.12, "duration_seconds": 7.866, "sample_count": 67}, "timestamp": "2026-01-25T15:50:28.018324"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8377.233, "latencies_ms": [8377.233], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image is in black and white, featuring a person in a dark shirt and jeans working on a toilet. The lighting is bright and even, illuminating the person and the toilet, which is made of porcelain.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24289.9, "ram_available_mb": 38551.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24300.8, "ram_available_mb": 38540.1, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.634}, "power_stats": {"power_gpu_soc_mean_watts": 22.031, "power_cpu_cv_mean_watts": 1.737, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 71.634, "power_watts_avg": 22.031, "energy_joules_est": 184.57, "duration_seconds": 8.378, "sample_count": 71}, "timestamp": "2026-01-25T15:50:38.431226"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12176.076, "latencies_ms": [12176.076], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a group of people standing on a snow-covered mountain. There are six people in the scene, with some of them wearing skis. They are positioned near the top of the mountain, possibly preparing for a skiing adventure or taking a break. The group is spread out, with some individuals closer to the foreground and others further back.\n\n", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24300.8, "ram_available_mb": 38540.1, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24296.8, "ram_available_mb": 38544.1, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.337}, "power_stats": {"power_gpu_soc_mean_watts": 21.991, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 9.137, "gpu_utilization_percent_mean": 73.337, "power_watts_avg": 21.991, "energy_joules_est": 267.78, "duration_seconds": 12.177, "sample_count": 104}, "timestamp": "2026-01-25T15:50:52.664710"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8769.765, "latencies_ms": [8769.765], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "people: 5\nskis: 2\nsnowboards: 0\nmountains: 1\ntracks: 1\nsnow: 1\ntrees: 0\nclouds: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24296.8, "ram_available_mb": 38544.1, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24299.6, "ram_available_mb": 38541.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.932}, "power_stats": {"power_gpu_soc_mean_watts": 23.883, "power_cpu_cv_mean_watts": 1.493, "power_sys_5v0_mean_watts": 9.1, "gpu_utilization_percent_mean": 76.932, "power_watts_avg": 23.883, "energy_joules_est": 209.46, "duration_seconds": 8.77, "sample_count": 74}, "timestamp": "2026-01-25T15:51:03.461731"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11082.194, "latencies_ms": [11082.194], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there is a group of people standing on skis, positioned near the base of a large snow-covered mountain. The mountain is in the background, with its peak reaching high into the clear blue sky. The people are closer to the viewer than the mountain, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24299.6, "ram_available_mb": 38541.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24299.1, "ram_available_mb": 38541.8, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.681}, "power_stats": {"power_gpu_soc_mean_watts": 22.866, "power_cpu_cv_mean_watts": 1.742, "power_sys_5v0_mean_watts": 9.165, "gpu_utilization_percent_mean": 73.681, "power_watts_avg": 22.866, "energy_joules_est": 253.42, "duration_seconds": 11.083, "sample_count": 94}, "timestamp": "2026-01-25T15:51:16.594782"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8011.765, "latencies_ms": [8011.765], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A group of people are standing on a snowy mountain, with a large snow-covered mountain in the background. They appear to be preparing to ski or snowboard down the mountain.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24299.1, "ram_available_mb": 38541.8, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24301.0, "ram_available_mb": 38539.9, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.149}, "power_stats": {"power_gpu_soc_mean_watts": 24.101, "power_cpu_cv_mean_watts": 1.399, "power_sys_5v0_mean_watts": 9.132, "gpu_utilization_percent_mean": 78.149, "power_watts_avg": 24.101, "energy_joules_est": 193.11, "duration_seconds": 8.012, "sample_count": 67}, "timestamp": "2026-01-25T15:51:26.618002"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8305.835, "latencies_ms": [8305.835], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image features a group of people standing on a snowy mountain with a clear blue sky in the background. The snow on the mountain appears to be fresh and untouched, with some tracks visible in the snow.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24301.0, "ram_available_mb": 38539.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24303.3, "ram_available_mb": 38537.6, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.4}, "power_stats": {"power_gpu_soc_mean_watts": 23.528, "power_cpu_cv_mean_watts": 1.51, "power_sys_5v0_mean_watts": 9.193, "gpu_utilization_percent_mean": 76.4, "power_watts_avg": 23.528, "energy_joules_est": 195.44, "duration_seconds": 8.306, "sample_count": 70}, "timestamp": "2026-01-25T15:51:36.939540"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12367.06, "latencies_ms": [12367.06], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a well-balanced meal served on a white plate. The plate is divided into three sections. On the left side, there's a serving of white rice, which is the base of the meal. The middle section of the plate is filled with a vibrant red and yellow chili, adding a pop of color and likely a spicy kick to the me", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24303.3, "ram_available_mb": 38537.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24284.6, "ram_available_mb": 38556.3, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.01}, "power_stats": {"power_gpu_soc_mean_watts": 22.817, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 9.179, "gpu_utilization_percent_mean": 74.01, "power_watts_avg": 22.817, "energy_joules_est": 282.19, "duration_seconds": 12.368, "sample_count": 105}, "timestamp": "2026-01-25T15:51:51.334488"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6281.141, "latencies_ms": [6281.141], "images_per_second": 0.159, "prompt_tokens": 39, "response_tokens_est": 22, "n_tiles": 16, "output_text": "rice: 1 portion, broccoli: 2 portions, chili: 1 portion", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24284.6, "ram_available_mb": 38556.3, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24307.4, "ram_available_mb": 38533.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 82.981}, "power_stats": {"power_gpu_soc_mean_watts": 25.955, "power_cpu_cv_mean_watts": 1.08, "power_sys_5v0_mean_watts": 9.204, "gpu_utilization_percent_mean": 82.981, "power_watts_avg": 25.955, "energy_joules_est": 163.04, "duration_seconds": 6.282, "sample_count": 53}, "timestamp": "2026-01-25T15:51:59.672748"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12443.09, "latencies_ms": [12443.09], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the foreground of the image, there is a white bowl containing a meal. To the left of the bowl, there is a piece of broccoli, and to the right, there is a serving of rice. The bowl is placed on a wooden surface, and the broccoli is positioned to the left of the rice, creating a balanced composition.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24307.4, "ram_available_mb": 38533.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24283.0, "ram_available_mb": 38557.9, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.434}, "power_stats": {"power_gpu_soc_mean_watts": 22.894, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 9.198, "gpu_utilization_percent_mean": 74.434, "power_watts_avg": 22.894, "energy_joules_est": 284.89, "duration_seconds": 12.444, "sample_count": 106}, "timestamp": "2026-01-25T15:52:14.143885"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8860.849, "latencies_ms": [8860.849], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows a white bowl filled with a meal consisting of white rice, a serving of stewed vegetables, and a piece of broccoli. The bowl is placed on a dark wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24283.0, "ram_available_mb": 38557.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24284.3, "ram_available_mb": 38556.6, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.013}, "power_stats": {"power_gpu_soc_mean_watts": 24.263, "power_cpu_cv_mean_watts": 1.452, "power_sys_5v0_mean_watts": 9.16, "gpu_utilization_percent_mean": 78.013, "power_watts_avg": 24.263, "energy_joules_est": 215.01, "duration_seconds": 8.862, "sample_count": 75}, "timestamp": "2026-01-25T15:52:25.057822"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11754.358, "latencies_ms": [11754.358], "images_per_second": 0.085, "prompt_tokens": 36, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The image shows a meal consisting of white rice, a piece of broccoli, and a red bean stew, all served on a white plate. The lighting in the image is warm and appears to be coming from the top left, casting a soft glow on the food and highlighting the textures of the rice and the stew.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24284.3, "ram_available_mb": 38556.6, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24288.1, "ram_available_mb": 38552.8, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.41}, "power_stats": {"power_gpu_soc_mean_watts": 23.023, "power_cpu_cv_mean_watts": 1.733, "power_sys_5v0_mean_watts": 9.208, "gpu_utilization_percent_mean": 74.41, "power_watts_avg": 23.023, "energy_joules_est": 270.63, "duration_seconds": 11.755, "sample_count": 100}, "timestamp": "2026-01-25T15:52:38.856211"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11140.131, "latencies_ms": [11140.131], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is skillfully skateboarding on a wooden platform. The skateboarder is wearing a pair of black and white sneakers, which are visible as they perform a trick on the skateboard. The skateboard itself is black and white, with a checkered pattern on the bottom. The wooden platform on which the skateboarder is", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24288.1, "ram_available_mb": 38552.8, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24263.9, "ram_available_mb": 38577.0, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.621}, "power_stats": {"power_gpu_soc_mean_watts": 20.904, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.621, "power_watts_avg": 20.904, "energy_joules_est": 232.89, "duration_seconds": 11.141, "sample_count": 95}, "timestamp": "2026-01-25T15:52:52.048712"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8324.491, "latencies_ms": [8324.491], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "skateboard: 1, wooden platform: 1, grass: multiple patches, person's legs: 2, shoes: 2, wheels: 4, checkered pattern: 1, sunlight: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24263.9, "ram_available_mb": 38577.0, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24404.3, "ram_available_mb": 38436.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.271}, "power_stats": {"power_gpu_soc_mean_watts": 22.178, "power_cpu_cv_mean_watts": 1.698, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 73.271, "power_watts_avg": 22.178, "energy_joules_est": 184.64, "duration_seconds": 8.325, "sample_count": 70}, "timestamp": "2026-01-25T15:53:02.392361"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8914.926, "latencies_ms": [8914.926], "images_per_second": 0.112, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, performing a trick on a wooden platform. The platform is elevated above the grassy ground, which is in the background. There is another wooden structure visible in the far background, slightly to the right of the skateboarder.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24272.3, "ram_available_mb": 38568.6, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24408.5, "ram_available_mb": 38432.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.2}, "power_stats": {"power_gpu_soc_mean_watts": 21.723, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 71.2, "power_watts_avg": 21.723, "energy_joules_est": 193.67, "duration_seconds": 8.916, "sample_count": 75}, "timestamp": "2026-01-25T15:53:13.324356"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5619.433, "latencies_ms": [5619.433], "images_per_second": 0.178, "prompt_tokens": 37, "response_tokens_est": 28, "n_tiles": 16, "output_text": "A person is skateboarding on a wooden platform outdoors, with a blurred background featuring grass and wooden structures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24408.5, "ram_available_mb": 38432.4, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24409.0, "ram_available_mb": 38431.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.128}, "power_stats": {"power_gpu_soc_mean_watts": 24.231, "power_cpu_cv_mean_watts": 1.363, "power_sys_5v0_mean_watts": 9.043, "gpu_utilization_percent_mean": 77.128, "power_watts_avg": 24.231, "energy_joules_est": 136.18, "duration_seconds": 5.62, "sample_count": 47}, "timestamp": "2026-01-25T15:53:20.998018"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8106.882, "latencies_ms": [8106.882], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image captures a moment of a skateboarder in mid-air, with the skateboard positioned on a wooden platform. The lighting is natural and bright, suggesting the photo was taken outdoors on a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24322.0, "ram_available_mb": 38518.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24325.9, "ram_available_mb": 38515.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.13}, "power_stats": {"power_gpu_soc_mean_watts": 21.996, "power_cpu_cv_mean_watts": 1.729, "power_sys_5v0_mean_watts": 9.042, "gpu_utilization_percent_mean": 71.13, "power_watts_avg": 21.996, "energy_joules_est": 178.33, "duration_seconds": 8.108, "sample_count": 69}, "timestamp": "2026-01-25T15:53:31.163098"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12088.05, "latencies_ms": [12088.05], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The image features a bunch of bananas sitting on a wooden table. There are three bananas in the bunch, with one banana placed on top of the other two. The bananas are yellow and appear ripe, indicating that they are ready to be eaten. The table provides a contrasting background for the bananas, making them the focal point of the scene.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24272.4, "ram_available_mb": 38568.5, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24275.6, "ram_available_mb": 38565.3, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.087}, "power_stats": {"power_gpu_soc_mean_watts": 21.854, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 9.167, "gpu_utilization_percent_mean": 73.087, "power_watts_avg": 21.854, "energy_joules_est": 264.19, "duration_seconds": 12.089, "sample_count": 103}, "timestamp": "2026-01-25T15:53:45.281357"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4308.132, "latencies_ms": [4308.132], "images_per_second": 0.232, "prompt_tokens": 39, "response_tokens_est": 7, "n_tiles": 16, "output_text": "banana: 5\n", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24275.6, "ram_available_mb": 38565.3, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24276.0, "ram_available_mb": 38564.9, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 89.694}, "power_stats": {"power_gpu_soc_mean_watts": 28.05, "power_cpu_cv_mean_watts": 0.578, "power_sys_5v0_mean_watts": 9.236, "gpu_utilization_percent_mean": 89.694, "power_watts_avg": 28.05, "energy_joules_est": 120.86, "duration_seconds": 4.309, "sample_count": 36}, "timestamp": "2026-01-25T15:53:51.642746"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9268.77, "latencies_ms": [9268.77], "images_per_second": 0.108, "prompt_tokens": 44, "response_tokens_est": 54, "n_tiles": 16, "output_text": "In the foreground, there is a bunch of bananas with one banana lying on its side, closer to the viewer. The background features a blurred computer monitor and keyboard, indicating that the main objects are placed on a desk or table.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24276.0, "ram_available_mb": 38564.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24426.2, "ram_available_mb": 38414.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.291}, "power_stats": {"power_gpu_soc_mean_watts": 23.485, "power_cpu_cv_mean_watts": 1.611, "power_sys_5v0_mean_watts": 9.169, "gpu_utilization_percent_mean": 75.291, "power_watts_avg": 23.485, "energy_joules_est": 217.69, "duration_seconds": 9.269, "sample_count": 79}, "timestamp": "2026-01-25T15:54:02.946507"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7531.179, "latencies_ms": [7531.179], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "A bunch of bananas is precariously balanced on top of another banana on a wooden table. In the background, there is a computer monitor and a keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.2, "ram_available_mb": 38414.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24414.1, "ram_available_mb": 38426.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.375}, "power_stats": {"power_gpu_soc_mean_watts": 24.532, "power_cpu_cv_mean_watts": 1.363, "power_sys_5v0_mean_watts": 9.147, "gpu_utilization_percent_mean": 79.375, "power_watts_avg": 24.532, "energy_joules_est": 184.77, "duration_seconds": 7.532, "sample_count": 64}, "timestamp": "2026-01-25T15:54:12.494874"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7909.314, "latencies_ms": [7909.314], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The bananas are yellow with some brown spots, indicating ripeness. They are placed on a wooden surface, likely a table, with a blurred background that includes a computer monitor and keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24414.1, "ram_available_mb": 38426.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24275.5, "ram_available_mb": 38565.4, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.09}, "power_stats": {"power_gpu_soc_mean_watts": 23.81, "power_cpu_cv_mean_watts": 1.488, "power_sys_5v0_mean_watts": 9.258, "gpu_utilization_percent_mean": 77.09, "power_watts_avg": 23.81, "energy_joules_est": 188.34, "duration_seconds": 7.91, "sample_count": 67}, "timestamp": "2026-01-25T15:54:22.443703"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11211.813, "latencies_ms": [11211.813], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large plate of food placed on a dining table. The plate contains a variety of food items, including a generous portion of rice, several pieces of broccoli, and a mix of carrots and other vegetables. The dish appears to be a delicious and well-balanced meal, with a combination of vegetables and rice.\n\nIn", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24275.5, "ram_available_mb": 38565.4, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24271.4, "ram_available_mb": 38569.5, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.74}, "power_stats": {"power_gpu_soc_mean_watts": 20.422, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.922, "gpu_utilization_percent_mean": 69.74, "power_watts_avg": 20.422, "energy_joules_est": 228.98, "duration_seconds": 11.212, "sample_count": 96}, "timestamp": "2026-01-25T15:54:35.676724"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8204.932, "latencies_ms": [8204.932], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "plate: 1\nglass: 1\nwater: 1\nfork: 1\nknife: 1\nchicken: 1\nbroccoli: 1\ncarrot: 1\nrice: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24271.4, "ram_available_mb": 38569.5, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24310.3, "ram_available_mb": 38530.6, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.478}, "power_stats": {"power_gpu_soc_mean_watts": 22.18, "power_cpu_cv_mean_watts": 1.711, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 72.478, "power_watts_avg": 22.18, "energy_joules_est": 182.0, "duration_seconds": 8.206, "sample_count": 69}, "timestamp": "2026-01-25T15:54:45.927216"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7347.304, "latencies_ms": [7347.304], "images_per_second": 0.136, "prompt_tokens": 44, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The plate of food is in the foreground of the image, placed on a wooden table. In the background, there is a glass of water and a fork and knife placed to the left side of the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24310.3, "ram_available_mb": 38530.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24375.7, "ram_available_mb": 38465.2, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.484}, "power_stats": {"power_gpu_soc_mean_watts": 22.475, "power_cpu_cv_mean_watts": 1.646, "power_sys_5v0_mean_watts": 9.065, "gpu_utilization_percent_mean": 72.484, "power_watts_avg": 22.475, "energy_joules_est": 165.14, "duration_seconds": 7.348, "sample_count": 62}, "timestamp": "2026-01-25T15:54:55.314289"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7544.956, "latencies_ms": [7544.956], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A plate of food is on a wooden table, with a glass of water and a fork and knife placed beside it. The plate contains a serving of white rice, stir-fried vegetables, and chicken.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24375.7, "ram_available_mb": 38465.2, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24280.1, "ram_available_mb": 38560.8, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.062}, "power_stats": {"power_gpu_soc_mean_watts": 22.561, "power_cpu_cv_mean_watts": 1.633, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 74.062, "power_watts_avg": 22.561, "energy_joules_est": 170.24, "duration_seconds": 7.546, "sample_count": 64}, "timestamp": "2026-01-25T15:55:04.879194"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7454.377, "latencies_ms": [7454.377], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The plate is colorful with a mix of orange, green, and white hues, and it's placed on a wooden table. There's a glass of water in the background, suggesting a dining setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24280.1, "ram_available_mb": 38560.8, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24385.1, "ram_available_mb": 38455.8, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.73}, "power_stats": {"power_gpu_soc_mean_watts": 22.31, "power_cpu_cv_mean_watts": 1.678, "power_sys_5v0_mean_watts": 9.033, "gpu_utilization_percent_mean": 71.73, "power_watts_avg": 22.31, "energy_joules_est": 166.32, "duration_seconds": 7.455, "sample_count": 63}, "timestamp": "2026-01-25T15:55:14.354535"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11141.044, "latencies_ms": [11141.044], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a young girl in a colorful dress playing with a Wii remote in a living room. She is standing in front of a couch, holding the remote in her hand. There are several other people in the room, including a man and a woman standing near the couch, and another person standing further back. \n\nThe living room is furnished with a couch", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24269.9, "ram_available_mb": 38571.0, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24272.0, "ram_available_mb": 38568.9, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.4}, "power_stats": {"power_gpu_soc_mean_watts": 20.89, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.4, "power_watts_avg": 20.89, "energy_joules_est": 232.75, "duration_seconds": 11.142, "sample_count": 95}, "timestamp": "2026-01-25T15:55:27.543695"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9486.823, "latencies_ms": [9486.823], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "- people: 5\n\n- couch: 1\n\n- sofa: 1\n\n- rug: 1\n\n- wheelbarrow: 1\n\n- remote control: 1\n\n- wine glass: 1\n\n- bottle: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24272.0, "ram_available_mb": 38568.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24277.2, "ram_available_mb": 38563.7, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.457}, "power_stats": {"power_gpu_soc_mean_watts": 21.53, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 8.934, "gpu_utilization_percent_mean": 71.457, "power_watts_avg": 21.53, "energy_joules_est": 204.27, "duration_seconds": 9.487, "sample_count": 81}, "timestamp": "2026-01-25T15:55:39.048574"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11201.468, "latencies_ms": [11201.468], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a young girl is standing on a gray shaggy rug, holding a remote control and appears to be in motion, possibly playing a video game. Behind her, on the right side of the image, there is a couch with red and white patterned pillows. In the background, there are several people standing and interacting with each other, with one person", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24277.2, "ram_available_mb": 38563.7, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24274.8, "ram_available_mb": 38566.1, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.126}, "power_stats": {"power_gpu_soc_mean_watts": 20.983, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 70.126, "power_watts_avg": 20.983, "energy_joules_est": 235.05, "duration_seconds": 11.202, "sample_count": 95}, "timestamp": "2026-01-25T15:55:52.295940"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8453.992, "latencies_ms": [8453.992], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "A young girl is energetically playing with a red ball in a living room, while a group of people watch her. The room is furnished with a couch, a coffee table, and a rug, creating a cozy and lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24274.8, "ram_available_mb": 38566.1, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24270.1, "ram_available_mb": 38570.8, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.861}, "power_stats": {"power_gpu_soc_mean_watts": 22.158, "power_cpu_cv_mean_watts": 1.718, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 71.861, "power_watts_avg": 22.158, "energy_joules_est": 187.34, "duration_seconds": 8.455, "sample_count": 72}, "timestamp": "2026-01-25T15:56:02.791942"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8172.05, "latencies_ms": [8172.05], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The room is well-lit with natural light coming from the windows, and the carpet is a soft, plush grey. The couch is adorned with red and white patterned pillows, adding a pop of color to the space.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24270.1, "ram_available_mb": 38570.8, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24272.5, "ram_available_mb": 38568.4, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.343}, "power_stats": {"power_gpu_soc_mean_watts": 22.07, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 71.343, "power_watts_avg": 22.07, "energy_joules_est": 180.37, "duration_seconds": 8.173, "sample_count": 70}, "timestamp": "2026-01-25T15:56:13.017865"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11154.458, "latencies_ms": [11154.458], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features two men shaking hands in a room filled with people. The man on the left is wearing a tie, and the man on the right is wearing a suit. They are both smiling, indicating a positive interaction. The room is filled with numerous people, some of whom are also wearing ties.\n\nThere are several dining tables in the room, with", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24274.0, "ram_available_mb": 38566.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24283.0, "ram_available_mb": 38557.9, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.495}, "power_stats": {"power_gpu_soc_mean_watts": 20.913, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 69.495, "power_watts_avg": 20.913, "energy_joules_est": 233.29, "duration_seconds": 11.155, "sample_count": 95}, "timestamp": "2026-01-25T15:56:26.207909"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10085.785, "latencies_ms": [10085.785], "images_per_second": 0.099, "prompt_tokens": 39, "response_tokens_est": 68, "n_tiles": 16, "output_text": "1. People: 12\n2. Tables: 4\n3. Wine glasses: 3\n4. Plates: 2\n5. Napkins: 2\n6. Menus: 1\n7. Glasses: 1\n8. Suits: 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24283.0, "ram_available_mb": 38557.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24307.9, "ram_available_mb": 38533.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.294}, "power_stats": {"power_gpu_soc_mean_watts": 21.338, "power_cpu_cv_mean_watts": 1.837, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 70.294, "power_watts_avg": 21.338, "energy_joules_est": 215.22, "duration_seconds": 10.086, "sample_count": 85}, "timestamp": "2026-01-25T15:56:38.319697"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11172.208, "latencies_ms": [11172.208], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, two men are shaking hands, one wearing a patterned shirt and the other in a suit with a yellow tie. They are standing close to each other, indicating a personal interaction. In the background, there are multiple tables set up with white tablecloths, and several other people are scattered around, suggesting this is a social event or gathering. The", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24307.9, "ram_available_mb": 38533.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24275.0, "ram_available_mb": 38565.9, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.83}, "power_stats": {"power_gpu_soc_mean_watts": 20.894, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.934, "gpu_utilization_percent_mean": 69.83, "power_watts_avg": 20.894, "energy_joules_est": 233.45, "duration_seconds": 11.173, "sample_count": 94}, "timestamp": "2026-01-25T15:56:51.547232"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4939.169, "latencies_ms": [4939.169], "images_per_second": 0.202, "prompt_tokens": 37, "response_tokens_est": 22, "n_tiles": 16, "output_text": "Two men are shaking hands in a formal event setting, with a group of people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24275.0, "ram_available_mb": 38565.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24288.4, "ram_available_mb": 38552.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.683}, "power_stats": {"power_gpu_soc_mean_watts": 25.103, "power_cpu_cv_mean_watts": 1.24, "power_sys_5v0_mean_watts": 9.118, "gpu_utilization_percent_mean": 77.683, "power_watts_avg": 25.103, "energy_joules_est": 124.0, "duration_seconds": 4.94, "sample_count": 41}, "timestamp": "2026-01-25T15:56:58.513401"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8444.783, "latencies_ms": [8444.783], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows two men shaking hands in an indoor setting with warm lighting. The man on the left is wearing a patterned shirt with a blue collar, while the man on the right is dressed in a dark suit with a yellow tie.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24288.4, "ram_available_mb": 38552.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24269.4, "ram_available_mb": 38571.5, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.11}, "power_stats": {"power_gpu_soc_mean_watts": 21.768, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 9.033, "gpu_utilization_percent_mean": 71.11, "power_watts_avg": 21.768, "energy_joules_est": 183.84, "duration_seconds": 8.446, "sample_count": 73}, "timestamp": "2026-01-25T15:57:08.983280"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11134.956, "latencies_ms": [11134.956], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is captured in a moment of quiet contemplation. He is dressed in a crisp white shirt, which contrasts with his dark tie. His gaze is directed off to the side, suggesting he is lost in thought. The background is blurred, drawing focus to the man and his introspective state. The lighting is soft, casting a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24269.4, "ram_available_mb": 38571.5, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24296.1, "ram_available_mb": 38544.8, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.443}, "power_stats": {"power_gpu_soc_mean_watts": 20.872, "power_cpu_cv_mean_watts": 1.928, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 69.443, "power_watts_avg": 20.872, "energy_joules_est": 232.42, "duration_seconds": 11.136, "sample_count": 97}, "timestamp": "2026-01-25T15:57:22.167506"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7330.817, "latencies_ms": [7330.817], "images_per_second": 0.136, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "person: 1, shirt: 1, tie: 1, collar: 1, button: 1, cuff: 1, pocket: 1, hand: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24296.1, "ram_available_mb": 38544.8, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24315.8, "ram_available_mb": 38525.1, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.403}, "power_stats": {"power_gpu_soc_mean_watts": 22.677, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 73.403, "power_watts_avg": 22.677, "energy_joules_est": 166.25, "duration_seconds": 7.331, "sample_count": 62}, "timestamp": "2026-01-25T15:57:31.515512"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11177.658, "latencies_ms": [11177.658], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The person in the image is wearing a white shirt and a striped tie. The shirt is buttoned up, and the tie is tied in a neat knot. The person's left hand is slightly raised, as if they are gesturing or making a point. The background is blurred, but it appears to be an indoor setting with some objects in the distance", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24315.8, "ram_available_mb": 38525.1, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24321.6, "ram_available_mb": 38519.3, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.375}, "power_stats": {"power_gpu_soc_mean_watts": 20.95, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.375, "power_watts_avg": 20.95, "energy_joules_est": 234.18, "duration_seconds": 11.178, "sample_count": 96}, "timestamp": "2026-01-25T15:57:44.738319"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8202.777, "latencies_ms": [8202.777], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A person is standing in a dimly lit environment, wearing a white shirt and a striped tie. The background is blurred, but it appears to be an indoor setting with some objects that could be furniture or equipment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24321.6, "ram_available_mb": 38519.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24312.5, "ram_available_mb": 38528.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.429}, "power_stats": {"power_gpu_soc_mean_watts": 22.077, "power_cpu_cv_mean_watts": 1.699, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 72.429, "power_watts_avg": 22.077, "energy_joules_est": 181.11, "duration_seconds": 8.203, "sample_count": 70}, "timestamp": "2026-01-25T15:57:54.966176"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6932.422, "latencies_ms": [6932.422], "images_per_second": 0.144, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image features a person wearing a white shirt and a dark striped tie. The lighting in the image is dim, with a focus on the person's face and upper body.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24312.5, "ram_available_mb": 38528.4, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24301.4, "ram_available_mb": 38539.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.441}, "power_stats": {"power_gpu_soc_mean_watts": 22.773, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 9.095, "gpu_utilization_percent_mean": 73.441, "power_watts_avg": 22.773, "energy_joules_est": 157.89, "duration_seconds": 6.933, "sample_count": 59}, "timestamp": "2026-01-25T15:58:03.921114"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11157.215, "latencies_ms": [11157.215], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a cozy living room scene. Dominating the space is a blue and red plaid sofa, inviting and comfortable. To the left of the sofa, a wooden chair with a red and white checkered pattern adds a touch of homeliness. The chair and sofa are positioned in front of a wooden entertainment center, which houses a television set.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24301.4, "ram_available_mb": 38539.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.372}, "power_stats": {"power_gpu_soc_mean_watts": 20.905, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 69.372, "power_watts_avg": 20.905, "energy_joules_est": 233.26, "duration_seconds": 11.158, "sample_count": 94}, "timestamp": "2026-01-25T15:58:17.112407"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7414.225, "latencies_ms": [7414.225], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "Chair: 1, Sofa: 1, Entertainment center: 1, Television: 1, Picture: 1, List: 1, Speaker: 1, Rug: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24273.3, "ram_available_mb": 38567.6, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24326.0, "ram_available_mb": 38514.9, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.016}, "power_stats": {"power_gpu_soc_mean_watts": 22.597, "power_cpu_cv_mean_watts": 1.621, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 73.016, "power_watts_avg": 22.597, "energy_joules_est": 167.55, "duration_seconds": 7.415, "sample_count": 63}, "timestamp": "2026-01-25T15:58:26.555166"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11160.198, "latencies_ms": [11160.198], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a red plaid chair positioned to the left and a blue plaid sofa to the right, both facing forward. The chair and sofa are in the middle ground and are the main objects in the image. In the background, there is a television set on a wooden stand and a whiteboard with writing on it, both situated above the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24326.0, "ram_available_mb": 38514.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24275.0, "ram_available_mb": 38565.9, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.677}, "power_stats": {"power_gpu_soc_mean_watts": 20.85, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 69.677, "power_watts_avg": 20.85, "energy_joules_est": 232.7, "duration_seconds": 11.161, "sample_count": 96}, "timestamp": "2026-01-25T15:58:39.750920"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8959.167, "latencies_ms": [8959.167], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image depicts a cozy living room setting with a red and blue plaid sofa, a red armchair, and a television on a wooden stand. A whiteboard with writing on it is mounted on the wall, and a framed picture is also visible.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24275.0, "ram_available_mb": 38565.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24270.4, "ram_available_mb": 38570.5, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.831}, "power_stats": {"power_gpu_soc_mean_watts": 21.903, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 72.831, "power_watts_avg": 21.903, "energy_joules_est": 196.25, "duration_seconds": 8.96, "sample_count": 77}, "timestamp": "2026-01-25T15:58:50.738420"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8094.213, "latencies_ms": [8094.213], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The room has a plaid patterned chair and ottoman, with a whiteboard and a black and white photo on the wall. The lighting in the room is dim, and the materials used for the furniture appear to be wood and fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24270.4, "ram_available_mb": 38570.5, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24379.4, "ram_available_mb": 38461.5, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.725}, "power_stats": {"power_gpu_soc_mean_watts": 21.94, "power_cpu_cv_mean_watts": 1.752, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 71.725, "power_watts_avg": 21.94, "energy_joules_est": 177.6, "duration_seconds": 8.095, "sample_count": 69}, "timestamp": "2026-01-25T15:59:00.853783"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12352.217, "latencies_ms": [12352.217], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a dynamic scene of a surfer riding a wave. The surfer, clad in a vibrant yellow shirt and black shorts, is skillfully maneuvering a white surfboard. The wave, a powerful force of nature, is breaking to the right, creating a spray of white foam that contrasts with the deep blue-green", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24282.6, "ram_available_mb": 38558.3, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24277.7, "ram_available_mb": 38563.2, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.123}, "power_stats": {"power_gpu_soc_mean_watts": 22.815, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 9.186, "gpu_utilization_percent_mean": 74.123, "power_watts_avg": 22.815, "energy_joules_est": 281.83, "duration_seconds": 12.353, "sample_count": 106}, "timestamp": "2026-01-25T15:59:15.266063"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9651.601, "latencies_ms": [9651.601], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "water: 1\nsurfboard: 1\nyellow shirt: 1\nred shorts: 1\nblack gloves: 1\nwhite surfboard: 1\nsurfer: 1\nwave: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24277.7, "ram_available_mb": 38563.2, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24294.0, "ram_available_mb": 38546.9, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.927}, "power_stats": {"power_gpu_soc_mean_watts": 23.885, "power_cpu_cv_mean_watts": 1.528, "power_sys_5v0_mean_watts": 9.157, "gpu_utilization_percent_mean": 77.927, "power_watts_avg": 23.885, "energy_joules_est": 230.55, "duration_seconds": 9.652, "sample_count": 82}, "timestamp": "2026-01-25T15:59:26.972378"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11913.366, "latencies_ms": [11913.366], "images_per_second": 0.084, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is breaking to the right of the frame. The water in the background appears calmer and is a darker shade of blue, indicating depth and distance from the surfer. The surfboard is angled towards the left side of the image, suggesting movement in that direction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24294.0, "ram_available_mb": 38546.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24287.2, "ram_available_mb": 38553.7, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.584}, "power_stats": {"power_gpu_soc_mean_watts": 22.956, "power_cpu_cv_mean_watts": 1.72, "power_sys_5v0_mean_watts": 9.19, "gpu_utilization_percent_mean": 74.584, "power_watts_avg": 22.956, "energy_joules_est": 273.5, "duration_seconds": 11.914, "sample_count": 101}, "timestamp": "2026-01-25T15:59:40.917217"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7058.758, "latencies_ms": [7058.758], "images_per_second": 0.142, "prompt_tokens": 37, "response_tokens_est": 29, "n_tiles": 16, "output_text": "A person is surfing on a wave in the ocean. The surfer is wearing a yellow shirt and red shorts.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24287.2, "ram_available_mb": 38553.7, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24345.9, "ram_available_mb": 38495.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 81.15}, "power_stats": {"power_gpu_soc_mean_watts": 25.234, "power_cpu_cv_mean_watts": 1.188, "power_sys_5v0_mean_watts": 9.146, "gpu_utilization_percent_mean": 81.15, "power_watts_avg": 25.234, "energy_joules_est": 178.14, "duration_seconds": 7.059, "sample_count": 60}, "timestamp": "2026-01-25T15:59:50.019979"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9157.911, "latencies_ms": [9157.911], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The surfer is wearing a bright yellow shirt and red shorts, which stand out against the white foam of the wave. The lighting in the image is natural, suggesting it was taken during the day under clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24289.3, "ram_available_mb": 38551.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24363.8, "ram_available_mb": 38477.1, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.974}, "power_stats": {"power_gpu_soc_mean_watts": 23.879, "power_cpu_cv_mean_watts": 1.545, "power_sys_5v0_mean_watts": 9.24, "gpu_utilization_percent_mean": 76.974, "power_watts_avg": 23.879, "energy_joules_est": 218.7, "duration_seconds": 9.159, "sample_count": 78}, "timestamp": "2026-01-25T16:00:01.219344"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11164.518, "latencies_ms": [11164.518], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a black cat is sitting on a desk, its head tilted to the side as it gazes intently at a computer screen. The screen is displaying a webpage with a white background and black text. The cat's position and the direction of its gaze suggest it is trying to understand or interact with the content on the screen.\n\nTo", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24363.8, "ram_available_mb": 38477.1, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24340.2, "ram_available_mb": 38500.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.632}, "power_stats": {"power_gpu_soc_mean_watts": 20.886, "power_cpu_cv_mean_watts": 1.934, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.632, "power_watts_avg": 20.886, "energy_joules_est": 233.2, "duration_seconds": 11.165, "sample_count": 95}, "timestamp": "2026-01-25T16:00:14.414175"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7068.927, "latencies_ms": [7068.927], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "computer: 2, monitor: 1, keyboard: 1, mouse: 1, cat: 1, calculator: 1, phone: 1, book: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24340.2, "ram_available_mb": 38500.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24434.9, "ram_available_mb": 38406.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.317}, "power_stats": {"power_gpu_soc_mean_watts": 22.72, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 9.009, "gpu_utilization_percent_mean": 74.317, "power_watts_avg": 22.72, "energy_joules_est": 160.62, "duration_seconds": 7.07, "sample_count": 60}, "timestamp": "2026-01-25T16:00:23.504767"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11132.111, "latencies_ms": [11132.111], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a black cat sitting in front of a computer monitor, which is positioned slightly to the left of the center of the image. The cat is facing the monitor, and its body is angled towards it, suggesting it is looking at the screen. In the background, there is a keyboard to the left of the monitor and a telephone to the right, both", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24356.3, "ram_available_mb": 38484.6, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24360.7, "ram_available_mb": 38480.2, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.674}, "power_stats": {"power_gpu_soc_mean_watts": 20.946, "power_cpu_cv_mean_watts": 1.934, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 69.674, "power_watts_avg": 20.946, "energy_joules_est": 233.19, "duration_seconds": 11.133, "sample_count": 95}, "timestamp": "2026-01-25T16:00:36.658797"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6168.814, "latencies_ms": [6168.814], "images_per_second": 0.162, "prompt_tokens": 37, "response_tokens_est": 33, "n_tiles": 16, "output_text": "A cat is sitting in front of a computer monitor, looking at the screen. The computer is placed on a desk with a keyboard and a mouse nearby.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24360.7, "ram_available_mb": 38480.2, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24352.6, "ram_available_mb": 38488.3, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.904}, "power_stats": {"power_gpu_soc_mean_watts": 23.646, "power_cpu_cv_mean_watts": 1.463, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 74.904, "power_watts_avg": 23.646, "energy_joules_est": 145.88, "duration_seconds": 6.169, "sample_count": 52}, "timestamp": "2026-01-25T16:00:44.882404"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7022.28, "latencies_ms": [7022.28], "images_per_second": 0.142, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image shows a cat with a predominantly grey coat, sitting in front of a computer screen. The lighting in the room is bright, illuminating the cat and the computer screen clearly.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 24282.5, "ram_available_mb": 38558.4, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24307.7, "ram_available_mb": 38533.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.695}, "power_stats": {"power_gpu_soc_mean_watts": 22.662, "power_cpu_cv_mean_watts": 1.628, "power_sys_5v0_mean_watts": 9.06, "gpu_utilization_percent_mean": 72.695, "power_watts_avg": 22.662, "energy_joules_est": 159.15, "duration_seconds": 7.023, "sample_count": 59}, "timestamp": "2026-01-25T16:00:53.942006"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11170.819, "latencies_ms": [11170.819], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people gathered around a little girl who is cutting a red ribbon. The girl is wearing a helmet and is the center of attention as she holds the scissors, ready to cut the ribbon. Several people are standing around her, watching the event unfold.\n\nIn the scene, there are a few bicycles visible, one on the left", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24307.7, "ram_available_mb": 38533.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24284.4, "ram_available_mb": 38556.5, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.389}, "power_stats": {"power_gpu_soc_mean_watts": 20.848, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 69.389, "power_watts_avg": 20.848, "energy_joules_est": 232.91, "duration_seconds": 11.172, "sample_count": 95}, "timestamp": "2026-01-25T16:01:07.167336"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10020.098, "latencies_ms": [10020.098], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 67, "n_tiles": 16, "output_text": "1. People: 15\n2. Balloons: 1\n3. Ribbon: 1\n4. Camera: 1\n5. Helmet: 1\n6. Tie: 1\n7. Suitcase: 1\n8. Bicycle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24284.4, "ram_available_mb": 38556.5, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24284.7, "ram_available_mb": 38556.2, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.129}, "power_stats": {"power_gpu_soc_mean_watts": 20.672, "power_cpu_cv_mean_watts": 1.846, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 72.129, "power_watts_avg": 20.672, "energy_joules_est": 207.15, "duration_seconds": 10.021, "sample_count": 85}, "timestamp": "2026-01-25T16:01:19.216988"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11225.49, "latencies_ms": [11225.49], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a child wearing a blue helmet is being assisted by an adult to walk through a red ribbon, indicating a ceremonial event. Behind them, a group of people are gathered, with some standing closer to the camera and others further back, creating a sense of depth. The balloons are held by an adult in the center, slightly behind the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24284.7, "ram_available_mb": 38556.2, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24283.9, "ram_available_mb": 38557.0, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.443}, "power_stats": {"power_gpu_soc_mean_watts": 19.985, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 69.443, "power_watts_avg": 19.985, "energy_joules_est": 224.35, "duration_seconds": 11.226, "sample_count": 97}, "timestamp": "2026-01-25T16:01:32.464951"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9327.862, "latencies_ms": [9327.862], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 61, "n_tiles": 16, "output_text": "A group of people, including children, are gathered on a sidewalk in front of a building with a sign that reads \"Johnny's.\" They are participating in a ribbon-cutting ceremony, with some individuals holding a pair of scissors and a blue balloon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24283.9, "ram_available_mb": 38557.0, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24278.7, "ram_available_mb": 38562.2, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.532}, "power_stats": {"power_gpu_soc_mean_watts": 21.672, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 71.532, "power_watts_avg": 21.672, "energy_joules_est": 202.17, "duration_seconds": 9.328, "sample_count": 79}, "timestamp": "2026-01-25T16:01:43.846793"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7801.983, "latencies_ms": [7801.983], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a group of people gathered outdoors on a sunny day, as indicated by the bright lighting and shadows cast on the ground. The weather appears to be clear and pleasant, suitable for an outdoor event.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24278.7, "ram_available_mb": 38562.2, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24279.3, "ram_available_mb": 38561.6, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.348}, "power_stats": {"power_gpu_soc_mean_watts": 22.123, "power_cpu_cv_mean_watts": 1.705, "power_sys_5v0_mean_watts": 9.033, "gpu_utilization_percent_mean": 71.348, "power_watts_avg": 22.123, "energy_joules_est": 172.62, "duration_seconds": 7.803, "sample_count": 66}, "timestamp": "2026-01-25T16:01:53.679147"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11188.198, "latencies_ms": [11188.198], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large, modern bus parked on the side of a street. The bus is predominantly white and blue, with a pink stripe running along its side. It is a Scania bus, as indicated by the text on the side of the vehicle. The bus is parked in a designated bus stop area, which is marked by yellow lines on the road.\n", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24279.3, "ram_available_mb": 38561.6, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24416.7, "ram_available_mb": 38424.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.126}, "power_stats": {"power_gpu_soc_mean_watts": 20.812, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 69.126, "power_watts_avg": 20.812, "energy_joules_est": 232.86, "duration_seconds": 11.189, "sample_count": 95}, "timestamp": "2026-01-25T16:02:06.933947"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6958.071, "latencies_ms": [6958.071], "images_per_second": 0.144, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "bus: 1, window: multiple, license plate: 1, building: multiple, pedestrian: 1, car: 1, street: multiple, sign: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24326.4, "ram_available_mb": 38514.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24434.2, "ram_available_mb": 38406.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.293}, "power_stats": {"power_gpu_soc_mean_watts": 22.854, "power_cpu_cv_mean_watts": 1.587, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 72.293, "power_watts_avg": 22.854, "energy_joules_est": 159.04, "duration_seconds": 6.959, "sample_count": 58}, "timestamp": "2026-01-25T16:02:15.934621"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10915.328, "latencies_ms": [10915.328], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The bus is in the foreground of the image, positioned on the left side of the frame, and appears to be moving towards the right. It is parked in a designated bus stop area marked with yellow lines on the road. In the background, there are buildings and a pedestrian walking on the sidewalk, indicating that the bus is in an urban setting.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24434.2, "ram_available_mb": 38406.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24434.7, "ram_available_mb": 38406.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.43}, "power_stats": {"power_gpu_soc_mean_watts": 20.974, "power_cpu_cv_mean_watts": 1.903, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.43, "power_watts_avg": 20.974, "energy_joules_est": 228.95, "duration_seconds": 10.916, "sample_count": 93}, "timestamp": "2026-01-25T16:02:28.919316"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6083.355, "latencies_ms": [6083.355], "images_per_second": 0.164, "prompt_tokens": 37, "response_tokens_est": 32, "n_tiles": 16, "output_text": "A Scania bus with the sign \"First Group\" is parked on the side of a street. The bus offers free Wi-Fi on board.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24286.0, "ram_available_mb": 38554.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24414.9, "ram_available_mb": 38426.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.137}, "power_stats": {"power_gpu_soc_mean_watts": 23.673, "power_cpu_cv_mean_watts": 1.405, "power_sys_5v0_mean_watts": 9.015, "gpu_utilization_percent_mean": 76.137, "power_watts_avg": 23.673, "energy_joules_est": 144.03, "duration_seconds": 6.084, "sample_count": 51}, "timestamp": "2026-01-25T16:02:37.064335"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8771.406, "latencies_ms": [8771.406], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The bus is predominantly white with pink and blue accents, and it is equipped with free Wi-Fi as indicated by the sign on the front. The bus is parked on a street with a yellow \"BUS STOP\" marking on the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24414.9, "ram_available_mb": 38426.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24282.8, "ram_available_mb": 38558.1, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.176}, "power_stats": {"power_gpu_soc_mean_watts": 21.712, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 9.042, "gpu_utilization_percent_mean": 71.176, "power_watts_avg": 21.712, "energy_joules_est": 190.46, "duration_seconds": 8.772, "sample_count": 74}, "timestamp": "2026-01-25T16:02:47.885372"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11152.735, "latencies_ms": [11152.735], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the image, a man is sitting on the floor in front of a large, ornate mirror. He is wearing a green shirt and is holding a cell phone in his hands. The mirror reflects the room, showing a living room with a couch and a lamp. The man appears to be focused on his phone, possibly taking a photo or browsing the internet.", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24282.8, "ram_available_mb": 38558.1, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24376.8, "ram_available_mb": 38464.1, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.649}, "power_stats": {"power_gpu_soc_mean_watts": 20.892, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 69.649, "power_watts_avg": 20.892, "energy_joules_est": 233.02, "duration_seconds": 11.154, "sample_count": 94}, "timestamp": "2026-01-25T16:03:01.071209"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8564.72, "latencies_ms": [8564.72], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Mirror: 1\n- Sofa: 1\n- Table: 1\n- Lamp: 1\n- Cell phone: 1\n- Chair: 1\n- Rug: 1\n- Floor: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24303.2, "ram_available_mb": 38537.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24429.3, "ram_available_mb": 38411.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.041}, "power_stats": {"power_gpu_soc_mean_watts": 21.976, "power_cpu_cv_mean_watts": 1.744, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 71.041, "power_watts_avg": 21.976, "energy_joules_est": 188.23, "duration_seconds": 8.565, "sample_count": 73}, "timestamp": "2026-01-25T16:03:11.662509"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10525.07, "latencies_ms": [10525.07], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, a person is seated on the floor in front of a large, ornate mirror. The mirror reflects the interior of a room, showing a person sitting on a couch in the background. The person in the foreground is closer to the camera than the person in the reflection, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24357.4, "ram_available_mb": 38483.5, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24341.0, "ram_available_mb": 38499.9, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.333}, "power_stats": {"power_gpu_soc_mean_watts": 21.148, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 70.333, "power_watts_avg": 21.148, "energy_joules_est": 222.6, "duration_seconds": 10.526, "sample_count": 90}, "timestamp": "2026-01-25T16:03:24.237652"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7540.259, "latencies_ms": [7540.259], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A man is sitting on the floor in front of a large, ornate mirror, holding a smartphone in his hands. The room has a wooden floor, a green wall, and a red wall in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24341.0, "ram_available_mb": 38499.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24295.7, "ram_available_mb": 38545.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.516}, "power_stats": {"power_gpu_soc_mean_watts": 22.37, "power_cpu_cv_mean_watts": 1.652, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 72.516, "power_watts_avg": 22.37, "energy_joules_est": 168.7, "duration_seconds": 7.541, "sample_count": 64}, "timestamp": "2026-01-25T16:03:33.837937"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7463.882, "latencies_ms": [7463.882], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a person sitting on the floor in front of a large, ornate mirror. The room has wooden flooring and walls painted in a light green color, with a red accent wall visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24295.7, "ram_available_mb": 38545.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24371.8, "ram_available_mb": 38469.1, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.524}, "power_stats": {"power_gpu_soc_mean_watts": 22.503, "power_cpu_cv_mean_watts": 1.684, "power_sys_5v0_mean_watts": 9.024, "gpu_utilization_percent_mean": 72.524, "power_watts_avg": 22.503, "energy_joules_est": 167.98, "duration_seconds": 7.465, "sample_count": 63}, "timestamp": "2026-01-25T16:03:43.340641"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11142.271, "latencies_ms": [11142.271], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of five men standing together, each holding a surfboard. They are posing for a picture, with one of the men taking a photo of the group using a cell phone. The surfboards are of various sizes and colors, adding a vibrant touch to the scene.\n\nIn the background, there is a refrigerator, which suggests that", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24314.9, "ram_available_mb": 38526.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24428.5, "ram_available_mb": 38412.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.526}, "power_stats": {"power_gpu_soc_mean_watts": 20.854, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 69.526, "power_watts_avg": 20.854, "energy_joules_est": 232.37, "duration_seconds": 11.143, "sample_count": 95}, "timestamp": "2026-01-25T16:03:56.559274"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10086.806, "latencies_ms": [10086.806], "images_per_second": 0.099, "prompt_tokens": 39, "response_tokens_est": 68, "n_tiles": 16, "output_text": "1. Surfboard: 4\n2. Camera: 1\n3. Cell phone: 1\n4. Flag: 2\n5. Surfboard: 1\n6. Surfboard: 1\n7. Surfboard: 1\n8. Surfboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24288.0, "ram_available_mb": 38552.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24392.3, "ram_available_mb": 38448.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.839}, "power_stats": {"power_gpu_soc_mean_watts": 21.317, "power_cpu_cv_mean_watts": 1.845, "power_sys_5v0_mean_watts": 8.928, "gpu_utilization_percent_mean": 70.839, "power_watts_avg": 21.317, "energy_joules_est": 215.03, "duration_seconds": 10.087, "sample_count": 87}, "timestamp": "2026-01-25T16:04:08.689607"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10144.952, "latencies_ms": [10144.952], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there is a person taking a photo of a group of individuals who are standing close together. The group is positioned in the center of the image, with some of them holding surfboards. In the background, there is a wall with various items hanging on it, including a surfboard and a flag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24320.2, "ram_available_mb": 38520.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24427.6, "ram_available_mb": 38413.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.897}, "power_stats": {"power_gpu_soc_mean_watts": 21.229, "power_cpu_cv_mean_watts": 1.877, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 69.897, "power_watts_avg": 21.229, "energy_joules_est": 215.38, "duration_seconds": 10.146, "sample_count": 87}, "timestamp": "2026-01-25T16:04:20.874912"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7411.06, "latencies_ms": [7411.06], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A group of young men are gathered in a room, holding surfboards and taking a photo with a camera. The room appears to be a casual setting, possibly a hangout or a surf shop.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24335.7, "ram_available_mb": 38505.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24430.4, "ram_available_mb": 38410.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.844}, "power_stats": {"power_gpu_soc_mean_watts": 22.458, "power_cpu_cv_mean_watts": 1.651, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 72.844, "power_watts_avg": 22.458, "energy_joules_est": 166.45, "duration_seconds": 7.412, "sample_count": 64}, "timestamp": "2026-01-25T16:04:30.328219"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8550.329, "latencies_ms": [8550.329], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a group of individuals indoors, with one person taking a photo of the others who are holding surfboards. The lighting appears to be artificial, and the surfboards are of various colors and designs, including yellow, red, and blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24430.4, "ram_available_mb": 38410.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24431.4, "ram_available_mb": 38409.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.932}, "power_stats": {"power_gpu_soc_mean_watts": 21.756, "power_cpu_cv_mean_watts": 1.782, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 70.932, "power_watts_avg": 21.756, "energy_joules_est": 186.04, "duration_seconds": 8.551, "sample_count": 73}, "timestamp": "2026-01-25T16:04:40.931021"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12187.905, "latencies_ms": [12187.905], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large yellow passenger airplane parked on the tarmac at an airport. The airplane is a LOT (Polish Airlines) plane, as indicated by the logo on the side of the aircraft. The plane is positioned in the center of the image, occupying a significant portion of the frame.\n\nIn the background, there are several other airplan", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24359.6, "ram_available_mb": 38481.3, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24358.2, "ram_available_mb": 38482.7, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.404}, "power_stats": {"power_gpu_soc_mean_watts": 21.864, "power_cpu_cv_mean_watts": 1.798, "power_sys_5v0_mean_watts": 9.123, "gpu_utilization_percent_mean": 73.404, "power_watts_avg": 21.864, "energy_joules_est": 266.49, "duration_seconds": 12.189, "sample_count": 104}, "timestamp": "2026-01-25T16:04:55.148517"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8314.663, "latencies_ms": [8314.663], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "airplane: 1, cloud: numerous, runway: 1, engine: 1, wing: 1, tail: 1, door: numerous, runway marking: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24288.5, "ram_available_mb": 38552.4, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24287.3, "ram_available_mb": 38553.6, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.465}, "power_stats": {"power_gpu_soc_mean_watts": 24.022, "power_cpu_cv_mean_watts": 1.455, "power_sys_5v0_mean_watts": 9.121, "gpu_utilization_percent_mean": 77.465, "power_watts_avg": 24.022, "energy_joules_est": 199.75, "duration_seconds": 8.315, "sample_count": 71}, "timestamp": "2026-01-25T16:05:05.498075"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11799.979, "latencies_ms": [11799.979], "images_per_second": 0.085, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The airplane is positioned in the foreground of the image, appearing large and prominent. It is situated on the tarmac, with a clear view of its side profile. In the background, there are other airplanes and a landscape that appears to be a hilly area with some buildings, indicating that the airport is located in a relatively open space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24287.3, "ram_available_mb": 38553.6, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24315.5, "ram_available_mb": 38525.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.743}, "power_stats": {"power_gpu_soc_mean_watts": 22.01, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 9.145, "gpu_utilization_percent_mean": 73.743, "power_watts_avg": 22.01, "energy_joules_est": 259.74, "duration_seconds": 11.801, "sample_count": 101}, "timestamp": "2026-01-25T16:05:19.334220"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7429.008, "latencies_ms": [7429.008], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A LOT Polish Airlines airplane is on the tarmac, likely preparing for departure. The sky is partly cloudy, suggesting fair weather conditions for flying.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24315.5, "ram_available_mb": 38525.4, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24303.4, "ram_available_mb": 38537.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.349}, "power_stats": {"power_gpu_soc_mean_watts": 24.579, "power_cpu_cv_mean_watts": 1.328, "power_sys_5v0_mean_watts": 9.112, "gpu_utilization_percent_mean": 79.349, "power_watts_avg": 24.579, "energy_joules_est": 182.61, "duration_seconds": 7.43, "sample_count": 63}, "timestamp": "2026-01-25T16:05:28.824473"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9169.658, "latencies_ms": [9169.658], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image features a large, yellow passenger airplane with the logo \"LOT\" on its side, indicating it is operated by Lot Polish Airlines. The aircraft is on a runway with a partly cloudy sky above, suggesting fair weather conditions for flying.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24303.4, "ram_available_mb": 38537.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24271.5, "ram_available_mb": 38569.4, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.089}, "power_stats": {"power_gpu_soc_mean_watts": 23.079, "power_cpu_cv_mean_watts": 1.622, "power_sys_5v0_mean_watts": 9.192, "gpu_utilization_percent_mean": 75.089, "power_watts_avg": 23.079, "energy_joules_est": 211.64, "duration_seconds": 9.17, "sample_count": 79}, "timestamp": "2026-01-25T16:05:40.008054"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11133.516, "latencies_ms": [11133.516], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a bathroom, bathed in the stark contrast of black and white. Dominating the scene is a toilet, its lid raised in an invitation to use. The seat of the toilet is adorned with a white towel, a common sight in many bathrooms, providing a soft barrier between the user and the porcelain", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24271.5, "ram_available_mb": 38569.4, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24271.3, "ram_available_mb": 38569.6, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.053}, "power_stats": {"power_gpu_soc_mean_watts": 20.9, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 69.053, "power_watts_avg": 20.9, "energy_joules_est": 232.7, "duration_seconds": 11.134, "sample_count": 95}, "timestamp": "2026-01-25T16:05:53.161531"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7541.362, "latencies_ms": [7541.362], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "toilet: 1, flush handle: 1, towel: 1, wall: 1, floor: 1, person: 1, shoes: 1, door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24271.3, "ram_available_mb": 38569.6, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24272.0, "ram_available_mb": 38568.9, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.922}, "power_stats": {"power_gpu_soc_mean_watts": 22.589, "power_cpu_cv_mean_watts": 1.639, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 72.922, "power_watts_avg": 22.589, "energy_joules_est": 170.37, "duration_seconds": 7.542, "sample_count": 64}, "timestamp": "2026-01-25T16:06:02.718597"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10285.6, "latencies_ms": [10285.6], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The toilet is located in the foreground of the image, with its lid open and seat up. In the background, there is a wall-mounted toilet paper holder with a roll of paper attached to it. The person's legs are visible in the foreground, suggesting they are standing next to the toilet.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24272.0, "ram_available_mb": 38568.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24274.9, "ram_available_mb": 38566.0, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.114}, "power_stats": {"power_gpu_soc_mean_watts": 21.21, "power_cpu_cv_mean_watts": 1.879, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 70.114, "power_watts_avg": 21.21, "energy_joules_est": 218.17, "duration_seconds": 10.286, "sample_count": 88}, "timestamp": "2026-01-25T16:06:15.043299"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7881.172, "latencies_ms": [7881.172], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image shows a close-up of a toilet with the seat cover up, revealing the toilet bowl. A person's legs are visible, suggesting that they are standing next to the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24274.9, "ram_available_mb": 38566.0, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24273.0, "ram_available_mb": 38567.9, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.194}, "power_stats": {"power_gpu_soc_mean_watts": 22.39, "power_cpu_cv_mean_watts": 1.667, "power_sys_5v0_mean_watts": 9.015, "gpu_utilization_percent_mean": 73.194, "power_watts_avg": 22.39, "energy_joules_est": 176.48, "duration_seconds": 7.882, "sample_count": 67}, "timestamp": "2026-01-25T16:06:24.974629"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7165.497, "latencies_ms": [7165.497], "images_per_second": 0.14, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image is in black and white, featuring a toilet with a closed lid and a partially open lid. The lighting appears to be artificial, coming from above, casting shadows on the floor.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24273.0, "ram_available_mb": 38567.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24272.4, "ram_available_mb": 38568.5, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.246}, "power_stats": {"power_gpu_soc_mean_watts": 22.629, "power_cpu_cv_mean_watts": 1.634, "power_sys_5v0_mean_watts": 9.031, "gpu_utilization_percent_mean": 73.246, "power_watts_avg": 22.629, "energy_joules_est": 162.16, "duration_seconds": 7.166, "sample_count": 61}, "timestamp": "2026-01-25T16:06:34.164861"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11169.563, "latencies_ms": [11169.563], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a skier is captured in the midst of a thrilling descent down a snowy mountain slope. The skier, clad in a blue jacket and black pants, is holding two ski poles, aiding in their navigation through the snow. The skier's helmet, a crucial piece of safety gear, is white and matches the color of", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24272.9, "ram_available_mb": 38568.0, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24302.7, "ram_available_mb": 38538.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.316}, "power_stats": {"power_gpu_soc_mean_watts": 20.832, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.316, "power_watts_avg": 20.832, "energy_joules_est": 232.7, "duration_seconds": 11.17, "sample_count": 95}, "timestamp": "2026-01-25T16:06:47.404809"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8232.635, "latencies_ms": [8232.635], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "tree: multiple, ski: 2, skier: 1, snow: multiple, ski tracks: multiple, snowflakes: multiple, helmet: 1, goggles: 1, gloves: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24302.7, "ram_available_mb": 38538.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24292.3, "ram_available_mb": 38548.6, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.571}, "power_stats": {"power_gpu_soc_mean_watts": 22.173, "power_cpu_cv_mean_watts": 1.681, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 72.571, "power_watts_avg": 22.173, "energy_joules_est": 182.56, "duration_seconds": 8.233, "sample_count": 70}, "timestamp": "2026-01-25T16:06:57.649663"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9720.933, "latencies_ms": [9720.933], "images_per_second": 0.103, "prompt_tokens": 44, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The skier is positioned in the foreground on the left side of the image, moving towards the right. The snow-covered trees form the background, appearing denser and more uniform in the distance. The skier is closer to the viewer than the trees, creating a sense of depth in the scene.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24292.3, "ram_available_mb": 38548.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24399.5, "ram_available_mb": 38441.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.253}, "power_stats": {"power_gpu_soc_mean_watts": 21.446, "power_cpu_cv_mean_watts": 1.847, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 70.253, "power_watts_avg": 21.446, "energy_joules_est": 208.49, "duration_seconds": 9.722, "sample_count": 83}, "timestamp": "2026-01-25T16:07:09.406491"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8000.718, "latencies_ms": [8000.718], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A person is skiing down a snowy slope with trees covered in snow in the background. The skier is wearing a blue jacket, black pants, and a white helmet, and is holding two ski poles.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24304.4, "ram_available_mb": 38536.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24418.1, "ram_available_mb": 38422.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.087}, "power_stats": {"power_gpu_soc_mean_watts": 22.244, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 73.087, "power_watts_avg": 22.244, "energy_joules_est": 177.98, "duration_seconds": 8.001, "sample_count": 69}, "timestamp": "2026-01-25T16:07:19.473548"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6942.037, "latencies_ms": [6942.037], "images_per_second": 0.144, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The skier is wearing a blue jacket and black pants, and the snow is a bright white color. The trees in the background are covered in snow and the sky is overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24281.1, "ram_available_mb": 38559.8, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24406.0, "ram_available_mb": 38434.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.949}, "power_stats": {"power_gpu_soc_mean_watts": 22.765, "power_cpu_cv_mean_watts": 1.622, "power_sys_5v0_mean_watts": 9.079, "gpu_utilization_percent_mean": 72.949, "power_watts_avg": 22.765, "energy_joules_est": 158.05, "duration_seconds": 6.943, "sample_count": 59}, "timestamp": "2026-01-25T16:07:28.450349"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11150.601, "latencies_ms": [11150.601], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment from a tennis match. A player in a yellow shirt and black shorts is in the midst of a backhand swing, holding a tennis racket. The court is a vibrant blue, and the player is positioned near the baseline. In the background, there are spectators seated on the stands, and the court is adorned with", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 24279.3, "ram_available_mb": 38561.6, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24301.0, "ram_available_mb": 38539.9, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.165}, "power_stats": {"power_gpu_soc_mean_watts": 20.773, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.922, "gpu_utilization_percent_mean": 69.165, "power_watts_avg": 20.773, "energy_joules_est": 231.64, "duration_seconds": 11.151, "sample_count": 97}, "timestamp": "2026-01-25T16:07:41.626278"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7289.116, "latencies_ms": [7289.116], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "player: 1, racket: 1, ball: 0, chair: 2, audience: 1, advertisement: 4, court: 1, logo: 3", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24301.0, "ram_available_mb": 38539.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24307.8, "ram_available_mb": 38533.1, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.952}, "power_stats": {"power_gpu_soc_mean_watts": 22.664, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 72.952, "power_watts_avg": 22.664, "energy_joules_est": 165.22, "duration_seconds": 7.29, "sample_count": 62}, "timestamp": "2026-01-25T16:07:50.938319"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11140.64, "latencies_ms": [11140.64], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a tennis player is positioned near the baseline on the left side of the court, preparing to hit a ball. In the background, there are spectators seated on the right side of the court, watching the game. The court itself is surrounded by a blue wall with advertisements, and the floor markings are visible, indicating the boundaries of the playing", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24307.8, "ram_available_mb": 38533.1, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24306.5, "ram_available_mb": 38534.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.558}, "power_stats": {"power_gpu_soc_mean_watts": 20.92, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 69.558, "power_watts_avg": 20.92, "energy_joules_est": 233.07, "duration_seconds": 11.141, "sample_count": 95}, "timestamp": "2026-01-25T16:08:04.105930"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9429.096, "latencies_ms": [9429.096], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image captures a moment from a tennis match, with a player in a yellow shirt and black shorts in the midst of a backhand swing. The court is surrounded by a crowd of spectators, and there are advertisements for Kia and Garnier visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24306.5, "ram_available_mb": 38534.4, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24304.8, "ram_available_mb": 38536.1, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.575}, "power_stats": {"power_gpu_soc_mean_watts": 21.655, "power_cpu_cv_mean_watts": 1.782, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 71.575, "power_watts_avg": 21.655, "energy_joules_est": 204.21, "duration_seconds": 9.43, "sample_count": 80}, "timestamp": "2026-01-25T16:08:15.576064"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7764.799, "latencies_ms": [7764.799], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image shows an indoor tennis court with a blue surface and white boundary lines. The lighting appears to be artificial, as it is evenly distributed across the court, and there are no visible shadows that would indicate natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24304.8, "ram_available_mb": 38536.1, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24301.7, "ram_available_mb": 38539.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.015}, "power_stats": {"power_gpu_soc_mean_watts": 22.067, "power_cpu_cv_mean_watts": 1.715, "power_sys_5v0_mean_watts": 9.034, "gpu_utilization_percent_mean": 71.015, "power_watts_avg": 22.067, "energy_joules_est": 171.36, "duration_seconds": 7.765, "sample_count": 67}, "timestamp": "2026-01-25T16:08:25.372043"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12161.431, "latencies_ms": [12161.431], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of culinary delight, featuring a black plate with a scalloped edge, resting on a white tablecloth. The plate holds a small metal bowl filled with a vibrant red sauce, its rich color suggesting a tomato base. Adjacent to the sauce, a smaller metal bowl cradles a collection of peach slices", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24301.7, "ram_available_mb": 38539.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24375.8, "ram_available_mb": 38465.1, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.365}, "power_stats": {"power_gpu_soc_mean_watts": 21.988, "power_cpu_cv_mean_watts": 1.798, "power_sys_5v0_mean_watts": 9.127, "gpu_utilization_percent_mean": 73.365, "power_watts_avg": 21.988, "energy_joules_est": 267.42, "duration_seconds": 12.162, "sample_count": 104}, "timestamp": "2026-01-25T16:08:39.573325"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8882.092, "latencies_ms": [8882.092], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "bowl of food: 1, bowl of fruit: 1, fork: 1, knife: 1, glass: 1, napkin: 1, plate: 1, table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24375.8, "ram_available_mb": 38465.1, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24411.0, "ram_available_mb": 38429.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.68}, "power_stats": {"power_gpu_soc_mean_watts": 23.836, "power_cpu_cv_mean_watts": 1.505, "power_sys_5v0_mean_watts": 9.103, "gpu_utilization_percent_mean": 76.68, "power_watts_avg": 23.836, "energy_joules_est": 211.73, "duration_seconds": 8.883, "sample_count": 75}, "timestamp": "2026-01-25T16:08:50.491825"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10871.006, "latencies_ms": [10871.006], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, there is a bowl of sliced peaches placed near the edge of a black plate. Behind it, slightly to the left, is a smaller bowl containing what appears to be a meat-based dish. The larger bowl is closer to the viewer than the smaller bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24411.0, "ram_available_mb": 38429.9, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24381.6, "ram_available_mb": 38459.3, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.108}, "power_stats": {"power_gpu_soc_mean_watts": 22.865, "power_cpu_cv_mean_watts": 1.73, "power_sys_5v0_mean_watts": 9.158, "gpu_utilization_percent_mean": 73.108, "power_watts_avg": 22.865, "energy_joules_est": 248.58, "duration_seconds": 10.872, "sample_count": 93}, "timestamp": "2026-01-25T16:09:03.391855"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11049.952, "latencies_ms": [11049.952], "images_per_second": 0.09, "prompt_tokens": 37, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image shows a meal setting with a bowl of sliced peaches and a bowl of what appears to be a meat-based dish, possibly stew, placed on a table. The table has a black surface, and there is a glass of water and a folded napkin on the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24381.6, "ram_available_mb": 38459.3, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24449.2, "ram_available_mb": 38391.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.34}, "power_stats": {"power_gpu_soc_mean_watts": 22.699, "power_cpu_cv_mean_watts": 1.678, "power_sys_5v0_mean_watts": 9.084, "gpu_utilization_percent_mean": 74.34, "power_watts_avg": 22.699, "energy_joules_est": 250.84, "duration_seconds": 11.051, "sample_count": 94}, "timestamp": "2026-01-25T16:09:16.470809"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10927.386, "latencies_ms": [10927.386], "images_per_second": 0.092, "prompt_tokens": 36, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image shows a meal with a dark, richly colored meat dish in a metal bowl and a lighter, pale yellow dessert in a smaller metal bowl. The lighting is warm and artificial, coming from a source not visible in the image, casting a soft glow on the food and the table surface.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24387.2, "ram_available_mb": 38453.7, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24363.6, "ram_available_mb": 38477.3, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.839}, "power_stats": {"power_gpu_soc_mean_watts": 22.844, "power_cpu_cv_mean_watts": 1.752, "power_sys_5v0_mean_watts": 9.183, "gpu_utilization_percent_mean": 73.839, "power_watts_avg": 22.844, "energy_joules_est": 249.64, "duration_seconds": 10.928, "sample_count": 93}, "timestamp": "2026-01-25T16:09:29.450835"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11143.928, "latencies_ms": [11143.928], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of sheep standing together in a grassy area near a brick building. There are at least five sheep visible in the scene, with some of them standing close to each other, while others are a bit more spread out. The sheep appear to be of various sizes, and they are all looking in different directions, possibly observing their surroundings or interacting with one another", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24363.6, "ram_available_mb": 38477.3, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24403.5, "ram_available_mb": 38437.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.271}, "power_stats": {"power_gpu_soc_mean_watts": 20.871, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 69.271, "power_watts_avg": 20.871, "energy_joules_est": 232.6, "duration_seconds": 11.145, "sample_count": 96}, "timestamp": "2026-01-25T16:09:42.628108"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9769.691, "latencies_ms": [9769.691], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. Sheep: 5\n2. Grass: 1\n3. Brick wall: 1\n4. Tree: 1\n5. Fence: 1\n6. Barn: 1\n7. Shed: 1\n8. Ground: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24403.5, "ram_available_mb": 38437.4, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24334.4, "ram_available_mb": 38506.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.735}, "power_stats": {"power_gpu_soc_mean_watts": 21.534, "power_cpu_cv_mean_watts": 1.833, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 70.735, "power_watts_avg": 21.534, "energy_joules_est": 210.39, "duration_seconds": 9.77, "sample_count": 83}, "timestamp": "2026-01-25T16:09:54.449238"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10348.289, "latencies_ms": [10348.289], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground of the image, there are three sheep standing close together, with one on the left, one in the middle, and one on the right. They are positioned near the grassy area in front of a brick building. In the background, there is another sheep standing further away, and a fence can be seen behind it.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24334.4, "ram_available_mb": 38506.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24328.6, "ram_available_mb": 38512.3, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.841}, "power_stats": {"power_gpu_soc_mean_watts": 21.211, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 69.841, "power_watts_avg": 21.211, "energy_joules_est": 219.51, "duration_seconds": 10.349, "sample_count": 88}, "timestamp": "2026-01-25T16:10:06.813220"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6059.604, "latencies_ms": [6059.604], "images_per_second": 0.165, "prompt_tokens": 37, "response_tokens_est": 32, "n_tiles": 16, "output_text": "A group of sheep are standing in a grassy area near a brick building. The sheep appear to be grazing or resting in the shade.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24328.6, "ram_available_mb": 38512.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24323.0, "ram_available_mb": 38517.9, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.667}, "power_stats": {"power_gpu_soc_mean_watts": 23.565, "power_cpu_cv_mean_watts": 1.476, "power_sys_5v0_mean_watts": 9.019, "gpu_utilization_percent_mean": 75.667, "power_watts_avg": 23.565, "energy_joules_est": 142.81, "duration_seconds": 6.06, "sample_count": 51}, "timestamp": "2026-01-25T16:10:14.888890"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7222.152, "latencies_ms": [7222.152], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image features a group of sheep with thick, curly wool in various shades of brown and beige. The lighting is bright and natural, suggesting the photo was taken on a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24323.0, "ram_available_mb": 38517.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24333.4, "ram_available_mb": 38507.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.049}, "power_stats": {"power_gpu_soc_mean_watts": 22.531, "power_cpu_cv_mean_watts": 1.667, "power_sys_5v0_mean_watts": 9.033, "gpu_utilization_percent_mean": 72.049, "power_watts_avg": 22.531, "energy_joules_est": 162.74, "duration_seconds": 7.223, "sample_count": 61}, "timestamp": "2026-01-25T16:10:24.131727"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10523.204, "latencies_ms": [10523.204], "images_per_second": 0.095, "prompt_tokens": 24, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image features a red apple sitting in the center, surrounded by a bunch of green bananas. The bananas are arranged in a circular pattern around the apple, creating a visually appealing composition. The background consists of a blue and white pattern, which adds a contrasting color to the overall scene.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24333.4, "ram_available_mb": 38507.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24322.6, "ram_available_mb": 38518.3, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.539}, "power_stats": {"power_gpu_soc_mean_watts": 23.339, "power_cpu_cv_mean_watts": 1.669, "power_sys_5v0_mean_watts": 9.207, "gpu_utilization_percent_mean": 75.539, "power_watts_avg": 23.339, "energy_joules_est": 245.61, "duration_seconds": 10.524, "sample_count": 89}, "timestamp": "2026-01-25T16:10:36.704351"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5029.915, "latencies_ms": [5029.915], "images_per_second": 0.199, "prompt_tokens": 39, "response_tokens_est": 11, "n_tiles": 16, "output_text": "banana: 5, apple: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24322.6, "ram_available_mb": 38518.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24408.0, "ram_available_mb": 38432.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 87.5}, "power_stats": {"power_gpu_soc_mean_watts": 27.555, "power_cpu_cv_mean_watts": 0.696, "power_sys_5v0_mean_watts": 9.208, "gpu_utilization_percent_mean": 87.5, "power_watts_avg": 27.555, "energy_joules_est": 138.62, "duration_seconds": 5.031, "sample_count": 42}, "timestamp": "2026-01-25T16:10:43.781338"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12452.996, "latencies_ms": [12452.996], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a red apple centrally positioned with a cluster of green bananas surrounding it, one on the left and two on the right, all appearing to be in the near vicinity of the apple. The background features a textured pattern that resembles a close-up of a leaf or a frosted glass surface, which is further away", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24408.0, "ram_available_mb": 38432.9, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24321.2, "ram_available_mb": 38519.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.34}, "power_stats": {"power_gpu_soc_mean_watts": 22.901, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 9.192, "gpu_utilization_percent_mean": 74.34, "power_watts_avg": 22.901, "energy_joules_est": 285.2, "duration_seconds": 12.454, "sample_count": 106}, "timestamp": "2026-01-25T16:10:58.285099"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10755.08, "latencies_ms": [10755.08], "images_per_second": 0.093, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image shows a group of bananas arranged in a circle around a red apple, all set against a blue textured background. The arrangement creates a visually appealing pattern, with the bananas' yellow color contrasting with the apple's red hue and the background's cool tone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24321.2, "ram_available_mb": 38519.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24408.3, "ram_available_mb": 38432.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.108}, "power_stats": {"power_gpu_soc_mean_watts": 23.487, "power_cpu_cv_mean_watts": 1.636, "power_sys_5v0_mean_watts": 9.138, "gpu_utilization_percent_mean": 76.108, "power_watts_avg": 23.487, "energy_joules_est": 252.62, "duration_seconds": 10.756, "sample_count": 93}, "timestamp": "2026-01-25T16:11:11.091673"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11438.706, "latencies_ms": [11438.706], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image features a vibrant red apple centrally placed among four green bananas, all set against a textured background with a cool blue tone. The lighting in the image highlights the glossy surface of the apple and the smooth skin of the bananas, creating a contrast with the intricate patterns of the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24329.5, "ram_available_mb": 38511.4, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24410.4, "ram_available_mb": 38430.5, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.265}, "power_stats": {"power_gpu_soc_mean_watts": 22.847, "power_cpu_cv_mean_watts": 1.736, "power_sys_5v0_mean_watts": 9.2, "gpu_utilization_percent_mean": 74.265, "power_watts_avg": 22.847, "energy_joules_est": 261.35, "duration_seconds": 11.439, "sample_count": 98}, "timestamp": "2026-01-25T16:11:24.585170"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12153.801, "latencies_ms": [12153.801], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a blue and white train traveling down the tracks, with several people visible inside the train. The train is moving through a city, and there are multiple cars parked along the side of the road. \n\nIn addition to the train, there are a few other vehicles in the scene, including a truck and a car. A person can be seen standing near the train", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24410.4, "ram_available_mb": 38430.5, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24410.9, "ram_available_mb": 38430.0, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.825}, "power_stats": {"power_gpu_soc_mean_watts": 22.026, "power_cpu_cv_mean_watts": 1.804, "power_sys_5v0_mean_watts": 9.125, "gpu_utilization_percent_mean": 72.825, "power_watts_avg": 22.026, "energy_joules_est": 267.72, "duration_seconds": 12.155, "sample_count": 103}, "timestamp": "2026-01-25T16:11:38.802414"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8539.995, "latencies_ms": [8539.995], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "train: 1\ntruck: 1\nwindow: 12\nseat: 10\npassenger: 9\ndoor: 4\ntrack: 2\nplatform: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24410.9, "ram_available_mb": 38430.0, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24363.9, "ram_available_mb": 38477.0, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.247}, "power_stats": {"power_gpu_soc_mean_watts": 23.93, "power_cpu_cv_mean_watts": 1.514, "power_sys_5v0_mean_watts": 9.149, "gpu_utilization_percent_mean": 77.247, "power_watts_avg": 23.93, "energy_joules_est": 204.38, "duration_seconds": 8.541, "sample_count": 73}, "timestamp": "2026-01-25T16:11:49.360361"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9040.092, "latencies_ms": [9040.092], "images_per_second": 0.111, "prompt_tokens": 44, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The train is in the foreground of the image, positioned on the left side, and appears to be moving towards the right. There is a clear sky in the background, and a building can be seen in the far right corner of the image.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24363.9, "ram_available_mb": 38477.0, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.13}, "power_stats": {"power_gpu_soc_mean_watts": 23.607, "power_cpu_cv_mean_watts": 1.612, "power_sys_5v0_mean_watts": 9.215, "gpu_utilization_percent_mean": 75.13, "power_watts_avg": 23.607, "energy_joules_est": 213.42, "duration_seconds": 9.041, "sample_count": 77}, "timestamp": "2026-01-25T16:12:00.456562"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7445.521, "latencies_ms": [7445.521], "images_per_second": 0.134, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A blue and white train with the number 2 on the front is on a track with trees in the background. There are people visible through the windows of the train.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24334.3, "ram_available_mb": 38506.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24328.9, "ram_available_mb": 38512.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.032}, "power_stats": {"power_gpu_soc_mean_watts": 24.6, "power_cpu_cv_mean_watts": 1.347, "power_sys_5v0_mean_watts": 9.127, "gpu_utilization_percent_mean": 79.032, "power_watts_avg": 24.6, "energy_joules_est": 183.18, "duration_seconds": 7.446, "sample_count": 63}, "timestamp": "2026-01-25T16:12:09.920096"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6717.239, "latencies_ms": [6717.239], "images_per_second": 0.149, "prompt_tokens": 36, "response_tokens_est": 31, "n_tiles": 16, "output_text": "The train is predominantly blue and white with a red interior. It is a clear day with shadows cast on the ground indicating sunlight.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24328.9, "ram_available_mb": 38512.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24325.9, "ram_available_mb": 38515.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.696}, "power_stats": {"power_gpu_soc_mean_watts": 24.787, "power_cpu_cv_mean_watts": 1.301, "power_sys_5v0_mean_watts": 9.26, "gpu_utilization_percent_mean": 79.696, "power_watts_avg": 24.787, "energy_joules_est": 166.52, "duration_seconds": 6.718, "sample_count": 56}, "timestamp": "2026-01-25T16:12:18.677213"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11156.695, "latencies_ms": [11156.695], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and well-organized bathroom. Dominating the scene is a white bathtub, its interior glowing with a beige hue. A red shower curtain, adorned with a floral pattern, hangs gracefully from the bathtub, adding a pop of color to the otherwise neutral palette. \n\nAd", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24325.9, "ram_available_mb": 38515.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24326.6, "ram_available_mb": 38514.3, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.295}, "power_stats": {"power_gpu_soc_mean_watts": 20.853, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 69.295, "power_watts_avg": 20.853, "energy_joules_est": 232.66, "duration_seconds": 11.157, "sample_count": 95}, "timestamp": "2026-01-25T16:12:31.863822"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7647.248, "latencies_ms": [7647.248], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "mirror: 1\nsink: 1\ntub: 1\ncurtain: 1\ntowel: 1\nmat: 1\ndoor: 1\nmirror frame: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24326.6, "ram_available_mb": 38514.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24404.3, "ram_available_mb": 38436.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.547}, "power_stats": {"power_gpu_soc_mean_watts": 22.482, "power_cpu_cv_mean_watts": 1.62, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 72.547, "power_watts_avg": 22.482, "energy_joules_est": 171.94, "duration_seconds": 7.648, "sample_count": 64}, "timestamp": "2026-01-25T16:12:41.541681"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8222.017, "latencies_ms": [8222.017], "images_per_second": 0.122, "prompt_tokens": 44, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The shower area is located on the left side of the image, adjacent to the bathroom sink which is on the right. The red bath mat is placed in the foreground on the floor, leading the eye towards the shower area in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24332.6, "ram_available_mb": 38508.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24382.6, "ram_available_mb": 38458.3, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.042}, "power_stats": {"power_gpu_soc_mean_watts": 21.906, "power_cpu_cv_mean_watts": 1.748, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 71.042, "power_watts_avg": 21.906, "energy_joules_est": 180.13, "duration_seconds": 8.223, "sample_count": 71}, "timestamp": "2026-01-25T16:12:51.783718"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8393.444, "latencies_ms": [8393.444], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image depicts a well-lit, clean bathroom with a large mirror above a double sink vanity. A red shower curtain is partially drawn, and a red bath mat is placed on the floor in front of the bathtub.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24329.0, "ram_available_mb": 38511.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24398.1, "ram_available_mb": 38442.8, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.845}, "power_stats": {"power_gpu_soc_mean_watts": 22.099, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 71.845, "power_watts_avg": 22.099, "energy_joules_est": 185.5, "duration_seconds": 8.394, "sample_count": 71}, "timestamp": "2026-01-25T16:13:02.235594"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9116.572, "latencies_ms": [9116.572], "images_per_second": 0.11, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The bathroom features a beige color scheme with a red patterned shower curtain. The lighting is provided by two wall-mounted sconces with a vintage design, casting a warm glow on the cream-colored walls and the dark wood vanity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24326.4, "ram_available_mb": 38514.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24383.8, "ram_available_mb": 38457.1, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.321}, "power_stats": {"power_gpu_soc_mean_watts": 21.512, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 70.321, "power_watts_avg": 21.512, "energy_joules_est": 196.13, "duration_seconds": 9.117, "sample_count": 78}, "timestamp": "2026-01-25T16:13:13.412458"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11116.102, "latencies_ms": [11116.102], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a dynamic scene of a surfer in action. The surfer, clad in a black wetsuit, is skillfully riding a wave. The wave, a powerful force of nature, is white and frothy, indicating its strength and the speed at which it's moving. The surfer is crouched on the surfboard, arms outstretch", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24383.8, "ram_available_mb": 38457.1, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24417.3, "ram_available_mb": 38423.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.698}, "power_stats": {"power_gpu_soc_mean_watts": 20.94, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 69.698, "power_watts_avg": 20.94, "energy_joules_est": 232.79, "duration_seconds": 11.117, "sample_count": 96}, "timestamp": "2026-01-25T16:13:26.562404"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7505.422, "latencies_ms": [7505.422], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "water: numerous\nsurfboard: 1\nsurfer: 1\nwave: 1\nspray: numerous\nsea: entire background\nblack and white: entire image\ntext: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24363.7, "ram_available_mb": 38477.2, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24412.3, "ram_available_mb": 38428.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.859}, "power_stats": {"power_gpu_soc_mean_watts": 22.413, "power_cpu_cv_mean_watts": 1.651, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 72.859, "power_watts_avg": 22.413, "energy_joules_est": 168.24, "duration_seconds": 7.506, "sample_count": 64}, "timestamp": "2026-01-25T16:13:36.118550"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11143.978, "latencies_ms": [11143.978], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is breaking to the right of the frame. The wave originates from the background and extends towards the left side of the image, creating a dynamic contrast between the surfer's movement and the stationary water. The text 'STB' is placed in the upper right corner, far from the action, suggesting it", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24412.3, "ram_available_mb": 38428.6, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24413.6, "ram_available_mb": 38427.3, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.305}, "power_stats": {"power_gpu_soc_mean_watts": 20.949, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 69.305, "power_watts_avg": 20.949, "energy_joules_est": 233.47, "duration_seconds": 11.145, "sample_count": 95}, "timestamp": "2026-01-25T16:13:49.307998"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7965.875, "latencies_ms": [7965.875], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A surfer is captured in mid-air, performing a jump above a wave in the ocean. The image is in black and white, with the surfer's silhouette standing out against the white foam of the wave.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24413.6, "ram_available_mb": 38427.3, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24325.7, "ram_available_mb": 38515.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.362}, "power_stats": {"power_gpu_soc_mean_watts": 22.257, "power_cpu_cv_mean_watts": 1.7, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 73.362, "power_watts_avg": 22.257, "energy_joules_est": 177.31, "duration_seconds": 7.967, "sample_count": 69}, "timestamp": "2026-01-25T16:13:59.324102"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6654.273, "latencies_ms": [6654.273], "images_per_second": 0.15, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image is in black and white, featuring a surfer riding a wave. The wave is large and powerful, with water splashing around the surfer as they navigate it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24325.7, "ram_available_mb": 38515.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24329.5, "ram_available_mb": 38511.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.982}, "power_stats": {"power_gpu_soc_mean_watts": 23.018, "power_cpu_cv_mean_watts": 1.587, "power_sys_5v0_mean_watts": 9.055, "gpu_utilization_percent_mean": 72.982, "power_watts_avg": 23.018, "energy_joules_est": 153.18, "duration_seconds": 6.655, "sample_count": 56}, "timestamp": "2026-01-25T16:14:08.023334"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11177.01, "latencies_ms": [11177.01], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a baseball game in progress, with a batter standing at home plate, holding a baseball bat and waiting for the pitch. The catcher is crouched behind the batter, wearing a baseball glove, and the umpire is positioned behind the catcher, observing the game. \n\nThere are several other people in the scene, including players and spect", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24329.5, "ram_available_mb": 38511.4, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24395.6, "ram_available_mb": 38445.3, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.568}, "power_stats": {"power_gpu_soc_mean_watts": 20.755, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 69.568, "power_watts_avg": 20.755, "energy_joules_est": 231.99, "duration_seconds": 11.178, "sample_count": 95}, "timestamp": "2026-01-25T16:14:21.225499"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7763.978, "latencies_ms": [7763.978], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "player: 1, catcher: 1, umpire: 1, baseball glove: 1, bat: 1, jersey: 1, number: 10, audience: multiple", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24342.1, "ram_available_mb": 38498.8, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24435.9, "ram_available_mb": 38405.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.773}, "power_stats": {"power_gpu_soc_mean_watts": 22.404, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 73.773, "power_watts_avg": 22.404, "energy_joules_est": 173.96, "duration_seconds": 7.765, "sample_count": 66}, "timestamp": "2026-01-25T16:14:31.032518"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11135.785, "latencies_ms": [11135.785], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a baseball player in a white uniform with the number 10 is standing at home plate, ready to swing. Behind him, the catcher and umpire are in position, with the catcher crouched behind home plate and the umpire standing to the left of the catcher. In the background, there are spectators seated in the stands", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24330.7, "ram_available_mb": 38510.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24402.2, "ram_available_mb": 38438.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.968}, "power_stats": {"power_gpu_soc_mean_watts": 20.935, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 69.968, "power_watts_avg": 20.935, "energy_joules_est": 233.14, "duration_seconds": 11.136, "sample_count": 95}, "timestamp": "2026-01-25T16:14:44.225756"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8523.115, "latencies_ms": [8523.115], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image captures a moment during a baseball game where a batter is at the plate, ready to swing. The catcher and umpire are in position behind the batter, with the catcher crouched and the umpire standing upright.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24332.2, "ram_available_mb": 38508.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24402.3, "ram_available_mb": 38438.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.389}, "power_stats": {"power_gpu_soc_mean_watts": 22.084, "power_cpu_cv_mean_watts": 1.712, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 72.389, "power_watts_avg": 22.084, "energy_joules_est": 188.24, "duration_seconds": 8.524, "sample_count": 72}, "timestamp": "2026-01-25T16:14:54.768186"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9004.318, "latencies_ms": [9004.318], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image shows a baseball game in progress with a player in a white uniform at bat, a catcher in a black and red uniform, and an umpire in a blue uniform. The lighting appears to be artificial, likely from stadium lights, and the weather is overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24338.9, "ram_available_mb": 38501.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24423.3, "ram_available_mb": 38417.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.883}, "power_stats": {"power_gpu_soc_mean_watts": 21.602, "power_cpu_cv_mean_watts": 1.814, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 70.883, "power_watts_avg": 21.602, "energy_joules_est": 194.52, "duration_seconds": 9.005, "sample_count": 77}, "timestamp": "2026-01-25T16:15:05.803743"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12327.859, "latencies_ms": [12327.859], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a black and white drawing of a variety of fruits and nuts. There are two apples, one on the left and one on the right, with the left apple being larger and the right one smaller. A bunch of grapes is placed between the two apples, and a single orange is situated in the center of the composition. Additionally, there are several pean", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 24334.6, "ram_available_mb": 38506.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24287.0, "ram_available_mb": 38553.9, "ram_percent": 38.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.2}, "power_stats": {"power_gpu_soc_mean_watts": 22.817, "power_cpu_cv_mean_watts": 1.784, "power_sys_5v0_mean_watts": 9.183, "gpu_utilization_percent_mean": 74.2, "power_watts_avg": 22.817, "energy_joules_est": 281.3, "duration_seconds": 12.329, "sample_count": 105}, "timestamp": "2026-01-25T16:15:20.192068"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7246.238, "latencies_ms": [7246.238], "images_per_second": 0.138, "prompt_tokens": 39, "response_tokens_est": 31, "n_tiles": 16, "output_text": "peanuts: 12, grapes: 5, orange: 1, apple: 1, pear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24287.0, "ram_available_mb": 38553.9, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24382.0, "ram_available_mb": 38458.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 81.5}, "power_stats": {"power_gpu_soc_mean_watts": 25.188, "power_cpu_cv_mean_watts": 1.24, "power_sys_5v0_mean_watts": 9.179, "gpu_utilization_percent_mean": 81.5, "power_watts_avg": 25.188, "energy_joules_est": 182.53, "duration_seconds": 7.247, "sample_count": 62}, "timestamp": "2026-01-25T16:15:29.463826"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12093.558, "latencies_ms": [12093.558], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground, there is a pile of peanuts, which are the closest to the viewer. Behind the peanuts, there is a large orange, and further back, there are clusters of grapes. The grapes are positioned on the left side, while the orange is more centrally located among the fruits.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24320.0, "ram_available_mb": 38520.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24332.5, "ram_available_mb": 38508.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.644}, "power_stats": {"power_gpu_soc_mean_watts": 22.931, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 9.214, "gpu_utilization_percent_mean": 73.644, "power_watts_avg": 22.931, "energy_joules_est": 277.33, "duration_seconds": 12.094, "sample_count": 104}, "timestamp": "2026-01-25T16:15:43.586449"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9815.395, "latencies_ms": [9815.395], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a still life arrangement of various fruits and nuts on a white background. There is a large orange, a bunch of grapes, a few peanuts, and a few other fruits that are not clearly identifiable.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24275.9, "ram_available_mb": 38565.0, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24327.4, "ram_available_mb": 38513.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.905}, "power_stats": {"power_gpu_soc_mean_watts": 23.82, "power_cpu_cv_mean_watts": 1.563, "power_sys_5v0_mean_watts": 9.149, "gpu_utilization_percent_mean": 76.905, "power_watts_avg": 23.82, "energy_joules_est": 233.82, "duration_seconds": 9.816, "sample_count": 84}, "timestamp": "2026-01-25T16:15:55.425375"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8567.761, "latencies_ms": [8567.761], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image is a black and white drawing featuring a variety of fruits. The lighting appears to be coming from the upper left, casting subtle shadows on the right sides of the fruits and nuts.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24327.4, "ram_available_mb": 38513.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24375.5, "ram_available_mb": 38465.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.425}, "power_stats": {"power_gpu_soc_mean_watts": 24.129, "power_cpu_cv_mean_watts": 1.497, "power_sys_5v0_mean_watts": 9.242, "gpu_utilization_percent_mean": 77.425, "power_watts_avg": 24.129, "energy_joules_est": 206.75, "duration_seconds": 8.568, "sample_count": 73}, "timestamp": "2026-01-25T16:16:06.019325"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12150.949, "latencies_ms": [12150.949], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a city street with a mix of modern and older buildings. There are several cars parked along the street, and a few cars are driving down the road. A sidewalk runs alongside the street, providing a pedestrian path. A bike lane is also present, running parallel to the sidewalk.\n\nIn the distance, there is a bus stop with", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 24280.4, "ram_available_mb": 38560.5, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24321.1, "ram_available_mb": 38519.8, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.971, "power_cpu_cv_mean_watts": 1.819, "power_sys_5v0_mean_watts": 9.16, "gpu_utilization_percent_mean": 72.0, "power_watts_avg": 21.971, "energy_joules_est": 266.98, "duration_seconds": 12.152, "sample_count": 105}, "timestamp": "2026-01-25T16:16:20.204146"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9015.675, "latencies_ms": [9015.675], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "buildings: 10, cars: 5, street lights: 4, trees: 2, sidewalks: 1, buses: 1, pedestrians: 0, buildings: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24321.1, "ram_available_mb": 38519.8, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24319.9, "ram_available_mb": 38521.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.805}, "power_stats": {"power_gpu_soc_mean_watts": 23.386, "power_cpu_cv_mean_watts": 1.539, "power_sys_5v0_mean_watts": 9.113, "gpu_utilization_percent_mean": 76.805, "power_watts_avg": 23.386, "energy_joules_est": 210.86, "duration_seconds": 9.016, "sample_count": 77}, "timestamp": "2026-01-25T16:16:31.252357"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11713.861, "latencies_ms": [11713.861], "images_per_second": 0.085, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The image shows a street view with buildings in the background, a road in the foreground, and a clear sky above. The road has multiple lanes with cars parked on the side, and there is a sidewalk parallel to the road. The buildings appear to be residential or commercial structures, and there is a tree line on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24319.9, "ram_available_mb": 38521.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24316.2, "ram_available_mb": 38524.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.14}, "power_stats": {"power_gpu_soc_mean_watts": 22.778, "power_cpu_cv_mean_watts": 1.785, "power_sys_5v0_mean_watts": 9.17, "gpu_utilization_percent_mean": 73.14, "power_watts_avg": 22.778, "energy_joules_est": 266.83, "duration_seconds": 11.715, "sample_count": 100}, "timestamp": "2026-01-25T16:16:44.989274"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8324.41, "latencies_ms": [8324.41], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image depicts a street scene with a clear sky and a few clouds. There are multiple buildings on the left side of the street, and a few cars are parked and driving on the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24316.2, "ram_available_mb": 38524.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24317.7, "ram_available_mb": 38523.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.732}, "power_stats": {"power_gpu_soc_mean_watts": 23.768, "power_cpu_cv_mean_watts": 1.46, "power_sys_5v0_mean_watts": 9.128, "gpu_utilization_percent_mean": 76.732, "power_watts_avg": 23.768, "energy_joules_est": 197.87, "duration_seconds": 8.325, "sample_count": 71}, "timestamp": "2026-01-25T16:16:55.351110"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6801.137, "latencies_ms": [6801.137], "images_per_second": 0.147, "prompt_tokens": 36, "response_tokens_est": 32, "n_tiles": 16, "output_text": "The image shows a clear day with a blue sky and some clouds. The buildings are multi-story with a mix of brick and painted exteriors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24317.7, "ram_available_mb": 38523.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24316.7, "ram_available_mb": 38524.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.466}, "power_stats": {"power_gpu_soc_mean_watts": 24.736, "power_cpu_cv_mean_watts": 1.318, "power_sys_5v0_mean_watts": 9.241, "gpu_utilization_percent_mean": 78.466, "power_watts_avg": 24.736, "energy_joules_est": 168.25, "duration_seconds": 6.802, "sample_count": 58}, "timestamp": "2026-01-25T16:17:04.179390"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11153.622, "latencies_ms": [11153.622], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a man and a woman sitting at a table in a restaurant. The man is wearing a blue striped shirt and a black tie with a gold emblem, and he is holding a phone in his hand. The woman is wearing a white tank top and has her arm around the man's shoulder. They are both smiling and appear to be happy", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24316.7, "ram_available_mb": 38524.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24302.8, "ram_available_mb": 38538.1, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.745}, "power_stats": {"power_gpu_soc_mean_watts": 20.828, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 69.745, "power_watts_avg": 20.828, "energy_joules_est": 232.32, "duration_seconds": 11.154, "sample_count": 94}, "timestamp": "2026-01-25T16:17:17.369016"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7107.198, "latencies_ms": [7107.198], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "person: 2, table: 1, menu: 1, drink: 1, cell phone: 1, wall: 1, window: 1, television: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24302.8, "ram_available_mb": 38538.1, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24303.7, "ram_available_mb": 38537.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.417}, "power_stats": {"power_gpu_soc_mean_watts": 22.857, "power_cpu_cv_mean_watts": 1.568, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 73.417, "power_watts_avg": 22.857, "energy_joules_est": 162.46, "duration_seconds": 7.108, "sample_count": 60}, "timestamp": "2026-01-25T16:17:26.509400"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11190.495, "latencies_ms": [11190.495], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The man is standing in the foreground on the left side of the image, while the woman is seated in the foreground on the right side. They are positioned close to each other, suggesting a sense of intimacy or companionship. In the background, there is a wall with framed pictures and a television screen, indicating that they are inside a room, possibly a living room", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24303.7, "ram_available_mb": 38537.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24331.2, "ram_available_mb": 38509.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.463}, "power_stats": {"power_gpu_soc_mean_watts": 21.004, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 70.463, "power_watts_avg": 21.004, "energy_joules_est": 235.06, "duration_seconds": 11.191, "sample_count": 95}, "timestamp": "2026-01-25T16:17:39.735677"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8913.651, "latencies_ms": [8913.651], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "A man and a woman are sitting closely together at a table in a dimly lit restaurant, with the man holding a phone in his hand. The woman is wearing a white tank top and the man is wearing a blue striped shirt with a patterned tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24331.2, "ram_available_mb": 38509.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24406.5, "ram_available_mb": 38434.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.737}, "power_stats": {"power_gpu_soc_mean_watts": 21.166, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 8.927, "gpu_utilization_percent_mean": 71.737, "power_watts_avg": 21.166, "energy_joules_est": 188.68, "duration_seconds": 8.914, "sample_count": 76}, "timestamp": "2026-01-25T16:17:50.669490"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7501.329, "latencies_ms": [7501.329], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows two individuals in an indoor setting with warm lighting. The man is wearing a blue striped shirt with a patterned tie, and the woman is in a white sleeveless top.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24299.4, "ram_available_mb": 38541.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24316.9, "ram_available_mb": 38524.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.54}, "power_stats": {"power_gpu_soc_mean_watts": 22.473, "power_cpu_cv_mean_watts": 1.684, "power_sys_5v0_mean_watts": 9.028, "gpu_utilization_percent_mean": 72.54, "power_watts_avg": 22.473, "energy_joules_est": 168.59, "duration_seconds": 7.502, "sample_count": 63}, "timestamp": "2026-01-25T16:18:00.205797"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10819.406, "latencies_ms": [10819.406], "images_per_second": 0.092, "prompt_tokens": 24, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The image features a woman dressed in a costume, possibly as a warrior or a character from a movie, talking on a cell phone. She is surrounded by a group of people, with some of them standing close to her and others further away. The woman is the main focus of the scene, and the people around her seem to be engaged in their own activities.", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24316.9, "ram_available_mb": 38524.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24330.4, "ram_available_mb": 38510.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.478}, "power_stats": {"power_gpu_soc_mean_watts": 20.931, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 69.478, "power_watts_avg": 20.931, "energy_joules_est": 226.47, "duration_seconds": 10.82, "sample_count": 92}, "timestamp": "2026-01-25T16:18:13.069664"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9517.1, "latencies_ms": [9517.1], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Woman: 1\n2. Phone: 1\n3. Man: 1\n4. Building: 1\n5. Hat: 1\n6. Necklace: 1\n7. Eye shadow: 1\n8. Lipstick: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24330.4, "ram_available_mb": 38510.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24429.8, "ram_available_mb": 38411.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.951}, "power_stats": {"power_gpu_soc_mean_watts": 21.549, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 70.951, "power_watts_avg": 21.549, "energy_joules_est": 205.1, "duration_seconds": 9.518, "sample_count": 81}, "timestamp": "2026-01-25T16:18:24.612835"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11141.057, "latencies_ms": [11141.057], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is holding a phone to their ear, seemingly in the middle of a conversation. Behind them, there are several other individuals, some of whom are partially visible and appear to be engaged in their own activities. The person with the phone is positioned slightly to the left of the center of the image, while the others are scattered around, with some in the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.8, "ram_available_mb": 38411.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24291.8, "ram_available_mb": 38549.1, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.372}, "power_stats": {"power_gpu_soc_mean_watts": 20.94, "power_cpu_cv_mean_watts": 1.929, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 69.372, "power_watts_avg": 20.94, "energy_joules_est": 233.31, "duration_seconds": 11.142, "sample_count": 94}, "timestamp": "2026-01-25T16:18:37.777683"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7888.389, "latencies_ms": [7888.389], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A woman dressed in a costume resembling a character from a movie is talking on a phone while standing in a crowd of people. The setting appears to be an outdoor event or gathering, possibly a convention or festival.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24291.8, "ram_available_mb": 38549.1, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24314.9, "ram_available_mb": 38526.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.091}, "power_stats": {"power_gpu_soc_mean_watts": 22.342, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 74.091, "power_watts_avg": 22.342, "energy_joules_est": 176.26, "duration_seconds": 7.889, "sample_count": 66}, "timestamp": "2026-01-25T16:18:47.679725"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9127.682, "latencies_ms": [9127.682], "images_per_second": 0.11, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image features a person wearing a costume with gold and black colors, and the lighting appears to be natural daylight. The costume seems to be made of a shiny, possibly metallic material, and the person is wearing a headpiece with a gold design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24314.9, "ram_available_mb": 38526.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24310.3, "ram_available_mb": 38530.6, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.13}, "power_stats": {"power_gpu_soc_mean_watts": 21.499, "power_cpu_cv_mean_watts": 1.804, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 71.13, "power_watts_avg": 21.499, "energy_joules_est": 196.25, "duration_seconds": 9.128, "sample_count": 77}, "timestamp": "2026-01-25T16:18:58.834299"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11154.813, "latencies_ms": [11154.813], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene in a bathroom, dominated by a white toilet with a white tank and a white shower head. The toilet is positioned on the left side of the image, while the shower head is on the right. The floor beneath these fixtures is tiled in white, matching the color of the toilet and shower head", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24310.3, "ram_available_mb": 38530.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24328.5, "ram_available_mb": 38512.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.553}, "power_stats": {"power_gpu_soc_mean_watts": 20.827, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 69.553, "power_watts_avg": 20.827, "energy_joules_est": 232.34, "duration_seconds": 11.155, "sample_count": 94}, "timestamp": "2026-01-25T16:19:12.024739"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7332.884, "latencies_ms": [7332.884], "images_per_second": 0.136, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "shower head: 1, pipe: 3, tiles: many, bucket: 2, drain: 1, toilet: 1, wall: many, floor: many", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24275.2, "ram_available_mb": 38565.7, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24326.6, "ram_available_mb": 38514.3, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.131}, "power_stats": {"power_gpu_soc_mean_watts": 22.741, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 73.131, "power_watts_avg": 22.741, "energy_joules_est": 166.77, "duration_seconds": 7.334, "sample_count": 61}, "timestamp": "2026-01-25T16:19:21.382593"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11197.091, "latencies_ms": [11197.091], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a white toilet with a blue pipe connected to it, situated near a white pillar that appears to be part of a shower system. To the right of the toilet, there is a green bucket placed on the floor, and further back, there is a red bucket. The shower system with a white showerhead is", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24326.6, "ram_available_mb": 38514.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24339.2, "ram_available_mb": 38501.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.221}, "power_stats": {"power_gpu_soc_mean_watts": 20.951, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 70.221, "power_watts_avg": 20.951, "energy_joules_est": 234.6, "duration_seconds": 11.198, "sample_count": 95}, "timestamp": "2026-01-25T16:19:34.628197"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9099.553, "latencies_ms": [9099.553], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image depicts a small, cramped bathroom with white tiled walls and floor. There is a white toilet with a blue pipe connected to it, a green bucket, and a red bucket on the floor, and a showerhead mounted on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24285.9, "ram_available_mb": 38555.0, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24408.6, "ram_available_mb": 38432.3, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.962}, "power_stats": {"power_gpu_soc_mean_watts": 21.779, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 70.962, "power_watts_avg": 21.779, "energy_joules_est": 198.19, "duration_seconds": 9.1, "sample_count": 78}, "timestamp": "2026-01-25T16:19:45.776416"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8045.291, "latencies_ms": [8045.291], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a bathroom with white tiled walls and a white toilet with a blue pipe. There is a green bucket and a red bucket on the floor, and a showerhead is visible on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24353.6, "ram_available_mb": 38487.3, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24397.3, "ram_available_mb": 38443.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.912}, "power_stats": {"power_gpu_soc_mean_watts": 22.132, "power_cpu_cv_mean_watts": 1.707, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 71.912, "power_watts_avg": 22.132, "energy_joules_est": 178.07, "duration_seconds": 8.046, "sample_count": 68}, "timestamp": "2026-01-25T16:19:55.884521"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11187.609, "latencies_ms": [11187.609], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing next to an elephant, with the elephant's trunk wrapped around the man's neck. The man is wearing glasses and appears to be smiling, possibly enjoying the interaction with the elephant. The elephant is also smiling, creating a heartwarming scene.\n\nThere are a few", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24281.9, "ram_available_mb": 38559.0, "ram_percent": 38.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24339.4, "ram_available_mb": 38501.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.895}, "power_stats": {"power_gpu_soc_mean_watts": 20.871, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 68.895, "power_watts_avg": 20.871, "energy_joules_est": 233.51, "duration_seconds": 11.188, "sample_count": 95}, "timestamp": "2026-01-25T16:20:09.126886"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7101.176, "latencies_ms": [7101.176], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "man: 1, elephant: 2, trees: many, sky: 1, mountains: 1, sun: 1, grass: 1, clouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24339.4, "ram_available_mb": 38501.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24427.6, "ram_available_mb": 38413.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.283}, "power_stats": {"power_gpu_soc_mean_watts": 22.813, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 74.283, "power_watts_avg": 22.813, "energy_joules_est": 162.01, "duration_seconds": 7.102, "sample_count": 60}, "timestamp": "2026-01-25T16:20:18.292049"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11142.889, "latencies_ms": [11142.889], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a man wearing glasses and a light-colored shirt, who is standing close to an elephant. The elephant is positioned to the left of the man, with its trunk extended towards him. In the background, there is a lush green forest, indicating that the man and the elephant are in a natural,", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 24339.0, "ram_available_mb": 38501.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24330.4, "ram_available_mb": 38510.5, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.206}, "power_stats": {"power_gpu_soc_mean_watts": 20.899, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.206, "power_watts_avg": 20.899, "energy_joules_est": 232.89, "duration_seconds": 11.144, "sample_count": 97}, "timestamp": "2026-01-25T16:20:31.461260"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7070.68, "latencies_ms": [7070.68], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A man is standing in front of an elephant, with the elephant's trunk wrapped around his neck. The background shows a lush green forest and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24330.4, "ram_available_mb": 38510.5, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24331.2, "ram_available_mb": 38509.7, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.017}, "power_stats": {"power_gpu_soc_mean_watts": 22.763, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 9.016, "gpu_utilization_percent_mean": 73.017, "power_watts_avg": 22.763, "energy_joules_est": 160.96, "duration_seconds": 7.071, "sample_count": 60}, "timestamp": "2026-01-25T16:20:40.569223"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7222.649, "latencies_ms": [7222.649], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image features a man wearing glasses and a light-colored t-shirt, standing in front of an elephant. The background shows a lush green forest and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24331.2, "ram_available_mb": 38509.7, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24294.1, "ram_available_mb": 38546.8, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.951}, "power_stats": {"power_gpu_soc_mean_watts": 22.485, "power_cpu_cv_mean_watts": 1.647, "power_sys_5v0_mean_watts": 9.032, "gpu_utilization_percent_mean": 71.951, "power_watts_avg": 22.485, "energy_joules_est": 162.42, "duration_seconds": 7.223, "sample_count": 61}, "timestamp": "2026-01-25T16:20:49.846593"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10584.605, "latencies_ms": [10584.605], "images_per_second": 0.094, "prompt_tokens": 24, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The image depicts a group of five children sitting on the grass, each holding a white frisbee. They are all wearing different colored shirts, and the scene appears to be set in a park or a grassy area. The children seem to be enjoying their time together, possibly playing a game or just hanging out.", "error": null, "sys_before": {"cpu_percent": 10.7, "ram_used_mb": 24294.1, "ram_available_mb": 38546.8, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24305.8, "ram_available_mb": 38535.1, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.44}, "power_stats": {"power_gpu_soc_mean_watts": 20.022, "power_cpu_cv_mean_watts": 1.883, "power_sys_5v0_mean_watts": 8.909, "gpu_utilization_percent_mean": 70.44, "power_watts_avg": 20.022, "energy_joules_est": 211.94, "duration_seconds": 10.585, "sample_count": 91}, "timestamp": "2026-01-25T16:21:02.488896"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8315.381, "latencies_ms": [8315.381], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "children: 5\nfrisbees: 2\ngrass: numerous\ntrees: 3\nsweatshirts: 2\nsocks: 2\nshirts: 3\npants: 3", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24305.8, "ram_available_mb": 38535.1, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24300.5, "ram_available_mb": 38540.4, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.31}, "power_stats": {"power_gpu_soc_mean_watts": 21.928, "power_cpu_cv_mean_watts": 1.714, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 71.31, "power_watts_avg": 21.928, "energy_joules_est": 182.35, "duration_seconds": 8.316, "sample_count": 71}, "timestamp": "2026-01-25T16:21:12.854907"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11124.373, "latencies_ms": [11124.373], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are five children sitting on the grass, with one child on the far left and the others spread out towards the right. The child on the far right is holding a white frisbee with the word \"Ultimate\" on it, which is the main object in the image. The background consists of trees and shrubs, indicating that the setting is outdo", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24300.5, "ram_available_mb": 38540.4, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24335.7, "ram_available_mb": 38505.2, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.819}, "power_stats": {"power_gpu_soc_mean_watts": 20.983, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.819, "power_watts_avg": 20.983, "energy_joules_est": 233.44, "duration_seconds": 11.125, "sample_count": 94}, "timestamp": "2026-01-25T16:21:26.000059"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6628.824, "latencies_ms": [6628.824], "images_per_second": 0.151, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A group of children is sitting on the grass, each holding a frisbee. They appear to be in a park or a grassy area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24335.7, "ram_available_mb": 38505.2, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24330.0, "ram_available_mb": 38510.9, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.232}, "power_stats": {"power_gpu_soc_mean_watts": 23.226, "power_cpu_cv_mean_watts": 1.515, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 74.232, "power_watts_avg": 23.226, "energy_joules_est": 153.98, "duration_seconds": 6.629, "sample_count": 56}, "timestamp": "2026-01-25T16:21:34.655552"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7570.232, "latencies_ms": [7570.232], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a group of children sitting on grass with a clear sky in the background. They are holding frisbees, one of which is white with a black logo, and another is white with red and blue text.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24330.0, "ram_available_mb": 38510.9, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24311.6, "ram_available_mb": 38529.3, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.062}, "power_stats": {"power_gpu_soc_mean_watts": 22.273, "power_cpu_cv_mean_watts": 1.689, "power_sys_5v0_mean_watts": 9.019, "gpu_utilization_percent_mean": 72.062, "power_watts_avg": 22.273, "energy_joules_est": 168.63, "duration_seconds": 7.571, "sample_count": 64}, "timestamp": "2026-01-25T16:21:44.261127"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11133.64, "latencies_ms": [11133.64], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young girl is standing on a sidewalk, holding a black umbrella to protect herself from the rain. She is wearing a red coat and white shoes, and she appears to be smiling. The sidewalk is wet, indicating that it has been raining.\n\nThe scene is set in a residential area, with a house visible in the background", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24311.6, "ram_available_mb": 38529.3, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24342.9, "ram_available_mb": 38498.0, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.427}, "power_stats": {"power_gpu_soc_mean_watts": 20.853, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.427, "power_watts_avg": 20.853, "energy_joules_est": 232.19, "duration_seconds": 11.135, "sample_count": 96}, "timestamp": "2026-01-25T16:21:57.429398"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9230.36, "latencies_ms": [9230.36], "images_per_second": 0.108, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- umbrella: 1\n- child: 1\n- red coat: 1\n- blue jeans: 1\n- white sneakers: 1\n- green bush: 2\n- puddle: 1\n- houses: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24342.9, "ram_available_mb": 38498.0, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24354.6, "ram_available_mb": 38486.3, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.114}, "power_stats": {"power_gpu_soc_mean_watts": 21.684, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 8.935, "gpu_utilization_percent_mean": 72.114, "power_watts_avg": 21.684, "energy_joules_est": 200.16, "duration_seconds": 9.231, "sample_count": 79}, "timestamp": "2026-01-25T16:22:08.718698"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9631.518, "latencies_ms": [9631.518], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "In the foreground, a child is standing on a wet sidewalk, holding an umbrella to protect from the rain. The child is positioned near a large, well-manicured bush. In the background, there are houses and vehicles, indicating that the scene takes place in a residential area.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24354.6, "ram_available_mb": 38486.3, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24345.3, "ram_available_mb": 38495.6, "ram_percent": 38.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.485, "power_cpu_cv_mean_watts": 1.821, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 71.0, "power_watts_avg": 21.485, "energy_joules_est": 206.95, "duration_seconds": 9.632, "sample_count": 82}, "timestamp": "2026-01-25T16:22:20.364640"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6511.475, "latencies_ms": [6511.475], "images_per_second": 0.154, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "A young child is standing on a wet sidewalk, holding an umbrella to shield themselves from the rain. The background shows a residential area with houses and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24345.3, "ram_available_mb": 38495.6, "ram_percent": 38.7}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24353.5, "ram_available_mb": 38487.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.815}, "power_stats": {"power_gpu_soc_mean_watts": 23.613, "power_cpu_cv_mean_watts": 1.519, "power_sys_5v0_mean_watts": 9.048, "gpu_utilization_percent_mean": 73.815, "power_watts_avg": 23.613, "energy_joules_est": 153.77, "duration_seconds": 6.512, "sample_count": 54}, "timestamp": "2026-01-25T16:22:28.905463"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7991.055, "latencies_ms": [7991.055], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A child in a red coat is holding a patterned umbrella, standing on a wet sidewalk with a hedge and a red truck in the background. The sky is overcast, indicating it is likely a rainy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24353.5, "ram_available_mb": 38487.4, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24359.0, "ram_available_mb": 38481.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.676}, "power_stats": {"power_gpu_soc_mean_watts": 21.41, "power_cpu_cv_mean_watts": 1.695, "power_sys_5v0_mean_watts": 9.004, "gpu_utilization_percent_mean": 73.676, "power_watts_avg": 21.41, "energy_joules_est": 171.1, "duration_seconds": 7.992, "sample_count": 68}, "timestamp": "2026-01-25T16:22:38.935426"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11172.918, "latencies_ms": [11172.918], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a group of elephants is walking along a dirt road. The main elephant in the foreground is walking towards the camera, while the other elephants are following behind. The elephants are of various sizes, with some being larger and others smaller. The scene appears to be in a natural environment, possibly a savannah or a wildlife reserve", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24359.0, "ram_available_mb": 38481.9, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24364.8, "ram_available_mb": 38476.1, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.747}, "power_stats": {"power_gpu_soc_mean_watts": 20.827, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 68.747, "power_watts_avg": 20.827, "energy_joules_est": 232.71, "duration_seconds": 11.174, "sample_count": 95}, "timestamp": "2026-01-25T16:22:52.161888"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7295.232, "latencies_ms": [7295.232], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "elephant: 3, water: 1, trees: 1, dirt: 1, sky: 1, clouds: 1, grass: 1, ground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24364.8, "ram_available_mb": 38476.1, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24364.5, "ram_available_mb": 38476.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.339}, "power_stats": {"power_gpu_soc_mean_watts": 22.736, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 74.339, "power_watts_avg": 22.736, "energy_joules_est": 165.88, "duration_seconds": 7.296, "sample_count": 62}, "timestamp": "2026-01-25T16:23:01.495062"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11149.177, "latencies_ms": [11149.177], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a prominent elephant with its trunk extended towards the camera, giving a sense of being close to the viewer. Behind this elephant, there are two more elephants, one partially visible on the left and another more obscured on the right, creating a sense of depth. The background features a body of water and some vegetation,", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24364.5, "ram_available_mb": 38476.4, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24367.7, "ram_available_mb": 38473.2, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.442}, "power_stats": {"power_gpu_soc_mean_watts": 20.881, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 69.442, "power_watts_avg": 20.881, "energy_joules_est": 232.82, "duration_seconds": 11.15, "sample_count": 95}, "timestamp": "2026-01-25T16:23:14.691001"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7975.651, "latencies_ms": [7975.651], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A group of elephants, including a young one, are walking along a dirt path near a body of water. The elephants appear to be in a natural habitat, possibly a savannah or a wildlife reserve.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24367.7, "ram_available_mb": 38473.2, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24381.3, "ram_available_mb": 38459.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.985}, "power_stats": {"power_gpu_soc_mean_watts": 22.118, "power_cpu_cv_mean_watts": 1.695, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 71.985, "power_watts_avg": 22.118, "energy_joules_est": 176.42, "duration_seconds": 7.976, "sample_count": 68}, "timestamp": "2026-01-25T16:23:24.696932"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9818.441, "latencies_ms": [9818.441], "images_per_second": 0.102, "prompt_tokens": 36, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The elephant in the foreground has a rough, sandy-colored skin with patches of lighter and darker shades, indicating a dry and dusty environment. The lighting is soft and diffused, suggesting an overcast sky or a time of day when the sun is not at its peak.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24381.3, "ram_available_mb": 38459.6, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24381.4, "ram_available_mb": 38459.5, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.251, "power_cpu_cv_mean_watts": 1.852, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 70.0, "power_watts_avg": 21.251, "energy_joules_est": 208.67, "duration_seconds": 9.819, "sample_count": 83}, "timestamp": "2026-01-25T16:23:36.555320"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12340.934, "latencies_ms": [12340.934], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a thrilling moment of a surfer riding a wave. The surfer, clad in a vibrant red and green wetsuit, is skillfully maneuvering a white surfboard. The wave, a deep green color, is breaking to the right, creating a dynamic and powerful scene. The surfer is positioned on the left side of", "error": null, "sys_before": {"cpu_percent": 3.3, "ram_used_mb": 24381.4, "ram_available_mb": 38459.5, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24412.2, "ram_available_mb": 38428.7, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.39}, "power_stats": {"power_gpu_soc_mean_watts": 22.809, "power_cpu_cv_mean_watts": 1.788, "power_sys_5v0_mean_watts": 9.16, "gpu_utilization_percent_mean": 73.39, "power_watts_avg": 22.809, "energy_joules_est": 281.5, "duration_seconds": 12.342, "sample_count": 105}, "timestamp": "2026-01-25T16:23:50.961823"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8704.107, "latencies_ms": [8704.107], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "surfer: 1, wave: multiple, water droplets: numerous, surfboard: 1, air: 1, foam: multiple, ocean: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24412.2, "ram_available_mb": 38428.7, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24409.4, "ram_available_mb": 38431.5, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.824}, "power_stats": {"power_gpu_soc_mean_watts": 24.311, "power_cpu_cv_mean_watts": 1.444, "power_sys_5v0_mean_watts": 9.176, "gpu_utilization_percent_mean": 78.824, "power_watts_avg": 24.311, "energy_joules_est": 211.62, "duration_seconds": 8.705, "sample_count": 74}, "timestamp": "2026-01-25T16:24:01.679957"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12416.339, "latencies_ms": [12416.339], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is breaking to the right of the frame. The wave itself is the main object in the background, with its crest curling over to the left side of the image. The spray from the wave is captured in the midground, creating a dynamic contrast between the surfer and the wave's motion.", "error": null, "sys_before": {"cpu_percent": 27.3, "ram_used_mb": 24409.4, "ram_available_mb": 38431.5, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24416.6, "ram_available_mb": 38424.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.59}, "power_stats": {"power_gpu_soc_mean_watts": 22.887, "power_cpu_cv_mean_watts": 1.769, "power_sys_5v0_mean_watts": 9.209, "gpu_utilization_percent_mean": 73.59, "power_watts_avg": 22.887, "energy_joules_est": 284.19, "duration_seconds": 12.417, "sample_count": 105}, "timestamp": "2026-01-25T16:24:16.134882"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8603.147, "latencies_ms": [8603.147], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A surfer is captured in mid-air, performing a trick above a large wave. The photo is taken from a distance, showcasing the surfer's skill and the power of the ocean.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24416.6, "ram_available_mb": 38424.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24417.2, "ram_available_mb": 38423.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.836}, "power_stats": {"power_gpu_soc_mean_watts": 24.375, "power_cpu_cv_mean_watts": 1.409, "power_sys_5v0_mean_watts": 9.166, "gpu_utilization_percent_mean": 78.836, "power_watts_avg": 24.375, "energy_joules_est": 209.72, "duration_seconds": 8.604, "sample_count": 73}, "timestamp": "2026-01-25T16:24:26.796006"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9137.109, "latencies_ms": [9137.109], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The surfer is wearing a bright red and green wetsuit, which stands out against the darker tones of the ocean. The lighting in the image is natural, suggesting it was taken during the day under clear skies.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24417.2, "ram_available_mb": 38423.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24428.2, "ram_available_mb": 38412.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.532}, "power_stats": {"power_gpu_soc_mean_watts": 23.941, "power_cpu_cv_mean_watts": 1.528, "power_sys_5v0_mean_watts": 9.276, "gpu_utilization_percent_mean": 76.532, "power_watts_avg": 23.941, "energy_joules_est": 218.77, "duration_seconds": 9.138, "sample_count": 77}, "timestamp": "2026-01-25T16:24:37.967578"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12159.191, "latencies_ms": [12159.191], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a man and a woman riding horses on a sandy beach. The man is riding a white horse, while the woman is on a brown horse. They are both wearing white clothes, and the man is holding a stick in his hand. The beach is bustling with activity, as there are several other people visible in the background. Some of them are standing near", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24428.2, "ram_available_mb": 38412.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24419.8, "ram_available_mb": 38421.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.404}, "power_stats": {"power_gpu_soc_mean_watts": 22.005, "power_cpu_cv_mean_watts": 1.79, "power_sys_5v0_mean_watts": 9.127, "gpu_utilization_percent_mean": 73.404, "power_watts_avg": 22.005, "energy_joules_est": 267.58, "duration_seconds": 12.16, "sample_count": 104}, "timestamp": "2026-01-25T16:24:52.179812"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7991.041, "latencies_ms": [7991.041], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "people: 2, horses: 2, beach: 1, ocean: 1, sky: 1, clouds: 1, sand: 1, camera: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24419.8, "ram_available_mb": 38421.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24415.8, "ram_available_mb": 38425.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.618}, "power_stats": {"power_gpu_soc_mean_watts": 24.291, "power_cpu_cv_mean_watts": 1.407, "power_sys_5v0_mean_watts": 9.142, "gpu_utilization_percent_mean": 78.618, "power_watts_avg": 24.291, "energy_joules_est": 194.13, "duration_seconds": 7.992, "sample_count": 68}, "timestamp": "2026-01-25T16:25:02.196276"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12082.472, "latencies_ms": [12082.472], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, there are two individuals riding horses, with one horse positioned to the left and the other to the right of the frame. The background features a sandy beach with people in the water and a clear blue sky above. The horses and riders are closer to the camera than the people in the water, making them appear larger and more prominent in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24415.8, "ram_available_mb": 38425.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24418.7, "ram_available_mb": 38422.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.029}, "power_stats": {"power_gpu_soc_mean_watts": 22.588, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 9.151, "gpu_utilization_percent_mean": 73.029, "power_watts_avg": 22.588, "energy_joules_est": 272.93, "duration_seconds": 12.083, "sample_count": 102}, "timestamp": "2026-01-25T16:25:16.309563"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6884.721, "latencies_ms": [6884.721], "images_per_second": 0.145, "prompt_tokens": 37, "response_tokens_est": 30, "n_tiles": 16, "output_text": "Two people are riding horses on a sandy beach near the ocean. The sky is blue and there are a few clouds in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.7, "ram_available_mb": 38422.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24435.9, "ram_available_mb": 38405.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.879}, "power_stats": {"power_gpu_soc_mean_watts": 24.968, "power_cpu_cv_mean_watts": 1.249, "power_sys_5v0_mean_watts": 9.155, "gpu_utilization_percent_mean": 79.879, "power_watts_avg": 24.968, "energy_joules_est": 171.91, "duration_seconds": 6.885, "sample_count": 58}, "timestamp": "2026-01-25T16:25:25.251611"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8210.548, "latencies_ms": [8210.548], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The sky is a clear blue with a few scattered clouds, indicating a sunny day. The sand is a light beige color, and the water appears a deep blue-green, suggesting a tropical beach setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.9, "ram_available_mb": 38405.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24453.0, "ram_available_mb": 38387.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.913}, "power_stats": {"power_gpu_soc_mean_watts": 23.675, "power_cpu_cv_mean_watts": 1.508, "power_sys_5v0_mean_watts": 9.202, "gpu_utilization_percent_mean": 76.913, "power_watts_avg": 23.675, "energy_joules_est": 194.41, "duration_seconds": 8.211, "sample_count": 69}, "timestamp": "2026-01-25T16:25:35.524799"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11120.115, "latencies_ms": [11120.115], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a motorcycle parked in front of a garage. The motorcycle is leaning against a wall, and it appears to be a vintage model. A small dog is standing nearby, looking at the motorcycle. The garage is filled with various items, including bicycles and other objects.\n\nIn the background, there is a car park", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24453.0, "ram_available_mb": 38387.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24492.3, "ram_available_mb": 38348.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.726}, "power_stats": {"power_gpu_soc_mean_watts": 20.961, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 69.726, "power_watts_avg": 20.961, "energy_joules_est": 233.1, "duration_seconds": 11.121, "sample_count": 95}, "timestamp": "2026-01-25T16:25:48.708475"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8857.437, "latencies_ms": [8857.437], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 57, "n_tiles": 16, "output_text": "- Motorcycle: 1\n- Dog: 1\n- Tire: 2\n- Car: 1\n- Bike: 1\n- Grill: 1\n- Motorcycle seat: 1\n- Motorcycle handlebar: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.7, "ram_available_mb": 38420.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.427}, "power_stats": {"power_gpu_soc_mean_watts": 21.806, "power_cpu_cv_mean_watts": 1.751, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 71.427, "power_watts_avg": 21.806, "energy_joules_est": 193.16, "duration_seconds": 8.858, "sample_count": 75}, "timestamp": "2026-01-25T16:25:59.585832"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10219.282, "latencies_ms": [10219.282], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there is a motorcycle with a sidecar parked on the grass. The sidecar is attached to a red vehicle, which is positioned in front of a garage. The garage is located in the background, with a car covered in a blue tarp parked to the right of the garage.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24429.0, "ram_available_mb": 38411.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.721}, "power_stats": {"power_gpu_soc_mean_watts": 20.811, "power_cpu_cv_mean_watts": 1.871, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 70.721, "power_watts_avg": 20.811, "energy_joules_est": 212.69, "duration_seconds": 10.22, "sample_count": 86}, "timestamp": "2026-01-25T16:26:11.823561"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6736.731, "latencies_ms": [6736.731], "images_per_second": 0.148, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A motorcycle is parked in front of a garage with a dog standing nearby. The garage is filled with various items, including bicycles hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.0, "ram_available_mb": 38411.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24428.4, "ram_available_mb": 38412.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.036}, "power_stats": {"power_gpu_soc_mean_watts": 23.062, "power_cpu_cv_mean_watts": 1.523, "power_sys_5v0_mean_watts": 9.018, "gpu_utilization_percent_mean": 75.036, "power_watts_avg": 23.062, "energy_joules_est": 155.38, "duration_seconds": 6.737, "sample_count": 56}, "timestamp": "2026-01-25T16:26:20.591103"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5302.621, "latencies_ms": [5302.621], "images_per_second": 0.189, "prompt_tokens": 36, "response_tokens_est": 27, "n_tiles": 16, "output_text": "The motorcycle is primarily silver with a black seat and handlebars. The weather appears to be sunny with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24428.4, "ram_available_mb": 38412.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24425.4, "ram_available_mb": 38415.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.273}, "power_stats": {"power_gpu_soc_mean_watts": 24.336, "power_cpu_cv_mean_watts": 1.337, "power_sys_5v0_mean_watts": 9.134, "gpu_utilization_percent_mean": 76.273, "power_watts_avg": 24.336, "energy_joules_est": 129.06, "duration_seconds": 5.303, "sample_count": 44}, "timestamp": "2026-01-25T16:26:27.908995"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11131.765, "latencies_ms": [11131.765], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a man flying a kite on a sandy beach. He is standing in the sand, holding onto the kite string, and appears to be enjoying the activity. There are several other people scattered around the beach, some closer to the water and others further away. \n\nIn the background, a few boats can be seen on the water, adding to the beach atmosphere", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24425.4, "ram_available_mb": 38415.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24422.9, "ram_available_mb": 38418.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.543}, "power_stats": {"power_gpu_soc_mean_watts": 20.9, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.543, "power_watts_avg": 20.9, "energy_joules_est": 232.67, "duration_seconds": 11.133, "sample_count": 94}, "timestamp": "2026-01-25T16:26:41.071133"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7412.433, "latencies_ms": [7412.433], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "kite: 1, person: 1, sand: many, water: 1, buildings: 2, trees: many, beach chairs: 0, umbrellas: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.9, "ram_available_mb": 38418.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24443.0, "ram_available_mb": 38397.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.548}, "power_stats": {"power_gpu_soc_mean_watts": 22.503, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 72.548, "power_watts_avg": 22.503, "energy_joules_est": 166.82, "duration_seconds": 7.413, "sample_count": 62}, "timestamp": "2026-01-25T16:26:50.517086"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9115.067, "latencies_ms": [9115.067], "images_per_second": 0.11, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "In the foreground, a person is flying a kite on a sandy beach. The beach is adjacent to a body of water, with several people scattered along the shoreline. In the background, there are trees and buildings, indicating that the beach is likely located near a populated area.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24443.0, "ram_available_mb": 38397.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24419.9, "ram_available_mb": 38421.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.156}, "power_stats": {"power_gpu_soc_mean_watts": 21.614, "power_cpu_cv_mean_watts": 1.804, "power_sys_5v0_mean_watts": 9.014, "gpu_utilization_percent_mean": 71.156, "power_watts_avg": 21.614, "energy_joules_est": 197.03, "duration_seconds": 9.116, "sample_count": 77}, "timestamp": "2026-01-25T16:27:01.666801"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6491.077, "latencies_ms": [6491.077], "images_per_second": 0.154, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "A man is flying a kite on a sandy beach with the ocean in the background. There are several people scattered around the beach, enjoying the sunny day.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24419.9, "ram_available_mb": 38421.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24425.4, "ram_available_mb": 38415.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.944}, "power_stats": {"power_gpu_soc_mean_watts": 23.212, "power_cpu_cv_mean_watts": 1.49, "power_sys_5v0_mean_watts": 9.026, "gpu_utilization_percent_mean": 73.944, "power_watts_avg": 23.212, "energy_joules_est": 150.69, "duration_seconds": 6.492, "sample_count": 54}, "timestamp": "2026-01-25T16:27:10.172890"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7673.641, "latencies_ms": [7673.641], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image shows a sunny day at the beach with clear blue skies and a few scattered clouds. The sand is a light beige color, and the water appears to be a deep blue, reflecting the bright sunlight.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24425.4, "ram_available_mb": 38415.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.906}, "power_stats": {"power_gpu_soc_mean_watts": 22.247, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 9.046, "gpu_utilization_percent_mean": 71.906, "power_watts_avg": 22.247, "energy_joules_est": 170.73, "duration_seconds": 7.674, "sample_count": 64}, "timestamp": "2026-01-25T16:27:19.862618"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11140.252, "latencies_ms": [11140.252], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a kitchen with wooden cabinets and a black countertop. The countertop is cluttered with various items, including a green bottle, a white plate, and several bottles. There is a sink in the kitchen, and a refrigerator is visible on the left side of the scene. \n\nIn addition to the kitchen items, there are a few decor", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24423.5, "ram_available_mb": 38417.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.326}, "power_stats": {"power_gpu_soc_mean_watts": 20.848, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.934, "gpu_utilization_percent_mean": 69.326, "power_watts_avg": 20.848, "energy_joules_est": 232.27, "duration_seconds": 11.141, "sample_count": 95}, "timestamp": "2026-01-25T16:27:33.038808"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8306.262, "latencies_ms": [8306.262], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "refrigerator: 1, sink: 2, dishwasher: 1, microwave: 1, oven: 1, candle: 1, plate: 1, bottle: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.5, "ram_available_mb": 38417.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.606}, "power_stats": {"power_gpu_soc_mean_watts": 22.018, "power_cpu_cv_mean_watts": 1.708, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 72.606, "power_watts_avg": 22.018, "energy_joules_est": 182.9, "duration_seconds": 8.307, "sample_count": 71}, "timestamp": "2026-01-25T16:27:43.401618"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10665.857, "latencies_ms": [10665.857], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground of the image, there is a kitchen counter with various items scattered on it, including a green bottle, a white plate with a red object on it, and several bottles. The refrigerator is in the background, to the left of the counter. The sink is located in the foreground, to the left of the counter.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24435.2, "ram_available_mb": 38405.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.407}, "power_stats": {"power_gpu_soc_mean_watts": 21.117, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 70.407, "power_watts_avg": 21.117, "energy_joules_est": 225.25, "duration_seconds": 10.667, "sample_count": 91}, "timestamp": "2026-01-25T16:27:56.081908"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8176.476, "latencies_ms": [8176.476], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image depicts a kitchen with wooden cabinets and a stainless steel refrigerator. The countertop is cluttered with various items such as bottles, a plate with a red bow, and a green bottle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.2, "ram_available_mb": 38405.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24430.9, "ram_available_mb": 38410.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.362}, "power_stats": {"power_gpu_soc_mean_watts": 22.088, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 72.362, "power_watts_avg": 22.088, "energy_joules_est": 180.62, "duration_seconds": 8.177, "sample_count": 69}, "timestamp": "2026-01-25T16:28:06.308997"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7674.728, "latencies_ms": [7674.728], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The kitchen is well-lit with natural light coming from the window, and the cabinets are made of wood with a light brown color. The countertop is black, and there are various cleaning supplies and food items scattered around.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24430.9, "ram_available_mb": 38410.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24426.2, "ram_available_mb": 38414.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.431}, "power_stats": {"power_gpu_soc_mean_watts": 22.233, "power_cpu_cv_mean_watts": 1.687, "power_sys_5v0_mean_watts": 9.056, "gpu_utilization_percent_mean": 71.431, "power_watts_avg": 22.233, "energy_joules_est": 170.65, "duration_seconds": 7.675, "sample_count": 65}, "timestamp": "2026-01-25T16:28:16.031976"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11103.15, "latencies_ms": [11103.15], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the vast expanse of the clear blue sky, a vibrant kite soars high. The kite, a striking combination of white and red, is adorned with black and white designs that add a touch of whimsy to its appearance. The kite is shaped like a diamond, with a pointed nose and a tail that extends behind it, creating a sense", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24426.2, "ram_available_mb": 38414.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24415.3, "ram_available_mb": 38425.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.6}, "power_stats": {"power_gpu_soc_mean_watts": 20.921, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 69.6, "power_watts_avg": 20.921, "energy_joules_est": 232.3, "duration_seconds": 11.104, "sample_count": 95}, "timestamp": "2026-01-25T16:28:29.163275"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8177.549, "latencies_ms": [8177.549], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "kite: 1, sky: 1, string: 2, kite shape: 1, kite color: 1, kite design: 1, kite brand: 1, kite material: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24415.3, "ram_available_mb": 38425.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24431.5, "ram_available_mb": 38409.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.174}, "power_stats": {"power_gpu_soc_mean_watts": 22.198, "power_cpu_cv_mean_watts": 1.688, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 72.174, "power_watts_avg": 22.198, "energy_joules_est": 181.54, "duration_seconds": 8.178, "sample_count": 69}, "timestamp": "2026-01-25T16:28:39.384709"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7918.98, "latencies_ms": [7918.98], "images_per_second": 0.126, "prompt_tokens": 44, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The kite is in the foreground and appears to be flying high in the sky. The background is a clear blue sky with no clouds. The kite is positioned in the center of the image, with its tail trailing behind it.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24431.5, "ram_available_mb": 38409.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24427.2, "ram_available_mb": 38413.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.029}, "power_stats": {"power_gpu_soc_mean_watts": 21.534, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 9.01, "gpu_utilization_percent_mean": 72.029, "power_watts_avg": 21.534, "energy_joules_est": 170.54, "duration_seconds": 7.92, "sample_count": 68}, "timestamp": "2026-01-25T16:28:49.337447"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8271.775, "latencies_ms": [8271.775], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image captures a vibrant scene of a kite soaring high in the clear blue sky. The kite, with its striking design of black and white patterns on a red and white background, is the focal point of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.2, "ram_available_mb": 38413.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24422.5, "ram_available_mb": 38418.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.085}, "power_stats": {"power_gpu_soc_mean_watts": 22.119, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 73.085, "power_watts_avg": 22.119, "energy_joules_est": 182.98, "duration_seconds": 8.272, "sample_count": 71}, "timestamp": "2026-01-25T16:28:59.624676"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5194.437, "latencies_ms": [5194.437], "images_per_second": 0.193, "prompt_tokens": 36, "response_tokens_est": 26, "n_tiles": 16, "output_text": "The kite is predominantly white with red and black accents. It is flying high in the clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.5, "ram_available_mb": 38418.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24427.8, "ram_available_mb": 38413.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.558}, "power_stats": {"power_gpu_soc_mean_watts": 24.399, "power_cpu_cv_mean_watts": 1.35, "power_sys_5v0_mean_watts": 9.161, "gpu_utilization_percent_mean": 76.558, "power_watts_avg": 24.399, "energy_joules_est": 126.75, "duration_seconds": 5.195, "sample_count": 43}, "timestamp": "2026-01-25T16:29:06.835431"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11107.775, "latencies_ms": [11107.775], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large, well-lit bedroom with two neatly made beds. The beds are positioned side by side, and the room has a cozy and inviting atmosphere. The beds are adorned with pillows and blankets, and there are two lamps on either side of the beds, providing ample lighting.\n\nIn addition to", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24427.8, "ram_available_mb": 38413.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24439.1, "ram_available_mb": 38401.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.202}, "power_stats": {"power_gpu_soc_mean_watts": 20.92, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 69.202, "power_watts_avg": 20.92, "energy_joules_est": 232.39, "duration_seconds": 11.108, "sample_count": 94}, "timestamp": "2026-01-25T16:29:19.966884"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7720.692, "latencies_ms": [7720.692], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "bed: 2\nlamps: 2\npillows: 6\nblankets: 2\nrugs: 2\nartwork: 1\nwindows: 2\ndoors: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24439.1, "ram_available_mb": 38401.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24429.0, "ram_available_mb": 38411.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.908}, "power_stats": {"power_gpu_soc_mean_watts": 22.462, "power_cpu_cv_mean_watts": 1.657, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 72.908, "power_watts_avg": 22.462, "energy_joules_est": 173.44, "duration_seconds": 7.721, "sample_count": 65}, "timestamp": "2026-01-25T16:29:29.721329"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11084.478, "latencies_ms": [11084.478], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a room with two beds positioned parallel to each other, one in the foreground and the other in the background. The beds are separated by a small space, and there is a door on the right side of the image that is slightly farther away from the viewer than the beds. The room has a wooden floor and walls, and there are two windows, one", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24429.0, "ram_available_mb": 38411.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24419.8, "ram_available_mb": 38421.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.394}, "power_stats": {"power_gpu_soc_mean_watts": 20.997, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 69.394, "power_watts_avg": 20.997, "energy_joules_est": 232.75, "duration_seconds": 11.085, "sample_count": 94}, "timestamp": "2026-01-25T16:29:42.845658"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8049.61, "latencies_ms": [8049.61], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a well-lit bedroom with two neatly made beds, each adorned with pillows and blankets. The room features a large window with a view of trees outside, and a door leading to another room.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24419.8, "ram_available_mb": 38421.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24416.7, "ram_available_mb": 38424.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.824}, "power_stats": {"power_gpu_soc_mean_watts": 22.183, "power_cpu_cv_mean_watts": 1.69, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 71.824, "power_watts_avg": 22.183, "energy_joules_est": 178.58, "duration_seconds": 8.05, "sample_count": 68}, "timestamp": "2026-01-25T16:29:52.907613"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7860.077, "latencies_ms": [7860.077], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The room is well-lit with warm lighting from lamps on either side of the bed, creating a cozy atmosphere. The walls are painted in a light color, and the wooden floor adds a touch of warmth to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24416.7, "ram_available_mb": 38424.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.697}, "power_stats": {"power_gpu_soc_mean_watts": 22.187, "power_cpu_cv_mean_watts": 1.698, "power_sys_5v0_mean_watts": 9.064, "gpu_utilization_percent_mean": 71.697, "power_watts_avg": 22.187, "energy_joules_est": 174.41, "duration_seconds": 7.861, "sample_count": 66}, "timestamp": "2026-01-25T16:30:02.809054"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11098.566, "latencies_ms": [11098.566], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is riding a motorcycle on a road, with a group of people watching him. The man is wearing a white helmet and a white jacket, and he is riding a white motorcycle with a green stripe. The people watching him are standing on the side of the road, and they are all looking at the man as he rides by", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24411.5, "ram_available_mb": 38429.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.894}, "power_stats": {"power_gpu_soc_mean_watts": 20.981, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.894, "power_watts_avg": 20.981, "energy_joules_est": 232.87, "duration_seconds": 11.099, "sample_count": 94}, "timestamp": "2026-01-25T16:30:15.955822"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7615.431, "latencies_ms": [7615.431], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "motorcycle: 1, rider: 1, helmet: 1, fence: 4, grass: 1, people: 4, road: 1, motorcycle brand: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24411.5, "ram_available_mb": 38429.4, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24411.4, "ram_available_mb": 38429.5, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.292}, "power_stats": {"power_gpu_soc_mean_watts": 22.576, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 73.292, "power_watts_avg": 22.576, "energy_joules_est": 171.94, "duration_seconds": 7.616, "sample_count": 65}, "timestamp": "2026-01-25T16:30:25.628918"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 6982.299, "latencies_ms": [6982.299], "images_per_second": 0.143, "prompt_tokens": 44, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A motorcyclist is in the foreground, riding on the left side of the road. Spectators are gathered on the right side of the road, watching the rider from a distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24411.4, "ram_available_mb": 38429.5, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.847}, "power_stats": {"power_gpu_soc_mean_watts": 22.736, "power_cpu_cv_mean_watts": 1.622, "power_sys_5v0_mean_watts": 9.067, "gpu_utilization_percent_mean": 72.847, "power_watts_avg": 22.736, "energy_joules_est": 158.77, "duration_seconds": 6.983, "sample_count": 59}, "timestamp": "2026-01-25T16:30:34.627933"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8043.749, "latencies_ms": [8043.749], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A motorcyclist is seen riding a white motorcycle with green accents on a road, wearing a white helmet and racing suit. Spectators are gathered on the side of the road, watching the rider in action.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24419.5, "ram_available_mb": 38421.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.735}, "power_stats": {"power_gpu_soc_mean_watts": 22.224, "power_cpu_cv_mean_watts": 1.69, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 72.735, "power_watts_avg": 22.224, "energy_joules_est": 178.78, "duration_seconds": 8.044, "sample_count": 68}, "timestamp": "2026-01-25T16:30:44.727228"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7001.196, "latencies_ms": [7001.196], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The motorcycle rider is wearing a white helmet and a white and green racing suit. The motorcycle is white with green accents and has the word \"PAJ\" written on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24542.7, "ram_available_mb": 38298.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.22}, "power_stats": {"power_gpu_soc_mean_watts": 22.62, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 9.04, "gpu_utilization_percent_mean": 72.22, "power_watts_avg": 22.62, "energy_joules_est": 158.38, "duration_seconds": 7.002, "sample_count": 59}, "timestamp": "2026-01-25T16:30:53.749280"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11114.334, "latencies_ms": [11114.334], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene dining scene set in a dimly lit restaurant. The main focus is a table, draped in a pristine white tablecloth, which is adorned with a single white flower in a clear vase. The table is set meticulously with a white plate, silverware, and a glass of water, all arranged with an air", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24420.7, "ram_available_mb": 38420.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24423.2, "ram_available_mb": 38417.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.568}, "power_stats": {"power_gpu_soc_mean_watts": 20.954, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 69.568, "power_watts_avg": 20.954, "energy_joules_est": 232.9, "duration_seconds": 11.115, "sample_count": 95}, "timestamp": "2026-01-25T16:31:06.895809"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7510.686, "latencies_ms": [7510.686], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "table: 1\nglass: 3\nplate: 2\nknife: 2\nfork: 2\nspoon: 1\nvase: 1\nflower: 5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.2, "ram_available_mb": 38417.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24408.9, "ram_available_mb": 38432.0, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.794}, "power_stats": {"power_gpu_soc_mean_watts": 22.68, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 72.794, "power_watts_avg": 22.68, "energy_joules_est": 170.36, "duration_seconds": 7.511, "sample_count": 63}, "timestamp": "2026-01-25T16:31:16.436190"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11108.565, "latencies_ms": [11108.565], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a table set for a meal with a white tablecloth. On the table, there are three wine glasses, two of which are placed closer to the viewer and one further away, creating a sense of depth. The main object, a vase with white flowers, is centrally located on the table, with the wine glasses arranged symmet", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24408.9, "ram_available_mb": 38432.0, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24414.5, "ram_available_mb": 38426.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.957}, "power_stats": {"power_gpu_soc_mean_watts": 20.959, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.957, "power_watts_avg": 20.959, "energy_joules_est": 232.84, "duration_seconds": 11.109, "sample_count": 94}, "timestamp": "2026-01-25T16:31:29.561474"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8824.708, "latencies_ms": [8824.708], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image depicts a serene dining setting with a table elegantly set for a meal. The table is adorned with a white tablecloth, and there are three glass vases with white flowers in them, placed centrally on the table.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24414.5, "ram_available_mb": 38426.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24418.8, "ram_available_mb": 38422.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.213}, "power_stats": {"power_gpu_soc_mean_watts": 21.806, "power_cpu_cv_mean_watts": 1.761, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 71.213, "power_watts_avg": 21.806, "energy_joules_est": 192.45, "duration_seconds": 8.825, "sample_count": 75}, "timestamp": "2026-01-25T16:31:40.406931"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10113.395, "latencies_ms": [10113.395], "images_per_second": 0.099, "prompt_tokens": 36, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image features a table setting with a central vase holding white flowers, illuminated by soft, warm lighting that creates a cozy atmosphere. The table is adorned with elegant glassware, including wine glasses and a decanter, and is set with white plates and silverware, suggesting a formal dining experience.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.8, "ram_available_mb": 38422.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24422.7, "ram_available_mb": 38418.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.674}, "power_stats": {"power_gpu_soc_mean_watts": 21.015, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 70.674, "power_watts_avg": 21.015, "energy_joules_est": 212.55, "duration_seconds": 10.114, "sample_count": 86}, "timestamp": "2026-01-25T16:31:52.532839"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12344.567, "latencies_ms": [12344.567], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment frozen in time, featuring a black and white photograph of a street clock. The clock, standing tall on a pole, is the central focus of the image. It's a hexagonal structure, with each of its six sides adorned with a clock face. Each face is marked with numbers from 1 to 12, indicating the hours.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24422.7, "ram_available_mb": 38418.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24445.9, "ram_available_mb": 38395.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.362}, "power_stats": {"power_gpu_soc_mean_watts": 22.862, "power_cpu_cv_mean_watts": 1.773, "power_sys_5v0_mean_watts": 9.186, "gpu_utilization_percent_mean": 74.362, "power_watts_avg": 22.862, "energy_joules_est": 282.24, "duration_seconds": 12.345, "sample_count": 105}, "timestamp": "2026-01-25T16:32:06.901577"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9282.159, "latencies_ms": [9282.159], "images_per_second": 0.108, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "Clock: 2\nGrass: 1\nPole: 1\nPerson: 1\nWheat: 1\nSky: 1\nGround: 1\nClock face: 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24445.9, "ram_available_mb": 38395.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24437.7, "ram_available_mb": 38403.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.167}, "power_stats": {"power_gpu_soc_mean_watts": 24.081, "power_cpu_cv_mean_watts": 1.489, "power_sys_5v0_mean_watts": 9.161, "gpu_utilization_percent_mean": 78.167, "power_watts_avg": 24.081, "energy_joules_est": 223.54, "duration_seconds": 9.283, "sample_count": 78}, "timestamp": "2026-01-25T16:32:18.202460"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12185.427, "latencies_ms": [12185.427], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The clock is positioned in the foreground of the image, appearing larger and more detailed compared to the background. The background features a field of crops, which is less distinct due to its distance from the viewer. The clock is closer to the viewer, making it the main object of focus, while the field extends into the distance, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.7, "ram_available_mb": 38403.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24448.9, "ram_available_mb": 38392.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.837}, "power_stats": {"power_gpu_soc_mean_watts": 22.934, "power_cpu_cv_mean_watts": 1.767, "power_sys_5v0_mean_watts": 9.216, "gpu_utilization_percent_mean": 73.837, "power_watts_avg": 22.934, "energy_joules_est": 279.48, "duration_seconds": 12.186, "sample_count": 104}, "timestamp": "2026-01-25T16:32:32.413657"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8943.471, "latencies_ms": [8943.471], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image features a unique, octagonal-shaped clock mounted on a pole, with a grassy field in the background. The clock is black and white, adding a classic and timeless feel to the scene.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24448.9, "ram_available_mb": 38392.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24446.7, "ram_available_mb": 38394.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.329}, "power_stats": {"power_gpu_soc_mean_watts": 24.222, "power_cpu_cv_mean_watts": 1.464, "power_sys_5v0_mean_watts": 9.164, "gpu_utilization_percent_mean": 78.329, "power_watts_avg": 24.222, "energy_joules_est": 216.64, "duration_seconds": 8.944, "sample_count": 76}, "timestamp": "2026-01-25T16:32:43.408617"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8426.175, "latencies_ms": [8426.175], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image is a black and white photograph featuring a street clock with three faces, each displaying the time. The clock is mounted on a pole and the background shows a field of crops under a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24446.7, "ram_available_mb": 38394.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24450.8, "ram_available_mb": 38390.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.583}, "power_stats": {"power_gpu_soc_mean_watts": 24.223, "power_cpu_cv_mean_watts": 1.485, "power_sys_5v0_mean_watts": 9.272, "gpu_utilization_percent_mean": 77.583, "power_watts_avg": 24.223, "energy_joules_est": 204.13, "duration_seconds": 8.427, "sample_count": 72}, "timestamp": "2026-01-25T16:32:53.867543"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11090.765, "latencies_ms": [11090.765], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is skillfully riding a skateboard on a sidewalk. He is wearing a black shirt and a baseball cap, and his feet are firmly planted on the skateboard. The skateboard is positioned in the center of the image, with the young man's body leaning slightly to the left. The sidewalk on", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24450.8, "ram_available_mb": 38390.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24417.8, "ram_available_mb": 38423.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.894}, "power_stats": {"power_gpu_soc_mean_watts": 20.962, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 69.894, "power_watts_avg": 20.962, "energy_joules_est": 232.5, "duration_seconds": 11.091, "sample_count": 94}, "timestamp": "2026-01-25T16:33:07.001807"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10192.581, "latencies_ms": [10192.581], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 69, "n_tiles": 16, "output_text": "- Skateboard: 1\n\n- Skate park: 1\n\n- Skateboarder: 1\n\n- Pants: 1\n\n- Cap: 1\n\n- T-shirt: 1\n\n- Graffiti: 1\n\n- Trees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24417.8, "ram_available_mb": 38423.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24416.3, "ram_available_mb": 38424.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.655}, "power_stats": {"power_gpu_soc_mean_watts": 21.405, "power_cpu_cv_mean_watts": 1.827, "power_sys_5v0_mean_watts": 8.925, "gpu_utilization_percent_mean": 71.655, "power_watts_avg": 21.405, "energy_joules_est": 218.19, "duration_seconds": 10.193, "sample_count": 87}, "timestamp": "2026-01-25T16:33:19.240284"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10699.295, "latencies_ms": [10699.295], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, performing a trick on a skateboard. The background features a concrete surface with graffiti and trees, indicating an outdoor skate park setting. The skateboarder is positioned near the center of the image, with ample space around him, suggesting he is the main focus of the scene.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24416.3, "ram_available_mb": 38424.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24422.5, "ram_available_mb": 38418.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.121}, "power_stats": {"power_gpu_soc_mean_watts": 21.189, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 71.121, "power_watts_avg": 21.189, "energy_joules_est": 226.72, "duration_seconds": 10.7, "sample_count": 91}, "timestamp": "2026-01-25T16:33:31.959104"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7631.35, "latencies_ms": [7631.35], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "A person is skateboarding in an outdoor skate park, performing a trick on a red brick surface. The skate park is surrounded by trees and there are a few tents and people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.5, "ram_available_mb": 38418.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24431.1, "ram_available_mb": 38409.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.625}, "power_stats": {"power_gpu_soc_mean_watts": 22.601, "power_cpu_cv_mean_watts": 1.664, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 72.625, "power_watts_avg": 22.601, "energy_joules_est": 172.49, "duration_seconds": 7.632, "sample_count": 64}, "timestamp": "2026-01-25T16:33:41.624147"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7345.681, "latencies_ms": [7345.681], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows a person skateboarding on a concrete surface with a graffiti-covered wall in the background. The lighting is natural and appears to be overcast, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.1, "ram_available_mb": 38409.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.855}, "power_stats": {"power_gpu_soc_mean_watts": 22.569, "power_cpu_cv_mean_watts": 1.647, "power_sys_5v0_mean_watts": 9.069, "gpu_utilization_percent_mean": 72.855, "power_watts_avg": 22.569, "energy_joules_est": 165.8, "duration_seconds": 7.346, "sample_count": 62}, "timestamp": "2026-01-25T16:33:50.984907"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12075.249, "latencies_ms": [12075.249], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a white plate filled with a variety of vegetables, including carrots and peas. The carrots are spread out across the plate, with some of them overlapping each other. The peas are placed in the center of the plate, surrounded by the carrots. The plate is placed on a countertop, and there is a knife nearby, suggesting", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24429.5, "ram_available_mb": 38411.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.961}, "power_stats": {"power_gpu_soc_mean_watts": 22.683, "power_cpu_cv_mean_watts": 1.792, "power_sys_5v0_mean_watts": 9.178, "gpu_utilization_percent_mean": 72.961, "power_watts_avg": 22.683, "energy_joules_est": 273.92, "duration_seconds": 12.076, "sample_count": 103}, "timestamp": "2026-01-25T16:34:05.110521"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9429.003, "latencies_ms": [9429.003], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "carrots: 20, peas: 10, beets: 3, carrot peeler: 1, white bowl: 1, black container: 1, sink: 1, spoon: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24429.5, "ram_available_mb": 38411.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24433.1, "ram_available_mb": 38407.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.675}, "power_stats": {"power_gpu_soc_mean_watts": 23.622, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 9.125, "gpu_utilization_percent_mean": 75.675, "power_watts_avg": 23.622, "energy_joules_est": 222.75, "duration_seconds": 9.43, "sample_count": 80}, "timestamp": "2026-01-25T16:34:16.553727"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12064.401, "latencies_ms": [12064.401], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a white plate filled with orange carrots, which are the main focus of the image. To the right of the plate, there is a blue vegetable peeler, and further to the right, there are some green peas and a bunch of red beets. In the background, there is a white plastic container and a white cup", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.8, "ram_available_mb": 38408.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24428.3, "ram_available_mb": 38412.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.137}, "power_stats": {"power_gpu_soc_mean_watts": 22.658, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 9.168, "gpu_utilization_percent_mean": 73.137, "power_watts_avg": 22.658, "energy_joules_est": 273.37, "duration_seconds": 12.065, "sample_count": 102}, "timestamp": "2026-01-25T16:34:30.646027"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7422.953, "latencies_ms": [7422.953], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A white plate filled with orange carrots is placed on a kitchen countertop. There is a blue peeler and some green peas on the plate as well.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24428.3, "ram_available_mb": 38412.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24423.9, "ram_available_mb": 38417.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.274}, "power_stats": {"power_gpu_soc_mean_watts": 24.718, "power_cpu_cv_mean_watts": 1.311, "power_sys_5v0_mean_watts": 9.137, "gpu_utilization_percent_mean": 79.274, "power_watts_avg": 24.718, "energy_joules_est": 183.5, "duration_seconds": 7.424, "sample_count": 62}, "timestamp": "2026-01-25T16:34:40.096481"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8145.349, "latencies_ms": [8145.349], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image shows a bowl of orange carrots on a kitchen counter. The lighting in the image is bright and natural, coming from the left side, casting a shadow to the right of the objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.9, "ram_available_mb": 38417.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24434.1, "ram_available_mb": 38406.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_gpu_soc_mean_watts": 23.941, "power_cpu_cv_mean_watts": 1.497, "power_sys_5v0_mean_watts": 9.22, "gpu_utilization_percent_mean": 77.0, "power_watts_avg": 23.941, "energy_joules_est": 195.02, "duration_seconds": 8.146, "sample_count": 69}, "timestamp": "2026-01-25T16:34:50.296951"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10009.842, "latencies_ms": [10009.842], "images_per_second": 0.1, "prompt_tokens": 24, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the image, a man is standing on a stage, dressed in a suit and tie, and appears to be giving a speech or presentation. He is holding a microphone in his hand, and there are people in the audience watching him. The stage is well-lit, and the audience is attentively listening to the speaker.", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24434.1, "ram_available_mb": 38406.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24428.9, "ram_available_mb": 38412.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.282}, "power_stats": {"power_gpu_soc_mean_watts": 21.215, "power_cpu_cv_mean_watts": 1.86, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 70.282, "power_watts_avg": 21.215, "energy_joules_est": 212.37, "duration_seconds": 10.01, "sample_count": 85}, "timestamp": "2026-01-25T16:35:02.344705"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9265.472, "latencies_ms": [9265.472], "images_per_second": 0.108, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "1. Screen: 1\n2. Man: 1\n3. Microphone: 1\n4. Stage: 1\n5. Audience: 2\n6. Projector: 1\n7. Camera: 1\n8. Screen: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24428.9, "ram_available_mb": 38412.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24415.4, "ram_available_mb": 38425.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.241}, "power_stats": {"power_gpu_soc_mean_watts": 21.712, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 71.241, "power_watts_avg": 21.712, "energy_joules_est": 201.19, "duration_seconds": 9.266, "sample_count": 79}, "timestamp": "2026-01-25T16:35:13.639766"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9888.13, "latencies_ms": [9888.13], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, there is a person with red hair sitting in front of a large screen displaying a man in a suit. The man in the suit is positioned in the center of the screen, appearing to be giving a presentation. The audience is seated in front of the screen, with their attention directed towards the screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24415.4, "ram_available_mb": 38425.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.655}, "power_stats": {"power_gpu_soc_mean_watts": 21.329, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 9.007, "gpu_utilization_percent_mean": 70.655, "power_watts_avg": 21.329, "energy_joules_est": 210.92, "duration_seconds": 9.889, "sample_count": 84}, "timestamp": "2026-01-25T16:35:25.568832"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5605.49, "latencies_ms": [5605.49], "images_per_second": 0.178, "prompt_tokens": 37, "response_tokens_est": 28, "n_tiles": 16, "output_text": "A man is giving a presentation on a large screen in front of an audience, with a woman in the foreground watching intently.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24416.3, "ram_available_mb": 38424.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.638}, "power_stats": {"power_gpu_soc_mean_watts": 24.28, "power_cpu_cv_mean_watts": 1.337, "power_sys_5v0_mean_watts": 9.043, "gpu_utilization_percent_mean": 77.638, "power_watts_avg": 24.28, "energy_joules_est": 136.12, "duration_seconds": 5.606, "sample_count": 47}, "timestamp": "2026-01-25T16:35:33.213818"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7462.179, "latencies_ms": [7462.179], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a person on a stage with a blue background and a colorful geometric design on the right side. The stage lighting casts a warm glow on the speaker, highlighting their suit and gestures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24416.3, "ram_available_mb": 38424.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24418.4, "ram_available_mb": 38422.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.302}, "power_stats": {"power_gpu_soc_mean_watts": 21.827, "power_cpu_cv_mean_watts": 1.665, "power_sys_5v0_mean_watts": 9.025, "gpu_utilization_percent_mean": 72.302, "power_watts_avg": 21.827, "energy_joules_est": 162.89, "duration_seconds": 7.463, "sample_count": 63}, "timestamp": "2026-01-25T16:35:42.686831"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11093.843, "latencies_ms": [11093.843], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features two men sitting on a bench in a parking lot, with a motorcycle parked nearby. One man is wearing a blue shirt and the other is wearing a white shirt. They appear to be having a conversation while sitting on the bench. \n\nIn the background, there are several other motorcycles parked around the area, with one", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24418.4, "ram_available_mb": 38422.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24424.4, "ram_available_mb": 38416.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.585}, "power_stats": {"power_gpu_soc_mean_watts": 20.981, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 69.585, "power_watts_avg": 20.981, "energy_joules_est": 232.77, "duration_seconds": 11.094, "sample_count": 94}, "timestamp": "2026-01-25T16:35:55.857490"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9855.369, "latencies_ms": [9855.369], "images_per_second": 0.101, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "1. Motorcycles: 2\n2. Scooters: 2\n3. Bicycles: 0\n4. People: 2\n5. Chair: 1\n6. Pole: 1\n7. Sign: 1\n8. Broom: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24424.4, "ram_available_mb": 38416.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24421.1, "ram_available_mb": 38419.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.36}, "power_stats": {"power_gpu_soc_mean_watts": 21.474, "power_cpu_cv_mean_watts": 1.825, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 70.36, "power_watts_avg": 21.474, "energy_joules_est": 211.65, "duration_seconds": 9.856, "sample_count": 86}, "timestamp": "2026-01-25T16:36:07.747113"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11089.546, "latencies_ms": [11089.546], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a man sitting on a chair, facing another man who is standing and holding a shovel. The shovel is positioned near the man standing. In the background, there are two scooters parked on the side of the street, with one closer to the left side and the other slightly further to the right. There is also a building with", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24421.1, "ram_available_mb": 38419.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24423.7, "ram_available_mb": 38417.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.792}, "power_stats": {"power_gpu_soc_mean_watts": 20.958, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 69.792, "power_watts_avg": 20.958, "energy_joules_est": 232.43, "duration_seconds": 11.09, "sample_count": 96}, "timestamp": "2026-01-25T16:36:20.864350"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10392.548, "latencies_ms": [10392.548], "images_per_second": 0.096, "prompt_tokens": 37, "response_tokens_est": 71, "n_tiles": 16, "output_text": "Two men are sitting on a bench in front of a building with a blue and white sign that reads \"X \u5373\u515a\" and a large blue character on the left side of the building. There are two motorcycles parked on the sidewalk next to them, and a man is holding a shovel.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24423.7, "ram_available_mb": 38417.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24419.9, "ram_available_mb": 38421.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.261}, "power_stats": {"power_gpu_soc_mean_watts": 21.278, "power_cpu_cv_mean_watts": 1.851, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 70.261, "power_watts_avg": 21.278, "energy_joules_est": 221.15, "duration_seconds": 10.393, "sample_count": 88}, "timestamp": "2026-01-25T16:36:33.273166"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7597.931, "latencies_ms": [7597.931], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a street scene with a blue and white sign with a large letter \"L\" on it. There are two men sitting on a bench, one wearing a blue uniform and the other in casual clothes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.9, "ram_available_mb": 38421.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.667}, "power_stats": {"power_gpu_soc_mean_watts": 22.16, "power_cpu_cv_mean_watts": 1.665, "power_sys_5v0_mean_watts": 9.004, "gpu_utilization_percent_mean": 72.667, "power_watts_avg": 22.16, "energy_joules_est": 168.38, "duration_seconds": 7.599, "sample_count": 63}, "timestamp": "2026-01-25T16:36:42.907803"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10819.355, "latencies_ms": [10819.355], "images_per_second": 0.092, "prompt_tokens": 24, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows a black plastic container filled with a green, chunky vegetable dish, likely a curry, placed on a white paper plate. Beside it, there's a pile of shredded chicken. The plate is on a kitchen counter with a granite-like surface.", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24440.0, "ram_available_mb": 38400.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.835}, "power_stats": {"power_gpu_soc_mean_watts": 22.933, "power_cpu_cv_mean_watts": 1.689, "power_sys_5v0_mean_watts": 9.191, "gpu_utilization_percent_mean": 74.835, "power_watts_avg": 22.933, "energy_joules_est": 248.14, "duration_seconds": 10.82, "sample_count": 91}, "timestamp": "2026-01-25T16:36:55.769298"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9314.297, "latencies_ms": [9314.297], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "plate: 1, bowl: 1, fork: 1, chicken: 1, shredded chicken: 1, broccoli: 1, sauce: 1, container: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24440.0, "ram_available_mb": 38400.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24447.0, "ram_available_mb": 38393.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.39}, "power_stats": {"power_gpu_soc_mean_watts": 24.083, "power_cpu_cv_mean_watts": 1.502, "power_sys_5v0_mean_watts": 9.158, "gpu_utilization_percent_mean": 78.39, "power_watts_avg": 24.083, "energy_joules_est": 224.34, "duration_seconds": 9.315, "sample_count": 77}, "timestamp": "2026-01-25T16:37:07.113291"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12065.343, "latencies_ms": [12065.343], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground of the image, there is a pile of shredded chicken on the left side, which is closer to the viewer. Behind it, on the right side, there is a black container filled with a green sauce and chunks of vegetables. The container is placed on top of the plate, which is on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24447.0, "ram_available_mb": 38393.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24446.3, "ram_available_mb": 38394.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.971, "power_cpu_cv_mean_watts": 1.754, "power_sys_5v0_mean_watts": 9.212, "gpu_utilization_percent_mean": 74.0, "power_watts_avg": 22.971, "energy_joules_est": 277.17, "duration_seconds": 12.066, "sample_count": 102}, "timestamp": "2026-01-25T16:37:21.189841"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9546.184, "latencies_ms": [9546.184], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a meal consisting of shredded chicken and a green vegetable dish, possibly a curry, served on a white paper plate with a fork on the side. The meal is presented on a kitchen countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24446.3, "ram_available_mb": 38394.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24449.9, "ram_available_mb": 38390.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.062}, "power_stats": {"power_gpu_soc_mean_watts": 23.493, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 9.151, "gpu_utilization_percent_mean": 77.062, "power_watts_avg": 23.493, "energy_joules_est": 224.29, "duration_seconds": 9.547, "sample_count": 81}, "timestamp": "2026-01-25T16:37:32.748741"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10153.611, "latencies_ms": [10153.611], "images_per_second": 0.098, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a meal consisting of shredded chicken and a green sauce in a black container, placed on a white paper plate with a fork on the side. The lighting appears to be artificial, and the surface under the plate seems to be a kitchen countertop.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24449.9, "ram_available_mb": 38390.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.64}, "power_stats": {"power_gpu_soc_mean_watts": 23.548, "power_cpu_cv_mean_watts": 1.62, "power_sys_5v0_mean_watts": 9.244, "gpu_utilization_percent_mean": 75.64, "power_watts_avg": 23.548, "energy_joules_est": 239.11, "duration_seconds": 10.154, "sample_count": 86}, "timestamp": "2026-01-25T16:37:44.915095"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11125.163, "latencies_ms": [11125.163], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is the central figure, exuding an air of professionalism and confidence. He is dressed in a blue shirt and a red tie, a combination that suggests a formal or business setting. His head is adorned with a gray and white plaid hat, adding a touch of personal style to his otherwise formal attire. The hat is slightly tilted", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24450.2, "ram_available_mb": 38390.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.642}, "power_stats": {"power_gpu_soc_mean_watts": 20.925, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.642, "power_watts_avg": 20.925, "energy_joules_est": 232.81, "duration_seconds": 11.126, "sample_count": 95}, "timestamp": "2026-01-25T16:37:58.108981"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11283.041, "latencies_ms": [11283.041], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 80, "n_tiles": 16, "output_text": "1. Building: 1 visible in the background\n\n2. Chair: 1 visible in the background\n\n3. Man: 1\n\n4. Cap: 1\n\n5. Glasses: 1\n\n6. Shirt: 1\n\n7. Tie: 1\n\n8. Pool: 1 visible in the background", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24450.2, "ram_available_mb": 38390.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24431.8, "ram_available_mb": 38409.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.25}, "power_stats": {"power_gpu_soc_mean_watts": 21.094, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 70.25, "power_watts_avg": 21.094, "energy_joules_est": 238.02, "duration_seconds": 11.284, "sample_count": 96}, "timestamp": "2026-01-25T16:38:11.405524"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9440.487, "latencies_ms": [9440.487], "images_per_second": 0.106, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The man is positioned in the foreground of the image, appearing to be the main subject. He is standing in front of a building with windows, which serves as the background. The man is wearing a hat and glasses, and he is smiling, indicating a positive and friendly demeanor.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24431.8, "ram_available_mb": 38409.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24426.3, "ram_available_mb": 38414.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.42}, "power_stats": {"power_gpu_soc_mean_watts": 21.485, "power_cpu_cv_mean_watts": 1.839, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 70.42, "power_watts_avg": 21.485, "energy_joules_est": 202.84, "duration_seconds": 9.441, "sample_count": 81}, "timestamp": "2026-01-25T16:38:22.860183"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5953.742, "latencies_ms": [5953.742], "images_per_second": 0.168, "prompt_tokens": 37, "response_tokens_est": 31, "n_tiles": 16, "output_text": "A man is standing outdoors, wearing a blue shirt and a red tie. He is smiling and looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.3, "ram_available_mb": 38414.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24433.6, "ram_available_mb": 38407.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.26}, "power_stats": {"power_gpu_soc_mean_watts": 23.888, "power_cpu_cv_mean_watts": 1.425, "power_sys_5v0_mean_watts": 9.042, "gpu_utilization_percent_mean": 76.26, "power_watts_avg": 23.888, "energy_joules_est": 142.24, "duration_seconds": 5.954, "sample_count": 50}, "timestamp": "2026-01-25T16:38:30.845106"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7423.705, "latencies_ms": [7423.705], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image features a person wearing a blue shirt and a red tie, with a plaid hat on their head. The lighting appears to be natural, suggesting the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.6, "ram_available_mb": 38407.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24447.8, "ram_available_mb": 38393.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.73}, "power_stats": {"power_gpu_soc_mean_watts": 22.399, "power_cpu_cv_mean_watts": 1.671, "power_sys_5v0_mean_watts": 9.028, "gpu_utilization_percent_mean": 71.73, "power_watts_avg": 22.399, "energy_joules_est": 166.3, "duration_seconds": 7.424, "sample_count": 63}, "timestamp": "2026-01-25T16:38:40.326929"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12311.156, "latencies_ms": [12311.156], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a collage of six photographs showcasing a pizza with various toppings. The pizza appears to be a deep-dish style with a golden-brown crust. The toppings include melted cheese, tomato sauce, and possibly some herbs and spices. The pizza is cut into slices, and some slices are", "error": null, "sys_before": {"cpu_percent": 9.7, "ram_used_mb": 24447.8, "ram_available_mb": 38393.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.162}, "power_stats": {"power_gpu_soc_mean_watts": 22.832, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 9.197, "gpu_utilization_percent_mean": 74.162, "power_watts_avg": 22.832, "energy_joules_est": 281.1, "duration_seconds": 12.312, "sample_count": 105}, "timestamp": "2026-01-25T16:38:54.669055"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8801.186, "latencies_ms": [8801.186], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "cheese: 5, tomato: 3, crust: 4, plate: 5, fork: 2, bite: 1, slice: 4, crust: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24432.6, "ram_available_mb": 38408.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.907}, "power_stats": {"power_gpu_soc_mean_watts": 24.283, "power_cpu_cv_mean_watts": 1.468, "power_sys_5v0_mean_watts": 9.184, "gpu_utilization_percent_mean": 77.907, "power_watts_avg": 24.283, "energy_joules_est": 213.74, "duration_seconds": 8.802, "sample_count": 75}, "timestamp": "2026-01-25T16:39:05.519967"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12409.697, "latencies_ms": [12409.697], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The main object, which appears to be a slice of pizza, is positioned in the foreground of the image, occupying the central and lower parts of the frame. It is placed on a white plate, which is in the background. The pizza slice is the closest to the viewer, with other similar slices appearing progressively smaller and further away, creating a sense of depth", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.6, "ram_available_mb": 38408.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24439.6, "ram_available_mb": 38401.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.231}, "power_stats": {"power_gpu_soc_mean_watts": 22.856, "power_cpu_cv_mean_watts": 1.79, "power_sys_5v0_mean_watts": 9.211, "gpu_utilization_percent_mean": 74.231, "power_watts_avg": 22.856, "energy_joules_est": 283.65, "duration_seconds": 12.41, "sample_count": 108}, "timestamp": "2026-01-25T16:39:19.959246"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 12651.725, "latencies_ms": [12651.725], "images_per_second": 0.079, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a collage of six photographs showing a pizza with various toppings being eaten. The pizza appears to be freshly baked with a golden-brown crust, and the toppings include cheese, tomato sauce, and possibly some herbs and vegetables. The photographs are arranged in a 2x3 grid, with each photo showing", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.6, "ram_available_mb": 38401.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24439.6, "ram_available_mb": 38401.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.778}, "power_stats": {"power_gpu_soc_mean_watts": 22.956, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 9.142, "gpu_utilization_percent_mean": 73.778, "power_watts_avg": 22.956, "energy_joules_est": 290.45, "duration_seconds": 12.652, "sample_count": 108}, "timestamp": "2026-01-25T16:39:34.670955"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12401.199, "latencies_ms": [12401.199], "images_per_second": 0.081, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a collage of six photos showing different views of a pizza. The pizza appears to have a golden-brown crust, melted cheese, and various toppings that could include vegetables and possibly meats. The lighting in the photos highlights the textures and colors of the pizza, with the cheese looking particularly gooey and the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.6, "ram_available_mb": 38401.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24442.5, "ram_available_mb": 38398.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.895}, "power_stats": {"power_gpu_soc_mean_watts": 22.917, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 9.227, "gpu_utilization_percent_mean": 73.895, "power_watts_avg": 22.917, "energy_joules_est": 284.21, "duration_seconds": 12.402, "sample_count": 105}, "timestamp": "2026-01-25T16:39:49.113642"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11122.262, "latencies_ms": [11122.262], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two young girls are standing next to a black goat and a white goat in a fenced area. The girls are petting the goats, and one of them is holding a stick. The goats seem to be enjoying the attention from the children. The scene takes place in a grassy area, with the goats and the girls being the main focus of", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24442.5, "ram_available_mb": 38398.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24439.8, "ram_available_mb": 38401.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.464}, "power_stats": {"power_gpu_soc_mean_watts": 20.899, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 69.464, "power_watts_avg": 20.899, "energy_joules_est": 232.46, "duration_seconds": 11.123, "sample_count": 97}, "timestamp": "2026-01-25T16:40:02.309130"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9726.271, "latencies_ms": [9726.271], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. Girl: 2\n2. Goat: 2\n3. Fence: 1\n4. Grass: 1\n5. Dress: 2\n6. Pole: 1\n7. Strawberry: 1\n8. Trees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.8, "ram_available_mb": 38401.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.268}, "power_stats": {"power_gpu_soc_mean_watts": 21.609, "power_cpu_cv_mean_watts": 1.806, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 71.268, "power_watts_avg": 21.609, "energy_joules_est": 210.19, "duration_seconds": 9.727, "sample_count": 82}, "timestamp": "2026-01-25T16:40:14.054150"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11088.008, "latencies_ms": [11088.008], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a black goat with white patches on its head and neck, being petted by a child. The goat is positioned near the center of the image, close to the viewer. In the background, there are two children, one wearing a floral dress and the other in a blue dress with a pink bow, both standing behind", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24413.7, "ram_available_mb": 38427.2, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.884}, "power_stats": {"power_gpu_soc_mean_watts": 20.989, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 69.884, "power_watts_avg": 20.989, "energy_joules_est": 232.74, "duration_seconds": 11.089, "sample_count": 95}, "timestamp": "2026-01-25T16:40:27.183051"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5381.78, "latencies_ms": [5381.78], "images_per_second": 0.186, "prompt_tokens": 37, "response_tokens_est": 26, "n_tiles": 16, "output_text": "Two young girls are petting a black goat in a fenced area, likely at a petting zoo or farm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24413.7, "ram_available_mb": 38427.2, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24444.5, "ram_available_mb": 38396.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.467}, "power_stats": {"power_gpu_soc_mean_watts": 24.674, "power_cpu_cv_mean_watts": 1.317, "power_sys_5v0_mean_watts": 9.136, "gpu_utilization_percent_mean": 77.467, "power_watts_avg": 24.674, "energy_joules_est": 132.81, "duration_seconds": 5.383, "sample_count": 45}, "timestamp": "2026-01-25T16:40:34.615515"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11116.793, "latencies_ms": [11116.793], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features two young girls in colorful dresses, one in pink with a floral pattern and the other in blue with a strawberry pattern, standing in front of a black goat with white patches. The goat is being petted by one of the girls, and there is a white goat with black patches in the background. The setting appears to be", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24444.5, "ram_available_mb": 38396.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24416.3, "ram_available_mb": 38424.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.755}, "power_stats": {"power_gpu_soc_mean_watts": 20.951, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 69.755, "power_watts_avg": 20.951, "energy_joules_est": 232.92, "duration_seconds": 11.118, "sample_count": 94}, "timestamp": "2026-01-25T16:40:47.746904"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11120.057, "latencies_ms": [11120.057], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene evening scene at a traffic intersection. The sky, painted in a deep shade of blue, serves as a backdrop to the silhouette of a mountain range. The mountains, bathed in the soft glow of the setting sun, add a sense of tranquility to the scene.\n\nIn the foreground, a traffic light hangs from", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 24416.3, "ram_available_mb": 38424.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24417.7, "ram_available_mb": 38423.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.688}, "power_stats": {"power_gpu_soc_mean_watts": 20.894, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.688, "power_watts_avg": 20.894, "energy_joules_est": 232.36, "duration_seconds": 11.121, "sample_count": 96}, "timestamp": "2026-01-25T16:41:00.893586"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11288.273, "latencies_ms": [11288.273], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "traffic light: 2, street sign: 1, traffic light: 1, street sign: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24417.7, "ram_available_mb": 38423.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.031}, "power_stats": {"power_gpu_soc_mean_watts": 21.078, "power_cpu_cv_mean_watts": 1.881, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 70.031, "power_watts_avg": 21.078, "energy_joules_est": 237.95, "duration_seconds": 11.289, "sample_count": 96}, "timestamp": "2026-01-25T16:41:14.241666"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8017.535, "latencies_ms": [8017.535], "images_per_second": 0.125, "prompt_tokens": 44, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The traffic lights are positioned in the foreground on the right side of the image, while the sun is setting in the background on the left side. The street sign is located near the traffic lights, slightly above and to the right of them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.603}, "power_stats": {"power_gpu_soc_mean_watts": 22.031, "power_cpu_cv_mean_watts": 1.725, "power_sys_5v0_mean_watts": 9.019, "gpu_utilization_percent_mean": 71.603, "power_watts_avg": 22.031, "energy_joules_est": 176.65, "duration_seconds": 8.018, "sample_count": 68}, "timestamp": "2026-01-25T16:41:24.315105"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11286.427, "latencies_ms": [11286.427], "images_per_second": 0.089, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene evening at a traffic intersection, bathed in the soft glow of a setting sun. The sky, painted in hues of deep blue, serves as a stunning backdrop to the silhouette of distant mountains. The traffic lights, glowing in a vibrant green, guide the flow of vehicles, while a street sign reading \"BEL", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24437.3, "ram_available_mb": 38403.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.198}, "power_stats": {"power_gpu_soc_mean_watts": 20.957, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 8.928, "gpu_utilization_percent_mean": 70.198, "power_watts_avg": 20.957, "energy_joules_est": 236.54, "duration_seconds": 11.287, "sample_count": 96}, "timestamp": "2026-01-25T16:41:37.654890"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8423.135, "latencies_ms": [8423.135], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image captures a night scene with a clear sky, as evidenced by the visible stars. The traffic lights are illuminated, casting a green glow, and the street is bathed in a soft light from the traffic signal and street lamps.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.3, "ram_available_mb": 38403.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24536.4, "ram_available_mb": 38304.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.708}, "power_stats": {"power_gpu_soc_mean_watts": 21.78, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 9.03, "gpu_utilization_percent_mean": 71.708, "power_watts_avg": 21.78, "energy_joules_est": 183.47, "duration_seconds": 8.424, "sample_count": 72}, "timestamp": "2026-01-25T16:41:48.118462"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 8887.942, "latencies_ms": [8887.942], "images_per_second": 0.113, "prompt_tokens": 24, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a smiling woman standing behind a table laden with bunches of bananas. She is wearing a patterned dress and is in a building with a yellow wall and a sign that reads 'BANANAS'. The bananas are yellow and appear ripe.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24414.4, "ram_available_mb": 38426.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24439.3, "ram_available_mb": 38401.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.467}, "power_stats": {"power_gpu_soc_mean_watts": 21.775, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 9.004, "gpu_utilization_percent_mean": 71.467, "power_watts_avg": 21.775, "energy_joules_est": 193.55, "duration_seconds": 8.889, "sample_count": 75}, "timestamp": "2026-01-25T16:41:59.053270"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8948.047, "latencies_ms": [8948.047], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 58, "n_tiles": 16, "output_text": "bananas: 2 bunches, approximately 20-30 bananas each\nwoman: 1\nbuilding: 1\nred sign: 1\nblue sign: 1\nwindow: 1\ndoor: 1\nother people: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.3, "ram_available_mb": 38401.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24444.2, "ram_available_mb": 38396.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.208}, "power_stats": {"power_gpu_soc_mean_watts": 21.832, "power_cpu_cv_mean_watts": 1.762, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 71.208, "power_watts_avg": 21.832, "energy_joules_est": 195.37, "duration_seconds": 8.949, "sample_count": 77}, "timestamp": "2026-01-25T16:42:10.019789"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8907.946, "latencies_ms": [8907.946], "images_per_second": 0.112, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "In the foreground, there is a large bunch of bananas placed on a wooden surface. To the left of the bananas, there is a building with a red circular sign above the entrance. In the background, there is another individual standing near the doorway of the same building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24444.2, "ram_available_mb": 38396.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.013}, "power_stats": {"power_gpu_soc_mean_watts": 21.679, "power_cpu_cv_mean_watts": 1.804, "power_sys_5v0_mean_watts": 9.062, "gpu_utilization_percent_mean": 71.013, "power_watts_avg": 21.679, "energy_joules_est": 193.13, "duration_seconds": 8.909, "sample_count": 75}, "timestamp": "2026-01-25T16:42:20.970444"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9069.759, "latencies_ms": [9069.759], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "A woman is standing behind a table laden with bunches of bananas, likely at a market or a stall. The background shows a building with a sign that reads \"BANANAS\", suggesting that this is a place where bananas are sold or a themed establishment.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.494}, "power_stats": {"power_gpu_soc_mean_watts": 21.807, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 71.494, "power_watts_avg": 21.807, "energy_joules_est": 197.8, "duration_seconds": 9.07, "sample_count": 77}, "timestamp": "2026-01-25T16:42:32.092790"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10644.689, "latencies_ms": [10644.689], "images_per_second": 0.094, "prompt_tokens": 36, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The image features a vibrant yellow banana bunch in sharp focus against a blurred background, with natural lighting highlighting the texture of the bananas and the patterned fabric of the person's clothing. The background shows a building with a rough texture and a mix of warm and cool tones, suggesting an outdoor setting with natural light.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24421.3, "ram_available_mb": 38419.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.533}, "power_stats": {"power_gpu_soc_mean_watts": 21.052, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 69.533, "power_watts_avg": 21.052, "energy_joules_est": 224.1, "duration_seconds": 10.645, "sample_count": 90}, "timestamp": "2026-01-25T16:42:44.776111"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12307.923, "latencies_ms": [12307.923], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene on a city street. Dominating the view is a three-story brick building, its facade a vibrant green. The building is adorned with a fire escape on the second floor, a common sight in urban architecture. The ground floor features a green door, above which hang two closed garage doors, their surfaces a canvas for graffiti.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24421.3, "ram_available_mb": 38419.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.808}, "power_stats": {"power_gpu_soc_mean_watts": 22.884, "power_cpu_cv_mean_watts": 1.778, "power_sys_5v0_mean_watts": 9.213, "gpu_utilization_percent_mean": 73.808, "power_watts_avg": 22.884, "energy_joules_est": 281.67, "duration_seconds": 12.309, "sample_count": 104}, "timestamp": "2026-01-25T16:42:59.152076"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10974.877, "latencies_ms": [10974.877], "images_per_second": 0.091, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "- Fire hydrant: 1\n\n- Door: 2\n\n- Graffiti: 10\n\n- Windows: 4\n\n- Bike: 1\n\n- Tree: 1\n\n- Trash can: 1\n\n- Sign: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24439.3, "ram_available_mb": 38401.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_gpu_soc_mean_watts": 23.438, "power_cpu_cv_mean_watts": 1.636, "power_sys_5v0_mean_watts": 9.154, "gpu_utilization_percent_mean": 76.0, "power_watts_avg": 23.438, "energy_joules_est": 257.24, "duration_seconds": 10.975, "sample_count": 93}, "timestamp": "2026-01-25T16:43:12.187362"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11735.199, "latencies_ms": [11735.199], "images_per_second": 0.085, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The fire hydrant is located in the foreground on the left side of the image, near the sidewalk. The building with graffiti is in the background, behind the fire hydrant. The green door is on the right side of the building, and the graffiti is spread across the garage doors on both sides of the green door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.3, "ram_available_mb": 38401.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24443.3, "ram_available_mb": 38397.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.667}, "power_stats": {"power_gpu_soc_mean_watts": 23.108, "power_cpu_cv_mean_watts": 1.723, "power_sys_5v0_mean_watts": 9.218, "gpu_utilization_percent_mean": 74.667, "power_watts_avg": 23.108, "energy_joules_est": 271.19, "duration_seconds": 11.736, "sample_count": 99}, "timestamp": "2026-01-25T16:43:25.942425"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8725.904, "latencies_ms": [8725.904], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image depicts a red brick building with a green awning and a fire hydrant on the sidewalk in front of it. The building has multiple windows and a green fire escape on the second floor.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24443.3, "ram_available_mb": 38397.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24438.5, "ram_available_mb": 38402.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.493}, "power_stats": {"power_gpu_soc_mean_watts": 24.32, "power_cpu_cv_mean_watts": 1.437, "power_sys_5v0_mean_watts": 9.18, "gpu_utilization_percent_mean": 78.493, "power_watts_avg": 24.32, "energy_joules_est": 212.23, "duration_seconds": 8.727, "sample_count": 73}, "timestamp": "2026-01-25T16:43:36.690366"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8684.271, "latencies_ms": [8684.271], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a red brick building with a green awning and a fire escape on the second floor. The garage doors on the ground floor are covered in various graffiti tags in blue, red, and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.5, "ram_available_mb": 38402.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24431.2, "ram_available_mb": 38409.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.26}, "power_stats": {"power_gpu_soc_mean_watts": 24.109, "power_cpu_cv_mean_watts": 1.492, "power_sys_5v0_mean_watts": 9.255, "gpu_utilization_percent_mean": 77.26, "power_watts_avg": 24.109, "energy_joules_est": 209.39, "duration_seconds": 8.685, "sample_count": 73}, "timestamp": "2026-01-25T16:43:47.412137"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11089.168, "latencies_ms": [11089.168], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man with long, curly hair is the main focus. He is wearing a black shirt and a blue and white striped beanie. The man is holding a bright yellow frisbee in his right hand, preparing to throw it. The background is a simple, unadorned wall. The man's expression is serious, suggesting concentration or", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24431.2, "ram_available_mb": 38409.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.469}, "power_stats": {"power_gpu_soc_mean_watts": 21.022, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 69.469, "power_watts_avg": 21.022, "energy_joules_est": 233.13, "duration_seconds": 11.09, "sample_count": 96}, "timestamp": "2026-01-25T16:44:00.542942"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7754.491, "latencies_ms": [7754.491], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "person: 1, camera: 1, yellow frisbee: 1, wall: 1, room: 1, ceiling: 1, light fixture: 1, audience: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24417.6, "ram_available_mb": 38423.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.908}, "power_stats": {"power_gpu_soc_mean_watts": 22.167, "power_cpu_cv_mean_watts": 1.657, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 72.908, "power_watts_avg": 22.167, "energy_joules_est": 171.91, "duration_seconds": 7.755, "sample_count": 65}, "timestamp": "2026-01-25T16:44:10.320742"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11111.68, "latencies_ms": [11111.68], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The person in the foreground is holding a yellow frisbee with their right hand, extending it towards the camera, while the background shows a group of people sitting and standing, with some facing the camera and others turned away. The person with the frisbee is positioned in the center of the image, making them the main focus, with the audience members behind them appearing smaller and", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24417.6, "ram_available_mb": 38423.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24426.5, "ram_available_mb": 38414.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.649}, "power_stats": {"power_gpu_soc_mean_watts": 20.907, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 69.649, "power_watts_avg": 20.907, "energy_joules_est": 232.33, "duration_seconds": 11.112, "sample_count": 97}, "timestamp": "2026-01-25T16:44:23.457838"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7707.874, "latencies_ms": [7707.874], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A person with long curly hair is holding a neon green frisbee in a dimly lit indoor setting, possibly a room or a hallway. The person's face is not visible in the image.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24426.5, "ram_available_mb": 38414.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24421.0, "ram_available_mb": 38419.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.554}, "power_stats": {"power_gpu_soc_mean_watts": 22.451, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 72.554, "power_watts_avg": 22.451, "energy_joules_est": 173.06, "duration_seconds": 7.709, "sample_count": 65}, "timestamp": "2026-01-25T16:44:33.195124"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8548.433, "latencies_ms": [8548.433], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image features a person with long, curly hair wearing a black shirt and a blue and white striped headband. The person is holding a bright yellow frisbee in a dimly lit indoor setting with a crowd of people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.0, "ram_available_mb": 38419.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.417}, "power_stats": {"power_gpu_soc_mean_watts": 21.814, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 9.041, "gpu_utilization_percent_mean": 71.417, "power_watts_avg": 21.814, "energy_joules_est": 186.49, "duration_seconds": 8.549, "sample_count": 72}, "timestamp": "2026-01-25T16:44:43.771793"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11131.437, "latencies_ms": [11131.437], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people sitting around a dining table, each with their own laptop open in front of them. There are at least five people visible, with some sitting closer to the left side of the table and others on the right side. The laptops are placed on the table, and the people seem to be engaged in their work or activities.\n\nIn addition to the", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24438.2, "ram_available_mb": 38402.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.979}, "power_stats": {"power_gpu_soc_mean_watts": 20.96, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 69.979, "power_watts_avg": 20.96, "energy_joules_est": 233.33, "duration_seconds": 11.132, "sample_count": 94}, "timestamp": "2026-01-25T16:44:56.930861"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9302.655, "latencies_ms": [9302.655], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Laptop: 3\n\n- Computer mouse: 2\n\n- Keyboard: 2\n\n- Computer monitor: 2\n\n- Laptop screen: 2\n\n- Chair: 2\n\n- Table: 2\n\n- Person: 5", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24438.2, "ram_available_mb": 38402.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24421.0, "ram_available_mb": 38419.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.316}, "power_stats": {"power_gpu_soc_mean_watts": 21.73, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 72.316, "power_watts_avg": 21.73, "energy_joules_est": 202.16, "duration_seconds": 9.303, "sample_count": 79}, "timestamp": "2026-01-25T16:45:08.256680"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10510.457, "latencies_ms": [10510.457], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, there is a laptop with a blurred screen on the left side, and a clear laptop screen in the center. In the background, there are multiple people seated around tables, with some facing the camera and others turned away. The tables are arranged in a way that suggests a communal workspace or a caf\u00e9 setting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24421.0, "ram_available_mb": 38419.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24424.9, "ram_available_mb": 38416.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.852}, "power_stats": {"power_gpu_soc_mean_watts": 21.079, "power_cpu_cv_mean_watts": 1.874, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.852, "power_watts_avg": 21.079, "energy_joules_est": 221.56, "duration_seconds": 10.511, "sample_count": 88}, "timestamp": "2026-01-25T16:45:20.796687"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8417.211, "latencies_ms": [8417.211], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image depicts a group of people sitting around a table in a casual setting, possibly a cafe or a restaurant, working on their laptops. The atmosphere appears to be relaxed and collaborative, with people engaged in their tasks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24424.9, "ram_available_mb": 38416.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24422.2, "ram_available_mb": 38418.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.722}, "power_stats": {"power_gpu_soc_mean_watts": 22.075, "power_cpu_cv_mean_watts": 1.718, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 72.722, "power_watts_avg": 22.075, "energy_joules_est": 185.82, "duration_seconds": 8.418, "sample_count": 72}, "timestamp": "2026-01-25T16:45:31.239483"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7432.636, "latencies_ms": [7432.636], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image is taken indoors with warm lighting, likely from overhead fixtures. Various objects are scattered on the table, including a laptop, a mug, and a bottle of ketchup.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.2, "ram_available_mb": 38418.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24443.0, "ram_available_mb": 38397.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.19}, "power_stats": {"power_gpu_soc_mean_watts": 22.356, "power_cpu_cv_mean_watts": 1.671, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 72.19, "power_watts_avg": 22.356, "energy_joules_est": 166.18, "duration_seconds": 7.433, "sample_count": 63}, "timestamp": "2026-01-25T16:45:40.727884"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12287.989, "latencies_ms": [12287.989], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young girl is the main subject. She is standing on a sidewalk, which is made of concrete. The girl is holding a blue umbrella with a wooden handle. The umbrella is open, providing a canopy over her. She is dressed in a pink jacket and blue jeans, adding a pop of color to the scene. The background is", "error": null, "sys_before": {"cpu_percent": 6.1, "ram_used_mb": 24443.0, "ram_available_mb": 38397.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24445.6, "ram_available_mb": 38395.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.548}, "power_stats": {"power_gpu_soc_mean_watts": 22.908, "power_cpu_cv_mean_watts": 1.79, "power_sys_5v0_mean_watts": 9.218, "gpu_utilization_percent_mean": 73.548, "power_watts_avg": 22.908, "energy_joules_est": 281.51, "duration_seconds": 12.289, "sample_count": 104}, "timestamp": "2026-01-25T16:45:55.065842"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9397.746, "latencies_ms": [9397.746], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "umbrella: 1, girl: 1, pink jacket: 1, blue jeans: 1, brown purse: 1, sandals: 1, ground: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24445.6, "ram_available_mb": 38395.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24444.1, "ram_available_mb": 38396.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.8}, "power_stats": {"power_gpu_soc_mean_watts": 23.965, "power_cpu_cv_mean_watts": 1.522, "power_sys_5v0_mean_watts": 9.172, "gpu_utilization_percent_mean": 76.8, "power_watts_avg": 23.965, "energy_joules_est": 225.23, "duration_seconds": 9.398, "sample_count": 80}, "timestamp": "2026-01-25T16:46:06.501370"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12415.933, "latencies_ms": [12415.933], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The girl is standing in the foreground holding a blue umbrella with her right hand, which is positioned near the top of the umbrella. The umbrella is open and covers a significant portion of the background, which appears to be a paved area. The girl is facing towards the camera, and her body is slightly angled to the left, creating a sense of depth", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24444.1, "ram_available_mb": 38396.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24445.8, "ram_available_mb": 38395.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.736}, "power_stats": {"power_gpu_soc_mean_watts": 22.891, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 9.191, "gpu_utilization_percent_mean": 73.736, "power_watts_avg": 22.891, "energy_joules_est": 284.23, "duration_seconds": 12.417, "sample_count": 106}, "timestamp": "2026-01-25T16:46:20.980178"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9305.701, "latencies_ms": [9305.701], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A young girl is standing under a blue umbrella, likely to protect herself from rain or sun. She is wearing a pink jacket and blue jeans, and appears to be outdoors on a sunny day.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24445.8, "ram_available_mb": 38395.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24450.6, "ram_available_mb": 38390.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.862}, "power_stats": {"power_gpu_soc_mean_watts": 23.998, "power_cpu_cv_mean_watts": 1.496, "power_sys_5v0_mean_watts": 9.15, "gpu_utilization_percent_mean": 77.862, "power_watts_avg": 23.998, "energy_joules_est": 223.33, "duration_seconds": 9.306, "sample_count": 80}, "timestamp": "2026-01-25T16:46:32.338850"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7013.521, "latencies_ms": [7013.521], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 31, "n_tiles": 16, "output_text": "A young child is holding a blue umbrella with a wooden handle. The child is wearing a pink jacket and blue jeans.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24450.6, "ram_available_mb": 38390.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24444.9, "ram_available_mb": 38396.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.61}, "power_stats": {"power_gpu_soc_mean_watts": 25.173, "power_cpu_cv_mean_watts": 1.289, "power_sys_5v0_mean_watts": 9.305, "gpu_utilization_percent_mean": 79.61, "power_watts_avg": 25.173, "energy_joules_est": 176.57, "duration_seconds": 7.014, "sample_count": 59}, "timestamp": "2026-01-25T16:46:41.373133"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11098.698, "latencies_ms": [11098.698], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man in a suit is standing in front of a desk with a computer. The man is wearing a tie and appears to be a businessman or a professional. The desk has a computer monitor, keyboard, and mouse, indicating that it is a workspace. There is a window in the background, providing natural light to the room.\n\nIn addition to", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24444.9, "ram_available_mb": 38396.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24420.4, "ram_available_mb": 38420.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.589}, "power_stats": {"power_gpu_soc_mean_watts": 20.933, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.589, "power_watts_avg": 20.933, "energy_joules_est": 232.34, "duration_seconds": 11.099, "sample_count": 95}, "timestamp": "2026-01-25T16:46:54.510364"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11281.466, "latencies_ms": [11281.466], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 79, "n_tiles": 16, "output_text": "man: 1, suit: 1, tie: 1, shirt: 1, jacket: 1, tie: 1, chair: 2, window: 1, computer: 1, monitor: 1, keyboard: 1, mouse: 1, desk: 2, outlet: 1, wall: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24420.4, "ram_available_mb": 38420.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24412.5, "ram_available_mb": 38428.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.146}, "power_stats": {"power_gpu_soc_mean_watts": 21.11, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.925, "gpu_utilization_percent_mean": 71.146, "power_watts_avg": 21.11, "energy_joules_est": 238.16, "duration_seconds": 11.282, "sample_count": 96}, "timestamp": "2026-01-25T16:47:07.831234"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11103.657, "latencies_ms": [11103.657], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, there is a man standing in front of a desk with a computer and a monitor. The desk is located near a window in the background, which has a reflection of a person and a chair visible on it. The man is standing to the left of the desk, and the computer and monitor are positioned on the right side of the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24412.5, "ram_available_mb": 38428.4, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.779}, "power_stats": {"power_gpu_soc_mean_watts": 21.008, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.779, "power_watts_avg": 21.008, "energy_joules_est": 233.28, "duration_seconds": 11.104, "sample_count": 95}, "timestamp": "2026-01-25T16:47:20.955969"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8174.076, "latencies_ms": [8174.076], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A man in a suit stands in front of a desk with a computer and a monitor displaying an image of a room with a couch and a chair. The room appears to be an office or a conference room with a window in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24413.7, "ram_available_mb": 38427.2, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.884}, "power_stats": {"power_gpu_soc_mean_watts": 22.127, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 71.884, "power_watts_avg": 22.127, "energy_joules_est": 180.89, "duration_seconds": 8.175, "sample_count": 69}, "timestamp": "2026-01-25T16:47:31.168272"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8292.258, "latencies_ms": [8292.258], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The room has a neutral color scheme with white walls and a light-colored carpet. The lighting appears to be artificial, coming from ceiling fixtures, and the room contains a wooden desk with a computer and a monitor displaying an image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24413.7, "ram_available_mb": 38427.2, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24414.4, "ram_available_mb": 38426.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.056}, "power_stats": {"power_gpu_soc_mean_watts": 21.892, "power_cpu_cv_mean_watts": 1.759, "power_sys_5v0_mean_watts": 9.023, "gpu_utilization_percent_mean": 71.056, "power_watts_avg": 21.892, "energy_joules_est": 181.55, "duration_seconds": 8.293, "sample_count": 71}, "timestamp": "2026-01-25T16:47:41.501025"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11104.807, "latencies_ms": [11104.807], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of four men sitting around a wooden dining table, enjoying a meal together. They are all dressed in casual clothing, and the atmosphere appears to be relaxed and friendly. The table is filled with various dishes, including bowls, cups, and bottles, as well as utensils like forks and knives.\n\nThere", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24414.4, "ram_available_mb": 38426.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24439.1, "ram_available_mb": 38401.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.532}, "power_stats": {"power_gpu_soc_mean_watts": 20.962, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 69.532, "power_watts_avg": 20.962, "energy_joules_est": 232.79, "duration_seconds": 11.106, "sample_count": 94}, "timestamp": "2026-01-25T16:47:54.631048"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8178.703, "latencies_ms": [8178.703], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "table: 1\ncups: 4\nbowls: 3\nplates: 5\nsugar packets: 2\nsalt packets: 2\nmugs: 2\ncandies: 1\n", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24439.1, "ram_available_mb": 38401.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24431.5, "ram_available_mb": 38409.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.754}, "power_stats": {"power_gpu_soc_mean_watts": 22.238, "power_cpu_cv_mean_watts": 1.688, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 72.754, "power_watts_avg": 22.238, "energy_joules_est": 181.89, "duration_seconds": 8.179, "sample_count": 69}, "timestamp": "2026-01-25T16:48:04.829529"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11091.849, "latencies_ms": [11091.849], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden table with various items on it, including bowls, cups, and a bottle, indicating a meal is being shared. In the background, there are four individuals seated around the table, with one person on the left side, two in the center, and one on the right side. The table is positioned in the middle of the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.5, "ram_available_mb": 38409.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24421.7, "ram_available_mb": 38419.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.521}, "power_stats": {"power_gpu_soc_mean_watts": 21.045, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 69.521, "power_watts_avg": 21.045, "energy_joules_est": 233.45, "duration_seconds": 11.093, "sample_count": 94}, "timestamp": "2026-01-25T16:48:17.970224"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7731.215, "latencies_ms": [7731.215], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A group of people are gathered around a wooden table, enjoying a meal together. The table is set with various dishes, cups, and utensils, and there is a clock on the wall behind them.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24421.7, "ram_available_mb": 38419.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24419.3, "ram_available_mb": 38421.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.446}, "power_stats": {"power_gpu_soc_mean_watts": 22.532, "power_cpu_cv_mean_watts": 1.638, "power_sys_5v0_mean_watts": 9.016, "gpu_utilization_percent_mean": 73.446, "power_watts_avg": 22.532, "energy_joules_est": 174.21, "duration_seconds": 7.732, "sample_count": 65}, "timestamp": "2026-01-25T16:48:27.727253"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6761.016, "latencies_ms": [6761.016], "images_per_second": 0.148, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The room has a warm and cozy atmosphere with wooden walls and a wooden ceiling. The lighting is soft and natural, coming from the large window on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.3, "ram_available_mb": 38421.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24435.2, "ram_available_mb": 38405.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.719}, "power_stats": {"power_gpu_soc_mean_watts": 22.856, "power_cpu_cv_mean_watts": 1.594, "power_sys_5v0_mean_watts": 9.079, "gpu_utilization_percent_mean": 72.719, "power_watts_avg": 22.856, "energy_joules_est": 154.54, "duration_seconds": 6.762, "sample_count": 57}, "timestamp": "2026-01-25T16:48:36.522680"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11109.654, "latencies_ms": [11109.654], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a red pickup truck is parked on a snowy street, with a person standing next to it. The truck is equipped with a snow plow, which is attached to the front of the vehicle. The person appears to be either preparing to or has just finished using the plow to clear the snow from the street.\n\nThere are several houses", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24435.2, "ram_available_mb": 38405.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24421.2, "ram_available_mb": 38419.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.271}, "power_stats": {"power_gpu_soc_mean_watts": 20.897, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 69.271, "power_watts_avg": 20.897, "energy_joules_est": 232.17, "duration_seconds": 11.11, "sample_count": 96}, "timestamp": "2026-01-25T16:48:49.665215"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7425.116, "latencies_ms": [7425.116], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "house: 3, truck: 1, snow plow: 1, person: 1, tree: 4, house: 2, snow: 1, truck: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.2, "ram_available_mb": 38419.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24423.7, "ram_available_mb": 38417.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.095}, "power_stats": {"power_gpu_soc_mean_watts": 22.501, "power_cpu_cv_mean_watts": 1.62, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 73.095, "power_watts_avg": 22.501, "energy_joules_est": 167.09, "duration_seconds": 7.426, "sample_count": 63}, "timestamp": "2026-01-25T16:48:59.120544"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9315.161, "latencies_ms": [9315.161], "images_per_second": 0.107, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "A red pickup truck is in the foreground, positioned on the right side of the image, moving towards the left. The truck is closer to the viewer than the houses in the background. A person is standing behind the truck, partially obscured by the snow plow attachment.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24423.7, "ram_available_mb": 38417.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24418.9, "ram_available_mb": 38422.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.797}, "power_stats": {"power_gpu_soc_mean_watts": 21.562, "power_cpu_cv_mean_watts": 1.819, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 70.797, "power_watts_avg": 21.562, "energy_joules_est": 200.87, "duration_seconds": 9.316, "sample_count": 79}, "timestamp": "2026-01-25T16:49:10.453524"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6635.781, "latencies_ms": [6635.781], "images_per_second": 0.151, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A red pickup truck is being used to clear snow from a residential street. A person is standing next to the truck, assisting in the snow removal process.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24418.9, "ram_available_mb": 38422.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24415.2, "ram_available_mb": 38425.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.464}, "power_stats": {"power_gpu_soc_mean_watts": 23.226, "power_cpu_cv_mean_watts": 1.523, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 75.464, "power_watts_avg": 23.226, "energy_joules_est": 154.14, "duration_seconds": 6.636, "sample_count": 56}, "timestamp": "2026-01-25T16:49:19.111113"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7189.025, "latencies_ms": [7189.025], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A red pickup truck is being used to clear snow from a residential street. The truck has a snowplow attached to the front and is being operated by a person in a yellow jacket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24415.2, "ram_available_mb": 38425.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24411.5, "ram_available_mb": 38429.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.623}, "power_stats": {"power_gpu_soc_mean_watts": 22.568, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 9.054, "gpu_utilization_percent_mean": 72.623, "power_watts_avg": 22.568, "energy_joules_est": 162.26, "duration_seconds": 7.19, "sample_count": 61}, "timestamp": "2026-01-25T16:49:28.351653"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12323.978, "latencies_ms": [12323.978], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a luxurious bathroom. The room is bathed in soft light, casting a warm glow on the beige walls and marble countertop. A large mirror dominates the wall, reflecting the room's opulence. Above the mirror, a television is mounted, adding a modern touch to the space.\n\nThe countertop", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 24411.5, "ram_available_mb": 38429.4, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24443.9, "ram_available_mb": 38397.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.274}, "power_stats": {"power_gpu_soc_mean_watts": 22.821, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 9.194, "gpu_utilization_percent_mean": 74.274, "power_watts_avg": 22.821, "energy_joules_est": 281.26, "duration_seconds": 12.325, "sample_count": 106}, "timestamp": "2026-01-25T16:49:42.718136"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8937.967, "latencies_ms": [8937.967], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "mirror: 1\ncamera: 1\nlight fixture: 4\ntowel: 3\nfaucet: 2\nbucket: 1\ntile: 1\nmat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24443.9, "ram_available_mb": 38397.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24436.2, "ram_available_mb": 38404.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.675}, "power_stats": {"power_gpu_soc_mean_watts": 24.172, "power_cpu_cv_mean_watts": 1.487, "power_sys_5v0_mean_watts": 9.165, "gpu_utilization_percent_mean": 78.675, "power_watts_avg": 24.172, "energy_joules_est": 216.06, "duration_seconds": 8.939, "sample_count": 77}, "timestamp": "2026-01-25T16:49:53.671160"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9937.364, "latencies_ms": [9937.364], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The main objects in the image are the bathroom counter, sink, mirror, and the person taking a photo. The counter and sink are in the foreground, while the mirror and person are in the background. The person is standing near the mirror, capturing their reflection.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.2, "ram_available_mb": 38404.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24450.4, "ram_available_mb": 38390.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.81}, "power_stats": {"power_gpu_soc_mean_watts": 23.583, "power_cpu_cv_mean_watts": 1.625, "power_sys_5v0_mean_watts": 9.24, "gpu_utilization_percent_mean": 75.81, "power_watts_avg": 23.583, "energy_joules_est": 234.37, "duration_seconds": 9.938, "sample_count": 84}, "timestamp": "2026-01-25T16:50:05.627751"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9175.964, "latencies_ms": [9175.964], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A person is taking a photo of a luxurious bathroom with a large mirror, double sinks, and a bathtub. The bathroom is well-lit with multiple light fixtures and has a modern design.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24450.4, "ram_available_mb": 38390.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24439.3, "ram_available_mb": 38401.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.228}, "power_stats": {"power_gpu_soc_mean_watts": 24.062, "power_cpu_cv_mean_watts": 1.5, "power_sys_5v0_mean_watts": 9.133, "gpu_utilization_percent_mean": 76.228, "power_watts_avg": 24.062, "energy_joules_est": 220.81, "duration_seconds": 9.177, "sample_count": 79}, "timestamp": "2026-01-25T16:50:16.846374"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9324.083, "latencies_ms": [9324.083], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The bathroom features a large mirror with a dark frame, and the countertop is made of marble with gold faucets. The lighting is warm and ambient, with multiple sconces on the walls and a television mounted above the mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.3, "ram_available_mb": 38401.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.025}, "power_stats": {"power_gpu_soc_mean_watts": 23.811, "power_cpu_cv_mean_watts": 1.571, "power_sys_5v0_mean_watts": 9.254, "gpu_utilization_percent_mean": 76.025, "power_watts_avg": 23.811, "energy_joules_est": 222.04, "duration_seconds": 9.325, "sample_count": 80}, "timestamp": "2026-01-25T16:50:28.232580"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11110.49, "latencies_ms": [11110.49], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a busy airport scene with several people standing around, some of them loading luggage into a white car. There are at least three people in the scene, with one man standing near a cart filled with luggage, another man standing close to the car, and a third person standing further back. \n\nThere are multiple suitcases scattered around the area,", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24414.4, "ram_available_mb": 38426.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.474}, "power_stats": {"power_gpu_soc_mean_watts": 20.925, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 69.474, "power_watts_avg": 20.925, "energy_joules_est": 232.51, "duration_seconds": 11.111, "sample_count": 95}, "timestamp": "2026-01-25T16:50:41.395865"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7725.416, "latencies_ms": [7725.416], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "cart: 1\nman: 2\nsuitcase: 3\ncar: 1\nlicense plate: 1\ndenim: 1\nshoes: 1\nbags: 2", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24414.4, "ram_available_mb": 38426.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24414.3, "ram_available_mb": 38426.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.231}, "power_stats": {"power_gpu_soc_mean_watts": 22.504, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 72.231, "power_watts_avg": 22.504, "energy_joules_est": 173.87, "duration_seconds": 7.726, "sample_count": 65}, "timestamp": "2026-01-25T16:50:51.151522"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11110.614, "latencies_ms": [11110.614], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, a man is standing and appears to be in the process of loading luggage into a white SUV parked in the background. Another man is standing nearby, also with luggage, and a luggage cart is positioned to the left of the scene. The background features a sign indicating the direction to the Short Bus to McCorman Airport.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24414.3, "ram_available_mb": 38426.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24417.4, "ram_available_mb": 38423.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.695}, "power_stats": {"power_gpu_soc_mean_watts": 20.991, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.695, "power_watts_avg": 20.991, "energy_joules_est": 233.24, "duration_seconds": 11.111, "sample_count": 95}, "timestamp": "2026-01-25T16:51:04.320688"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9293.247, "latencies_ms": [9293.247], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 61, "n_tiles": 16, "output_text": "In the image, two men are unloading luggage from a white car in a parking garage. The car is parked in a designated parking spot, and the men are standing on the concrete floor, with one man holding a suitcase and the other holding a handbag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24417.4, "ram_available_mb": 38423.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.513}, "power_stats": {"power_gpu_soc_mean_watts": 21.624, "power_cpu_cv_mean_watts": 1.791, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 70.513, "power_watts_avg": 21.624, "energy_joules_est": 200.97, "duration_seconds": 9.294, "sample_count": 78}, "timestamp": "2026-01-25T16:51:15.632015"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8430.096, "latencies_ms": [8430.096], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows an indoor setting with artificial lighting, predominantly white walls, and a concrete floor. There is a white car parked on the right side of the image, and the environment appears to be a covered parking area or a garage.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24415.4, "ram_available_mb": 38425.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.141}, "power_stats": {"power_gpu_soc_mean_watts": 21.884, "power_cpu_cv_mean_watts": 1.759, "power_sys_5v0_mean_watts": 9.032, "gpu_utilization_percent_mean": 71.141, "power_watts_avg": 21.884, "energy_joules_est": 184.5, "duration_seconds": 8.431, "sample_count": 71}, "timestamp": "2026-01-25T16:51:26.117782"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11116.642, "latencies_ms": [11116.642], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a delicious meal consisting of a sandwich, fries, and a side of ketchup. The sandwich is placed in the center of the plate, with the fries surrounding it. The ketchup is positioned to the left of the sandwich. The meal is served on a white plate, which is placed on a dining table. The sand", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24415.4, "ram_available_mb": 38425.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.766}, "power_stats": {"power_gpu_soc_mean_watts": 20.95, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 69.766, "power_watts_avg": 20.95, "energy_joules_est": 232.91, "duration_seconds": 11.117, "sample_count": 94}, "timestamp": "2026-01-25T16:51:39.271221"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8523.336, "latencies_ms": [8523.336], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "Hamburger: 2\nFries: many\nKetchup: 1\nOnion: 1\nBun: 1\nBread: 1\nBun: 1\nBread: 1\nBun: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24413.9, "ram_available_mb": 38427.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.042}, "power_stats": {"power_gpu_soc_mean_watts": 22.1, "power_cpu_cv_mean_watts": 1.724, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 72.042, "power_watts_avg": 22.1, "energy_joules_est": 188.38, "duration_seconds": 8.524, "sample_count": 72}, "timestamp": "2026-01-25T16:51:49.837885"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11161.365, "latencies_ms": [11161.365], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a plate with two burgers, one of which is cut in half, revealing the inside. Behind the burgers, there is a generous portion of golden fries. To the left of the plate, there is a small bowl of ketchup and a small bowl of mayonnaise, both within easy reach for di", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24413.9, "ram_available_mb": 38427.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24418.9, "ram_available_mb": 38422.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.568}, "power_stats": {"power_gpu_soc_mean_watts": 20.904, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 69.568, "power_watts_avg": 20.904, "energy_joules_est": 233.33, "duration_seconds": 11.162, "sample_count": 95}, "timestamp": "2026-01-25T16:52:03.056809"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7631.264, "latencies_ms": [7631.264], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a plate of food consisting of a burger, fries, and a small bowl of ketchup. The burger is topped with sesame seeds and the fries are golden brown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.9, "ram_available_mb": 38422.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24435.8, "ram_available_mb": 38405.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.438}, "power_stats": {"power_gpu_soc_mean_watts": 22.429, "power_cpu_cv_mean_watts": 1.651, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 72.438, "power_watts_avg": 22.429, "energy_joules_est": 171.18, "duration_seconds": 7.632, "sample_count": 64}, "timestamp": "2026-01-25T16:52:12.743272"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7696.162, "latencies_ms": [7696.162], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image features a meal with a sesame seed bun, a side of fries, and a small bowl of ketchup. The bun is golden brown and the fries are golden and crispy.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24435.8, "ram_available_mb": 38405.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24433.2, "ram_available_mb": 38407.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.231}, "power_stats": {"power_gpu_soc_mean_watts": 21.686, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 9.038, "gpu_utilization_percent_mean": 72.231, "power_watts_avg": 21.686, "energy_joules_est": 166.91, "duration_seconds": 7.697, "sample_count": 65}, "timestamp": "2026-01-25T16:52:22.501263"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11087.069, "latencies_ms": [11087.069], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and inviting bedroom. Dominating the center of the room is a bed, adorned with a green mosquito net that cascades over the headboard, providing a sense of tranquility. The bed is dressed in a white comforter, and a white pillow rests at its foot, inviting rest and relaxation.", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24433.2, "ram_available_mb": 38407.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24413.5, "ram_available_mb": 38427.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.406}, "power_stats": {"power_gpu_soc_mean_watts": 20.965, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.406, "power_watts_avg": 20.965, "energy_joules_est": 232.45, "duration_seconds": 11.088, "sample_count": 96}, "timestamp": "2026-01-25T16:52:35.617349"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7298.639, "latencies_ms": [7298.639], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "bed: 1, canopy: 1, window: 4, curtain: 4, painting: 2, table: 1, chair: 1, candle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24413.5, "ram_available_mb": 38427.4, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24418.7, "ram_available_mb": 38422.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.361}, "power_stats": {"power_gpu_soc_mean_watts": 22.648, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 73.361, "power_watts_avg": 22.648, "energy_joules_est": 165.32, "duration_seconds": 7.3, "sample_count": 61}, "timestamp": "2026-01-25T16:52:44.937979"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11104.434, "latencies_ms": [11104.434], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The bed with the mosquito net is positioned in the foreground of the image, occupying a central space in the room. In the background, there is a wooden table and a small bedside table with a lamp, both of which are to the right of the bed. The room has large windows on the left side, allowing natural light to enter and creating a bright atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.7, "ram_available_mb": 38422.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24415.5, "ram_available_mb": 38425.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.128}, "power_stats": {"power_gpu_soc_mean_watts": 20.947, "power_cpu_cv_mean_watts": 1.925, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 70.128, "power_watts_avg": 20.947, "energy_joules_est": 232.62, "duration_seconds": 11.105, "sample_count": 94}, "timestamp": "2026-01-25T16:52:58.072933"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9300.87, "latencies_ms": [9300.87], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image depicts a cozy bedroom with a large bed covered in a green mosquito net, a wooden nightstand with a candle on it, and a table with chairs. The room has a thatched roof and is decorated with framed pictures on the walls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24415.5, "ram_available_mb": 38425.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24423.8, "ram_available_mb": 38417.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.848}, "power_stats": {"power_gpu_soc_mean_watts": 21.695, "power_cpu_cv_mean_watts": 1.789, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 71.848, "power_watts_avg": 21.695, "energy_joules_est": 201.8, "duration_seconds": 9.302, "sample_count": 79}, "timestamp": "2026-01-25T16:53:09.389268"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7300.14, "latencies_ms": [7300.14], "images_per_second": 0.137, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The room has a warm and inviting atmosphere with orange walls and a thatched roof. The bed is covered with a green mosquito net, and there is a small wooden table and chairs in the corner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.8, "ram_available_mb": 38417.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24436.2, "ram_available_mb": 38404.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.129}, "power_stats": {"power_gpu_soc_mean_watts": 22.498, "power_cpu_cv_mean_watts": 1.672, "power_sys_5v0_mean_watts": 9.053, "gpu_utilization_percent_mean": 72.129, "power_watts_avg": 22.498, "energy_joules_est": 164.25, "duration_seconds": 7.301, "sample_count": 62}, "timestamp": "2026-01-25T16:53:18.750025"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12075.054, "latencies_ms": [12075.054], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a gray and white cat is standing on top of a black car in a garage. The cat appears to be looking around, possibly observing its surroundings. The garage is filled with various items, including a bicycle, a bottle, a box, and a few other objects. The cat is positioned near the center of the car, with its", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24436.2, "ram_available_mb": 38404.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24416.1, "ram_available_mb": 38424.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.804}, "power_stats": {"power_gpu_soc_mean_watts": 22.686, "power_cpu_cv_mean_watts": 1.793, "power_sys_5v0_mean_watts": 9.154, "gpu_utilization_percent_mean": 72.804, "power_watts_avg": 22.686, "energy_joules_est": 273.95, "duration_seconds": 12.076, "sample_count": 102}, "timestamp": "2026-01-25T16:53:32.856161"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8624.248, "latencies_ms": [8624.248], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "car: 1, cat: 1, lamps: 2, boxes: 1, tires: 2, bicycles: 2, bottles: 2, drawers: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24416.1, "ram_available_mb": 38424.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24432.1, "ram_available_mb": 38408.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.466}, "power_stats": {"power_gpu_soc_mean_watts": 24.005, "power_cpu_cv_mean_watts": 1.475, "power_sys_5v0_mean_watts": 9.132, "gpu_utilization_percent_mean": 76.466, "power_watts_avg": 24.005, "energy_joules_est": 207.04, "duration_seconds": 8.625, "sample_count": 73}, "timestamp": "2026-01-25T16:53:43.530336"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12087.631, "latencies_ms": [12087.631], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a cat is standing on the hood of a black car, which is positioned in the center of the image. The car is in the foreground and appears to be the main subject of the photo. In the background, there are various items such as a bicycle, a lamp, and a box, which are less prominent and appear to be further away from", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24432.1, "ram_available_mb": 38408.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24426.0, "ram_available_mb": 38414.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.873}, "power_stats": {"power_gpu_soc_mean_watts": 22.666, "power_cpu_cv_mean_watts": 1.789, "power_sys_5v0_mean_watts": 9.14, "gpu_utilization_percent_mean": 73.873, "power_watts_avg": 22.666, "energy_joules_est": 274.0, "duration_seconds": 12.089, "sample_count": 102}, "timestamp": "2026-01-25T16:53:57.638212"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8422.032, "latencies_ms": [8422.032], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A cat is standing on the hood of a black car in a garage. The garage is cluttered with various items, including a bicycle, a lamp, and a cardboard box.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24426.0, "ram_available_mb": 38414.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24415.6, "ram_available_mb": 38425.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.611}, "power_stats": {"power_gpu_soc_mean_watts": 23.985, "power_cpu_cv_mean_watts": 1.457, "power_sys_5v0_mean_watts": 9.162, "gpu_utilization_percent_mean": 76.611, "power_watts_avg": 23.985, "energy_joules_est": 202.02, "duration_seconds": 8.423, "sample_count": 72}, "timestamp": "2026-01-25T16:54:08.119269"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7588.426, "latencies_ms": [7588.426], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image shows a gray and white striped cat standing on a black car in an indoor setting. The lighting is artificial, with a lamp and overhead lights visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24415.6, "ram_available_mb": 38425.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24446.6, "ram_available_mb": 38394.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.078}, "power_stats": {"power_gpu_soc_mean_watts": 24.253, "power_cpu_cv_mean_watts": 1.438, "power_sys_5v0_mean_watts": 9.22, "gpu_utilization_percent_mean": 77.078, "power_watts_avg": 24.253, "energy_joules_est": 184.06, "duration_seconds": 7.589, "sample_count": 64}, "timestamp": "2026-01-25T16:54:17.770190"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12330.682, "latencies_ms": [12330.682], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a white plate holding a delicious-looking sandwich. The sandwich is made with a toasted bun, filled with a generous amount of meat, and topped with a slice of tomato and a dollop of sauce. The plate is placed on a table, and there's a fork visible on the right side of the plate. The background is bl", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 24446.6, "ram_available_mb": 38394.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.038}, "power_stats": {"power_gpu_soc_mean_watts": 22.792, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 9.204, "gpu_utilization_percent_mean": 74.038, "power_watts_avg": 22.792, "energy_joules_est": 281.06, "duration_seconds": 12.331, "sample_count": 106}, "timestamp": "2026-01-25T16:54:32.130427"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9372.511, "latencies_ms": [9372.511], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "plate: 1\nmushroom: 1\ngravy: 1\nbread: 1\nmeat: 1\ntomato: 1\nparsley: 1\ngravy: 1\n", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24454.5, "ram_available_mb": 38386.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.838}, "power_stats": {"power_gpu_soc_mean_watts": 24.034, "power_cpu_cv_mean_watts": 1.511, "power_sys_5v0_mean_watts": 9.158, "gpu_utilization_percent_mean": 77.838, "power_watts_avg": 24.034, "energy_joules_est": 225.27, "duration_seconds": 9.373, "sample_count": 80}, "timestamp": "2026-01-25T16:54:43.536561"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11509.482, "latencies_ms": [11509.482], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "In the foreground, there is a plate with a sandwich that has a large piece of meat and a slice of tomato on top. The plate is placed on a table with a black and white checkered pattern. In the background, there is another plate with a different type of food, and a fork is also visible on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24454.5, "ram_available_mb": 38386.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24450.7, "ram_available_mb": 38390.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.673}, "power_stats": {"power_gpu_soc_mean_watts": 23.1, "power_cpu_cv_mean_watts": 1.716, "power_sys_5v0_mean_watts": 9.224, "gpu_utilization_percent_mean": 74.673, "power_watts_avg": 23.1, "energy_joules_est": 265.88, "duration_seconds": 11.51, "sample_count": 98}, "timestamp": "2026-01-25T16:54:57.064865"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11532.98, "latencies_ms": [11532.98], "images_per_second": 0.087, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image shows a plate of food with a sandwich topped with a slice of tomato and a generous amount of gravy, garnished with chopped herbs. The plate is placed on a table with a black and white checkered tablecloth, and there is a fork on the right side of the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24450.7, "ram_available_mb": 38390.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.408}, "power_stats": {"power_gpu_soc_mean_watts": 23.266, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 9.152, "gpu_utilization_percent_mean": 75.408, "power_watts_avg": 23.266, "energy_joules_est": 268.34, "duration_seconds": 11.534, "sample_count": 98}, "timestamp": "2026-01-25T16:55:10.636504"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11584.779, "latencies_ms": [11584.779], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The image shows a plate of food with a rich, dark brown gravy and a slice of tomato on top, garnished with green herbs, possibly parsley. The plate is white, and the food is presented on a table with a black and white checkered tablecloth, suggesting an outdoor setting with natural light.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24448.3, "ram_available_mb": 38392.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.439}, "power_stats": {"power_gpu_soc_mean_watts": 22.909, "power_cpu_cv_mean_watts": 1.72, "power_sys_5v0_mean_watts": 9.174, "gpu_utilization_percent_mean": 74.439, "power_watts_avg": 22.909, "energy_joules_est": 265.41, "duration_seconds": 11.585, "sample_count": 98}, "timestamp": "2026-01-25T16:55:24.255668"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11110.8, "latencies_ms": [11110.8], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a group of three men sitting on a couch in a living room, enjoying a beer and playing a video game together. They are all focused on the game, with one man holding a remote controller in his hand. The room is furnished with a couch, a dining table, and a chair.\n\nVarious items can be seen in the", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24448.3, "ram_available_mb": 38392.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24423.4, "ram_available_mb": 38417.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.394}, "power_stats": {"power_gpu_soc_mean_watts": 20.955, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.394, "power_watts_avg": 20.955, "energy_joules_est": 232.84, "duration_seconds": 11.111, "sample_count": 94}, "timestamp": "2026-01-25T16:55:37.430450"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10584.614, "latencies_ms": [10584.614], "images_per_second": 0.094, "prompt_tokens": 39, "response_tokens_est": 72, "n_tiles": 16, "output_text": "- Beer cans: 5\n\n- Beer bottles: 2\n\n- Glasses: 1\n\n- Laptop: 1\n\n- Couch: 1\n\n- Chair: 1\n\n- Beer keg: 1\n\n- Beer keg cap: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.4, "ram_available_mb": 38417.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24415.8, "ram_available_mb": 38425.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.256}, "power_stats": {"power_gpu_soc_mean_watts": 20.307, "power_cpu_cv_mean_watts": 1.873, "power_sys_5v0_mean_watts": 8.909, "gpu_utilization_percent_mean": 71.256, "power_watts_avg": 20.307, "energy_joules_est": 214.96, "duration_seconds": 10.585, "sample_count": 90}, "timestamp": "2026-01-25T16:55:50.049734"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10915.418, "latencies_ms": [10915.418], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a red couch with a person sitting on it, and a red table with various items on it. In the background, there is a window with blinds partially open, allowing natural light to enter the room. The person standing on the right side of the image is holding a remote control, and there is a laptop on the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24415.8, "ram_available_mb": 38425.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24434.4, "ram_available_mb": 38406.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.075}, "power_stats": {"power_gpu_soc_mean_watts": 20.959, "power_cpu_cv_mean_watts": 1.911, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 70.075, "power_watts_avg": 20.959, "energy_joules_est": 228.79, "duration_seconds": 10.916, "sample_count": 93}, "timestamp": "2026-01-25T16:56:03.019850"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9702.685, "latencies_ms": [9702.685], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 65, "n_tiles": 16, "output_text": "A group of people are gathered in a living room, with one person standing and holding a Wii remote, while the others are seated on a couch and a red couch. There are various items scattered around the room, including a red table, a laptop, a backpack, and a bottle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.4, "ram_available_mb": 38406.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24418.3, "ram_available_mb": 38422.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.277}, "power_stats": {"power_gpu_soc_mean_watts": 21.488, "power_cpu_cv_mean_watts": 1.833, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 71.277, "power_watts_avg": 21.488, "energy_joules_est": 208.51, "duration_seconds": 9.703, "sample_count": 83}, "timestamp": "2026-01-25T16:56:14.779308"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7986.678, "latencies_ms": [7986.678], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The room is dimly lit with a warm yellow light emanating from a lamp on the right side of the image. There is a red couch and a red table in the room, along with a white couch and a white table.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24418.3, "ram_available_mb": 38422.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24411.6, "ram_available_mb": 38429.3, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.881}, "power_stats": {"power_gpu_soc_mean_watts": 22.108, "power_cpu_cv_mean_watts": 1.715, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 71.881, "power_watts_avg": 22.108, "energy_joules_est": 176.58, "duration_seconds": 7.987, "sample_count": 67}, "timestamp": "2026-01-25T16:56:24.798361"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11108.203, "latencies_ms": [11108.203], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a vibrant baseball field, a catcher crouches in anticipation. Dressed in a uniform of black and white, the catcher's helmet gleams under the sunlight, reflecting the intensity of the game. The catcher's mitt, a symbol of readiness, is held firmly in both hands, ready to catch any incoming", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24411.6, "ram_available_mb": 38429.3, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24426.7, "ram_available_mb": 38414.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.484}, "power_stats": {"power_gpu_soc_mean_watts": 20.966, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 69.484, "power_watts_avg": 20.966, "energy_joules_est": 232.91, "duration_seconds": 11.109, "sample_count": 95}, "timestamp": "2026-01-25T16:56:37.949603"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9840.239, "latencies_ms": [9840.239], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "1. Catcher: 1\n2. Home plate: 1\n3. Mound: 1\n4. Baseball field: 1\n5. Baseball glove: 1\n6. Helmet: 1\n7. Uniform: 1\n8. Net: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.7, "ram_available_mb": 38414.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24443.0, "ram_available_mb": 38397.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.807}, "power_stats": {"power_gpu_soc_mean_watts": 21.482, "power_cpu_cv_mean_watts": 1.828, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 70.807, "power_watts_avg": 21.482, "energy_joules_est": 211.4, "duration_seconds": 9.841, "sample_count": 83}, "timestamp": "2026-01-25T16:56:49.805296"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10881.963, "latencies_ms": [10881.963], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The catcher is positioned in the foreground on the baseball field, crouched behind home plate, which is the focal point of the image. The background features a well-maintained grassy area, likely the outfield, which is part of the baseball diamond. The catcher is near the home plate, indicating readiness to catch a pitch.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24443.0, "ram_available_mb": 38397.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24427.9, "ram_available_mb": 38413.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.728}, "power_stats": {"power_gpu_soc_mean_watts": 21.043, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 69.728, "power_watts_avg": 21.043, "energy_joules_est": 229.0, "duration_seconds": 10.883, "sample_count": 92}, "timestamp": "2026-01-25T16:57:02.714086"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7278.992, "latencies_ms": [7278.992], "images_per_second": 0.137, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A baseball catcher is crouched down on the field, ready to catch the ball. The catcher is wearing a black and white uniform with a helmet and a catcher's mitt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.9, "ram_available_mb": 38413.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24422.2, "ram_available_mb": 38418.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.365}, "power_stats": {"power_gpu_soc_mean_watts": 22.739, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 9.028, "gpu_utilization_percent_mean": 74.365, "power_watts_avg": 22.739, "energy_joules_est": 165.53, "duration_seconds": 7.28, "sample_count": 63}, "timestamp": "2026-01-25T16:57:12.014367"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7758.314, "latencies_ms": [7758.314], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The baseball catcher is wearing a black and white uniform with a black helmet and mask. The field is covered in green grass and the dirt around the catcher's area is brown with white chalk markings.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24422.2, "ram_available_mb": 38418.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24439.5, "ram_available_mb": 38401.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.688}, "power_stats": {"power_gpu_soc_mean_watts": 21.592, "power_cpu_cv_mean_watts": 1.72, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 73.688, "power_watts_avg": 21.592, "energy_joules_est": 167.53, "duration_seconds": 7.759, "sample_count": 64}, "timestamp": "2026-01-25T16:57:21.798264"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11120.961, "latencies_ms": [11120.961], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a bathroom scene. The dominant color scheme is pink and blue, with the walls painted in these hues. A white toilet is situated in the center of the room, with a white bathtub positioned to its left. Above the bathtub, a window is visible, allowing natural light to enter the space. The floor is covered with a green", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24439.5, "ram_available_mb": 38401.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24423.9, "ram_available_mb": 38417.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.379}, "power_stats": {"power_gpu_soc_mean_watts": 20.933, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.379, "power_watts_avg": 20.933, "energy_joules_est": 232.81, "duration_seconds": 11.122, "sample_count": 95}, "timestamp": "2026-01-25T16:57:34.940103"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7741.57, "latencies_ms": [7741.57], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "door: 2, bathtub: 1, toilet: 1, window: 1, shower curtain: 1, sink: 1, cabinet: 1, mirror: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.9, "ram_available_mb": 38417.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24419.2, "ram_available_mb": 38421.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.938}, "power_stats": {"power_gpu_soc_mean_watts": 22.517, "power_cpu_cv_mean_watts": 1.638, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 72.938, "power_watts_avg": 22.517, "energy_joules_est": 174.33, "duration_seconds": 7.742, "sample_count": 65}, "timestamp": "2026-01-25T16:57:44.694514"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9666.228, "latencies_ms": [9666.228], "images_per_second": 0.103, "prompt_tokens": 44, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The toilet is located in the foreground of the image, near the left side. The bathtub is in the background, to the left of the toilet. The shower curtain is hanging vertically in the center of the image, between the bathtub and the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.2, "ram_available_mb": 38421.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24421.0, "ram_available_mb": 38419.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.446}, "power_stats": {"power_gpu_soc_mean_watts": 21.348, "power_cpu_cv_mean_watts": 1.852, "power_sys_5v0_mean_watts": 9.007, "gpu_utilization_percent_mean": 70.446, "power_watts_avg": 21.348, "energy_joules_est": 206.37, "duration_seconds": 9.667, "sample_count": 83}, "timestamp": "2026-01-25T16:57:56.378877"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9628.374, "latencies_ms": [9628.374], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image shows a small, well-lit bathroom with a white toilet, a bathtub with a shower curtain, and a wooden vanity with a sink. The walls are painted in a light blue and pink color scheme, and there is a window above the bathtub.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.0, "ram_available_mb": 38419.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24420.4, "ram_available_mb": 38420.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.827}, "power_stats": {"power_gpu_soc_mean_watts": 21.529, "power_cpu_cv_mean_watts": 1.804, "power_sys_5v0_mean_watts": 8.935, "gpu_utilization_percent_mean": 69.827, "power_watts_avg": 21.529, "energy_joules_est": 207.3, "duration_seconds": 9.629, "sample_count": 81}, "timestamp": "2026-01-25T16:58:08.043496"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6576.475, "latencies_ms": [6576.475], "images_per_second": 0.152, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image shows a bathroom with a pink tiled wall and a white toilet. There is a window with white curtains on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.4, "ram_available_mb": 38420.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24424.9, "ram_available_mb": 38416.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.618}, "power_stats": {"power_gpu_soc_mean_watts": 22.67, "power_cpu_cv_mean_watts": 1.565, "power_sys_5v0_mean_watts": 9.046, "gpu_utilization_percent_mean": 73.618, "power_watts_avg": 22.67, "energy_joules_est": 149.1, "duration_seconds": 6.577, "sample_count": 55}, "timestamp": "2026-01-25T16:58:16.641288"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11125.332, "latencies_ms": [11125.332], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and cozy bedroom scene. Dominating the right side of the frame is a bed, dressed in a bedspread of a warm, inviting yellow and white plaid pattern. The bed, with its dark wooden headboard, stands as the centerpiece of the room.\n\nTo the left, a window draped with red curtain", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24424.9, "ram_available_mb": 38416.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24421.4, "ram_available_mb": 38419.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.137}, "power_stats": {"power_gpu_soc_mean_watts": 20.923, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 69.137, "power_watts_avg": 20.923, "energy_joules_est": 232.79, "duration_seconds": 11.126, "sample_count": 95}, "timestamp": "2026-01-25T16:58:29.810101"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9624.504, "latencies_ms": [9624.504], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "1. Bed: 1\n2. Pillow: 2\n3. Nightstand: 1\n4. Lamp: 1\n5. Window: 1\n6. Curtains: 2\n7. Drape: 1\n8. Wall: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24421.4, "ram_available_mb": 38419.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24413.0, "ram_available_mb": 38427.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.16}, "power_stats": {"power_gpu_soc_mean_watts": 21.553, "power_cpu_cv_mean_watts": 1.789, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 71.16, "power_watts_avg": 21.553, "energy_joules_est": 207.45, "duration_seconds": 9.625, "sample_count": 81}, "timestamp": "2026-01-25T16:58:41.457193"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9446.968, "latencies_ms": [9446.968], "images_per_second": 0.106, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The bed is located in the foreground of the image, occupying the lower right portion. The window with curtains is in the background, positioned on the left side of the image. The lamp is placed on a small bedside table, situated to the right of the bed and slightly behind it.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24413.0, "ram_available_mb": 38427.9, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24422.6, "ram_available_mb": 38418.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.438}, "power_stats": {"power_gpu_soc_mean_watts": 21.406, "power_cpu_cv_mean_watts": 1.831, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 70.438, "power_watts_avg": 21.406, "energy_joules_est": 202.24, "duration_seconds": 9.448, "sample_count": 80}, "timestamp": "2026-01-25T16:58:52.919866"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7288.58, "latencies_ms": [7288.58], "images_per_second": 0.137, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image depicts a cozy bedroom with a bed placed against a wall, next to a window with curtains. The room is dimly lit, creating a calm and relaxing atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.6, "ram_available_mb": 38418.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24415.7, "ram_available_mb": 38425.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.262}, "power_stats": {"power_gpu_soc_mean_watts": 22.879, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 9.008, "gpu_utilization_percent_mean": 74.262, "power_watts_avg": 22.879, "energy_joules_est": 166.77, "duration_seconds": 7.289, "sample_count": 61}, "timestamp": "2026-01-25T16:59:02.225108"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8092.318, "latencies_ms": [8092.318], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The room is dimly lit with natural light coming in through the window, casting a soft glow on the plaid bedspread. The curtains are drawn back, allowing the light to filter into the room and create a serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24415.7, "ram_available_mb": 38425.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24439.1, "ram_available_mb": 38401.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.855}, "power_stats": {"power_gpu_soc_mean_watts": 21.982, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 70.855, "power_watts_avg": 21.982, "energy_joules_est": 177.9, "duration_seconds": 8.093, "sample_count": 69}, "timestamp": "2026-01-25T16:59:12.380369"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12315.522, "latencies_ms": [12315.522], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man and woman are standing close to each other, with the woman placing a white flower on the man's lapel. The man is wearing a black suit and tie, while the woman is dressed in a black dress. They are both smiling, indicating a joyful and celebratory atmosphere. The woman is holding the flower with her right hand, and the", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24439.1, "ram_available_mb": 38401.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24443.4, "ram_available_mb": 38397.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.162}, "power_stats": {"power_gpu_soc_mean_watts": 22.852, "power_cpu_cv_mean_watts": 1.769, "power_sys_5v0_mean_watts": 9.188, "gpu_utilization_percent_mean": 74.162, "power_watts_avg": 22.852, "energy_joules_est": 281.45, "duration_seconds": 12.316, "sample_count": 105}, "timestamp": "2026-01-25T16:59:26.756471"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8730.092, "latencies_ms": [8730.092], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "person: 2, flower: 1, tie: 1, suit: 1, dress: 1, bracelet: 1, earring: 1, necklace: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24443.4, "ram_available_mb": 38397.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24453.8, "ram_available_mb": 38387.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.797}, "power_stats": {"power_gpu_soc_mean_watts": 24.3, "power_cpu_cv_mean_watts": 1.444, "power_sys_5v0_mean_watts": 9.176, "gpu_utilization_percent_mean": 78.797, "power_watts_avg": 24.3, "energy_joules_est": 212.16, "duration_seconds": 8.731, "sample_count": 74}, "timestamp": "2026-01-25T16:59:37.500712"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8341.852, "latencies_ms": [8341.852], "images_per_second": 0.12, "prompt_tokens": 44, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The person on the left is standing in the foreground, while the person on the right is slightly behind and to the right of the person on the left. The background is a plain wall with a door.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24453.8, "ram_available_mb": 38387.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24440.9, "ram_available_mb": 38400.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.929}, "power_stats": {"power_gpu_soc_mean_watts": 24.295, "power_cpu_cv_mean_watts": 1.47, "power_sys_5v0_mean_watts": 9.262, "gpu_utilization_percent_mean": 77.929, "power_watts_avg": 24.295, "energy_joules_est": 202.68, "duration_seconds": 8.343, "sample_count": 70}, "timestamp": "2026-01-25T16:59:47.879369"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9823.827, "latencies_ms": [9823.827], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "In the image, a young man and woman are standing close to each other, with the woman placing a white flower on the man's lapel. They are both dressed in formal attire, suggesting that they are attending a special event or celebration.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24440.9, "ram_available_mb": 38400.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.536}, "power_stats": {"power_gpu_soc_mean_watts": 23.863, "power_cpu_cv_mean_watts": 1.568, "power_sys_5v0_mean_watts": 9.17, "gpu_utilization_percent_mean": 76.536, "power_watts_avg": 23.863, "energy_joules_est": 234.44, "duration_seconds": 9.824, "sample_count": 84}, "timestamp": "2026-01-25T16:59:59.749521"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10052.088, "latencies_ms": [10052.088], "images_per_second": 0.099, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image features a man in a black suit with a beige tie and a woman in a black and silver sequined dress. The lighting appears to be artificial, likely from indoor lighting, and the overall color scheme is dark with some shimmering silver accents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24442.2, "ram_available_mb": 38398.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.635}, "power_stats": {"power_gpu_soc_mean_watts": 23.548, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.258, "gpu_utilization_percent_mean": 75.635, "power_watts_avg": 23.548, "energy_joules_est": 236.72, "duration_seconds": 10.053, "sample_count": 85}, "timestamp": "2026-01-25T17:00:11.858355"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11117.555, "latencies_ms": [11117.555], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of urban tranquility. Dominating the foreground is a chain-link fence, its metallic gray color contrasting with the lush green of the grass and the vibrant green of the palm trees in the background. The fence, standing tall and sturdy, serves as a boundary between the viewer and the scene beyond.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24442.2, "ram_available_mb": 38398.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24428.5, "ram_available_mb": 38412.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.396}, "power_stats": {"power_gpu_soc_mean_watts": 20.955, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 69.396, "power_watts_avg": 20.955, "energy_joules_est": 232.98, "duration_seconds": 11.118, "sample_count": 96}, "timestamp": "2026-01-25T17:00:25.014722"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7518.389, "latencies_ms": [7518.389], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "fence: 1, stop sign: 1, palm trees: 5, trash: 2, bush: 1, building: 1, car: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24428.5, "ram_available_mb": 38412.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.397}, "power_stats": {"power_gpu_soc_mean_watts": 22.544, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 73.397, "power_watts_avg": 22.544, "energy_joules_est": 169.51, "duration_seconds": 7.519, "sample_count": 63}, "timestamp": "2026-01-25T17:00:34.586474"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9111.221, "latencies_ms": [9111.221], "images_per_second": 0.11, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The stop sign is positioned in the foreground on the left side of the image, behind the chain-link fence which occupies the middle ground. In the background, there are palm trees and a building, indicating that the fence is likely in an urban or residential area.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24443.5, "ram_available_mb": 38397.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.87}, "power_stats": {"power_gpu_soc_mean_watts": 21.659, "power_cpu_cv_mean_watts": 1.804, "power_sys_5v0_mean_watts": 9.008, "gpu_utilization_percent_mean": 70.87, "power_watts_avg": 21.659, "energy_joules_est": 197.35, "duration_seconds": 9.112, "sample_count": 77}, "timestamp": "2026-01-25T17:00:45.744178"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8746.482, "latencies_ms": [8746.482], "images_per_second": 0.114, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "A stop sign is mounted on a chain-link fence, indicating that vehicles must come to a complete stop before proceeding. The fence is located in a grassy area with palm trees and a building in the background, suggesting an urban or residential setting.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24443.5, "ram_available_mb": 38397.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24437.4, "ram_available_mb": 38403.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.324}, "power_stats": {"power_gpu_soc_mean_watts": 22.006, "power_cpu_cv_mean_watts": 1.736, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 72.324, "power_watts_avg": 22.006, "energy_joules_est": 192.49, "duration_seconds": 8.747, "sample_count": 74}, "timestamp": "2026-01-25T17:00:56.528485"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8870.579, "latencies_ms": [8870.579], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image features a red and white stop sign mounted on a metal pole, which is partially obscured by a chain-link fence. The fence is green and appears to be in a state of disrepair, with some debris and trash scattered around the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.4, "ram_available_mb": 38403.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24433.8, "ram_available_mb": 38407.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.467}, "power_stats": {"power_gpu_soc_mean_watts": 21.708, "power_cpu_cv_mean_watts": 1.793, "power_sys_5v0_mean_watts": 9.011, "gpu_utilization_percent_mean": 70.467, "power_watts_avg": 21.708, "energy_joules_est": 192.58, "duration_seconds": 8.871, "sample_count": 75}, "timestamp": "2026-01-25T17:01:07.442291"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11155.656, "latencies_ms": [11155.656], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a man standing next to a motorcycle and a bicycle. The man is wearing a white helmet and is positioned on the left side of the scene. The motorcycle is parked on the left side of the image, while the bicycle is on the right side. The bicycle has a basket attached to it, and it is", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24433.8, "ram_available_mb": 38407.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24435.3, "ram_available_mb": 38405.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.443}, "power_stats": {"power_gpu_soc_mean_watts": 19.961, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 70.443, "power_watts_avg": 19.961, "energy_joules_est": 222.69, "duration_seconds": 11.156, "sample_count": 97}, "timestamp": "2026-01-25T17:01:20.648205"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7496.873, "latencies_ms": [7496.873], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "motorcycle: 1, bicycle: 3, tree: 2, fence: 1, basket: 1, tire: 4, leaf: 1, person: 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24435.3, "ram_available_mb": 38405.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24436.9, "ram_available_mb": 38403.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.841}, "power_stats": {"power_gpu_soc_mean_watts": 22.697, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 73.841, "power_watts_avg": 22.697, "energy_joules_est": 170.17, "duration_seconds": 7.497, "sample_count": 63}, "timestamp": "2026-01-25T17:01:30.167430"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11057.659, "latencies_ms": [11057.659], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, there is a motorcycle on the left side of the image, and a bicycle on the right side. The bicycle is positioned closer to the viewer than the motorcycle. In the background, there is a person standing to the right of the bicycle, and another bicycle is partially visible behind the first bicycle.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 24436.9, "ram_available_mb": 38403.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24413.2, "ram_available_mb": 38427.7, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.872}, "power_stats": {"power_gpu_soc_mean_watts": 21.042, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 69.872, "power_watts_avg": 21.042, "energy_joules_est": 232.69, "duration_seconds": 11.058, "sample_count": 94}, "timestamp": "2026-01-25T17:01:43.279861"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7729.431, "latencies_ms": [7729.431], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "In a residential area, a man is standing next to a motorcycle while two bicycles are parked nearby. The scene suggests a casual outdoor setting where people are engaging in different modes of transportation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24413.2, "ram_available_mb": 38427.7, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24443.0, "ram_available_mb": 38397.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.662}, "power_stats": {"power_gpu_soc_mean_watts": 22.547, "power_cpu_cv_mean_watts": 1.651, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 73.662, "power_watts_avg": 22.547, "energy_joules_est": 174.29, "duration_seconds": 7.73, "sample_count": 65}, "timestamp": "2026-01-25T17:01:53.051920"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7641.641, "latencies_ms": [7641.641], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image shows a sunny day with clear skies, casting shadows on the ground. There are three bicycles, one yellow, one black, and one with a red seat, all parked on a concrete surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24443.0, "ram_available_mb": 38397.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24433.6, "ram_available_mb": 38407.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.015}, "power_stats": {"power_gpu_soc_mean_watts": 22.283, "power_cpu_cv_mean_watts": 1.7, "power_sys_5v0_mean_watts": 9.084, "gpu_utilization_percent_mean": 72.015, "power_watts_avg": 22.283, "energy_joules_est": 170.29, "duration_seconds": 7.642, "sample_count": 65}, "timestamp": "2026-01-25T17:02:02.753446"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11098.056, "latencies_ms": [11098.056], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a busy city street scene with a man standing on the sidewalk, looking at a street sign. He is wearing a blue shirt and jeans. There are several other people in the scene, including a woman carrying a handbag and a man with a backpack. \n\nThe street is filled with various vehicles, including cars and a truck. There are", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24433.6, "ram_available_mb": 38407.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24423.2, "ram_available_mb": 38417.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.632}, "power_stats": {"power_gpu_soc_mean_watts": 20.956, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.632, "power_watts_avg": 20.956, "energy_joules_est": 232.59, "duration_seconds": 11.099, "sample_count": 95}, "timestamp": "2026-01-25T17:02:15.912564"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9383.173, "latencies_ms": [9383.173], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "- pedestrian: 1\n\n- traffic light: 3\n\n- car: 1\n\n- bicycle: 1\n\n- building: 1\n\n- sign: 2\n\n- trash can: 1\n\n- road: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.2, "ram_available_mb": 38417.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24427.7, "ram_available_mb": 38413.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.362}, "power_stats": {"power_gpu_soc_mean_watts": 21.71, "power_cpu_cv_mean_watts": 1.791, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 71.362, "power_watts_avg": 21.71, "energy_joules_est": 203.72, "duration_seconds": 9.384, "sample_count": 80}, "timestamp": "2026-01-25T17:02:27.358451"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11111.049, "latencies_ms": [11111.049], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a pedestrian standing on the sidewalk, looking towards the street where a red car is parked on the right side. The car is near the curb, and there is a traffic light above it. In the background, there is a building with a sign that reads \"PROCTER 2\" and a street sign with a \"No Parking", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.7, "ram_available_mb": 38413.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.547}, "power_stats": {"power_gpu_soc_mean_watts": 20.945, "power_cpu_cv_mean_watts": 1.938, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 69.547, "power_watts_avg": 20.945, "energy_joules_est": 232.74, "duration_seconds": 11.112, "sample_count": 95}, "timestamp": "2026-01-25T17:02:40.517994"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8991.057, "latencies_ms": [8991.057], "images_per_second": 0.111, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image depicts a city street scene with a pedestrian standing on the sidewalk, a car parked on the side of the road, and a cyclist riding a bicycle. There are traffic lights and street signs visible, indicating a busy urban environment.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24428.7, "ram_available_mb": 38412.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.844}, "power_stats": {"power_gpu_soc_mean_watts": 21.328, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 71.844, "power_watts_avg": 21.328, "energy_joules_est": 191.78, "duration_seconds": 8.992, "sample_count": 77}, "timestamp": "2026-01-25T17:02:51.554230"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9557.327, "latencies_ms": [9557.327], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows a street scene with a pedestrian standing on the sidewalk, a red car in the background, and a building with a brick facade. The weather appears to be overcast, and the lighting is natural but subdued, suggesting it might be an early morning or late afternoon time.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24428.7, "ram_available_mb": 38412.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24427.8, "ram_available_mb": 38413.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.402}, "power_stats": {"power_gpu_soc_mean_watts": 21.394, "power_cpu_cv_mean_watts": 1.84, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 70.402, "power_watts_avg": 21.394, "energy_joules_est": 204.48, "duration_seconds": 9.558, "sample_count": 82}, "timestamp": "2026-01-25T17:03:03.132747"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11123.318, "latencies_ms": [11123.318], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a bronze statue of two people sitting on a bench, with one person holding a suitcase. The statue is located on a sidewalk, and there are several people standing nearby, observing the artwork. Some of the people are wearing backpacks, and one of them is holding a handbag.\n\nIn the background, there is a car parked further", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24427.8, "ram_available_mb": 38413.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24412.3, "ram_available_mb": 38428.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.684}, "power_stats": {"power_gpu_soc_mean_watts": 20.94, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 69.684, "power_watts_avg": 20.94, "energy_joules_est": 232.94, "duration_seconds": 11.124, "sample_count": 95}, "timestamp": "2026-01-25T17:03:16.279461"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7042.65, "latencies_ms": [7042.65], "images_per_second": 0.142, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "people: 3, statue: 1, bench: 1, bag: 1, building: 1, pole: 2, shadow: 1, sun: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24412.3, "ram_available_mb": 38428.6, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24426.0, "ram_available_mb": 38414.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.068}, "power_stats": {"power_gpu_soc_mean_watts": 22.954, "power_cpu_cv_mean_watts": 1.567, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 73.068, "power_watts_avg": 22.954, "energy_joules_est": 161.67, "duration_seconds": 7.043, "sample_count": 59}, "timestamp": "2026-01-25T17:03:25.342086"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11092.711, "latencies_ms": [11092.711], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a bronze statue of two people sitting closely together on a bench, with a suitcase placed in front of them. In the background, there are people standing near a closed garage door, and a stroller is visible to the right side of the image. The statue is positioned on a stone bench, and the people in the background are at a", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24426.0, "ram_available_mb": 38414.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24414.2, "ram_available_mb": 38426.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.032}, "power_stats": {"power_gpu_soc_mean_watts": 21.032, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 70.032, "power_watts_avg": 21.032, "energy_joules_est": 233.31, "duration_seconds": 11.093, "sample_count": 94}, "timestamp": "2026-01-25T17:03:38.467173"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6381.768, "latencies_ms": [6381.768], "images_per_second": 0.157, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A bronze statue of two people sitting on a bench is located on a brick sidewalk. In the background, there are people standing and a closed garage door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24414.2, "ram_available_mb": 38426.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24416.9, "ram_available_mb": 38424.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.836}, "power_stats": {"power_gpu_soc_mean_watts": 23.42, "power_cpu_cv_mean_watts": 1.521, "power_sys_5v0_mean_watts": 9.007, "gpu_utilization_percent_mean": 75.836, "power_watts_avg": 23.42, "energy_joules_est": 149.48, "duration_seconds": 6.382, "sample_count": 55}, "timestamp": "2026-01-25T17:03:46.877152"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10869.352, "latencies_ms": [10869.352], "images_per_second": 0.092, "prompt_tokens": 36, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The sculpture depicts two figures seated closely together, with one figure holding a suitcase. The figures are made of a dark material, possibly bronze, and the sculpture is situated on a stone pedestal. The lighting in the image is natural, with shadows cast on the ground, indicating that the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24416.9, "ram_available_mb": 38424.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24411.9, "ram_available_mb": 38429.0, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.979}, "power_stats": {"power_gpu_soc_mean_watts": 20.956, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 69.979, "power_watts_avg": 20.956, "energy_joules_est": 227.79, "duration_seconds": 10.87, "sample_count": 94}, "timestamp": "2026-01-25T17:03:59.795425"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11086.197, "latencies_ms": [11086.197], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a street sign post, standing tall against the backdrop of a clear blue sky. The post is adorned with a series of signs, each with its own unique color and symbol, guiding the way for different types of vehicles.\n\nAt the top, a blue sign with a white airplane symbol indicates the direction to an airport. Just below", "error": null, "sys_before": {"cpu_percent": 13.0, "ram_used_mb": 24411.9, "ram_available_mb": 38429.0, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24508.5, "ram_available_mb": 38332.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.553}, "power_stats": {"power_gpu_soc_mean_watts": 20.989, "power_cpu_cv_mean_watts": 1.942, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 69.553, "power_watts_avg": 20.989, "energy_joules_est": 232.7, "duration_seconds": 11.087, "sample_count": 94}, "timestamp": "2026-01-25T17:04:12.918293"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10286.365, "latencies_ms": [10286.365], "images_per_second": 0.097, "prompt_tokens": 39, "response_tokens_est": 70, "n_tiles": 16, "output_text": "- Sign: 5\n\n- Parking symbol: 1\n\n- Truck symbol: 1\n\n- Bus symbol: 1\n\n- Traffic light: 1\n\n- Parking sign: 1\n\n- No parking sign: 1\n\n- No truck sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.4, "ram_available_mb": 38422.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24429.5, "ram_available_mb": 38411.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.465}, "power_stats": {"power_gpu_soc_mean_watts": 21.42, "power_cpu_cv_mean_watts": 1.871, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.465, "power_watts_avg": 21.42, "energy_joules_est": 220.35, "duration_seconds": 10.287, "sample_count": 86}, "timestamp": "2026-01-25T17:04:25.248687"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10917.844, "latencies_ms": [10917.844], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The signs are arranged vertically with the 'Severinsbr\u00fccke' sign at the top, followed by 'Koelnmesse' in the middle, and 'Im Sionstal' at the bottom. The 'no truck' sign is in the foreground on the left, while the 'no parking' sign is in the background on the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.5, "ram_available_mb": 38411.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24417.7, "ram_available_mb": 38423.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.323}, "power_stats": {"power_gpu_soc_mean_watts": 21.09, "power_cpu_cv_mean_watts": 1.903, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 70.323, "power_watts_avg": 21.09, "energy_joules_est": 230.27, "duration_seconds": 10.918, "sample_count": 93}, "timestamp": "2026-01-25T17:04:38.184772"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11313.519, "latencies_ms": [11313.519], "images_per_second": 0.088, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a collection of traffic signs mounted on a pole, likely at a roadside or intersection. The signs indicate directions to various destinations such as Severinsbr\u00fccke, Koelnmesse, and Im Sionstal, with arrows pointing to the right for each. Additionally, there is a no parking sign and a no left turn sign, indicating traffic regulations at this location", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24417.7, "ram_available_mb": 38423.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24427.3, "ram_available_mb": 38413.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.948}, "power_stats": {"power_gpu_soc_mean_watts": 21.069, "power_cpu_cv_mean_watts": 1.902, "power_sys_5v0_mean_watts": 8.931, "gpu_utilization_percent_mean": 69.948, "power_watts_avg": 21.069, "energy_joules_est": 238.38, "duration_seconds": 11.314, "sample_count": 96}, "timestamp": "2026-01-25T17:04:51.543052"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10493.355, "latencies_ms": [10493.355], "images_per_second": 0.095, "prompt_tokens": 36, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The image shows a series of traffic signs mounted on a pole, with the top sign featuring an airplane and a bridge, indicating directions to an airport and a bridge. The signs are in various colors such as blue, white, green, and red, and there is a no parking sign at the bottom with a red circle and a blue background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.3, "ram_available_mb": 38413.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.506}, "power_stats": {"power_gpu_soc_mean_watts": 21.202, "power_cpu_cv_mean_watts": 1.876, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 70.506, "power_watts_avg": 21.202, "energy_joules_est": 222.49, "duration_seconds": 10.494, "sample_count": 89}, "timestamp": "2026-01-25T17:05:04.081778"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11100.328, "latencies_ms": [11100.328], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman and a young girl are standing next to each other on a train platform. They are both smiling and posing for the camera. The woman is wearing a red shirt, and the girl is wearing a blue shirt. They are standing close to a black suitcase, which is placed on the platform.\n\nThere are other people in the background", "error": null, "sys_before": {"cpu_percent": 10.7, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24436.8, "ram_available_mb": 38404.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.723}, "power_stats": {"power_gpu_soc_mean_watts": 20.907, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.723, "power_watts_avg": 20.907, "energy_joules_est": 232.09, "duration_seconds": 11.101, "sample_count": 94}, "timestamp": "2026-01-25T17:05:17.245162"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8878.834, "latencies_ms": [8878.834], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 57, "n_tiles": 16, "output_text": "- Suitcase: 1\n- Tag: 1\n- Woman: 2\n- Train: 1\n- Flower patterned bag: 1\n- Train platform: 1\n- Train door: 1\n- Train window: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24436.8, "ram_available_mb": 38404.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24555.1, "ram_available_mb": 38285.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.52}, "power_stats": {"power_gpu_soc_mean_watts": 21.885, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 71.52, "power_watts_avg": 21.885, "energy_joules_est": 194.33, "duration_seconds": 8.879, "sample_count": 75}, "timestamp": "2026-01-25T17:05:28.174088"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11161.793, "latencies_ms": [11161.793], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a black suitcase with a tag on it, placed on the ground. Behind the suitcase, there are two individuals standing close to each other, with one person wearing a red shirt and the other wearing a blue shirt. In the background, there is a red pillar and a train or bus with a window showing a view of trees", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24468.2, "ram_available_mb": 38372.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24472.6, "ram_available_mb": 38368.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.326}, "power_stats": {"power_gpu_soc_mean_watts": 21.0, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 70.326, "power_watts_avg": 21.0, "energy_joules_est": 234.41, "duration_seconds": 11.162, "sample_count": 95}, "timestamp": "2026-01-25T17:05:41.361675"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7180.985, "latencies_ms": [7180.985], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "Two women are standing next to a suitcase on a train platform, possibly waiting for their train or just arrived at their destination. They are both smiling and appear to be in a good mood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24472.6, "ram_available_mb": 38368.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24464.3, "ram_available_mb": 38376.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.459}, "power_stats": {"power_gpu_soc_mean_watts": 22.849, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 74.459, "power_watts_avg": 22.849, "energy_joules_est": 164.09, "duration_seconds": 7.182, "sample_count": 61}, "timestamp": "2026-01-25T17:05:50.570489"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9915.72, "latencies_ms": [9915.72], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image shows two individuals standing next to a black suitcase with a tag visible on it, in a well-lit indoor setting with a red and white color scheme. The person on the left is wearing a red hoodie and the person on the right is wearing a blue shirt with a floral pattern.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24464.3, "ram_available_mb": 38376.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.235}, "power_stats": {"power_gpu_soc_mean_watts": 21.359, "power_cpu_cv_mean_watts": 1.851, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 70.235, "power_watts_avg": 21.359, "energy_joules_est": 211.8, "duration_seconds": 9.916, "sample_count": 85}, "timestamp": "2026-01-25T17:06:02.542409"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10805.658, "latencies_ms": [10805.658], "images_per_second": 0.093, "prompt_tokens": 24, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the image, there are three zebras standing in a line, facing the camera. They are positioned in front of a tree with purple flowers, possibly a wisteria tree. The zebras are standing on a dirt ground, and their black and white stripes are clearly visible. The scene appears to be a natural habitat for these animals.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24423.1, "ram_available_mb": 38417.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.043}, "power_stats": {"power_gpu_soc_mean_watts": 20.967, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 69.043, "power_watts_avg": 20.967, "energy_joules_est": 226.58, "duration_seconds": 10.806, "sample_count": 92}, "timestamp": "2026-01-25T17:06:15.399565"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7181.055, "latencies_ms": [7181.055], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "zebra: 3, trees: numerous, flowers: numerous, branches: numerous, ground: dirt, rocks: few, grass: dirt, sun: visible, shadows: visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.1, "ram_available_mb": 38417.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24423.1, "ram_available_mb": 38417.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.23}, "power_stats": {"power_gpu_soc_mean_watts": 22.871, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 74.23, "power_watts_avg": 22.871, "energy_joules_est": 164.26, "duration_seconds": 7.182, "sample_count": 61}, "timestamp": "2026-01-25T17:06:24.613335"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10113.914, "latencies_ms": [10113.914], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The three zebras are positioned in a line, with the one on the left being the farthest from the camera, the middle zebra being slightly closer, and the zebra on the right being the closest to the camera. They are standing on a dirt ground with trees and purple flowers in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.1, "ram_available_mb": 38417.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24425.1, "ram_available_mb": 38415.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.471}, "power_stats": {"power_gpu_soc_mean_watts": 21.265, "power_cpu_cv_mean_watts": 1.86, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 70.471, "power_watts_avg": 21.265, "energy_joules_est": 215.09, "duration_seconds": 10.115, "sample_count": 85}, "timestamp": "2026-01-25T17:06:36.759300"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7648.681, "latencies_ms": [7648.681], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "Three zebras are standing in a natural setting with trees and purple flowers in the background. The zebras appear to be in a peaceful and serene environment, possibly a wildlife reserve or a zoo.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24425.1, "ram_available_mb": 38415.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24416.4, "ram_available_mb": 38424.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.462}, "power_stats": {"power_gpu_soc_mean_watts": 22.491, "power_cpu_cv_mean_watts": 1.638, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 73.462, "power_watts_avg": 22.491, "energy_joules_est": 172.04, "duration_seconds": 7.649, "sample_count": 65}, "timestamp": "2026-01-25T17:06:46.443499"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6864.86, "latencies_ms": [6864.86], "images_per_second": 0.146, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "Three zebras are standing in a natural habitat with trees and purple flowers in the background. The ground is covered in dirt and the zebras have distinct black and white stripes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24416.4, "ram_available_mb": 38424.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24420.0, "ram_available_mb": 38420.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.356}, "power_stats": {"power_gpu_soc_mean_watts": 22.765, "power_cpu_cv_mean_watts": 1.622, "power_sys_5v0_mean_watts": 9.05, "gpu_utilization_percent_mean": 72.356, "power_watts_avg": 22.765, "energy_joules_est": 156.29, "duration_seconds": 6.865, "sample_count": 59}, "timestamp": "2026-01-25T17:06:55.366832"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11098.205, "latencies_ms": [11098.205], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a tripod with a camera mounted on top of it, positioned in the foreground. The camera is focused on a laptop that is placed on a chair in the background. The chair is located near a dining table, which is also visible in the scene. \n\nIn addition to the laptop and the camera, there are two refrigerators in the background,", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24420.0, "ram_available_mb": 38420.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24414.9, "ram_available_mb": 38426.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.543}, "power_stats": {"power_gpu_soc_mean_watts": 20.926, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.543, "power_watts_avg": 20.926, "energy_joules_est": 232.26, "duration_seconds": 11.099, "sample_count": 94}, "timestamp": "2026-01-25T17:07:08.501462"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9411.839, "latencies_ms": [9411.839], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "- tripod: 1\n\n- camera: 1\n\n- laptop: 1\n\n- chair: 1\n\n- cooler: 1\n\n- refrigerator: 1\n\n- water dispenser: 1\n\n- table: 1", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24414.9, "ram_available_mb": 38426.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24416.7, "ram_available_mb": 38424.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.663, "power_cpu_cv_mean_watts": 1.766, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 72.0, "power_watts_avg": 21.663, "energy_joules_est": 203.9, "duration_seconds": 9.412, "sample_count": 80}, "timestamp": "2026-01-25T17:07:19.977500"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11183.053, "latencies_ms": [11183.053], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a tripod with a camera mounted on top, positioned near a laptop that is placed on a chair. The laptop is in the foreground and appears to be the main focus of the image. In the background, there is a vending machine and a table with various items on it, both of which are further away from the camera's viewpoint.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24416.7, "ram_available_mb": 38424.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24424.5, "ram_available_mb": 38416.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.463}, "power_stats": {"power_gpu_soc_mean_watts": 20.927, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 69.463, "power_watts_avg": 20.927, "energy_joules_est": 234.04, "duration_seconds": 11.184, "sample_count": 95}, "timestamp": "2026-01-25T17:07:33.186788"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7849.914, "latencies_ms": [7849.914], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "In the image, a tripod is set up in a room with a laptop on it, and a vending machine is visible in the background. It appears to be a workspace or a temporary setup for a project or presentation.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24424.5, "ram_available_mb": 38416.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24420.4, "ram_available_mb": 38420.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.848}, "power_stats": {"power_gpu_soc_mean_watts": 22.375, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 71.848, "power_watts_avg": 22.375, "energy_joules_est": 175.66, "duration_seconds": 7.851, "sample_count": 66}, "timestamp": "2026-01-25T17:07:43.059358"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8476.616, "latencies_ms": [8476.616], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image features a tripod with a camera mounted on top, positioned in an indoor setting with a vending machine in the background. The tripod is light brown, and the camera is black, while the vending machine is metallic and blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.4, "ram_available_mb": 38420.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.319}, "power_stats": {"power_gpu_soc_mean_watts": 21.963, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 71.319, "power_watts_avg": 21.963, "energy_joules_est": 186.19, "duration_seconds": 8.477, "sample_count": 72}, "timestamp": "2026-01-25T17:07:53.558343"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11112.73, "latencies_ms": [11112.73], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the image, a sheep is standing in a pen, surrounded by a large amount of wool. The sheep is positioned in the center of the pen, with its body facing the camera. The wool is scattered all around the sheep, covering the ground and the sheep itself. The pen appears to be a fenced area, providing a safe and contained space for the sheep.", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24423.6, "ram_available_mb": 38417.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.851}, "power_stats": {"power_gpu_soc_mean_watts": 20.985, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.851, "power_watts_avg": 20.985, "energy_joules_est": 233.21, "duration_seconds": 11.113, "sample_count": 94}, "timestamp": "2026-01-25T17:08:06.699610"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8066.041, "latencies_ms": [8066.041], "images_per_second": 0.124, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "sheep: 1, wool pile: 1, metal fence: 1, concrete ground: 1, white object: 1, black object: 1, gray object: 1, metal grid: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.6, "ram_available_mb": 38417.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24425.7, "ram_available_mb": 38415.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.235}, "power_stats": {"power_gpu_soc_mean_watts": 22.283, "power_cpu_cv_mean_watts": 1.701, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 73.235, "power_watts_avg": 22.283, "energy_joules_est": 179.75, "duration_seconds": 8.067, "sample_count": 68}, "timestamp": "2026-01-25T17:08:16.783354"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9761.137, "latencies_ms": [9761.137], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "In the foreground, there is a pile of wool that is being sorted by a sheep. The sheep is standing behind the pile of wool, with its head resting on the pile. The background shows a metal fence that is enclosing the area where the sheep and wool are located.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24425.7, "ram_available_mb": 38415.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24432.8, "ram_available_mb": 38408.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.707}, "power_stats": {"power_gpu_soc_mean_watts": 21.438, "power_cpu_cv_mean_watts": 1.845, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 70.707, "power_watts_avg": 21.438, "energy_joules_est": 209.27, "duration_seconds": 9.762, "sample_count": 82}, "timestamp": "2026-01-25T17:08:28.587419"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6498.389, "latencies_ms": [6498.389], "images_per_second": 0.154, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "A sheep is standing in a pen with a pile of wool in front of it. The sheep appears to be eating or nibbling on the wool.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.8, "ram_available_mb": 38408.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24422.2, "ram_available_mb": 38418.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.545}, "power_stats": {"power_gpu_soc_mean_watts": 23.284, "power_cpu_cv_mean_watts": 1.521, "power_sys_5v0_mean_watts": 9.015, "gpu_utilization_percent_mean": 73.545, "power_watts_avg": 23.284, "energy_joules_est": 151.33, "duration_seconds": 6.499, "sample_count": 55}, "timestamp": "2026-01-25T17:08:37.147340"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7657.139, "latencies_ms": [7657.139], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image shows a sheep standing in a pen with a wire fence, surrounded by a large amount of wool. The wool is in various shades of grey and white, indicating different breeds or stages of shearing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.2, "ram_available_mb": 38418.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24419.6, "ram_available_mb": 38421.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.662}, "power_stats": {"power_gpu_soc_mean_watts": 22.258, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 9.021, "gpu_utilization_percent_mean": 71.662, "power_watts_avg": 22.258, "energy_joules_est": 170.45, "duration_seconds": 7.658, "sample_count": 65}, "timestamp": "2026-01-25T17:08:46.818606"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11118.192, "latencies_ms": [11118.192], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a tennis match in progress on a blue court. There are two players, one on each side of the net, actively engaged in the game. Both players are holding tennis rackets and are focused on the ball, which is located near the center of the court. \n\nIn addition to the players, there are several other people present in the scene. Some of them", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24419.6, "ram_available_mb": 38421.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24423.0, "ram_available_mb": 38417.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.766}, "power_stats": {"power_gpu_soc_mean_watts": 20.972, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.766, "power_watts_avg": 20.972, "energy_joules_est": 233.18, "duration_seconds": 11.119, "sample_count": 94}, "timestamp": "2026-01-25T17:08:59.984214"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8606.606, "latencies_ms": [8606.606], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Players: 2\n- Ball: 1\n- Net: 1\n- Chair: 1\n- Spectators: 20\n- Banners: 4\n- Signs: 3\n- Towel: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24423.0, "ram_available_mb": 38417.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24425.5, "ram_available_mb": 38415.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.521}, "power_stats": {"power_gpu_soc_mean_watts": 22.013, "power_cpu_cv_mean_watts": 1.749, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 71.521, "power_watts_avg": 22.013, "energy_joules_est": 189.47, "duration_seconds": 8.607, "sample_count": 73}, "timestamp": "2026-01-25T17:09:10.612223"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10433.402, "latencies_ms": [10433.402], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, there is a tennis court with two players in the middle of a match, one near the baseline and the other closer to the net. The audience is seated in the background, surrounding the court on all sides. The scoreboard is positioned above the court, near the center, and is at a distance from the players.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.5, "ram_available_mb": 38415.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24421.6, "ram_available_mb": 38419.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.911}, "power_stats": {"power_gpu_soc_mean_watts": 21.062, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 69.911, "power_watts_avg": 21.062, "energy_joules_est": 219.76, "duration_seconds": 10.434, "sample_count": 90}, "timestamp": "2026-01-25T17:09:23.064382"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9481.697, "latencies_ms": [9481.697], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image captures a tennis match in progress on a blue court with a crowd of spectators watching. Two players are actively engaged in the game, with one player in a pink outfit and the other in a red outfit, both holding tennis rackets and preparing to hit the ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.6, "ram_available_mb": 38419.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24416.1, "ram_available_mb": 38424.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.049}, "power_stats": {"power_gpu_soc_mean_watts": 21.551, "power_cpu_cv_mean_watts": 1.819, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 71.049, "power_watts_avg": 21.551, "energy_joules_est": 204.35, "duration_seconds": 9.482, "sample_count": 81}, "timestamp": "2026-01-25T17:09:34.570628"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7899.875, "latencies_ms": [7899.875], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The tennis court is a vibrant blue color, likely due to the blue paint used on the surface. The lighting in the image appears to be artificial, as it is evenly distributed across the court, suggesting an indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24416.1, "ram_available_mb": 38424.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24423.9, "ram_available_mb": 38417.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.652}, "power_stats": {"power_gpu_soc_mean_watts": 21.668, "power_cpu_cv_mean_watts": 1.723, "power_sys_5v0_mean_watts": 9.021, "gpu_utilization_percent_mean": 72.652, "power_watts_avg": 21.668, "energy_joules_est": 171.19, "duration_seconds": 7.901, "sample_count": 66}, "timestamp": "2026-01-25T17:09:44.491432"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11175.332, "latencies_ms": [11175.332], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is walking through a large glass door in a building. She is wearing a black coat and carrying a suitcase, suggesting she might be traveling or commuting. The woman is walking towards the right side of the image, and the glass door is open, allowing her to pass through.\n\nThere are several other people in the scene, some of whom are", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24423.9, "ram_available_mb": 38417.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.787}, "power_stats": {"power_gpu_soc_mean_watts": 20.188, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 70.787, "power_watts_avg": 20.188, "energy_joules_est": 225.62, "duration_seconds": 11.176, "sample_count": 94}, "timestamp": "2026-01-25T17:09:57.718574"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9646.954, "latencies_ms": [9646.954], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "- Glass doors: 1\n\n- Suitcase: 1\n\n- Person: 1\n\n- Stairs: 1\n\n- Signage: 1\n\n- Tiles: 1\n\n- Floor: 1\n\n- Pipes: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24423.3, "ram_available_mb": 38417.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.78}, "power_stats": {"power_gpu_soc_mean_watts": 21.56, "power_cpu_cv_mean_watts": 1.792, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 71.78, "power_watts_avg": 21.56, "energy_joules_est": 208.0, "duration_seconds": 9.648, "sample_count": 82}, "timestamp": "2026-01-25T17:10:09.413705"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9160.996, "latencies_ms": [9160.996], "images_per_second": 0.109, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "A person is walking in the foreground, carrying a suitcase, towards the left side of the image, while the background features a staircase and various signs overhead. The person is near the left edge of the image, and the staircase is further back, towards the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.3, "ram_available_mb": 38417.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24419.4, "ram_available_mb": 38421.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.115}, "power_stats": {"power_gpu_soc_mean_watts": 21.691, "power_cpu_cv_mean_watts": 1.802, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 71.115, "power_watts_avg": 21.691, "energy_joules_est": 198.73, "duration_seconds": 9.162, "sample_count": 78}, "timestamp": "2026-01-25T17:10:20.634790"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6860.044, "latencies_ms": [6860.044], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A person is seen walking through a glass door in a modern building, carrying a suitcase. The interior appears to be a public transportation station with escalators and directional signs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.4, "ram_available_mb": 38421.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24420.7, "ram_available_mb": 38420.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.448}, "power_stats": {"power_gpu_soc_mean_watts": 23.056, "power_cpu_cv_mean_watts": 1.56, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 74.448, "power_watts_avg": 23.056, "energy_joules_est": 158.18, "duration_seconds": 6.861, "sample_count": 58}, "timestamp": "2026-01-25T17:10:29.527187"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9155.952, "latencies_ms": [9155.952], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image shows a person in a black outfit with a hat, carrying a large black suitcase, walking through a glass door. The environment appears to be an indoor setting with a modern design, featuring a tiled floor, metal pillars, and a staircase in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.7, "ram_available_mb": 38420.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24409.0, "ram_available_mb": 38431.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.338}, "power_stats": {"power_gpu_soc_mean_watts": 21.641, "power_cpu_cv_mean_watts": 1.804, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 70.338, "power_watts_avg": 21.641, "energy_joules_est": 198.16, "duration_seconds": 9.157, "sample_count": 77}, "timestamp": "2026-01-25T17:10:40.715834"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11143.49, "latencies_ms": [11143.49], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with two pizzas placed on it. One pizza is located towards the left side of the table, while the other is positioned more towards the center. There are also two cups on the table, one near the left pizza and the other near the right pizza. \n\nIn the background, there is a person sitting at the table", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 24409.0, "ram_available_mb": 38431.9, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24432.4, "ram_available_mb": 38408.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.489}, "power_stats": {"power_gpu_soc_mean_watts": 20.837, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 69.489, "power_watts_avg": 20.837, "energy_joules_est": 232.21, "duration_seconds": 11.144, "sample_count": 94}, "timestamp": "2026-01-25T17:10:53.913170"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7753.071, "latencies_ms": [7753.071], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "pizza: 2\nglass: 3\nbox: 2\nfork: 2\nknife: 2\npizza box: 2\npizza: 2\npizza: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.4, "ram_available_mb": 38408.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24431.0, "ram_available_mb": 38409.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.892}, "power_stats": {"power_gpu_soc_mean_watts": 22.554, "power_cpu_cv_mean_watts": 1.644, "power_sys_5v0_mean_watts": 9.005, "gpu_utilization_percent_mean": 73.892, "power_watts_avg": 22.554, "energy_joules_est": 174.88, "duration_seconds": 7.754, "sample_count": 65}, "timestamp": "2026-01-25T17:11:03.695293"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11163.488, "latencies_ms": [11163.488], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large pizza on a white box with the word 'PIZZA' printed on it, positioned on the left side of the image. In the background, there is a smaller pizza on a similar white box, also with 'PIZZA' printed on it, placed towards the right side of the image. A person is seated", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.0, "ram_available_mb": 38409.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24435.9, "ram_available_mb": 38405.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.284}, "power_stats": {"power_gpu_soc_mean_watts": 21.0, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 70.284, "power_watts_avg": 21.0, "energy_joules_est": 234.45, "duration_seconds": 11.164, "sample_count": 95}, "timestamp": "2026-01-25T17:11:16.872603"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8095.65, "latencies_ms": [8095.65], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a dining table with two pizza boxes, one of which is open, and a glass of a dark-colored beverage. There is a person sitting at the table, and a television is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.9, "ram_available_mb": 38405.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24436.6, "ram_available_mb": 38404.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.456}, "power_stats": {"power_gpu_soc_mean_watts": 22.367, "power_cpu_cv_mean_watts": 1.672, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 73.456, "power_watts_avg": 22.367, "energy_joules_est": 181.09, "duration_seconds": 8.096, "sample_count": 68}, "timestamp": "2026-01-25T17:11:27.009588"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8696.676, "latencies_ms": [8696.676], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image shows a cozy indoor setting with a focus on a pizza in a box, suggesting a casual dining atmosphere. The lighting is warm and artificial, likely from an indoor source, and the table is covered with a plaid tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.6, "ram_available_mb": 38404.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24430.7, "ram_available_mb": 38410.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.365}, "power_stats": {"power_gpu_soc_mean_watts": 21.856, "power_cpu_cv_mean_watts": 1.785, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 71.365, "power_watts_avg": 21.856, "energy_joules_est": 190.09, "duration_seconds": 8.697, "sample_count": 74}, "timestamp": "2026-01-25T17:11:37.733514"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11127.015, "latencies_ms": [11127.015], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a young boy in a baseball uniform, holding a baseball bat and standing at home plate, ready to swing. He is wearing a helmet and is surrounded by a group of people, including other children and adults, who are watching the game. The boy is in the center of attention, and the onlookers are spread out around him, some closer to the field and", "error": null, "sys_before": {"cpu_percent": 10.3, "ram_used_mb": 24430.7, "ram_available_mb": 38410.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.6}, "power_stats": {"power_gpu_soc_mean_watts": 20.886, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.6, "power_watts_avg": 20.886, "energy_joules_est": 232.41, "duration_seconds": 11.128, "sample_count": 95}, "timestamp": "2026-01-25T17:11:50.915348"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9083.242, "latencies_ms": [9083.242], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- children: 10\n- adults: 5\n- baseball bats: 2\n- baseball gloves: 2\n- helmets: 2\n- baseball uniforms: 2\n- benches: 1\n- trees: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24418.7, "ram_available_mb": 38422.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.628}, "power_stats": {"power_gpu_soc_mean_watts": 21.737, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 71.628, "power_watts_avg": 21.737, "energy_joules_est": 197.46, "duration_seconds": 9.084, "sample_count": 78}, "timestamp": "2026-01-25T17:12:02.032825"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11157.611, "latencies_ms": [11157.611], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a young baseball player is standing on a dirt path, holding a baseball bat and wearing a helmet, ready to swing. Behind him, another player is crouched down, wearing a catcher's mitt, positioned to catch the ball. In the background, a group of spectators is seated on the grass, watching the game unfold", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24418.7, "ram_available_mb": 38422.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24440.5, "ram_available_mb": 38400.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.168}, "power_stats": {"power_gpu_soc_mean_watts": 20.991, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 70.168, "power_watts_avg": 20.991, "energy_joules_est": 234.22, "duration_seconds": 11.158, "sample_count": 95}, "timestamp": "2026-01-25T17:12:15.246568"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10327.161, "latencies_ms": [10327.161], "images_per_second": 0.097, "prompt_tokens": 37, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image depicts a baseball game in progress with a young boy in a white shirt and gray pants at bat, wearing a black helmet, and a catcher in a red shirt and black pants crouched behind him. Spectators are seated on the grassy field, watching the game intently.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24440.5, "ram_available_mb": 38400.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24424.8, "ram_available_mb": 38416.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.307}, "power_stats": {"power_gpu_soc_mean_watts": 21.301, "power_cpu_cv_mean_watts": 1.811, "power_sys_5v0_mean_watts": 8.926, "gpu_utilization_percent_mean": 71.307, "power_watts_avg": 21.301, "energy_joules_est": 220.0, "duration_seconds": 10.328, "sample_count": 88}, "timestamp": "2026-01-25T17:12:27.600835"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8333.638, "latencies_ms": [8333.638], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a sunny day with clear skies, as evidenced by the bright lighting and shadows cast on the ground. The players are wearing helmets and catcher's gear, indicating a baseball game is in progress.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24424.8, "ram_available_mb": 38416.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24423.7, "ram_available_mb": 38417.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.268}, "power_stats": {"power_gpu_soc_mean_watts": 21.986, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 71.268, "power_watts_avg": 21.986, "energy_joules_est": 183.24, "duration_seconds": 8.334, "sample_count": 71}, "timestamp": "2026-01-25T17:12:37.973682"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11126.507, "latencies_ms": [11126.507], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a yellow banana is resting on a black corded telephone, which is placed on a white desk. The banana is positioned in such a way that it appears to be leaning against the telephone. The desk is also home to a white computer monitor and a white printer, both of which are situated in the background. The tele", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24423.7, "ram_available_mb": 38417.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24443.1, "ram_available_mb": 38397.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.305}, "power_stats": {"power_gpu_soc_mean_watts": 20.916, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 69.305, "power_watts_avg": 20.916, "energy_joules_est": 232.74, "duration_seconds": 11.127, "sample_count": 95}, "timestamp": "2026-01-25T17:12:51.122382"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7405.018, "latencies_ms": [7405.018], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "phone: 1\nbanana: 1\ncord: 1\nnumbers: 1\ncalculator: 1\npen: 1\nnotepad: 1\npencil: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24443.1, "ram_available_mb": 38397.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.484}, "power_stats": {"power_gpu_soc_mean_watts": 22.625, "power_cpu_cv_mean_watts": 1.621, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 74.484, "power_watts_avg": 22.625, "energy_joules_est": 167.55, "duration_seconds": 7.406, "sample_count": 62}, "timestamp": "2026-01-25T17:13:00.564779"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11120.293, "latencies_ms": [11120.293], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a banana with a black cord wrapped around it, positioned near a Philips telephone. The banana is closer to the viewer than the telephone. In the background, there is a white telephone and a white electrical outlet on a wall. The banana is in the foreground, while the telephone and outlet are further", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24445.0, "ram_available_mb": 38395.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.851}, "power_stats": {"power_gpu_soc_mean_watts": 20.961, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 69.851, "power_watts_avg": 20.961, "energy_joules_est": 233.11, "duration_seconds": 11.121, "sample_count": 94}, "timestamp": "2026-01-25T17:13:13.707715"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8191.633, "latencies_ms": [8191.633], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A banana is placed on top of a Philips telephone, which is on a desk with a white surface. The telephone has a corded handset and a small screen, and there is a white computer tower in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24445.0, "ram_available_mb": 38395.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24443.1, "ram_available_mb": 38397.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.29}, "power_stats": {"power_gpu_soc_mean_watts": 22.239, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 72.29, "power_watts_avg": 22.239, "energy_joules_est": 182.19, "duration_seconds": 8.192, "sample_count": 69}, "timestamp": "2026-01-25T17:13:23.926148"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8111.83, "latencies_ms": [8111.83], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A ripe banana is placed on top of a black corded phone, which is situated on a white desk. The desk also holds a white telephone and a small white object that appears to be a piece of paper or a label.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24443.1, "ram_available_mb": 38397.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24440.9, "ram_available_mb": 38400.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.809}, "power_stats": {"power_gpu_soc_mean_watts": 22.053, "power_cpu_cv_mean_watts": 1.725, "power_sys_5v0_mean_watts": 9.028, "gpu_utilization_percent_mean": 71.809, "power_watts_avg": 22.053, "energy_joules_est": 178.91, "duration_seconds": 8.113, "sample_count": 68}, "timestamp": "2026-01-25T17:13:34.057645"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11122.956, "latencies_ms": [11122.956], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a lively scene of a crowd of people gathered in what appears to be a public space, possibly a park or a street. The focus is on a woman in the center holding a large teddy bear, which stands out due to its size and the contrasting color of her outfit. The crowd is diverse, with individuals of various ages and styles, some wearing back", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24440.9, "ram_available_mb": 38400.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 24421.8, "ram_available_mb": 38419.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.638}, "power_stats": {"power_gpu_soc_mean_watts": 20.93, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 69.638, "power_watts_avg": 20.93, "energy_joules_est": 232.82, "duration_seconds": 11.124, "sample_count": 94}, "timestamp": "2026-01-25T17:13:47.233585"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4494.11, "latencies_ms": [4494.11], "images_per_second": 0.223, "prompt_tokens": 39, "response_tokens_est": 18, "n_tiles": 16, "output_text": "people: numerous\nbear: 1\nbackpacks: 2\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.8, "ram_available_mb": 38419.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24418.6, "ram_available_mb": 38422.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.784}, "power_stats": {"power_gpu_soc_mean_watts": 25.77, "power_cpu_cv_mean_watts": 1.082, "power_sys_5v0_mean_watts": 9.135, "gpu_utilization_percent_mean": 79.784, "power_watts_avg": 25.77, "energy_joules_est": 115.83, "duration_seconds": 4.495, "sample_count": 37}, "timestamp": "2026-01-25T17:13:53.769590"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11100.307, "latencies_ms": [11100.307], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person holding a large teddy bear, positioned near the center of the image. To the left of this person, there is a group of individuals standing close together, possibly engaged in conversation. In the background, there is a bus stop with a sign that reads \"Tomato station,\" and beyond that, there are more people scattered around, some standing and", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.6, "ram_available_mb": 38422.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.585}, "power_stats": {"power_gpu_soc_mean_watts": 21.01, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 69.585, "power_watts_avg": 21.01, "energy_joules_est": 233.23, "duration_seconds": 11.101, "sample_count": 94}, "timestamp": "2026-01-25T17:14:06.894270"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10616.974, "latencies_ms": [10616.974], "images_per_second": 0.094, "prompt_tokens": 37, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The image depicts a bustling outdoor scene with a large crowd of people, some of whom are carrying backpacks and one person is holding a teddy bear. The setting appears to be a public event or gathering, possibly in a park or open space, as suggested by the presence of trees and a bus stop in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24419.6, "ram_available_mb": 38421.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.868}, "power_stats": {"power_gpu_soc_mean_watts": 21.279, "power_cpu_cv_mean_watts": 1.87, "power_sys_5v0_mean_watts": 8.928, "gpu_utilization_percent_mean": 70.868, "power_watts_avg": 21.279, "energy_joules_est": 225.93, "duration_seconds": 10.618, "sample_count": 91}, "timestamp": "2026-01-25T17:14:19.543922"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8633.298, "latencies_ms": [8633.298], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image is taken on a bright and sunny day, with natural light illuminating the scene. The crowd is diverse, with people wearing a variety of colors, and the environment appears to be an outdoor public space with trees and a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24419.6, "ram_available_mb": 38421.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24424.2, "ram_available_mb": 38416.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.122}, "power_stats": {"power_gpu_soc_mean_watts": 21.764, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 71.122, "power_watts_avg": 21.764, "energy_joules_est": 187.91, "duration_seconds": 8.634, "sample_count": 74}, "timestamp": "2026-01-25T17:14:30.236334"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11087.56, "latencies_ms": [11087.56], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is holding a baby in his arms while standing next to a brown horse. The man is wearing a red shirt and is smiling as he interacts with the horse. The baby is also smiling, enjoying the experience. The scene takes place in a stable or barn, with a stone wall visible in the background. The man and the baby are", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 24424.2, "ram_available_mb": 38416.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24433.2, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.24}, "power_stats": {"power_gpu_soc_mean_watts": 20.903, "power_cpu_cv_mean_watts": 1.939, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.24, "power_watts_avg": 20.903, "energy_joules_est": 231.78, "duration_seconds": 11.088, "sample_count": 96}, "timestamp": "2026-01-25T17:14:43.389562"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9530.357, "latencies_ms": [9530.357], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Man: 1\n2. Child: 1\n3. Horse: 1\n4. Fence: 1\n5. Door: 1\n6. Straw: 1\n7. Grass: 1\n8. Trees: 1", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24433.2, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24435.2, "ram_available_mb": 38405.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.963}, "power_stats": {"power_gpu_soc_mean_watts": 21.609, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 71.963, "power_watts_avg": 21.609, "energy_joules_est": 205.96, "duration_seconds": 9.531, "sample_count": 81}, "timestamp": "2026-01-25T17:14:54.972780"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11151.841, "latencies_ms": [11151.841], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a man in a red shirt is holding a baby in his arms, positioned near the center of the image. The baby is facing the horse, which is in the background, and appears to be sniffing the baby's hand. The man and baby are standing on a porch with a tiled floor, and there is a stone wall to the left", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.2, "ram_available_mb": 38405.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24435.4, "ram_available_mb": 38405.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.284}, "power_stats": {"power_gpu_soc_mean_watts": 21.02, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 70.284, "power_watts_avg": 21.02, "energy_joules_est": 234.43, "duration_seconds": 11.153, "sample_count": 95}, "timestamp": "2026-01-25T17:15:08.178047"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7870.682, "latencies_ms": [7870.682], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A man is holding a baby while standing next to a brown horse, which is sniffing the baby's hand. They appear to be in a stable or barn with a stone wall and a wooden door in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24435.4, "ram_available_mb": 38405.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24436.7, "ram_available_mb": 38404.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.414, "power_cpu_cv_mean_watts": 1.643, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 74.0, "power_watts_avg": 22.414, "energy_joules_est": 176.43, "duration_seconds": 7.871, "sample_count": 67}, "timestamp": "2026-01-25T17:15:18.100221"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7366.991, "latencies_ms": [7366.991], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image features a man in a red shirt holding a child, with a brown horse standing close to them. The setting appears to be outdoors, with natural lighting and a stone wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.7, "ram_available_mb": 38404.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24453.7, "ram_available_mb": 38387.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.73}, "power_stats": {"power_gpu_soc_mean_watts": 22.434, "power_cpu_cv_mean_watts": 1.671, "power_sys_5v0_mean_watts": 9.025, "gpu_utilization_percent_mean": 71.73, "power_watts_avg": 22.434, "energy_joules_est": 165.29, "duration_seconds": 7.368, "sample_count": 63}, "timestamp": "2026-01-25T17:15:27.509112"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11084.219, "latencies_ms": [11084.219], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility, featuring a single banana resting on a white plate with a gold rim. The banana, curved in a gentle arc, is positioned on the right side of the plate. On the left side of the plate, there's a small amount of pinkish-brown sauce, adding a touch of color to the", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 24453.7, "ram_available_mb": 38387.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24426.0, "ram_available_mb": 38414.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.747}, "power_stats": {"power_gpu_soc_mean_watts": 20.903, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 69.747, "power_watts_avg": 20.903, "energy_joules_est": 231.71, "duration_seconds": 11.085, "sample_count": 95}, "timestamp": "2026-01-25T17:15:40.618320"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4183.301, "latencies_ms": [4183.301], "images_per_second": 0.239, "prompt_tokens": 39, "response_tokens_est": 15, "n_tiles": 16, "output_text": "banana: 1, peanut butter: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.0, "ram_available_mb": 38414.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24440.9, "ram_available_mb": 38400.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.686}, "power_stats": {"power_gpu_soc_mean_watts": 26.334, "power_cpu_cv_mean_watts": 1.006, "power_sys_5v0_mean_watts": 9.118, "gpu_utilization_percent_mean": 78.686, "power_watts_avg": 26.334, "energy_joules_est": 110.18, "duration_seconds": 4.184, "sample_count": 35}, "timestamp": "2026-01-25T17:15:46.815307"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9599.256, "latencies_ms": [9599.256], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The banana is positioned in the foreground on the left side of the plate, which is placed on a wooden surface. The peanut butter is in the center of the plate, creating a contrast between the smooth texture of the banana and the creamy texture of the peanut butter.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24440.9, "ram_available_mb": 38400.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24421.6, "ram_available_mb": 38419.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.407}, "power_stats": {"power_gpu_soc_mean_watts": 21.495, "power_cpu_cv_mean_watts": 1.814, "power_sys_5v0_mean_watts": 9.007, "gpu_utilization_percent_mean": 71.407, "power_watts_avg": 21.495, "energy_joules_est": 206.35, "duration_seconds": 9.6, "sample_count": 81}, "timestamp": "2026-01-25T17:15:58.452172"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10190.617, "latencies_ms": [10190.617], "images_per_second": 0.098, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "A single banana is placed on a white plate with a brown rim, and there is a small amount of pink substance, possibly a spread or jam, in the center of the plate. The plate is on a wooden surface, and the setting appears to be casual, possibly for a snack or a light meal.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24421.6, "ram_available_mb": 38419.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24438.6, "ram_available_mb": 38402.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.953}, "power_stats": {"power_gpu_soc_mean_watts": 21.41, "power_cpu_cv_mean_watts": 1.825, "power_sys_5v0_mean_watts": 8.932, "gpu_utilization_percent_mean": 71.953, "power_watts_avg": 21.41, "energy_joules_est": 218.2, "duration_seconds": 10.191, "sample_count": 86}, "timestamp": "2026-01-25T17:16:10.659535"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11134.577, "latencies_ms": [11134.577], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a banana placed on a plate with a pink substance in the center. The plate has a white base with a brown rim, and the banana is positioned in such a way that it appears to be peeling or sliced. The lighting in the image is warm, suggesting it may have been taken indoors with artificial light. The material of the", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24438.6, "ram_available_mb": 38402.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24425.6, "ram_available_mb": 38415.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.138}, "power_stats": {"power_gpu_soc_mean_watts": 21.04, "power_cpu_cv_mean_watts": 1.929, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 70.138, "power_watts_avg": 21.04, "energy_joules_est": 234.29, "duration_seconds": 11.135, "sample_count": 94}, "timestamp": "2026-01-25T17:16:23.829328"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11127.403, "latencies_ms": [11127.403], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is kneeling down and working on a motorcycle tire. He is using a wrench to loosen the lug nuts on the tire, which is a crucial step in changing a tire. The man appears to be focused on his task, ensuring that the lug nuts are properly removed.\n\nThe motorcycle is parked on a", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 24425.6, "ram_available_mb": 38415.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24422.6, "ram_available_mb": 38418.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.284}, "power_stats": {"power_gpu_soc_mean_watts": 20.883, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 69.284, "power_watts_avg": 20.883, "energy_joules_est": 232.39, "duration_seconds": 11.128, "sample_count": 95}, "timestamp": "2026-01-25T17:16:36.997744"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9407.826, "latencies_ms": [9407.826], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "- Man: 1\n\n- Bike: 2\n\n- Wheel: 2\n\n- Tool: 1\n\n- Ground: 1\n\n- Basket: 1\n\n- Bicycle: 1\n\n- Basket: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24422.6, "ram_available_mb": 38418.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24441.7, "ram_available_mb": 38399.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.112}, "power_stats": {"power_gpu_soc_mean_watts": 21.699, "power_cpu_cv_mean_watts": 1.776, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 72.112, "power_watts_avg": 21.699, "energy_joules_est": 204.15, "duration_seconds": 9.408, "sample_count": 80}, "timestamp": "2026-01-25T17:16:48.467397"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11083.903, "latencies_ms": [11083.903], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a man is kneeling on the ground, working on a motorcycle wheel. The motorcycle is positioned to the left of the image, with its front wheel visible and the rear wheel partially obscured by another motorcycle in the background. The man is focused on the wheel, which is in the center of the image, and there is a tire iron in his", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24441.7, "ram_available_mb": 38399.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24432.7, "ram_available_mb": 38408.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.011}, "power_stats": {"power_gpu_soc_mean_watts": 20.976, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 70.011, "power_watts_avg": 20.976, "energy_joules_est": 232.51, "duration_seconds": 11.085, "sample_count": 94}, "timestamp": "2026-01-25T17:17:01.602027"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7290.181, "latencies_ms": [7290.181], "images_per_second": 0.137, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A man is repairing a motorcycle tire in an outdoor setting, possibly a garage or workshop. He is kneeling on the ground and using a tool to work on the tire.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24432.7, "ram_available_mb": 38408.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24432.3, "ram_available_mb": 38408.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.443}, "power_stats": {"power_gpu_soc_mean_watts": 22.851, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 74.443, "power_watts_avg": 22.851, "energy_joules_est": 166.6, "duration_seconds": 7.291, "sample_count": 61}, "timestamp": "2026-01-25T17:17:10.906273"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11096.285, "latencies_ms": [11096.285], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "A man in a green shirt and blue pants is working on a motorcycle wheel. The motorcycle is blue with colorful decals on the side. The man is using a tool to work on the wheel, which is resting on a metal stand. The background shows a tiled floor and a bicycle. The weather appears to be overcast, as the lighting", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24432.3, "ram_available_mb": 38408.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24429.2, "ram_available_mb": 38411.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.681}, "power_stats": {"power_gpu_soc_mean_watts": 20.963, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.681, "power_watts_avg": 20.963, "energy_joules_est": 232.62, "duration_seconds": 11.097, "sample_count": 94}, "timestamp": "2026-01-25T17:17:24.033681"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11091.875, "latencies_ms": [11091.875], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a skateboarder is captured in the midst of performing a trick on a ramp. The skateboarder, dressed in a black t-shirt and black pants, is balancing on one foot on the skateboard while the other foot is lifted off the board. The skateboard, which is brown with a white logo on it, is position", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24429.2, "ram_available_mb": 38411.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24446.8, "ram_available_mb": 38394.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.532}, "power_stats": {"power_gpu_soc_mean_watts": 20.934, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 69.532, "power_watts_avg": 20.934, "energy_joules_est": 232.21, "duration_seconds": 11.093, "sample_count": 94}, "timestamp": "2026-01-25T17:17:37.193511"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10306.766, "latencies_ms": [10306.766], "images_per_second": 0.097, "prompt_tokens": 39, "response_tokens_est": 70, "n_tiles": 16, "output_text": "- Skateboard: 1\n\n- Skate park ramp: 1\n\n- Fence: 1\n\n- Tree: 1\n\n- Grass: 1\n\n- Skateboarder: 1\n\n- Wristband: 1\n\n- Wristband: 1", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 24446.8, "ram_available_mb": 38394.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24445.9, "ram_available_mb": 38395.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.494}, "power_stats": {"power_gpu_soc_mean_watts": 21.406, "power_cpu_cv_mean_watts": 1.827, "power_sys_5v0_mean_watts": 8.934, "gpu_utilization_percent_mean": 71.494, "power_watts_avg": 21.406, "energy_joules_est": 220.64, "duration_seconds": 10.307, "sample_count": 87}, "timestamp": "2026-01-25T17:17:49.524636"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10604.154, "latencies_ms": [10604.154], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, performing a trick on a ramp. In the background, there is a fence and some greenery, indicating the skate park is likely outdoors. The skateboarder is near the edge of the ramp, suggesting they are in the process of grinding or sliding down it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24445.9, "ram_available_mb": 38395.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24448.6, "ram_available_mb": 38392.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.178}, "power_stats": {"power_gpu_soc_mean_watts": 21.161, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 70.178, "power_watts_avg": 21.161, "energy_joules_est": 224.41, "duration_seconds": 10.605, "sample_count": 90}, "timestamp": "2026-01-25T17:18:02.159142"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6504.403, "latencies_ms": [6504.403], "images_per_second": 0.154, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "A skateboarder with long hair is performing a trick on a ramp in a park. The background shows a fence and trees, indicating an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24448.6, "ram_available_mb": 38392.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24460.2, "ram_available_mb": 38380.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.6}, "power_stats": {"power_gpu_soc_mean_watts": 23.375, "power_cpu_cv_mean_watts": 1.507, "power_sys_5v0_mean_watts": 9.003, "gpu_utilization_percent_mean": 75.6, "power_watts_avg": 23.375, "energy_joules_est": 152.05, "duration_seconds": 6.505, "sample_count": 55}, "timestamp": "2026-01-25T17:18:10.720996"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9155.525, "latencies_ms": [9155.525], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The skateboarder is wearing a black t-shirt and black pants, and the skateboard is a mix of colors with a prominent red and white design. The lighting is natural, suggesting it is daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24460.2, "ram_available_mb": 38380.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.449}, "power_stats": {"power_gpu_soc_mean_watts": 21.649, "power_cpu_cv_mean_watts": 1.791, "power_sys_5v0_mean_watts": 9.015, "gpu_utilization_percent_mean": 71.449, "power_watts_avg": 21.649, "energy_joules_est": 198.22, "duration_seconds": 9.156, "sample_count": 78}, "timestamp": "2026-01-25T17:18:21.889306"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11133.429, "latencies_ms": [11133.429], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of four men standing in a grassy field, each holding a frisbee. They are posing for a picture, with the frisbees in their hands, showcasing their shared interest in the sport. The men are standing close to each other, creating a sense of camaraderie and enjoyment. The field appears to be a perfect setting", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24420.5, "ram_available_mb": 38420.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.642}, "power_stats": {"power_gpu_soc_mean_watts": 20.881, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 69.642, "power_watts_avg": 20.881, "energy_joules_est": 232.49, "duration_seconds": 11.134, "sample_count": 95}, "timestamp": "2026-01-25T17:18:35.051655"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10404.404, "latencies_ms": [10404.404], "images_per_second": 0.096, "prompt_tokens": 39, "response_tokens_est": 71, "n_tiles": 16, "output_text": "1. Frisbee: 4\n2. People: 4\n3. Grass: 1 (field)\n4. Sky: 1\n5. Trees: 1 (in background)\n6. Clouds: 0\n7. Sun: 0\n8. Clouds: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.5, "ram_available_mb": 38420.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24425.0, "ram_available_mb": 38415.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.289}, "power_stats": {"power_gpu_soc_mean_watts": 21.27, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 8.925, "gpu_utilization_percent_mean": 71.289, "power_watts_avg": 21.27, "energy_joules_est": 221.32, "duration_seconds": 10.405, "sample_count": 90}, "timestamp": "2026-01-25T17:18:47.474542"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10660.961, "latencies_ms": [10660.961], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there are four individuals standing on a grassy field, each holding a frisbee. The person on the far left is standing with their back to the camera, while the others are facing the camera. In the background, there are two tall poles, one on the right side and one on the far right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.0, "ram_available_mb": 38415.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24427.1, "ram_available_mb": 38413.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.33}, "power_stats": {"power_gpu_soc_mean_watts": 21.094, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 69.33, "power_watts_avg": 21.094, "energy_joules_est": 224.9, "duration_seconds": 10.662, "sample_count": 91}, "timestamp": "2026-01-25T17:19:00.181321"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7257.521, "latencies_ms": [7257.521], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "Four people are standing in a grassy field holding frisbees, likely preparing to play a game of frisbee. The sky is clear and it appears to be a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.1, "ram_available_mb": 38413.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24423.3, "ram_available_mb": 38417.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.29}, "power_stats": {"power_gpu_soc_mean_watts": 22.789, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 73.29, "power_watts_avg": 22.789, "energy_joules_est": 165.41, "duration_seconds": 7.258, "sample_count": 62}, "timestamp": "2026-01-25T17:19:09.452644"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8986.599, "latencies_ms": [8986.599], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image shows a group of people in a field during what appears to be either sunrise or sunset, given the warm lighting and long shadows. They are wearing casual clothing and are holding frisbees, suggesting they are enjoying a recreational activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.3, "ram_available_mb": 38417.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24426.3, "ram_available_mb": 38414.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.855}, "power_stats": {"power_gpu_soc_mean_watts": 21.691, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 70.855, "power_watts_avg": 21.691, "energy_joules_est": 194.94, "duration_seconds": 8.987, "sample_count": 76}, "timestamp": "2026-01-25T17:19:20.470555"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11104.737, "latencies_ms": [11104.737], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large white airplane with a red tail parked on the tarmac at an airport. The airplane is surrounded by several people, who are likely airport staff or passengers. There are also a few trucks and a car visible in the scene, possibly providing support services for the airplane.\n\nIn addition to the airplane and the people, there", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24426.3, "ram_available_mb": 38414.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24419.4, "ram_available_mb": 38421.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.766}, "power_stats": {"power_gpu_soc_mean_watts": 20.975, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.766, "power_watts_avg": 20.975, "energy_joules_est": 232.94, "duration_seconds": 11.105, "sample_count": 94}, "timestamp": "2026-01-25T17:19:33.610574"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8835.158, "latencies_ms": [8835.158], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 57, "n_tiles": 16, "output_text": "- Airplane: 1\n- Windows: 100+\n- People: 5\n- Carts: 2\n- Trees: 1\n- Building: 1\n- Clouds: 10+\n- Sign: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24419.4, "ram_available_mb": 38421.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24422.9, "ram_available_mb": 38418.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.882}, "power_stats": {"power_gpu_soc_mean_watts": 21.906, "power_cpu_cv_mean_watts": 1.754, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 71.882, "power_watts_avg": 21.906, "energy_joules_est": 193.56, "duration_seconds": 8.836, "sample_count": 76}, "timestamp": "2026-01-25T17:19:44.484757"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11021.601, "latencies_ms": [11021.601], "images_per_second": 0.091, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The airplane is parked on the tarmac in the foreground of the image, with the ground crew working around it. In the background, there is a clear blue sky with some clouds, and a building can be seen on the right side of the image. The airplane is positioned near the center of the image, with the ground crew and equipment surrounding it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24422.9, "ram_available_mb": 38418.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24421.5, "ram_available_mb": 38419.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.581}, "power_stats": {"power_gpu_soc_mean_watts": 20.961, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.581, "power_watts_avg": 20.961, "energy_joules_est": 231.04, "duration_seconds": 11.022, "sample_count": 93}, "timestamp": "2026-01-25T17:19:57.533188"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8076.555, "latencies_ms": [8076.555], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A JAL airplane is parked on the tarmac at an airport, with ground crew members attending to it. The sky is blue with some clouds, and there are other airplanes and buildings visible in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24421.5, "ram_available_mb": 38419.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24423.4, "ram_available_mb": 38417.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.309}, "power_stats": {"power_gpu_soc_mean_watts": 22.177, "power_cpu_cv_mean_watts": 1.678, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 71.309, "power_watts_avg": 22.177, "energy_joules_est": 179.13, "duration_seconds": 8.077, "sample_count": 68}, "timestamp": "2026-01-25T17:20:07.652683"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6754.764, "latencies_ms": [6754.764], "images_per_second": 0.148, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image features a large commercial airplane with a white body and a red tail, parked on the tarmac. The sky is blue with scattered white clouds, indicating fair weather conditions.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24423.4, "ram_available_mb": 38417.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24421.8, "ram_available_mb": 38419.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.982}, "power_stats": {"power_gpu_soc_mean_watts": 22.915, "power_cpu_cv_mean_watts": 1.594, "power_sys_5v0_mean_watts": 9.077, "gpu_utilization_percent_mean": 72.982, "power_watts_avg": 22.915, "energy_joules_est": 154.8, "duration_seconds": 6.755, "sample_count": 57}, "timestamp": "2026-01-25T17:20:16.459963"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11140.069, "latencies_ms": [11140.069], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is skillfully riding a skateboard on a cement ledge in a park. He is performing a trick, possibly a grind, as he balances on the edge of the ledge. The skateboarder is wearing a yellow shirt and is the main focus of the scene.\n\nThe park is filled with various benches", "error": null, "sys_before": {"cpu_percent": 14.8, "ram_used_mb": 24421.8, "ram_available_mb": 38419.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.606}, "power_stats": {"power_gpu_soc_mean_watts": 20.835, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 69.606, "power_watts_avg": 20.835, "energy_joules_est": 232.12, "duration_seconds": 11.141, "sample_count": 94}, "timestamp": "2026-01-25T17:20:29.636753"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9137.29, "latencies_ms": [9137.29], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- Bench: 2\n- Trash can: 1\n- Skateboard: 1\n- Skateboarder: 1\n- Graffiti: 1\n- Park: 1\n- Trees: 1\n- Fence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24419.9, "ram_available_mb": 38421.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.266}, "power_stats": {"power_gpu_soc_mean_watts": 21.693, "power_cpu_cv_mean_watts": 1.784, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 71.266, "power_watts_avg": 21.693, "energy_joules_est": 198.23, "duration_seconds": 9.138, "sample_count": 79}, "timestamp": "2026-01-25T17:20:40.799561"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11107.928, "latencies_ms": [11107.928], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, a skateboarder is performing a trick on a concrete ledge. The ledge is located near the center of the image and is surrounded by a grassy area with a trash can and a bench in the background. There is a fence and a tree further back, and a person walking on the sidewalk can be seen in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.9, "ram_available_mb": 38421.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24420.5, "ram_available_mb": 38420.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.579}, "power_stats": {"power_gpu_soc_mean_watts": 20.922, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 69.579, "power_watts_avg": 20.922, "energy_joules_est": 232.42, "duration_seconds": 11.109, "sample_count": 95}, "timestamp": "2026-01-25T17:20:53.923787"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6746.382, "latencies_ms": [6746.382], "images_per_second": 0.148, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A person is skateboarding on a concrete ledge in a park. The ledge has graffiti on it and there are benches and a trash can nearby.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24420.5, "ram_available_mb": 38420.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24416.5, "ram_available_mb": 38424.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.179}, "power_stats": {"power_gpu_soc_mean_watts": 23.191, "power_cpu_cv_mean_watts": 1.53, "power_sys_5v0_mean_watts": 9.005, "gpu_utilization_percent_mean": 75.179, "power_watts_avg": 23.191, "energy_joules_est": 156.47, "duration_seconds": 6.747, "sample_count": 56}, "timestamp": "2026-01-25T17:21:02.682844"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6426.408, "latencies_ms": [6426.408], "images_per_second": 0.156, "prompt_tokens": 36, "response_tokens_est": 37, "n_tiles": 16, "output_text": "The image shows a person skateboarding on a concrete ledge with graffiti on it. The weather appears to be sunny with shadows cast on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24416.5, "ram_available_mb": 38424.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24419.5, "ram_available_mb": 38421.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.333}, "power_stats": {"power_gpu_soc_mean_watts": 22.982, "power_cpu_cv_mean_watts": 1.542, "power_sys_5v0_mean_watts": 9.069, "gpu_utilization_percent_mean": 73.333, "power_watts_avg": 22.982, "energy_joules_est": 147.71, "duration_seconds": 6.427, "sample_count": 54}, "timestamp": "2026-01-25T17:21:11.151241"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11135.013, "latencies_ms": [11135.013], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of indulgence, featuring a single slice of chocolate tart resting on a white plate adorned with gold floral patterns. The tart, with its rich, dark chocolate filling, is topped with a glossy chocolate glaze that is artistically drizzled in a zigzag pattern. The glaze's", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24419.5, "ram_available_mb": 38421.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24418.8, "ram_available_mb": 38422.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.723}, "power_stats": {"power_gpu_soc_mean_watts": 20.958, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 69.723, "power_watts_avg": 20.958, "energy_joules_est": 233.38, "duration_seconds": 11.136, "sample_count": 94}, "timestamp": "2026-01-25T17:21:24.331965"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10623.123, "latencies_ms": [10623.123], "images_per_second": 0.094, "prompt_tokens": 39, "response_tokens_est": 73, "n_tiles": 16, "output_text": "plate: 1, slice of cake: 1, caramel sauce: 1, drizzle of caramel sauce: 1, chocolate drizzle: 1, chocolate shavings: 1, chocolate cake: 1, decorative pattern on plate: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.8, "ram_available_mb": 38422.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24418.1, "ram_available_mb": 38422.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.578}, "power_stats": {"power_gpu_soc_mean_watts": 21.144, "power_cpu_cv_mean_watts": 1.868, "power_sys_5v0_mean_watts": 8.908, "gpu_utilization_percent_mean": 70.578, "power_watts_avg": 21.144, "energy_joules_est": 224.63, "duration_seconds": 10.624, "sample_count": 90}, "timestamp": "2026-01-25T17:21:36.971778"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11103.606, "latencies_ms": [11103.606], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The slice of chocolate pie is positioned in the foreground, appearing large and in focus. It is placed on the left side of the plate, which is adorned with a floral pattern. The caramel sauce is drizzled around the pie, with some of it pooling near the base of the slice, indicating it is on the right side of the pie", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.1, "ram_available_mb": 38422.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24418.6, "ram_available_mb": 38422.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.734}, "power_stats": {"power_gpu_soc_mean_watts": 20.983, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.734, "power_watts_avg": 20.983, "energy_joules_est": 233.0, "duration_seconds": 11.104, "sample_count": 94}, "timestamp": "2026-01-25T17:21:50.110899"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6260.961, "latencies_ms": [6260.961], "images_per_second": 0.16, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "A slice of chocolate tart with a caramel drizzle is placed on a white plate with gold floral patterns, set on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.6, "ram_available_mb": 38422.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24416.0, "ram_available_mb": 38424.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.132}, "power_stats": {"power_gpu_soc_mean_watts": 23.611, "power_cpu_cv_mean_watts": 1.488, "power_sys_5v0_mean_watts": 9.038, "gpu_utilization_percent_mean": 76.132, "power_watts_avg": 23.611, "energy_joules_est": 147.84, "duration_seconds": 6.262, "sample_count": 53}, "timestamp": "2026-01-25T17:21:58.391076"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9315.585, "latencies_ms": [9315.585], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image features a slice of chocolate tart on a white plate with a gold floral pattern around the edge. The tart has a glossy chocolate glaze on top with a pattern of white lines, and there is a drizzle of caramel sauce on the plate.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24416.0, "ram_available_mb": 38424.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24421.7, "ram_available_mb": 38419.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.625}, "power_stats": {"power_gpu_soc_mean_watts": 21.424, "power_cpu_cv_mean_watts": 1.836, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 70.625, "power_watts_avg": 21.424, "energy_joules_est": 199.59, "duration_seconds": 9.316, "sample_count": 80}, "timestamp": "2026-01-25T17:22:09.729326"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11178.658, "latencies_ms": [11178.658], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a busy office environment with several people working on their laptops. There are at least four people visible in the scene, with one man sitting at a desk in the foreground, and three others working in the background. The office is filled with desks and chairs, and there are multiple laptops on the tables.\n\nIn addition to the la", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24421.7, "ram_available_mb": 38419.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.042}, "power_stats": {"power_gpu_soc_mean_watts": 20.061, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 70.042, "power_watts_avg": 20.061, "energy_joules_est": 224.27, "duration_seconds": 11.179, "sample_count": 96}, "timestamp": "2026-01-25T17:22:22.979560"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10658.6, "latencies_ms": [10658.6], "images_per_second": 0.094, "prompt_tokens": 39, "response_tokens_est": 73, "n_tiles": 16, "output_text": "- Laptops: 3\n\n- Chairs: 5\n\n- Desks: 3\n\n- Papers: numerous, exact count unspecified\n\n- Boxes: 1\n\n- Cables: numerous, exact count unspecified\n\n- Computers: 2\n\n- People: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24419.8, "ram_available_mb": 38421.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.495}, "power_stats": {"power_gpu_soc_mean_watts": 21.049, "power_cpu_cv_mean_watts": 1.865, "power_sys_5v0_mean_watts": 8.917, "gpu_utilization_percent_mean": 69.495, "power_watts_avg": 21.049, "energy_joules_est": 224.37, "duration_seconds": 10.659, "sample_count": 91}, "timestamp": "2026-01-25T17:22:35.674280"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11090.453, "latencies_ms": [11090.453], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a man sitting at a desk with a laptop in front of him, and another laptop is on the desk to his left. In the background, there are several other people working at desks with computers, some of which are in the middle ground, while others are further back. The room appears to be a busy workspace with multiple workstations.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.8, "ram_available_mb": 38421.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24420.0, "ram_available_mb": 38420.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.851}, "power_stats": {"power_gpu_soc_mean_watts": 20.993, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 69.851, "power_watts_avg": 20.993, "energy_joules_est": 232.84, "duration_seconds": 11.091, "sample_count": 94}, "timestamp": "2026-01-25T17:22:48.814462"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7842.935, "latencies_ms": [7842.935], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image depicts a busy office environment with multiple individuals working on laptops and other electronic devices. There are several desks and chairs arranged in the room, with people sitting and standing around, engaged in their tasks.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24420.0, "ram_available_mb": 38420.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24426.2, "ram_available_mb": 38414.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.642}, "power_stats": {"power_gpu_soc_mean_watts": 22.285, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 72.642, "power_watts_avg": 22.285, "energy_joules_est": 174.8, "duration_seconds": 7.844, "sample_count": 67}, "timestamp": "2026-01-25T17:22:58.716514"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7877.767, "latencies_ms": [7877.767], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows an indoor setting with a yellowish light, possibly from fluorescent lights, illuminating the room. Various materials such as cardboard boxes, electronic devices, and papers are scattered across the tables and desks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.2, "ram_available_mb": 38414.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24431.1, "ram_available_mb": 38409.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.821}, "power_stats": {"power_gpu_soc_mean_watts": 22.094, "power_cpu_cv_mean_watts": 1.727, "power_sys_5v0_mean_watts": 9.031, "gpu_utilization_percent_mean": 71.821, "power_watts_avg": 22.094, "energy_joules_est": 174.06, "duration_seconds": 7.878, "sample_count": 67}, "timestamp": "2026-01-25T17:23:08.623931"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11146.346, "latencies_ms": [11146.346], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people gathered in a living room, playing a video game using Wii controllers. There are three men and a woman standing in the room, all holding Wii remotes and actively participating in the game. The room is furnished with a couch, a dining table, and a chair.\n\nOn the dining table, there are two bott", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24431.1, "ram_available_mb": 38409.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24433.2, "ram_available_mb": 38407.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.298}, "power_stats": {"power_gpu_soc_mean_watts": 20.86, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 69.298, "power_watts_avg": 20.86, "energy_joules_est": 232.53, "duration_seconds": 11.147, "sample_count": 94}, "timestamp": "2026-01-25T17:23:21.796586"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7630.001, "latencies_ms": [7630.001], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "person: 4\nwii controller: 2\ncouch: 1\nbottles: 2\nframes: 2\nbasket: 1\ntable: 1\ndoor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.2, "ram_available_mb": 38407.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24432.3, "ram_available_mb": 38408.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.797}, "power_stats": {"power_gpu_soc_mean_watts": 22.599, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 73.797, "power_watts_avg": 22.599, "energy_joules_est": 172.45, "duration_seconds": 7.631, "sample_count": 64}, "timestamp": "2026-01-25T17:23:31.450407"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11123.153, "latencies_ms": [11123.153], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a woman in a white dress is standing and facing a man in a striped shirt who is also standing. In the background, two more men are standing near a couch and a window with blinds. The room appears to be a living space with a wooden table, a bottle, and a picture frame on the table, and a basket on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.3, "ram_available_mb": 38408.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24423.8, "ram_available_mb": 38417.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.638}, "power_stats": {"power_gpu_soc_mean_watts": 20.959, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 69.638, "power_watts_avg": 20.959, "energy_joules_est": 233.14, "duration_seconds": 11.124, "sample_count": 94}, "timestamp": "2026-01-25T17:23:44.607994"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9723.258, "latencies_ms": [9723.258], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 65, "n_tiles": 16, "output_text": "In a cozy living room, a group of people are gathered to play a video game using Wii controllers. The room is furnished with a couch, a wooden table, and a few bottles on the table, creating a casual and relaxed atmosphere for the group to enjoy their gaming session.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.8, "ram_available_mb": 38417.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24431.3, "ram_available_mb": 38409.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.072}, "power_stats": {"power_gpu_soc_mean_watts": 21.483, "power_cpu_cv_mean_watts": 1.828, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 71.072, "power_watts_avg": 21.483, "energy_joules_est": 208.9, "duration_seconds": 9.724, "sample_count": 83}, "timestamp": "2026-01-25T17:23:56.375372"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6432.124, "latencies_ms": [6432.124], "images_per_second": 0.155, "prompt_tokens": 36, "response_tokens_est": 37, "n_tiles": 16, "output_text": "The room is dimly lit with natural light coming from the window with blinds. The walls are painted in a light color, and the carpet is a neutral shade.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.3, "ram_available_mb": 38409.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24452.1, "ram_available_mb": 38388.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.648}, "power_stats": {"power_gpu_soc_mean_watts": 23.075, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 9.106, "gpu_utilization_percent_mean": 73.648, "power_watts_avg": 23.075, "energy_joules_est": 148.44, "duration_seconds": 6.433, "sample_count": 54}, "timestamp": "2026-01-25T17:24:04.862117"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11113.445, "latencies_ms": [11113.445], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene beach scene at sunset. A person is standing on the wet sand, facing the ocean. The sun is setting in the background, casting a warm glow over the scene. The sky is a gradient of blue and orange hues, indicating the time of day. The ocean is calm with gentle waves lapping at the shore. The person appears to be looking", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24452.1, "ram_available_mb": 38388.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24426.7, "ram_available_mb": 38414.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.574}, "power_stats": {"power_gpu_soc_mean_watts": 20.97, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.574, "power_watts_avg": 20.97, "energy_joules_est": 233.06, "duration_seconds": 11.114, "sample_count": 94}, "timestamp": "2026-01-25T17:24:18.018978"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7066.601, "latencies_ms": [7066.601], "images_per_second": 0.142, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "person: 1, sun: 1, frisbee: 1, waves: numerous, sand: extensive, horizon: 1, sky: 1, reflection: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.7, "ram_available_mb": 38414.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.083}, "power_stats": {"power_gpu_soc_mean_watts": 22.878, "power_cpu_cv_mean_watts": 1.575, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 74.083, "power_watts_avg": 22.878, "energy_joules_est": 161.69, "duration_seconds": 7.067, "sample_count": 60}, "timestamp": "2026-01-25T17:24:27.112744"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11108.064, "latencies_ms": [11108.064], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person standing on a wet surface, likely the beach, with the sun low on the horizon directly behind them, creating a silhouette effect. The person is positioned near the center of the image, with the sun appearing in the background slightly to the right. The waves on the beach are in the middle ground, extending from the left to the right side", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24416.5, "ram_available_mb": 38424.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.043}, "power_stats": {"power_gpu_soc_mean_watts": 20.963, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 70.043, "power_watts_avg": 20.963, "energy_joules_est": 232.87, "duration_seconds": 11.109, "sample_count": 94}, "timestamp": "2026-01-25T17:24:40.253139"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6055.478, "latencies_ms": [6055.478], "images_per_second": 0.165, "prompt_tokens": 37, "response_tokens_est": 32, "n_tiles": 16, "output_text": "A person is standing on a beach, looking at the sunset. The sun is setting over the horizon, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24416.5, "ram_available_mb": 38424.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24424.4, "ram_available_mb": 38416.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.039}, "power_stats": {"power_gpu_soc_mean_watts": 23.724, "power_cpu_cv_mean_watts": 1.444, "power_sys_5v0_mean_watts": 9.017, "gpu_utilization_percent_mean": 76.039, "power_watts_avg": 23.724, "energy_joules_est": 143.67, "duration_seconds": 6.056, "sample_count": 51}, "timestamp": "2026-01-25T17:24:48.322007"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9684.634, "latencies_ms": [9684.634], "images_per_second": 0.103, "prompt_tokens": 36, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image captures a serene beach scene at sunset with the sun casting a warm orange glow across the sky and reflecting off the wet sand. A person is silhouetted against the bright horizon, standing on the beach with a frisbee in hand, suggesting a leisurely activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24424.4, "ram_available_mb": 38416.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24410.8, "ram_available_mb": 38430.1, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.277}, "power_stats": {"power_gpu_soc_mean_watts": 21.287, "power_cpu_cv_mean_watts": 1.847, "power_sys_5v0_mean_watts": 9.035, "gpu_utilization_percent_mean": 70.277, "power_watts_avg": 21.287, "energy_joules_est": 206.17, "duration_seconds": 9.685, "sample_count": 83}, "timestamp": "2026-01-25T17:25:00.041628"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11126.419, "latencies_ms": [11126.419], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a spacious living room with a variety of furniture and decorations. There is a white couch positioned against the wall, accompanied by a dining table with four chairs around it. The chairs are arranged in a semi-circle, creating a cozy and inviting atmosphere. \n\nIn the room, there are two potted plants, one placed", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24410.8, "ram_available_mb": 38430.1, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24417.1, "ram_available_mb": 38423.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.202}, "power_stats": {"power_gpu_soc_mean_watts": 20.95, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 69.202, "power_watts_avg": 20.95, "energy_joules_est": 233.11, "duration_seconds": 11.127, "sample_count": 94}, "timestamp": "2026-01-25T17:25:13.194609"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8406.059, "latencies_ms": [8406.059], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "- Chair: 2\n- Table: 1\n- Sofa: 1\n- Rug: 1\n- Plant: 2\n- Television: 1\n- Screen: 1\n- Coffee table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24417.1, "ram_available_mb": 38423.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24419.1, "ram_available_mb": 38421.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.556}, "power_stats": {"power_gpu_soc_mean_watts": 22.121, "power_cpu_cv_mean_watts": 1.707, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 72.556, "power_watts_avg": 22.121, "energy_joules_est": 185.96, "duration_seconds": 8.407, "sample_count": 72}, "timestamp": "2026-01-25T17:25:23.648775"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11106.187, "latencies_ms": [11106.187], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a white table with a vase of flowers on it, positioned near the center of the image. To the left of the table, there is a red chair, and to the right, there is a white couch. In the background, there is a black and white patterned rug on the floor, and behind the couch, there is a window", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.1, "ram_available_mb": 38421.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.284}, "power_stats": {"power_gpu_soc_mean_watts": 20.96, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 69.284, "power_watts_avg": 20.96, "energy_joules_est": 232.8, "duration_seconds": 11.107, "sample_count": 95}, "timestamp": "2026-01-25T17:25:36.801596"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8495.771, "latencies_ms": [8495.771], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a white sofa, a red chair, and a white coffee table. There is a flat-screen TV mounted on the wall, and the room is decorated with various artworks and decorative items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24416.6, "ram_available_mb": 38424.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.486}, "power_stats": {"power_gpu_soc_mean_watts": 21.985, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 71.486, "power_watts_avg": 21.985, "energy_joules_est": 186.8, "duration_seconds": 8.496, "sample_count": 72}, "timestamp": "2026-01-25T17:25:47.330415"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6537.306, "latencies_ms": [6537.306], "images_per_second": 0.153, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The living room is bright and airy with natural light coming in from the large windows. The furniture is modern and colorful, with red chairs and a white coffee table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24416.6, "ram_available_mb": 38424.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24420.9, "ram_available_mb": 38420.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.636}, "power_stats": {"power_gpu_soc_mean_watts": 23.042, "power_cpu_cv_mean_watts": 1.558, "power_sys_5v0_mean_watts": 9.084, "gpu_utilization_percent_mean": 73.636, "power_watts_avg": 23.042, "energy_joules_est": 150.65, "duration_seconds": 6.538, "sample_count": 55}, "timestamp": "2026-01-25T17:25:55.928910"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11150.046, "latencies_ms": [11150.046], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a cozy kitchen, a curious feline companion has found an elevated perch on top of a blue refrigerator. The cat, adorned with a vibrant blue collar, gazes intently towards the left side of the image, its eyes reflecting a sense of curiosity and alertness. The refrigerator, a pristine", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24420.9, "ram_available_mb": 38420.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24420.6, "ram_available_mb": 38420.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.242}, "power_stats": {"power_gpu_soc_mean_watts": 20.906, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.927, "gpu_utilization_percent_mean": 69.242, "power_watts_avg": 20.906, "energy_joules_est": 233.12, "duration_seconds": 11.151, "sample_count": 95}, "timestamp": "2026-01-25T17:26:09.142501"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7946.937, "latencies_ms": [7946.937], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "refrigerator: 2, cat: 1, light fixture: 1, cabinet: 1, drawer: 1, bracelet: 1, keychain: 1, key: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24420.6, "ram_available_mb": 38420.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24413.9, "ram_available_mb": 38427.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.209}, "power_stats": {"power_gpu_soc_mean_watts": 22.274, "power_cpu_cv_mean_watts": 1.679, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 72.209, "power_watts_avg": 22.274, "energy_joules_est": 177.02, "duration_seconds": 7.948, "sample_count": 67}, "timestamp": "2026-01-25T17:26:19.148195"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11131.635, "latencies_ms": [11131.635], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a white refrigerator on the left side of the image, and a blue refrigerator on the right side. The cat is positioned on top of the blue refrigerator, near the top right corner of the image. The light source is coming from the top right corner, casting a shadow on the left side of the white refr", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24413.9, "ram_available_mb": 38427.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24414.7, "ram_available_mb": 38426.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.936}, "power_stats": {"power_gpu_soc_mean_watts": 20.929, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.936, "power_watts_avg": 20.929, "energy_joules_est": 232.99, "duration_seconds": 11.132, "sample_count": 94}, "timestamp": "2026-01-25T17:26:32.311867"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7624.027, "latencies_ms": [7624.027], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "A cat is sitting on top of a blue refrigerator, looking curiously to the side. The refrigerator is in a kitchen, with a white cabinet and a light fixture visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24414.7, "ram_available_mb": 38426.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24430.2, "ram_available_mb": 38410.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.703}, "power_stats": {"power_gpu_soc_mean_watts": 22.626, "power_cpu_cv_mean_watts": 1.626, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 73.703, "power_watts_avg": 22.626, "energy_joules_est": 172.51, "duration_seconds": 7.625, "sample_count": 64}, "timestamp": "2026-01-25T17:26:41.952151"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7223.181, "latencies_ms": [7223.181], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A cat with a striped pattern is perched on top of a blue refrigerator. The refrigerator is in a room with beige walls and a ceiling with a light fixture.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24430.2, "ram_available_mb": 38410.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24423.6, "ram_available_mb": 38417.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.451, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 22.451, "energy_joules_est": 162.18, "duration_seconds": 7.224, "sample_count": 60}, "timestamp": "2026-01-25T17:26:51.216133"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11242.168, "latencies_ms": [11242.168], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a small, cluttered living room with a wooden floor. The room is filled with various items, including a refrigerator on the left side, a TV in the center, and a bookshelf on the right side. There are multiple balloons scattered throughout the room, with some hanging from the ceiling and others placed on the floor. ", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24423.6, "ram_available_mb": 38417.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24418.7, "ram_available_mb": 38422.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.423}, "power_stats": {"power_gpu_soc_mean_watts": 19.929, "power_cpu_cv_mean_watts": 1.936, "power_sys_5v0_mean_watts": 8.93, "gpu_utilization_percent_mean": 70.423, "power_watts_avg": 19.929, "energy_joules_est": 224.06, "duration_seconds": 11.243, "sample_count": 97}, "timestamp": "2026-01-25T17:27:04.528242"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8734.594, "latencies_ms": [8734.594], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Balloons: 4\n- Furniture: 3\n- Electronics: 2\n- Books: 1\n- Plant: 1\n- Lamp: 1\n- Curtains: 1\n- Bed: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24418.7, "ram_available_mb": 38422.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.573}, "power_stats": {"power_gpu_soc_mean_watts": 21.816, "power_cpu_cv_mean_watts": 1.756, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 71.573, "power_watts_avg": 21.816, "energy_joules_est": 190.57, "duration_seconds": 8.735, "sample_count": 75}, "timestamp": "2026-01-25T17:27:15.319430"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11113.224, "latencies_ms": [11113.224], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden entertainment center with a TV and various electronic devices, positioned near the center of the room. The refrigerator is on the left side of the room, and there is a bed in the background towards the right side. The room is decorated with balloons, including a smiley face balloon hanging from the ceiling", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24420.1, "ram_available_mb": 38420.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.653}, "power_stats": {"power_gpu_soc_mean_watts": 20.919, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.653, "power_watts_avg": 20.919, "energy_joules_est": 232.49, "duration_seconds": 11.114, "sample_count": 95}, "timestamp": "2026-01-25T17:27:28.456124"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7710.466, "latencies_ms": [7710.466], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a cozy and cluttered living room with a variety of objects scattered throughout the space. There are balloons, books, and other items on the shelves, and a bed in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24420.1, "ram_available_mb": 38420.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24428.4, "ram_available_mb": 38412.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.754}, "power_stats": {"power_gpu_soc_mean_watts": 22.438, "power_cpu_cv_mean_watts": 1.669, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 72.754, "power_watts_avg": 22.438, "energy_joules_est": 173.02, "duration_seconds": 7.711, "sample_count": 65}, "timestamp": "2026-01-25T17:27:38.199252"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7552.645, "latencies_ms": [7552.645], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The room is decorated with yellow and blue balloons, and has a chandelier hanging from the ceiling. The walls are painted white, and there is a bookshelf with various items on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24428.4, "ram_available_mb": 38412.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.587}, "power_stats": {"power_gpu_soc_mean_watts": 22.398, "power_cpu_cv_mean_watts": 1.69, "power_sys_5v0_mean_watts": 9.043, "gpu_utilization_percent_mean": 72.587, "power_watts_avg": 22.398, "energy_joules_est": 169.18, "duration_seconds": 7.553, "sample_count": 63}, "timestamp": "2026-01-25T17:27:47.766234"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11216.422, "latencies_ms": [11216.422], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is sitting on a train, engrossed in his laptop. He is wearing a green jacket and has a headset on, suggesting he might be working or studying. The laptop, which is silver in color, is placed on a gray table in front of him. The train's interior is visible in the background, with a window that offers a view", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24425.5, "ram_available_mb": 38415.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.457}, "power_stats": {"power_gpu_soc_mean_watts": 20.317, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 70.457, "power_watts_avg": 20.317, "energy_joules_est": 227.9, "duration_seconds": 11.217, "sample_count": 94}, "timestamp": "2026-01-25T17:28:01.020006"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8583.315, "latencies_ms": [8583.315], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Man: 1\n- Computer: 1\n- Train window: 1\n- Seat: 1\n- Headphones: 1\n- Train tracks: 2\n- Train: 1\n- Train seat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.5, "ram_available_mb": 38415.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24446.8, "ram_available_mb": 38394.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.986}, "power_stats": {"power_gpu_soc_mean_watts": 21.008, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 8.928, "gpu_utilization_percent_mean": 71.986, "power_watts_avg": 21.008, "energy_joules_est": 180.33, "duration_seconds": 8.584, "sample_count": 72}, "timestamp": "2026-01-25T17:28:11.618168"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11114.144, "latencies_ms": [11114.144], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The laptop is in the foreground, placed on a table that extends from the left side of the image to the right. The person is seated in the background, with their body oriented towards the laptop, indicating that the laptop is the main object of focus. The window in the background provides a sense of depth, showing a view of the outside world that is nearer than the person and", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24446.8, "ram_available_mb": 38394.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24411.2, "ram_available_mb": 38429.7, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.915}, "power_stats": {"power_gpu_soc_mean_watts": 20.948, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 69.915, "power_watts_avg": 20.948, "energy_joules_est": 232.83, "duration_seconds": 11.115, "sample_count": 94}, "timestamp": "2026-01-25T17:28:24.774753"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7414.245, "latencies_ms": [7414.245], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A person is sitting on a train seat, using a laptop placed on a table in front of them. The train appears to be in motion, as the window shows a blurred view of the tracks outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24411.2, "ram_available_mb": 38429.7, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24426.8, "ram_available_mb": 38414.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.71}, "power_stats": {"power_gpu_soc_mean_watts": 22.594, "power_cpu_cv_mean_watts": 1.634, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 73.71, "power_watts_avg": 22.594, "energy_joules_est": 167.53, "duration_seconds": 7.415, "sample_count": 62}, "timestamp": "2026-01-25T17:28:34.220357"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4737.411, "latencies_ms": [4737.411], "images_per_second": 0.211, "prompt_tokens": 36, "response_tokens_est": 22, "n_tiles": 16, "output_text": "The laptop is silver and has a reflective surface. The man is wearing a green shirt.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24426.8, "ram_available_mb": 38414.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24447.1, "ram_available_mb": 38393.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.45}, "power_stats": {"power_gpu_soc_mean_watts": 25.018, "power_cpu_cv_mean_watts": 1.271, "power_sys_5v0_mean_watts": 9.194, "gpu_utilization_percent_mean": 77.45, "power_watts_avg": 25.018, "energy_joules_est": 118.54, "duration_seconds": 4.738, "sample_count": 40}, "timestamp": "2026-01-25T17:28:40.982928"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11175.191, "latencies_ms": [11175.191], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene day at a train station. A silver train, gleaming under the clear blue sky, is stationed on the tracks. The train is facing towards the right side of the image, ready to embark on its journey. The tracks, a symbol of connectivity and movement, are nestled amidst lush green trees, adding a touch of nature to the", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 24447.1, "ram_available_mb": 38393.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24438.1, "ram_available_mb": 38402.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.277}, "power_stats": {"power_gpu_soc_mean_watts": 20.312, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 70.277, "power_watts_avg": 20.312, "energy_joules_est": 227.0, "duration_seconds": 11.176, "sample_count": 94}, "timestamp": "2026-01-25T17:28:54.199688"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9185.614, "latencies_ms": [9185.614], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- Train: 1\n\n- Train tracks: 2\n\n- Cars: 1\n\n- Bridge: 1\n\n- Houses: 1\n\n- Trees: 1\n\n- Clouds: 1\n\n- Sky: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24438.1, "ram_available_mb": 38402.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24423.9, "ram_available_mb": 38417.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.449}, "power_stats": {"power_gpu_soc_mean_watts": 21.584, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 70.449, "power_watts_avg": 21.584, "energy_joules_est": 198.28, "duration_seconds": 9.186, "sample_count": 78}, "timestamp": "2026-01-25T17:29:05.446455"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9900.494, "latencies_ms": [9900.494], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The train is in the foreground, moving along the tracks, while the bridge is in the background, spanning across the image. The road is in the foreground on the left side of the image, with the bridge connecting the two sides of the road. The town is in the far background, nestled among the hills.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.9, "ram_available_mb": 38417.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24427.7, "ram_available_mb": 38413.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.798}, "power_stats": {"power_gpu_soc_mean_watts": 21.304, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 69.798, "power_watts_avg": 21.304, "energy_joules_est": 210.93, "duration_seconds": 9.901, "sample_count": 84}, "timestamp": "2026-01-25T17:29:17.389800"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6406.726, "latencies_ms": [6406.726], "images_per_second": 0.156, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A modern white bridge spans over a railway track, with a train passing underneath. The sky is partly cloudy, and there are buildings in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.7, "ram_available_mb": 38413.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24419.1, "ram_available_mb": 38421.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.444}, "power_stats": {"power_gpu_soc_mean_watts": 23.16, "power_cpu_cv_mean_watts": 1.49, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 74.444, "power_watts_avg": 23.16, "energy_joules_est": 148.39, "duration_seconds": 6.407, "sample_count": 54}, "timestamp": "2026-01-25T17:29:25.845738"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6749.319, "latencies_ms": [6749.319], "images_per_second": 0.148, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The bridge in the image is a white suspension bridge with a clear blue sky and fluffy white clouds in the background. The train on the tracks appears to be a silver bullet train.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.1, "ram_available_mb": 38421.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24414.9, "ram_available_mb": 38426.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.982}, "power_stats": {"power_gpu_soc_mean_watts": 22.854, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 9.098, "gpu_utilization_percent_mean": 72.982, "power_watts_avg": 22.854, "energy_joules_est": 154.26, "duration_seconds": 6.75, "sample_count": 57}, "timestamp": "2026-01-25T17:29:34.653193"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11129.166, "latencies_ms": [11129.166], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people gathered in a park, enjoying a day of flying kites. There are several kites of various sizes and colors, with one large kite flying in the air and others on the ground. The people are spread out across the field, with some standing closer to the kites and others further away.\n\nIn the background, there are a few cars", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24414.9, "ram_available_mb": 38426.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24413.5, "ram_available_mb": 38427.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.67}, "power_stats": {"power_gpu_soc_mean_watts": 20.947, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.67, "power_watts_avg": 20.947, "energy_joules_est": 233.14, "duration_seconds": 11.13, "sample_count": 94}, "timestamp": "2026-01-25T17:29:47.811980"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10167.139, "latencies_ms": [10167.139], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 69, "n_tiles": 16, "output_text": "1. Kite: 1\n2. People: 10\n3. Grass: 1\n4. Chair: 1\n5. Frisbee: 0\n6. Trees: 1\n7. Soccer ball: 0\n8. Sweatshirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24413.5, "ram_available_mb": 38427.4, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24421.8, "ram_available_mb": 38419.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.953}, "power_stats": {"power_gpu_soc_mean_watts": 21.414, "power_cpu_cv_mean_watts": 1.829, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 70.953, "power_watts_avg": 21.414, "energy_joules_est": 217.73, "duration_seconds": 10.168, "sample_count": 86}, "timestamp": "2026-01-25T17:30:00.010683"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10211.006, "latencies_ms": [10211.006], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "In the foreground, there is a vibrant kite with a butterfly design flying in the air. In the background, there are people on a grassy field, some of whom are flying kites as well. The kite in the foreground is closer to the camera, while the people in the background are further away.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24421.8, "ram_available_mb": 38419.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24418.6, "ram_available_mb": 38422.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.943}, "power_stats": {"power_gpu_soc_mean_watts": 21.229, "power_cpu_cv_mean_watts": 1.883, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 69.943, "power_watts_avg": 21.229, "energy_joules_est": 216.79, "duration_seconds": 10.212, "sample_count": 88}, "timestamp": "2026-01-25T17:30:12.241178"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7722.736, "latencies_ms": [7722.736], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A group of people are gathered in a park, flying kites on a sunny day. The kites are colorful and soaring high in the sky, while some people are sitting on the grass, watching the fun.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24418.6, "ram_available_mb": 38422.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24413.3, "ram_available_mb": 38427.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.646}, "power_stats": {"power_gpu_soc_mean_watts": 22.402, "power_cpu_cv_mean_watts": 1.644, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 72.646, "power_watts_avg": 22.402, "energy_joules_est": 173.02, "duration_seconds": 7.723, "sample_count": 65}, "timestamp": "2026-01-25T17:30:22.005025"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9409.53, "latencies_ms": [9409.53], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image features a vibrant kite with a combination of blue, purple, yellow, and red colors flying in the sky. The kite appears to be made of a lightweight fabric, and the weather seems to be sunny and windy, making it an ideal day for kite flying.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24413.3, "ram_available_mb": 38427.6, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24427.1, "ram_available_mb": 38413.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.35}, "power_stats": {"power_gpu_soc_mean_watts": 21.474, "power_cpu_cv_mean_watts": 1.846, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 70.35, "power_watts_avg": 21.474, "energy_joules_est": 202.07, "duration_seconds": 9.41, "sample_count": 80}, "timestamp": "2026-01-25T17:30:33.462871"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11102.498, "latencies_ms": [11102.498], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a miniature model train set, meticulously crafted to resemble a real-life scenario. Dominating the scene is a vibrant red Virgin brand train, its sleek design accentuated by a white stripe running along its side. The train is in motion, traveling from the left to the right of the frame, as indicated by the bl", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24427.1, "ram_available_mb": 38413.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24430.3, "ram_available_mb": 38410.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.117}, "power_stats": {"power_gpu_soc_mean_watts": 20.93, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.117, "power_watts_avg": 20.93, "energy_joules_est": 232.39, "duration_seconds": 11.103, "sample_count": 94}, "timestamp": "2026-01-25T17:30:46.620183"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9371.133, "latencies_ms": [9371.133], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "1. Virgin: 1\n2. Train: 1\n3. Workers: 5\n4. Train tracks: 4\n5. Fence: 1\n6. Bushes: 1\n7. Hill: 1\n8. Cable: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24430.3, "ram_available_mb": 38410.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24419.7, "ram_available_mb": 38421.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.8}, "power_stats": {"power_gpu_soc_mean_watts": 21.62, "power_cpu_cv_mean_watts": 1.806, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 71.8, "power_watts_avg": 21.62, "energy_joules_est": 202.62, "duration_seconds": 9.372, "sample_count": 80}, "timestamp": "2026-01-25T17:30:58.046547"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10019.691, "latencies_ms": [10019.691], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a model train on a track with miniature figures of workers in orange uniforms standing on the tracks beside it. The train is positioned in the middle ground of the image, moving from left to right. In the background, there is a miniature landscape with a fence and some greenery.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24419.7, "ram_available_mb": 38421.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24425.5, "ram_available_mb": 38415.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.535}, "power_stats": {"power_gpu_soc_mean_watts": 21.252, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 69.535, "power_watts_avg": 21.252, "energy_joules_est": 212.96, "duration_seconds": 10.021, "sample_count": 86}, "timestamp": "2026-01-25T17:31:10.110677"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8281.773, "latencies_ms": [8281.773], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A model train set depicts a Virgin brand train traveling on tracks with miniature workers in orange uniforms standing on the tracks beside it. The scene is set in a miniature landscape with grassy hills and a fence in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.5, "ram_available_mb": 38415.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24419.2, "ram_available_mb": 38421.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.053, "power_cpu_cv_mean_watts": 1.716, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 72.0, "power_watts_avg": 22.053, "energy_joules_est": 182.65, "duration_seconds": 8.282, "sample_count": 70}, "timestamp": "2026-01-25T17:31:20.441125"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7096.105, "latencies_ms": [7096.105], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A model train set is displayed on a track with miniature figures of workers in orange uniforms. The train is predominantly red and black with a yellow front and the Virgin branding on the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.2, "ram_available_mb": 38421.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.95}, "power_stats": {"power_gpu_soc_mean_watts": 22.606, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 9.06, "gpu_utilization_percent_mean": 72.95, "power_watts_avg": 22.606, "energy_joules_est": 160.43, "duration_seconds": 7.097, "sample_count": 60}, "timestamp": "2026-01-25T17:31:29.596504"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11100.213, "latencies_ms": [11100.213], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a close-up view of a cat's fur, which is predominantly white with a distinct brown patch on its back. The fur appears soft and well-groomed, with a natural sheen that suggests it is healthy and well-cared for. The background is blurred, but it seems to be a textured surface, possibly a piece", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24441.0, "ram_available_mb": 38399.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.606}, "power_stats": {"power_gpu_soc_mean_watts": 20.916, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.606, "power_watts_avg": 20.916, "energy_joules_est": 232.19, "duration_seconds": 11.101, "sample_count": 94}, "timestamp": "2026-01-25T17:31:42.765418"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6979.013, "latencies_ms": [6979.013], "images_per_second": 0.143, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "animal: 1, fur: numerous, stripes: 2, pattern: 1, background: 1, texture: 1, color: 2, direction: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24441.0, "ram_available_mb": 38399.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24437.7, "ram_available_mb": 38403.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.186}, "power_stats": {"power_gpu_soc_mean_watts": 22.912, "power_cpu_cv_mean_watts": 1.547, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 74.186, "power_watts_avg": 22.912, "energy_joules_est": 159.92, "duration_seconds": 6.98, "sample_count": 59}, "timestamp": "2026-01-25T17:31:51.804829"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11187.873, "latencies_ms": [11187.873], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a close-up of an animal's fur in the foreground, with a soft, textured background that appears to be a patterned fabric, possibly a blanket or a piece of clothing. The fur is in sharp focus, while the background is out of focus, creating a sense of depth. The fur's texture and color contrast with the background, making the", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24437.7, "ram_available_mb": 38403.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24431.1, "ram_available_mb": 38409.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.084}, "power_stats": {"power_gpu_soc_mean_watts": 21.0, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 70.084, "power_watts_avg": 21.0, "energy_joules_est": 234.96, "duration_seconds": 11.188, "sample_count": 95}, "timestamp": "2026-01-25T17:32:05.042307"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10799.54, "latencies_ms": [10799.54], "images_per_second": 0.093, "prompt_tokens": 37, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image shows a close-up of a cat's fur, with a blurred background that appears to be a textured fabric, possibly a blanket or a piece of clothing. The focus is on the fur, which is predominantly brown with some white patches, suggesting that the cat may have a tabby or similar coat pattern.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24431.1, "ram_available_mb": 38409.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24437.2, "ram_available_mb": 38403.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.088}, "power_stats": {"power_gpu_soc_mean_watts": 21.133, "power_cpu_cv_mean_watts": 1.874, "power_sys_5v0_mean_watts": 8.917, "gpu_utilization_percent_mean": 70.088, "power_watts_avg": 21.133, "energy_joules_est": 228.24, "duration_seconds": 10.8, "sample_count": 91}, "timestamp": "2026-01-25T17:32:17.884708"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8388.611, "latencies_ms": [8388.611], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a close-up of a cat's fur, which is predominantly white with dark brown patches. The fur appears soft and well-groomed, and the lighting is soft, suggesting an indoor setting with natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.2, "ram_available_mb": 38403.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24440.8, "ram_available_mb": 38400.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.889}, "power_stats": {"power_gpu_soc_mean_watts": 21.854, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 70.889, "power_watts_avg": 21.854, "energy_joules_est": 183.34, "duration_seconds": 8.389, "sample_count": 72}, "timestamp": "2026-01-25T17:32:28.321874"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11145.425, "latencies_ms": [11145.425], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a black and white cow is standing in a barn, with its head lowered towards the ground. The cow is positioned next to a metal gate, which is slightly ajar. The cow's head is resting on a metal pipe, which is located on the ground. The pipe is white and red in color, and it appears to be a part of a", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24440.8, "ram_available_mb": 38400.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24450.4, "ram_available_mb": 38390.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.702}, "power_stats": {"power_gpu_soc_mean_watts": 20.888, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.932, "gpu_utilization_percent_mean": 69.702, "power_watts_avg": 20.888, "energy_joules_est": 232.82, "duration_seconds": 11.146, "sample_count": 94}, "timestamp": "2026-01-25T17:32:41.506562"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9623.541, "latencies_ms": [9623.541], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "1. Cow: 1\n2. Feeder: 1\n3. Straw: 1\n4. Floor: 1\n5. Stick: 1\n6. Sticker: 1\n7. Sticker: 1\n8. Sticker: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24450.4, "ram_available_mb": 38390.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.744}, "power_stats": {"power_gpu_soc_mean_watts": 21.482, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 70.744, "power_watts_avg": 21.482, "energy_joules_est": 206.75, "duration_seconds": 9.624, "sample_count": 82}, "timestamp": "2026-01-25T17:32:53.161726"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11225.745, "latencies_ms": [11225.745], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two red and white objects that appear to be milking cups attached to the udder of a cow, which is positioned near the center of the image. The cow's hind legs are visible in the background, and there is a yellow tag on the cow's ear. The floor is covered with straw and hay, indicating the cow is", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.277}, "power_stats": {"power_gpu_soc_mean_watts": 20.134, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 70.277, "power_watts_avg": 20.134, "energy_joules_est": 226.03, "duration_seconds": 11.226, "sample_count": 94}, "timestamp": "2026-01-25T17:33:06.397750"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8173.286, "latencies_ms": [8173.286], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A cow is standing in a barn with its hind legs visible, and there are three red and white cylindrical objects attached to its udders. The cow is standing on a black mat with some straw and hay scattered around.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.667}, "power_stats": {"power_gpu_soc_mean_watts": 22.13, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 72.667, "power_watts_avg": 22.13, "energy_joules_est": 180.89, "duration_seconds": 8.174, "sample_count": 69}, "timestamp": "2026-01-25T17:33:16.607528"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8430.945, "latencies_ms": [8430.945], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows a close-up of a cow's udder with two red and white milking machines attached. The cow's fur is predominantly black with some white patches, and the environment appears to be indoors with artificial lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24441.2, "ram_available_mb": 38399.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.338}, "power_stats": {"power_gpu_soc_mean_watts": 21.861, "power_cpu_cv_mean_watts": 1.759, "power_sys_5v0_mean_watts": 9.037, "gpu_utilization_percent_mean": 71.338, "power_watts_avg": 21.861, "energy_joules_est": 184.32, "duration_seconds": 8.432, "sample_count": 71}, "timestamp": "2026-01-25T17:33:27.078031"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11137.407, "latencies_ms": [11137.407], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a sandwich with a slice taken out of it rests on a white plate with a floral pattern. The sandwich is made with two slices of white bread, and the filling is a vibrant red, suggesting it might be made of beetroot or a similar ingredient. The plate is placed on a green tablecloth, which", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24441.2, "ram_available_mb": 38399.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24478.0, "ram_available_mb": 38362.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.532}, "power_stats": {"power_gpu_soc_mean_watts": 20.922, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 69.532, "power_watts_avg": 20.922, "energy_joules_est": 233.03, "duration_seconds": 11.138, "sample_count": 94}, "timestamp": "2026-01-25T17:33:40.237928"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8186.586, "latencies_ms": [8186.586], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "sandwich: 3\nbutter knife: 1\nplate: 1\ntablecloth: 1\ndark background: 1\nlight source: 1\nplate design: 1\nknife handle: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24478.0, "ram_available_mb": 38362.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24423.8, "ram_available_mb": 38417.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.145}, "power_stats": {"power_gpu_soc_mean_watts": 22.298, "power_cpu_cv_mean_watts": 1.683, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 73.145, "power_watts_avg": 22.298, "energy_joules_est": 182.56, "duration_seconds": 8.187, "sample_count": 69}, "timestamp": "2026-01-25T17:33:50.442325"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7921.647, "latencies_ms": [7921.647], "images_per_second": 0.126, "prompt_tokens": 44, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The sandwich is placed on the left side of the plate, which is in the foreground of the image. The knife is positioned on the right side of the plate, near the edge, indicating it is ready to be used.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.8, "ram_available_mb": 38417.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24428.8, "ram_available_mb": 38412.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.662}, "power_stats": {"power_gpu_soc_mean_watts": 22.002, "power_cpu_cv_mean_watts": 1.719, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 71.662, "power_watts_avg": 22.002, "energy_joules_est": 174.31, "duration_seconds": 7.922, "sample_count": 68}, "timestamp": "2026-01-25T17:34:00.424881"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6943.081, "latencies_ms": [6943.081], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A sandwich with a bite taken out of it is placed on a decorative plate with a knife beside it. The setting appears to be a table with a green tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24428.8, "ram_available_mb": 38412.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24422.5, "ram_available_mb": 38418.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.712}, "power_stats": {"power_gpu_soc_mean_watts": 22.993, "power_cpu_cv_mean_watts": 1.561, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 73.712, "power_watts_avg": 22.993, "energy_joules_est": 159.66, "duration_seconds": 6.944, "sample_count": 59}, "timestamp": "2026-01-25T17:34:09.399453"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11104.751, "latencies_ms": [11104.751], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a sandwich with a visible filling of red berries, possibly raspberries or strawberries, nestled within a lightly toasted bread. The sandwich is placed on a plate with a delicate floral pattern, accompanied by a knife with a dark handle, all set against a dark background that contrasts with the warm tones of the food.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24422.5, "ram_available_mb": 38418.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24422.2, "ram_available_mb": 38418.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.521}, "power_stats": {"power_gpu_soc_mean_watts": 20.885, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 69.521, "power_watts_avg": 20.885, "energy_joules_est": 231.94, "duration_seconds": 11.105, "sample_count": 96}, "timestamp": "2026-01-25T17:34:22.536226"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11097.211, "latencies_ms": [11097.211], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with a tray containing a variety of food items. There are four different bowls on the tray, each filled with different dishes. One bowl contains a salad, another has pasta with meat and cheese, and the third bowl has carrots. The fourth bowl is filled with grapes. The arrangement of the bow", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 24422.2, "ram_available_mb": 38418.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24422.4, "ram_available_mb": 38418.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.8}, "power_stats": {"power_gpu_soc_mean_watts": 20.944, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.8, "power_watts_avg": 20.944, "energy_joules_est": 232.43, "duration_seconds": 11.098, "sample_count": 95}, "timestamp": "2026-01-25T17:34:35.690709"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8395.487, "latencies_ms": [8395.487], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "salad: 1, carrots: 4, grapes: 6, pasta: 1, cheese: 1, tomato sauce: 1, meat: 1, zucchini: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.4, "ram_available_mb": 38418.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.417}, "power_stats": {"power_gpu_soc_mean_watts": 22.123, "power_cpu_cv_mean_watts": 1.724, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 72.417, "power_watts_avg": 22.123, "energy_joules_est": 185.75, "duration_seconds": 8.396, "sample_count": 72}, "timestamp": "2026-01-25T17:34:46.104156"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11114.546, "latencies_ms": [11114.546], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a purple tray with four compartments, each containing different food items. The leftmost compartment has a salad with various vegetables, the second compartment contains sliced carrots, the third compartment has a pasta dish with cheese on top, and the fourth compartment contains green grapes. The background is dark", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24428.7, "ram_available_mb": 38412.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.632}, "power_stats": {"power_gpu_soc_mean_watts": 20.954, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.632, "power_watts_avg": 20.954, "energy_joules_est": 232.91, "duration_seconds": 11.115, "sample_count": 95}, "timestamp": "2026-01-25T17:34:59.270379"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6725.69, "latencies_ms": [6725.69], "images_per_second": 0.149, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image shows a meal prep container with four different sections, each containing a different type of food. The container is placed on a dark surface, possibly a table or countertop.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24428.7, "ram_available_mb": 38412.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24421.1, "ram_available_mb": 38419.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.439}, "power_stats": {"power_gpu_soc_mean_watts": 23.132, "power_cpu_cv_mean_watts": 1.545, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 75.439, "power_watts_avg": 23.132, "energy_joules_est": 155.59, "duration_seconds": 6.726, "sample_count": 57}, "timestamp": "2026-01-25T17:35:08.016261"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8214.975, "latencies_ms": [8214.975], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a meal prep container with vibrant colors, including a bright orange, green, and purple. The lighting appears to be artificial, likely from an indoor source, as there are no shadows or natural light indicators.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.1, "ram_available_mb": 38419.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.594}, "power_stats": {"power_gpu_soc_mean_watts": 22.018, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 9.014, "gpu_utilization_percent_mean": 71.594, "power_watts_avg": 22.018, "energy_joules_est": 180.89, "duration_seconds": 8.216, "sample_count": 69}, "timestamp": "2026-01-25T17:35:18.262597"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11104.737, "latencies_ms": [11104.737], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a cherry blossom tree in full bloom, its branches adorned with delicate pink flowers. The tree stands tall against a backdrop of a clear blue sky. A traffic light, painted in shades of red and white, is affixed to a pole in the foreground. The traffic light is currently displaying a red signal", "error": null, "sys_before": {"cpu_percent": 9.4, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24437.4, "ram_available_mb": 38403.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.421}, "power_stats": {"power_gpu_soc_mean_watts": 20.937, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.421, "power_watts_avg": 20.937, "energy_joules_est": 232.51, "duration_seconds": 11.105, "sample_count": 95}, "timestamp": "2026-01-25T17:35:31.425638"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6286.045, "latencies_ms": [6286.045], "images_per_second": 0.159, "prompt_tokens": 39, "response_tokens_est": 34, "n_tiles": 16, "output_text": "- Trees: numerous\n- Flowers: numerous\n- Traffic lights: 2\n- Red light: 1\n- Black background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.4, "ram_available_mb": 38403.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24436.4, "ram_available_mb": 38404.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.943}, "power_stats": {"power_gpu_soc_mean_watts": 23.55, "power_cpu_cv_mean_watts": 1.48, "power_sys_5v0_mean_watts": 9.034, "gpu_utilization_percent_mean": 74.943, "power_watts_avg": 23.55, "energy_joules_est": 148.05, "duration_seconds": 6.287, "sample_count": 53}, "timestamp": "2026-01-25T17:35:39.754659"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9585.08, "latencies_ms": [9585.08], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The traffic lights are positioned in the foreground on the left side of the image, while the cherry blossom trees cover the background and extend to the right side of the image. The trees appear to be in the near foreground, with the traffic lights being closer to the viewer than the trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.4, "ram_available_mb": 38404.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24435.6, "ram_available_mb": 38405.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.506}, "power_stats": {"power_gpu_soc_mean_watts": 21.545, "power_cpu_cv_mean_watts": 1.819, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 70.506, "power_watts_avg": 21.545, "energy_joules_est": 206.52, "duration_seconds": 9.586, "sample_count": 81}, "timestamp": "2026-01-25T17:35:51.387456"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10196.696, "latencies_ms": [10196.696], "images_per_second": 0.098, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image captures a vibrant scene of a traffic light with a red signal, set against a backdrop of cherry blossom trees in full bloom. The trees, adorned with delicate pink flowers, create a picturesque and serene setting, enhancing the visual appeal of the traffic light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.6, "ram_available_mb": 38405.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24436.4, "ram_available_mb": 38404.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.529}, "power_stats": {"power_gpu_soc_mean_watts": 21.369, "power_cpu_cv_mean_watts": 1.854, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 71.529, "power_watts_avg": 21.369, "energy_joules_est": 217.91, "duration_seconds": 10.197, "sample_count": 87}, "timestamp": "2026-01-25T17:36:03.625863"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9309.669, "latencies_ms": [9309.669], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image features a traffic light with a red signal illuminated, set against a backdrop of cherry blossoms in full bloom. The blossoms are predominantly white with hints of pink, and the trees are densely packed, creating a canopy of flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.4, "ram_available_mb": 38404.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24433.8, "ram_available_mb": 38407.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.162}, "power_stats": {"power_gpu_soc_mean_watts": 20.933, "power_cpu_cv_mean_watts": 1.811, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 72.162, "power_watts_avg": 20.933, "energy_joules_est": 194.89, "duration_seconds": 9.31, "sample_count": 80}, "timestamp": "2026-01-25T17:36:14.978979"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11133.245, "latencies_ms": [11133.245], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a white plate filled with a delicious meal consisting of broccoli and meat. The plate is placed on a dining table, and the food is arranged in a visually appealing manner. The broccoli is spread across the plate, with some pieces located closer to the edges and others near the center. The meat, possibly chicken, is placed in the middle", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24433.8, "ram_available_mb": 38407.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24425.7, "ram_available_mb": 38415.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.642}, "power_stats": {"power_gpu_soc_mean_watts": 20.899, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 69.642, "power_watts_avg": 20.899, "energy_joules_est": 232.69, "duration_seconds": 11.134, "sample_count": 95}, "timestamp": "2026-01-25T17:36:28.155646"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8640.056, "latencies_ms": [8640.056], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "plate: 1\nchicken: 1\nbroccoli: 10\nonions: 5\ngarlic: 2\npepper: 1\nherbs: 1\nsalt: 1\npepper: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.7, "ram_available_mb": 38415.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.438}, "power_stats": {"power_gpu_soc_mean_watts": 22.043, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 72.438, "power_watts_avg": 22.043, "energy_joules_est": 190.47, "duration_seconds": 8.641, "sample_count": 73}, "timestamp": "2026-01-25T17:36:38.841808"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10704.122, "latencies_ms": [10704.122], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground of the image, there is a pile of cooked broccoli with some bits of red and white, possibly onions or garlic, scattered around. In the background, there is a piece of grilled salmon with grill marks on it. The broccoli is in the front and the salmon is in the back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24418.4, "ram_available_mb": 38422.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.407}, "power_stats": {"power_gpu_soc_mean_watts": 21.087, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 69.407, "power_watts_avg": 21.087, "energy_joules_est": 225.74, "duration_seconds": 10.705, "sample_count": 91}, "timestamp": "2026-01-25T17:36:51.610106"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7960.032, "latencies_ms": [7960.032], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A plate of food is shown with a piece of grilled salmon and a mix of cooked vegetables, including broccoli and cauliflower. The dish appears to be a healthy and balanced meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.4, "ram_available_mb": 38422.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24426.9, "ram_available_mb": 38414.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.985}, "power_stats": {"power_gpu_soc_mean_watts": 22.187, "power_cpu_cv_mean_watts": 1.679, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 72.985, "power_watts_avg": 22.187, "energy_joules_est": 176.62, "duration_seconds": 7.961, "sample_count": 67}, "timestamp": "2026-01-25T17:37:01.620747"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8797.163, "latencies_ms": [8797.163], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image shows a plate of food with a piece of grilled salmon and a mix of cooked broccoli. The lighting in the image highlights the vibrant green color of the broccoli and the golden-brown hue of the salmon.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24426.9, "ram_available_mb": 38414.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24419.2, "ram_available_mb": 38421.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.932}, "power_stats": {"power_gpu_soc_mean_watts": 21.719, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 70.932, "power_watts_avg": 21.719, "energy_joules_est": 191.08, "duration_seconds": 8.798, "sample_count": 74}, "timestamp": "2026-01-25T17:37:12.442996"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11135.721, "latencies_ms": [11135.721], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a dimly lit restaurant, where three individuals are seated at a table. The person on the left, clad in a black shirt, is engrossed in their phone, perhaps browsing or texting. The middle person, wearing a red shirt, is captured mid-bite, savoring a piece of food. The person on", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 24419.2, "ram_available_mb": 38421.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24428.1, "ram_available_mb": 38412.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.319}, "power_stats": {"power_gpu_soc_mean_watts": 20.947, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 69.319, "power_watts_avg": 20.947, "energy_joules_est": 233.28, "duration_seconds": 11.136, "sample_count": 94}, "timestamp": "2026-01-25T17:37:25.623688"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9423.984, "latencies_ms": [9423.984], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "1. Person: 3\n2. Table: 1\n3. Chair: 1\n4. Cell phone: 1\n5. Napkin: 1\n6. Glass: 1\n7. Plate: 1\n8. Silverware: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24428.1, "ram_available_mb": 38412.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24420.1, "ram_available_mb": 38420.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.203}, "power_stats": {"power_gpu_soc_mean_watts": 21.573, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 71.203, "power_watts_avg": 21.573, "energy_joules_est": 203.32, "duration_seconds": 9.425, "sample_count": 79}, "timestamp": "2026-01-25T17:37:37.077343"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11097.693, "latencies_ms": [11097.693], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person on the left side of the image, partially obscured by a metal headboard with a decorative design. In the background, there are two other individuals seated across from each other, with one person slightly closer to the camera than the other. The person on the right appears to be the farthest away from the camera, with a glass of", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.1, "ram_available_mb": 38420.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24415.5, "ram_available_mb": 38425.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.468}, "power_stats": {"power_gpu_soc_mean_watts": 20.96, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 69.468, "power_watts_avg": 20.96, "energy_joules_est": 232.62, "duration_seconds": 11.098, "sample_count": 94}, "timestamp": "2026-01-25T17:37:50.198175"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7743.808, "latencies_ms": [7743.808], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image depicts a group of people sitting at a table in a dimly lit restaurant or bar. The atmosphere appears to be casual and relaxed, with the individuals engaged in conversation and enjoying their time together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24415.5, "ram_available_mb": 38425.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.2}, "power_stats": {"power_gpu_soc_mean_watts": 22.469, "power_cpu_cv_mean_watts": 1.644, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 73.2, "power_watts_avg": 22.469, "energy_joules_est": 174.01, "duration_seconds": 7.744, "sample_count": 65}, "timestamp": "2026-01-25T17:37:59.978782"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7653.981, "latencies_ms": [7653.981], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image features a warm, dimly lit interior with a dominant orange hue, likely from artificial lighting. A metal railing with ornate designs is visible in the background, suggesting an indoor setting with decorative elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24416.2, "ram_available_mb": 38424.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.154}, "power_stats": {"power_gpu_soc_mean_watts": 22.264, "power_cpu_cv_mean_watts": 1.706, "power_sys_5v0_mean_watts": 9.028, "gpu_utilization_percent_mean": 72.154, "power_watts_avg": 22.264, "energy_joules_est": 170.42, "duration_seconds": 7.655, "sample_count": 65}, "timestamp": "2026-01-25T17:38:09.663682"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11143.617, "latencies_ms": [11143.617], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a bustling city scene. Dominating the foreground is a yellow bus, its vibrant color contrasting with the gray of the cityscape. The bus is in motion, driving on a road that cuts through the scene. \n\nTo the right of the bus, a sidewalk stretches out, lined with trees that provide a touch", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24416.2, "ram_available_mb": 38424.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24421.2, "ram_available_mb": 38419.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.189}, "power_stats": {"power_gpu_soc_mean_watts": 20.895, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.189, "power_watts_avg": 20.895, "energy_joules_est": 232.86, "duration_seconds": 11.144, "sample_count": 95}, "timestamp": "2026-01-25T17:38:22.854816"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7393.954, "latencies_ms": [7393.954], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "building: 1\nbus: 2\nvan: 2\ntree: 4\nbench: 1\ntrash can: 1\nsidewalk: 1\nroad: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.2, "ram_available_mb": 38419.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24419.2, "ram_available_mb": 38421.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.032}, "power_stats": {"power_gpu_soc_mean_watts": 22.735, "power_cpu_cv_mean_watts": 1.62, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 74.032, "power_watts_avg": 22.735, "energy_joules_est": 168.12, "duration_seconds": 7.395, "sample_count": 63}, "timestamp": "2026-01-25T17:38:32.303155"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10881.893, "latencies_ms": [10881.893], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a yellow bus on the right side of the image, near a bus stop with a bench. In the background, there is a large building with many windows, and a few cars and vans parked in front of it. The bus is on the right side of the image, while the building and cars are on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.2, "ram_available_mb": 38421.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24430.2, "ram_available_mb": 38410.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.581}, "power_stats": {"power_gpu_soc_mean_watts": 21.053, "power_cpu_cv_mean_watts": 1.911, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 69.581, "power_watts_avg": 21.053, "energy_joules_est": 229.11, "duration_seconds": 10.883, "sample_count": 93}, "timestamp": "2026-01-25T17:38:45.212641"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11064.342, "latencies_ms": [11064.342], "images_per_second": 0.09, "prompt_tokens": 37, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The image depicts a city street with a yellow bus parked on the side of the road. There are several buildings in the background, including a large multi-story building with a curved facade. The street is lined with trees and there are a few cars parked along the curb. The sky is clear and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24430.2, "ram_available_mb": 38410.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24422.5, "ram_available_mb": 38418.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.6}, "power_stats": {"power_gpu_soc_mean_watts": 21.102, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.919, "gpu_utilization_percent_mean": 70.6, "power_watts_avg": 21.102, "energy_joules_est": 233.49, "duration_seconds": 11.065, "sample_count": 95}, "timestamp": "2026-01-25T17:38:58.312861"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8645.076, "latencies_ms": [8645.076], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image shows a clear day with sunlight casting shadows on the ground, which is paved with rectangular bricks. The bus is predominantly white with a yellow front, and the building in the background has a modern design with a mix of glass and concrete.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.5, "ram_available_mb": 38418.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24429.2, "ram_available_mb": 38411.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.219}, "power_stats": {"power_gpu_soc_mean_watts": 21.838, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 9.017, "gpu_utilization_percent_mean": 71.219, "power_watts_avg": 21.838, "energy_joules_est": 188.8, "duration_seconds": 8.646, "sample_count": 73}, "timestamp": "2026-01-25T17:39:08.982772"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11119.617, "latencies_ms": [11119.617], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene on a city street. Dominating the foreground is a red stop sign, its octagonal shape and bold color standing out against the urban backdrop. The sign is affixed to a sturdy metal pole, which is slightly tilted to the left, adding a dynamic element to the composition. \n\nThe pole is not alone", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24429.2, "ram_available_mb": 38411.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 24433.0, "ram_available_mb": 38407.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.011}, "power_stats": {"power_gpu_soc_mean_watts": 20.889, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 70.011, "power_watts_avg": 20.889, "energy_joules_est": 232.29, "duration_seconds": 11.12, "sample_count": 94}, "timestamp": "2026-01-25T17:39:22.137520"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8520.437, "latencies_ms": [8520.437], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Stop sign: 1\n- Pole: 1\n- Street: 1\n- Sun: 1\n- Buildings: 1\n- Trees: 1\n- Bushes: 1\n- Fence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.0, "ram_available_mb": 38407.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24431.2, "ram_available_mb": 38409.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.542}, "power_stats": {"power_gpu_soc_mean_watts": 22.072, "power_cpu_cv_mean_watts": 1.707, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 72.542, "power_watts_avg": 22.072, "energy_joules_est": 188.08, "duration_seconds": 8.521, "sample_count": 72}, "timestamp": "2026-01-25T17:39:32.679181"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10728.27, "latencies_ms": [10728.27], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The stop sign is positioned in the foreground on the right side of the image, while the buildings and vehicles are in the background, indicating that the sign is closer to the viewer than the buildings and vehicles. The sun is shining from the left side, casting a shadow of the stop sign to the right, showing that the sun is behind the sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.2, "ram_available_mb": 38409.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24432.6, "ram_available_mb": 38408.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.424}, "power_stats": {"power_gpu_soc_mean_watts": 21.101, "power_cpu_cv_mean_watts": 1.902, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 70.424, "power_watts_avg": 21.101, "energy_joules_est": 226.39, "duration_seconds": 10.729, "sample_count": 92}, "timestamp": "2026-01-25T17:39:45.422288"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6951.35, "latencies_ms": [6951.35], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image shows a red stop sign mounted on a metal pole at an intersection. The sign is positioned in front of a metal fence, with buildings and a street visible in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24432.6, "ram_available_mb": 38408.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24441.3, "ram_available_mb": 38399.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.475}, "power_stats": {"power_gpu_soc_mean_watts": 23.033, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 73.475, "power_watts_avg": 23.033, "energy_joules_est": 160.13, "duration_seconds": 6.952, "sample_count": 59}, "timestamp": "2026-01-25T17:39:54.418573"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8126.972, "latencies_ms": [8126.972], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image features a red stop sign with white lettering, mounted on a metal pole. The sign is positioned on the side of a road with buildings in the background, and the sun is shining brightly, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24441.3, "ram_available_mb": 38399.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24429.0, "ram_available_mb": 38411.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.043}, "power_stats": {"power_gpu_soc_mean_watts": 22.088, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 71.043, "power_watts_avg": 22.088, "energy_joules_est": 179.52, "duration_seconds": 8.128, "sample_count": 69}, "timestamp": "2026-01-25T17:40:04.602378"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11122.759, "latencies_ms": [11122.759], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of this image, a white and brown cat is the main subject. The cat is lying on its side, its body stretched out in a relaxed manner. Its head is tilted slightly to the left, and its eyes are looking directly at the camera, giving an impression of curiosity or alertness. The cat's fur is a mix of white and brown, with", "error": null, "sys_before": {"cpu_percent": 17.4, "ram_used_mb": 24429.0, "ram_available_mb": 38411.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24417.1, "ram_available_mb": 38423.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.126}, "power_stats": {"power_gpu_soc_mean_watts": 20.869, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 69.126, "power_watts_avg": 20.869, "energy_joules_est": 232.13, "duration_seconds": 11.123, "sample_count": 95}, "timestamp": "2026-01-25T17:40:17.764740"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7398.774, "latencies_ms": [7398.774], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "cat: 1, mouse: 1, computer mouse: 1, cord: 1, black surface: 1, white surface: 1, paw: 1, ear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24417.1, "ram_available_mb": 38423.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24428.0, "ram_available_mb": 38412.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.683}, "power_stats": {"power_gpu_soc_mean_watts": 22.607, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 73.683, "power_watts_avg": 22.607, "energy_joules_est": 167.28, "duration_seconds": 7.399, "sample_count": 63}, "timestamp": "2026-01-25T17:40:27.209747"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11120.149, "latencies_ms": [11120.149], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The cat is lying in the foreground on a black surface, with its body stretched out and its head turned towards the camera. The computer mouse is positioned to the left of the cat, slightly in the background, and is connected to the cat by a white cord. The cord extends from the mouse towards the right side of the image, indicating that the mouse is near the cat.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24428.0, "ram_available_mb": 38412.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24418.9, "ram_available_mb": 38422.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.653}, "power_stats": {"power_gpu_soc_mean_watts": 20.869, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 69.653, "power_watts_avg": 20.869, "energy_joules_est": 232.08, "duration_seconds": 11.121, "sample_count": 95}, "timestamp": "2026-01-25T17:40:40.348240"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9177.318, "latencies_ms": [9177.318], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "A cat with a white and brown coat is lying on a black surface, possibly a couch or a bed, with its paws stretched out in front of it. Next to the cat is a computer mouse, suggesting that the cat may be a pet of someone who uses the computer.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24418.9, "ram_available_mb": 38422.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24427.5, "ram_available_mb": 38413.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.675}, "power_stats": {"power_gpu_soc_mean_watts": 21.835, "power_cpu_cv_mean_watts": 1.778, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 71.675, "power_watts_avg": 21.835, "energy_joules_est": 200.4, "duration_seconds": 9.178, "sample_count": 77}, "timestamp": "2026-01-25T17:40:51.538190"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6101.045, "latencies_ms": [6101.045], "images_per_second": 0.164, "prompt_tokens": 36, "response_tokens_est": 34, "n_tiles": 16, "output_text": "The image features a cat with a white and brown coat, lying on a dark surface. The lighting in the image is dim, creating a moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24425.5, "ram_available_mb": 38415.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24420.5, "ram_available_mb": 38420.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.654}, "power_stats": {"power_gpu_soc_mean_watts": 23.341, "power_cpu_cv_mean_watts": 1.532, "power_sys_5v0_mean_watts": 9.072, "gpu_utilization_percent_mean": 73.654, "power_watts_avg": 23.341, "energy_joules_est": 142.42, "duration_seconds": 6.102, "sample_count": 52}, "timestamp": "2026-01-25T17:40:59.677003"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11124.5, "latencies_ms": [11124.5], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a bustling city scene, dominated by a large bus terminal. The terminal is a hive of activity, with numerous buses parked in neat rows, ready to embark on their respective journeys. The buses, painted in a variety of colors, add a vibrant touch to the otherwise monochrome cityscape.\n\nThe terminal", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 24420.5, "ram_available_mb": 38420.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24444.7, "ram_available_mb": 38396.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.302}, "power_stats": {"power_gpu_soc_mean_watts": 20.873, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.302, "power_watts_avg": 20.873, "energy_joules_est": 232.22, "duration_seconds": 11.125, "sample_count": 96}, "timestamp": "2026-01-25T17:41:12.826702"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8955.59, "latencies_ms": [8955.59], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 58, "n_tiles": 16, "output_text": "- Buses: 10\n- Buildings: 12\n- Trees: 5\n- Power lines: 4\n- Clouds: 10\n- Sky: 1\n- Fence: 1\n- Signs: 3", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24444.7, "ram_available_mb": 38396.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24421.2, "ram_available_mb": 38419.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.645}, "power_stats": {"power_gpu_soc_mean_watts": 21.718, "power_cpu_cv_mean_watts": 1.754, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 70.645, "power_watts_avg": 21.718, "energy_joules_est": 194.51, "duration_seconds": 8.956, "sample_count": 76}, "timestamp": "2026-01-25T17:41:23.808882"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11093.912, "latencies_ms": [11093.912], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are several buses parked under a covered area, with one bus prominently in the center. The buses are positioned near a road that curves to the left in the background. Further back, there are multiple high-rise buildings, with the tallest one located on the right side of the image. The sky is visible in the upper part of", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24421.2, "ram_available_mb": 38419.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24416.3, "ram_available_mb": 38424.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.063}, "power_stats": {"power_gpu_soc_mean_watts": 20.995, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 70.063, "power_watts_avg": 20.995, "energy_joules_est": 232.93, "duration_seconds": 11.094, "sample_count": 95}, "timestamp": "2026-01-25T17:41:36.941261"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6958.581, "latencies_ms": [6958.581], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image depicts a bustling city scene with multiple buses parked at a bus station. The station is located in a densely populated urban area with tall buildings surrounding it.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24416.3, "ram_available_mb": 38424.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24435.1, "ram_available_mb": 38405.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.814}, "power_stats": {"power_gpu_soc_mean_watts": 22.951, "power_cpu_cv_mean_watts": 1.554, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 74.814, "power_watts_avg": 22.951, "energy_joules_est": 159.72, "duration_seconds": 6.959, "sample_count": 59}, "timestamp": "2026-01-25T17:41:45.920336"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8897.513, "latencies_ms": [8897.513], "images_per_second": 0.112, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a clear day with a few clouds in the sky, and the lighting suggests it is daytime. The buses are predominantly white with some having green and blue accents, and they are parked in a large, open area with a concrete structure overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.1, "ram_available_mb": 38405.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24423.2, "ram_available_mb": 38417.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.84}, "power_stats": {"power_gpu_soc_mean_watts": 21.682, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 9.017, "gpu_utilization_percent_mean": 70.84, "power_watts_avg": 21.682, "energy_joules_est": 192.93, "duration_seconds": 8.898, "sample_count": 75}, "timestamp": "2026-01-25T17:41:56.853475"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11127.06, "latencies_ms": [11127.06], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is skillfully riding a skateboard down the side of a ramp. He is wearing a cowboy hat and appears to be enjoying the activity. The skateboarder is positioned in the center of the scene, with the ramp beneath him. \n\nIn the background, there are several chairs placed around the area, possibly", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24423.2, "ram_available_mb": 38417.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24434.4, "ram_available_mb": 38406.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.606}, "power_stats": {"power_gpu_soc_mean_watts": 20.908, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 69.606, "power_watts_avg": 20.908, "energy_joules_est": 232.66, "duration_seconds": 11.128, "sample_count": 94}, "timestamp": "2026-01-25T17:42:10.036613"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8305.233, "latencies_ms": [8305.233], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "skateboard: 1\nskateboarder: 1\ntent: 3\nsandals: 1\nshade: 1\ntable: 1\nchairs: 1\numbrella: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24434.4, "ram_available_mb": 38406.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24438.9, "ram_available_mb": 38402.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.292}, "power_stats": {"power_gpu_soc_mean_watts": 22.097, "power_cpu_cv_mean_watts": 1.718, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 72.292, "power_watts_avg": 22.097, "energy_joules_est": 183.53, "duration_seconds": 8.306, "sample_count": 72}, "timestamp": "2026-01-25T17:42:20.380544"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9483.048, "latencies_ms": [9483.048], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, performing a trick on a ramp. In the background, there are large green tents set up, and a few people can be seen sitting or standing near them. The tents appear to be further away from the camera than the skateboarder.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24438.9, "ram_available_mb": 38402.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24437.5, "ram_available_mb": 38403.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.075}, "power_stats": {"power_gpu_soc_mean_watts": 21.549, "power_cpu_cv_mean_watts": 1.827, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 71.075, "power_watts_avg": 21.549, "energy_joules_est": 204.36, "duration_seconds": 9.484, "sample_count": 80}, "timestamp": "2026-01-25T17:42:31.882598"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7299.112, "latencies_ms": [7299.112], "images_per_second": 0.137, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A person is skateboarding on a ramp in an outdoor setting with large green tents in the background. The skateboarder is wearing a cowboy hat and black shorts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.5, "ram_available_mb": 38403.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24424.8, "ram_available_mb": 38416.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.623}, "power_stats": {"power_gpu_soc_mean_watts": 22.688, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 72.623, "power_watts_avg": 22.688, "energy_joules_est": 165.62, "duration_seconds": 7.3, "sample_count": 61}, "timestamp": "2026-01-25T17:42:41.207027"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8144.24, "latencies_ms": [8144.24], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a person skateboarding on a ramp with a clear blue sky in the background. The skateboarder is wearing a black tank top and shorts, and the ramp is made of wood with red markings.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24424.8, "ram_available_mb": 38416.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24426.2, "ram_available_mb": 38414.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.13}, "power_stats": {"power_gpu_soc_mean_watts": 22.087, "power_cpu_cv_mean_watts": 1.723, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 72.13, "power_watts_avg": 22.087, "energy_joules_est": 179.9, "duration_seconds": 8.145, "sample_count": 69}, "timestamp": "2026-01-25T17:42:51.382349"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12080.927, "latencies_ms": [12080.927], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a dynamic scene of a person windsurfing on a sunny day. The individual is standing on a white surfboard, which is equipped with a blue sail. The windsurfer is positioned in the water, with the sail fully extended, harnessing the power of the wind to glide across the waves. The ocean, painted in shades of", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24426.2, "ram_available_mb": 38414.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24423.6, "ram_available_mb": 38417.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.076}, "power_stats": {"power_gpu_soc_mean_watts": 22.622, "power_cpu_cv_mean_watts": 1.803, "power_sys_5v0_mean_watts": 9.156, "gpu_utilization_percent_mean": 73.076, "power_watts_avg": 22.622, "energy_joules_est": 273.31, "duration_seconds": 12.082, "sample_count": 105}, "timestamp": "2026-01-25T17:43:05.529387"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8313.492, "latencies_ms": [8313.492], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "person: 1, surfboard: 1, kite: 4, wave: multiple, ocean: multiple, sky: multiple, wind: multiple, kite surfing: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.6, "ram_available_mb": 38417.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24434.2, "ram_available_mb": 38406.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.529}, "power_stats": {"power_gpu_soc_mean_watts": 24.094, "power_cpu_cv_mean_watts": 1.458, "power_sys_5v0_mean_watts": 9.147, "gpu_utilization_percent_mean": 77.529, "power_watts_avg": 24.094, "energy_joules_est": 200.32, "duration_seconds": 8.314, "sample_count": 70}, "timestamp": "2026-01-25T17:43:15.879503"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12058.004, "latencies_ms": [12058.004], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person standing on a surfboard with a sail, positioned near the water's edge. The waves are in the middle ground, with the person closer to the surfboard than the waves. In the background, there are multiple kites flying high in the sky, with the closest kite being the highest and the farthest kite being", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.2, "ram_available_mb": 38406.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24421.6, "ram_available_mb": 38419.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.641}, "power_stats": {"power_gpu_soc_mean_watts": 22.669, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 9.143, "gpu_utilization_percent_mean": 72.641, "power_watts_avg": 22.669, "energy_joules_est": 273.36, "duration_seconds": 12.059, "sample_count": 103}, "timestamp": "2026-01-25T17:43:29.965899"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7209.785, "latencies_ms": [7209.785], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 33, "n_tiles": 16, "output_text": "A person is windsurfing on a sunny day with several kites flying in the sky. The ocean is rough with waves suitable for the sport.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.6, "ram_available_mb": 38419.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24416.1, "ram_available_mb": 38424.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.672}, "power_stats": {"power_gpu_soc_mean_watts": 24.636, "power_cpu_cv_mean_watts": 1.306, "power_sys_5v0_mean_watts": 9.117, "gpu_utilization_percent_mean": 79.672, "power_watts_avg": 24.636, "energy_joules_est": 177.64, "duration_seconds": 7.21, "sample_count": 61}, "timestamp": "2026-01-25T17:43:39.190114"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8587.086, "latencies_ms": [8587.086], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image captures a bright and sunny day at the beach with clear blue skies and a few clouds. The ocean is a deep blue-green color, and the waves are white-capped, indicating strong winds.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24416.1, "ram_available_mb": 38424.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.164}, "power_stats": {"power_gpu_soc_mean_watts": 23.744, "power_cpu_cv_mean_watts": 1.546, "power_sys_5v0_mean_watts": 9.189, "gpu_utilization_percent_mean": 76.164, "power_watts_avg": 23.744, "energy_joules_est": 203.91, "duration_seconds": 8.588, "sample_count": 73}, "timestamp": "2026-01-25T17:43:49.803370"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11112.803, "latencies_ms": [11112.803], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a lush garden, a vibrant red fire hydrant stands as a beacon of safety. Its black cap and nozzles are a stark contrast to its bright color, adding an element of intrigue to its appearance. The hydrant is nestled in a bed of vibrant green grass, which is dotted with delicate white flowers, adding a touch", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24423.0, "ram_available_mb": 38417.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.116}, "power_stats": {"power_gpu_soc_mean_watts": 20.889, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 69.116, "power_watts_avg": 20.889, "energy_joules_est": 232.15, "duration_seconds": 11.113, "sample_count": 95}, "timestamp": "2026-01-25T17:44:02.964129"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8196.532, "latencies_ms": [8196.532], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "fire hydrant: 1, grass: numerous, dandelions: 8, house: 1, tree: 1, window: 1, flowers: 1, roof: 1, wisteria: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24423.0, "ram_available_mb": 38417.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24422.3, "ram_available_mb": 38418.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.609}, "power_stats": {"power_gpu_soc_mean_watts": 22.141, "power_cpu_cv_mean_watts": 1.688, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 71.609, "power_watts_avg": 22.141, "energy_joules_est": 181.49, "duration_seconds": 8.197, "sample_count": 69}, "timestamp": "2026-01-25T17:44:13.211722"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9385.994, "latencies_ms": [9385.994], "images_per_second": 0.107, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The fire hydrant is located in the foreground of the image, standing on a grassy area. In the background, there is a white house with a thatched roof and a tree with purple flowers. The fire hydrant is positioned closer to the viewer than the house and the tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.3, "ram_available_mb": 38418.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24421.6, "ram_available_mb": 38419.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.513}, "power_stats": {"power_gpu_soc_mean_watts": 21.562, "power_cpu_cv_mean_watts": 1.811, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 71.513, "power_watts_avg": 21.562, "energy_joules_est": 202.39, "duration_seconds": 9.387, "sample_count": 80}, "timestamp": "2026-01-25T17:44:24.633213"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6535.333, "latencies_ms": [6535.333], "images_per_second": 0.153, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "A red fire hydrant is situated in a grassy area with a white house in the background. The house has a thatched roof and is surrounded by trees and flowers.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24421.6, "ram_available_mb": 38419.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24438.9, "ram_available_mb": 38401.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.291}, "power_stats": {"power_gpu_soc_mean_watts": 23.318, "power_cpu_cv_mean_watts": 1.477, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 74.291, "power_watts_avg": 23.318, "energy_joules_est": 152.41, "duration_seconds": 6.536, "sample_count": 55}, "timestamp": "2026-01-25T17:44:33.213782"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6447.404, "latencies_ms": [6447.404], "images_per_second": 0.155, "prompt_tokens": 36, "response_tokens_est": 37, "n_tiles": 16, "output_text": "The fire hydrant in the image is bright red and appears to be made of metal. It is situated in a grassy area with a white house and trees in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24438.9, "ram_available_mb": 38401.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24447.4, "ram_available_mb": 38393.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.278}, "power_stats": {"power_gpu_soc_mean_watts": 23.284, "power_cpu_cv_mean_watts": 1.549, "power_sys_5v0_mean_watts": 9.073, "gpu_utilization_percent_mean": 74.278, "power_watts_avg": 23.284, "energy_joules_est": 150.14, "duration_seconds": 6.448, "sample_count": 54}, "timestamp": "2026-01-25T17:44:41.688288"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11082.439, "latencies_ms": [11082.439], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of a bird in flight, with its wings spread wide and its head turned towards the camera. The bird is perched on a wooden surface, which is painted in a light blue color. The surface is composed of multiple wooden planks, each with a slightly different shade of blue, creating a subtle pattern. The bird's feathers are a mix of", "error": null, "sys_before": {"cpu_percent": 13.0, "ram_used_mb": 24447.4, "ram_available_mb": 38393.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24414.3, "ram_available_mb": 38426.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.925}, "power_stats": {"power_gpu_soc_mean_watts": 21.055, "power_cpu_cv_mean_watts": 1.937, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 69.925, "power_watts_avg": 21.055, "energy_joules_est": 233.35, "duration_seconds": 11.083, "sample_count": 93}, "timestamp": "2026-01-25T17:44:54.792135"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8383.496, "latencies_ms": [8383.496], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "bird: 2, wooden plank: 5, paint peeling: 3, color blue: 1, color green: 1, color brown: 1, bird's wing: 1, bird's head: 1", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 24414.3, "ram_available_mb": 38426.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24412.2, "ram_available_mb": 38428.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.944}, "power_stats": {"power_gpu_soc_mean_watts": 21.978, "power_cpu_cv_mean_watts": 1.725, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 71.944, "power_watts_avg": 21.978, "energy_joules_est": 184.26, "duration_seconds": 8.384, "sample_count": 71}, "timestamp": "2026-01-25T17:45:05.196413"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11112.926, "latencies_ms": [11112.926], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a bird with its wings spread, positioned on the left side of the image, closer to the viewer. In the background, there are two more birds, one partially visible on the left and another on the right, both are further away from the viewer. The main object, the bird with spread wings, is in the center and appears to be in", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24412.2, "ram_available_mb": 38428.6, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24416.9, "ram_available_mb": 38424.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.766}, "power_stats": {"power_gpu_soc_mean_watts": 20.908, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.766, "power_watts_avg": 20.908, "energy_joules_est": 232.36, "duration_seconds": 11.114, "sample_count": 94}, "timestamp": "2026-01-25T17:45:18.367166"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9179.924, "latencies_ms": [9179.924], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "A small bird is captured in mid-flight, with its wings spread wide, as it appears to be landing on a wooden surface with peeling blue paint. The surface has a rough texture and shows signs of wear and tear, with visible cracks and chips in the paint.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24416.9, "ram_available_mb": 38424.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24426.1, "ram_available_mb": 38414.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.487}, "power_stats": {"power_gpu_soc_mean_watts": 21.646, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 70.487, "power_watts_avg": 21.646, "energy_joules_est": 198.72, "duration_seconds": 9.181, "sample_count": 78}, "timestamp": "2026-01-25T17:45:29.585336"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9654.411, "latencies_ms": [9654.411], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image features a close-up of a bird with a dark body and a lighter underbelly, perched on a weathered wooden surface with peeling blue paint. The lighting is soft and diffused, casting gentle shadows and highlighting the texture of the wood and the bird's feathers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.1, "ram_available_mb": 38414.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24412.0, "ram_available_mb": 38428.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.646}, "power_stats": {"power_gpu_soc_mean_watts": 21.314, "power_cpu_cv_mean_watts": 1.846, "power_sys_5v0_mean_watts": 9.019, "gpu_utilization_percent_mean": 70.646, "power_watts_avg": 21.314, "energy_joules_est": 205.79, "duration_seconds": 9.655, "sample_count": 82}, "timestamp": "2026-01-25T17:45:41.281706"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11121.753, "latencies_ms": [11121.753], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing in a barn, watching a brown horse with a red halter walking around. The horse is positioned in the center of the barn, and the woman is standing to the right of it. The barn is filled with various items, including a clock on the wall, a truck, and a couple of buckets. There are also two", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24412.0, "ram_available_mb": 38428.9, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.463}, "power_stats": {"power_gpu_soc_mean_watts": 20.87, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 69.463, "power_watts_avg": 20.87, "energy_joules_est": 232.12, "duration_seconds": 11.122, "sample_count": 95}, "timestamp": "2026-01-25T17:45:54.439844"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7047.847, "latencies_ms": [7047.847], "images_per_second": 0.142, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "door: 1, horse: 1, bucket: 1, wall: 1, window: 1, floor: 1, person: 1, desk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24416.2, "ram_available_mb": 38424.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.867}, "power_stats": {"power_gpu_soc_mean_watts": 22.799, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 9.009, "gpu_utilization_percent_mean": 73.867, "power_watts_avg": 22.799, "energy_joules_est": 160.7, "duration_seconds": 7.048, "sample_count": 60}, "timestamp": "2026-01-25T17:46:03.528663"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11100.879, "latencies_ms": [11100.879], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a brown horse with a red halter, positioned near the center of the image, walking towards the right side. A person, wearing blue jeans and a grey shirt, is standing to the right of the horse, closer to the background. The background features a wooden barn with a red door on the left, a window, and various items", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24416.2, "ram_available_mb": 38424.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24410.0, "ram_available_mb": 38430.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.745}, "power_stats": {"power_gpu_soc_mean_watts": 20.963, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 69.745, "power_watts_avg": 20.963, "energy_joules_est": 232.72, "duration_seconds": 11.102, "sample_count": 94}, "timestamp": "2026-01-25T17:46:16.643477"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6738.193, "latencies_ms": [6738.193], "images_per_second": 0.148, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A woman is standing in a barn with a horse that is walking towards her. The barn has wooden walls and a red door, and there is a window in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24410.0, "ram_available_mb": 38430.9, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24417.6, "ram_available_mb": 38423.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.579}, "power_stats": {"power_gpu_soc_mean_watts": 23.079, "power_cpu_cv_mean_watts": 1.566, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 74.579, "power_watts_avg": 23.079, "energy_joules_est": 155.53, "duration_seconds": 6.739, "sample_count": 57}, "timestamp": "2026-01-25T17:46:25.426731"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6645.948, "latencies_ms": [6645.948], "images_per_second": 0.15, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image depicts an indoor setting with wooden walls and a concrete floor. The lighting appears to be natural daylight coming from a window, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24417.6, "ram_available_mb": 38423.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24412.9, "ram_available_mb": 38427.9, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.661}, "power_stats": {"power_gpu_soc_mean_watts": 22.954, "power_cpu_cv_mean_watts": 1.58, "power_sys_5v0_mean_watts": 9.1, "gpu_utilization_percent_mean": 73.661, "power_watts_avg": 22.954, "energy_joules_est": 152.57, "duration_seconds": 6.647, "sample_count": 56}, "timestamp": "2026-01-25T17:46:34.101111"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11118.933, "latencies_ms": [11118.933], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a lush green field with a variety of animals grazing and roaming around. There are several zebras and a couple of rhinos in the field, with some of them standing close to each other. A herd of zebras can be seen in the middle of the field, while a couple of rhinos are located towards the left side of the scene.\n", "error": null, "sys_before": {"cpu_percent": 10.3, "ram_used_mb": 24412.9, "ram_available_mb": 38427.9, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24420.9, "ram_available_mb": 38420.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.617}, "power_stats": {"power_gpu_soc_mean_watts": 20.964, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 69.617, "power_watts_avg": 20.964, "energy_joules_est": 233.11, "duration_seconds": 11.12, "sample_count": 94}, "timestamp": "2026-01-25T17:46:47.247693"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9066.333, "latencies_ms": [9066.333], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "grass: 1\n\ntree: 10\n\nrock: 5\n\nwild boar: 2\n\nzebra: 2\n\ngiraffe: 1\n\nantelope: 2\n\nelephant: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.9, "ram_available_mb": 38420.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24420.6, "ram_available_mb": 38420.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.935}, "power_stats": {"power_gpu_soc_mean_watts": 21.661, "power_cpu_cv_mean_watts": 1.757, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 70.935, "power_watts_avg": 21.661, "energy_joules_est": 196.4, "duration_seconds": 9.067, "sample_count": 77}, "timestamp": "2026-01-25T17:46:58.352488"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9878.397, "latencies_ms": [9878.397], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, there is a grassy field with a zebra grazing on the right side and a few other animals scattered around. In the background, there are trees and a pond, with a few more animals near the water's edge. The sky is visible in the top left corner of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.6, "ram_available_mb": 38420.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24437.2, "ram_available_mb": 38403.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.655}, "power_stats": {"power_gpu_soc_mean_watts": 21.329, "power_cpu_cv_mean_watts": 1.854, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 70.655, "power_watts_avg": 21.329, "energy_joules_est": 210.71, "duration_seconds": 9.879, "sample_count": 84}, "timestamp": "2026-01-25T17:47:10.266785"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7747.602, "latencies_ms": [7747.602], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image depicts a lush green field with a variety of animals grazing and roaming around. There are zebras, wildebeests, and other animals in the field, with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.2, "ram_available_mb": 38403.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24415.7, "ram_available_mb": 38425.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.606}, "power_stats": {"power_gpu_soc_mean_watts": 22.386, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 73.606, "power_watts_avg": 22.386, "energy_joules_est": 173.45, "duration_seconds": 7.748, "sample_count": 66}, "timestamp": "2026-01-25T17:47:20.056126"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8438.829, "latencies_ms": [8438.829], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image depicts a sunny day with clear skies, as evidenced by the bright lighting and shadows cast on the ground. The landscape is a mix of green grass and brown rocks, with trees in the background providing a natural backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24415.7, "ram_available_mb": 38425.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24419.1, "ram_available_mb": 38421.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.708}, "power_stats": {"power_gpu_soc_mean_watts": 21.818, "power_cpu_cv_mean_watts": 1.757, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 70.708, "power_watts_avg": 21.818, "energy_joules_est": 184.14, "duration_seconds": 8.44, "sample_count": 72}, "timestamp": "2026-01-25T17:47:30.517011"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11125.933, "latencies_ms": [11125.933], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a bustling theme park, where a white horse-drawn carriage, adorned with a green and gold canopy, is the centerpiece. The carriage, labeled \"Disneyland\", is being pulled by a majestic white horse, its muscles rippling under the canopy. The horse's harness is a striking", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24419.1, "ram_available_mb": 38421.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24424.1, "ram_available_mb": 38416.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.6}, "power_stats": {"power_gpu_soc_mean_watts": 20.9, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.6, "power_watts_avg": 20.9, "energy_joules_est": 232.55, "duration_seconds": 11.127, "sample_count": 95}, "timestamp": "2026-01-25T17:47:43.682129"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7707.23, "latencies_ms": [7707.23], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "people: 10, disneyland sign: 1, horse: 1, carriage: 1, bench: 1, trees: 5, sky: 1, umbrella: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24424.1, "ram_available_mb": 38416.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24419.4, "ram_available_mb": 38421.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.045}, "power_stats": {"power_gpu_soc_mean_watts": 22.455, "power_cpu_cv_mean_watts": 1.668, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 72.045, "power_watts_avg": 22.455, "energy_joules_est": 173.08, "duration_seconds": 7.708, "sample_count": 66}, "timestamp": "2026-01-25T17:47:53.431016"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9586.401, "latencies_ms": [9586.401], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 66, "n_tiles": 16, "output_text": "In the foreground, there is a white horse pulling a Disneyland trolley with several passengers on board. The trolley is on a street with a green canopy and a sign that reads \"Disneyland.\" In the background, there are people walking on the sidewalk and trees lining the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.4, "ram_available_mb": 38421.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24433.6, "ram_available_mb": 38407.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.346}, "power_stats": {"power_gpu_soc_mean_watts": 21.525, "power_cpu_cv_mean_watts": 1.873, "power_sys_5v0_mean_watts": 9.02, "gpu_utilization_percent_mean": 71.346, "power_watts_avg": 21.525, "energy_joules_est": 206.37, "duration_seconds": 9.587, "sample_count": 81}, "timestamp": "2026-01-25T17:48:05.069079"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7737.317, "latencies_ms": [7737.317], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image captures a vintage scene of a horse-drawn carriage on Main Street, U.S.A. in Disneyland, with a white horse pulling the carriage and a man seated at the front.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24433.6, "ram_available_mb": 38407.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24419.5, "ram_available_mb": 38421.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.892}, "power_stats": {"power_gpu_soc_mean_watts": 22.53, "power_cpu_cv_mean_watts": 1.638, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 72.892, "power_watts_avg": 22.53, "energy_joules_est": 174.34, "duration_seconds": 7.738, "sample_count": 65}, "timestamp": "2026-01-25T17:48:14.820508"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9891.85, "latencies_ms": [9891.85], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image features a vibrant scene with a horse-drawn carriage in the center, painted in green and gold with a red roof, indicating a festive or touristic setting. The lighting suggests it's a sunny day, casting shadows on the ground, and the weather appears to be clear and pleasant.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24419.5, "ram_available_mb": 38421.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24416.0, "ram_available_mb": 38424.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.429}, "power_stats": {"power_gpu_soc_mean_watts": 21.272, "power_cpu_cv_mean_watts": 1.835, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 70.429, "power_watts_avg": 21.272, "energy_joules_est": 210.43, "duration_seconds": 9.893, "sample_count": 84}, "timestamp": "2026-01-25T17:48:26.755641"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11111.007, "latencies_ms": [11111.007], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of older men sitting on benches in a public area, likely a park or a waiting area. There are at least five men visible, with some sitting on the benches and others standing nearby. They are all engaged in various activities, such as reading newspapers, looking at their cell phones, or simply enjoying the outdoors.\n\nThere are multiple", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 24416.0, "ram_available_mb": 38424.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24429.0, "ram_available_mb": 38411.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.777}, "power_stats": {"power_gpu_soc_mean_watts": 20.91, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 69.777, "power_watts_avg": 20.91, "energy_joules_est": 232.34, "duration_seconds": 11.112, "sample_count": 94}, "timestamp": "2026-01-25T17:48:39.918698"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7615.452, "latencies_ms": [7615.452], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "bench: 5\nman: 5\nnewspaper: 1\ntrash can: 1\nwindow: 1\ndoor: 1\nplant: 1\nposter: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.0, "ram_available_mb": 38411.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24418.5, "ram_available_mb": 38422.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.391}, "power_stats": {"power_gpu_soc_mean_watts": 22.388, "power_cpu_cv_mean_watts": 1.639, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 72.391, "power_watts_avg": 22.388, "energy_joules_est": 170.51, "duration_seconds": 7.616, "sample_count": 64}, "timestamp": "2026-01-25T17:48:49.566935"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9666.783, "latencies_ms": [9666.783], "images_per_second": 0.103, "prompt_tokens": 44, "response_tokens_est": 66, "n_tiles": 16, "output_text": "In the foreground, there is a man sitting on a green bench reading a newspaper. Behind him, there are several other benches with people sitting on them, and a building with a sign that reads \"SOCIETY\". The man on the left bench is closer to the camera than the others.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24418.5, "ram_available_mb": 38422.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24416.1, "ram_available_mb": 38424.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.049}, "power_stats": {"power_gpu_soc_mean_watts": 21.419, "power_cpu_cv_mean_watts": 1.841, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 70.049, "power_watts_avg": 21.419, "energy_joules_est": 207.07, "duration_seconds": 9.667, "sample_count": 82}, "timestamp": "2026-01-25T17:49:01.286612"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7968.414, "latencies_ms": [7968.414], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image depicts a group of older men sitting on benches in a public area, with one man reading a newspaper. The setting appears to be a park or a public square with multiple benches and a building in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24416.1, "ram_available_mb": 38424.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24422.3, "ram_available_mb": 38418.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.537}, "power_stats": {"power_gpu_soc_mean_watts": 22.245, "power_cpu_cv_mean_watts": 1.661, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 72.537, "power_watts_avg": 22.245, "energy_joules_est": 177.28, "duration_seconds": 7.969, "sample_count": 67}, "timestamp": "2026-01-25T17:49:11.309388"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5764.846, "latencies_ms": [5764.846], "images_per_second": 0.173, "prompt_tokens": 36, "response_tokens_est": 31, "n_tiles": 16, "output_text": "The image shows a sunny day with clear skies, casting shadows on the ground. The men are seated on green metal benches.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24422.3, "ram_available_mb": 38418.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24425.2, "ram_available_mb": 38415.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.083}, "power_stats": {"power_gpu_soc_mean_watts": 23.81, "power_cpu_cv_mean_watts": 1.426, "power_sys_5v0_mean_watts": 9.098, "gpu_utilization_percent_mean": 75.083, "power_watts_avg": 23.81, "energy_joules_est": 137.28, "duration_seconds": 5.765, "sample_count": 48}, "timestamp": "2026-01-25T17:49:19.103570"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11139.719, "latencies_ms": [11139.719], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a desk with a laptop computer sitting on top of it. The laptop is open, and the screen is turned on, displaying a desktop wallpaper. A glass of orange juice is placed on the desk, and a book is also visible nearby. A lamp is situated on the desk, providing light for the workspace.\n\nIn addition to the laptop, there", "error": null, "sys_before": {"cpu_percent": 13.6, "ram_used_mb": 24425.2, "ram_available_mb": 38415.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24414.2, "ram_available_mb": 38426.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.383}, "power_stats": {"power_gpu_soc_mean_watts": 20.933, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.383, "power_watts_avg": 20.933, "energy_joules_est": 233.2, "duration_seconds": 11.14, "sample_count": 94}, "timestamp": "2026-01-25T17:49:32.265118"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8641.408, "latencies_ms": [8641.408], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Desk: 1\n- Laptop: 1\n- Phone: 1\n- Glass: 1\n- Lamp: 1\n- Candle holder: 1\n- Picture: 1\n- Drawer: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24414.2, "ram_available_mb": 38426.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24413.9, "ram_available_mb": 38427.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.616}, "power_stats": {"power_gpu_soc_mean_watts": 21.946, "power_cpu_cv_mean_watts": 1.727, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 72.616, "power_watts_avg": 21.946, "energy_joules_est": 189.66, "duration_seconds": 8.642, "sample_count": 73}, "timestamp": "2026-01-25T17:49:42.949413"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9996.952, "latencies_ms": [9996.952], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The laptop is positioned on the left side of the desk, closer to the foreground, while the lamp is on the right side, further back. The phone is placed to the left of the laptop, and the glass of orange juice is on the right side of the laptop, closer to the edge of the desk.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24413.9, "ram_available_mb": 38427.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24415.3, "ram_available_mb": 38425.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.494}, "power_stats": {"power_gpu_soc_mean_watts": 21.275, "power_cpu_cv_mean_watts": 1.865, "power_sys_5v0_mean_watts": 9.028, "gpu_utilization_percent_mean": 70.494, "power_watts_avg": 21.275, "energy_joules_est": 212.7, "duration_seconds": 9.998, "sample_count": 85}, "timestamp": "2026-01-25T17:49:54.969448"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8152.558, "latencies_ms": [8152.558], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image depicts a well-lit workspace with a laptop open on a wooden desk, displaying a desktop screen. A lamp with a black shade is placed to the right of the laptop, providing additional lighting to the area.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24415.3, "ram_available_mb": 38425.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24413.5, "ram_available_mb": 38427.4, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.696}, "power_stats": {"power_gpu_soc_mean_watts": 22.225, "power_cpu_cv_mean_watts": 1.695, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 71.696, "power_watts_avg": 22.225, "energy_joules_est": 181.2, "duration_seconds": 8.153, "sample_count": 69}, "timestamp": "2026-01-25T17:50:05.146262"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7005.479, "latencies_ms": [7005.479], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image features a desk with a laptop, a lamp, and a glass of orange juice. The lamp is turned on, casting a warm glow on the desk and the laptop screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24413.5, "ram_available_mb": 38427.4, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24430.0, "ram_available_mb": 38410.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.322}, "power_stats": {"power_gpu_soc_mean_watts": 22.683, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.038, "gpu_utilization_percent_mean": 72.322, "power_watts_avg": 22.683, "energy_joules_est": 158.92, "duration_seconds": 7.006, "sample_count": 59}, "timestamp": "2026-01-25T17:50:14.200583"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12358.064, "latencies_ms": [12358.064], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene beach scene. In the foreground, there's a blue and white striped towel neatly spread out on the sand. A blue surfboard with a white stripe is placed next to the towel, ready for a day of fun in the water. A red and white striped bag is also visible, perhaps containing personal belongings or beach", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24430.0, "ram_available_mb": 38410.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.151}, "power_stats": {"power_gpu_soc_mean_watts": 22.796, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 9.187, "gpu_utilization_percent_mean": 73.151, "power_watts_avg": 22.796, "energy_joules_est": 281.73, "duration_seconds": 12.359, "sample_count": 106}, "timestamp": "2026-01-25T17:50:28.624551"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9616.865, "latencies_ms": [9616.865], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "beach chair: 2, umbrella: 1, surfboard: 2, towel: 1, cooler: 1, flip flops: 1, handbag: 1, backpack: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24423.9, "ram_available_mb": 38417.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.28}, "power_stats": {"power_gpu_soc_mean_watts": 23.885, "power_cpu_cv_mean_watts": 1.528, "power_sys_5v0_mean_watts": 9.152, "gpu_utilization_percent_mean": 77.28, "power_watts_avg": 23.885, "energy_joules_est": 229.71, "duration_seconds": 9.618, "sample_count": 82}, "timestamp": "2026-01-25T17:50:40.259091"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12401.123, "latencies_ms": [12401.123], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a blue and white striped towel laid out on the sand, with a pink and white surfboard placed next to it. In the background, there is a beach chair with an umbrella, and a person is visible in the water further back. The surfboard and towel are near the edge of the frame, while the chair and", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24423.9, "ram_available_mb": 38417.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24421.1, "ram_available_mb": 38419.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.5}, "power_stats": {"power_gpu_soc_mean_watts": 22.834, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 9.211, "gpu_utilization_percent_mean": 73.5, "power_watts_avg": 22.834, "energy_joules_est": 283.18, "duration_seconds": 12.402, "sample_count": 106}, "timestamp": "2026-01-25T17:50:54.718372"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11546.601, "latencies_ms": [11546.601], "images_per_second": 0.087, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image depicts a beach scene with a blue and white striped towel laid out on the sand, a pink and white surfboard, and a blue surfboard. There is a green umbrella and a chair set up for relaxation, and a person can be seen in the water in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24421.1, "ram_available_mb": 38419.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24415.7, "ram_available_mb": 38425.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.316}, "power_stats": {"power_gpu_soc_mean_watts": 23.284, "power_cpu_cv_mean_watts": 1.667, "power_sys_5v0_mean_watts": 9.138, "gpu_utilization_percent_mean": 75.316, "power_watts_avg": 23.284, "energy_joules_est": 268.87, "duration_seconds": 11.547, "sample_count": 98}, "timestamp": "2026-01-25T17:51:08.309495"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11516.356, "latencies_ms": [11516.356], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The beach scene features a bright blue sky and calm ocean waves, with a clear day that suggests good weather for outdoor activities. The sand is a light beige color, and there are various items scattered on it, including a blue and white striped towel, a pink and white surfboard, and a brown leather bag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24415.7, "ram_available_mb": 38425.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.867}, "power_stats": {"power_gpu_soc_mean_watts": 23.097, "power_cpu_cv_mean_watts": 1.72, "power_sys_5v0_mean_watts": 9.209, "gpu_utilization_percent_mean": 73.867, "power_watts_avg": 23.097, "energy_joules_est": 266.01, "duration_seconds": 11.517, "sample_count": 98}, "timestamp": "2026-01-25T17:51:21.843516"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11115.052, "latencies_ms": [11115.052], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a solitary sheep stands majestically on a rocky outcropping. The sheep, with its white wool, is facing to the left, as if gazing into the distance. The rocky outcropping, a rugged terrain of gray and black rocks, provides a stark contrast to the sheep's soft wool. The sky above is a clear blue", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24446.6, "ram_available_mb": 38394.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.84}, "power_stats": {"power_gpu_soc_mean_watts": 20.896, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 69.84, "power_watts_avg": 20.896, "energy_joules_est": 232.27, "duration_seconds": 11.116, "sample_count": 94}, "timestamp": "2026-01-25T17:51:34.985003"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7512.112, "latencies_ms": [7512.112], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "rock: 10\nsheep: 1\ncloud: 10\nblue: 1\nwhite: 1\ngrass: 1\nmountain: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24446.6, "ram_available_mb": 38394.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.952}, "power_stats": {"power_gpu_soc_mean_watts": 22.674, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 73.952, "power_watts_avg": 22.674, "energy_joules_est": 170.34, "duration_seconds": 7.513, "sample_count": 63}, "timestamp": "2026-01-25T17:51:44.527228"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8941.027, "latencies_ms": [8941.027], "images_per_second": 0.112, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The sheep is positioned in the foreground on the left side of the image, standing on a rocky outcrop. The sky occupies the background, with clouds scattered across it, and the grassy area is in the far background, providing a natural setting for the sheep.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24420.1, "ram_available_mb": 38420.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.618}, "power_stats": {"power_gpu_soc_mean_watts": 21.724, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 71.618, "power_watts_avg": 21.724, "energy_joules_est": 194.25, "duration_seconds": 8.942, "sample_count": 76}, "timestamp": "2026-01-25T17:51:55.483291"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6745.431, "latencies_ms": [6745.431], "images_per_second": 0.148, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A sheep stands alone on a rocky outcrop under a clear blue sky with scattered clouds. The terrain is rugged and the sheep appears to be gazing into the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.1, "ram_available_mb": 38420.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24418.5, "ram_available_mb": 38422.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.351}, "power_stats": {"power_gpu_soc_mean_watts": 23.228, "power_cpu_cv_mean_watts": 1.524, "power_sys_5v0_mean_watts": 9.029, "gpu_utilization_percent_mean": 74.351, "power_watts_avg": 23.228, "energy_joules_est": 156.7, "duration_seconds": 6.746, "sample_count": 57}, "timestamp": "2026-01-25T17:52:04.254895"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7372.178, "latencies_ms": [7372.178], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The sheep is standing on a rocky outcrop with a clear blue sky and fluffy white clouds in the background. The rocks are dark and jagged, and the sheep is a light tan color.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24418.5, "ram_available_mb": 38422.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24442.9, "ram_available_mb": 38398.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.935}, "power_stats": {"power_gpu_soc_mean_watts": 22.523, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 9.05, "gpu_utilization_percent_mean": 71.935, "power_watts_avg": 22.523, "energy_joules_est": 166.06, "duration_seconds": 7.373, "sample_count": 62}, "timestamp": "2026-01-25T17:52:13.663186"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11117.092, "latencies_ms": [11117.092], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person with striking blue hair is captured in a moment of self-reflection. They are wearing a blue shirt and a black tie, adding a touch of professionalism to their appearance. The person is holding a phone in their right hand, which is displaying a vibrant image of a red flower. The background is a simple beige wall, providing a neutral", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 24442.9, "ram_available_mb": 38398.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24433.5, "ram_available_mb": 38407.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.702}, "power_stats": {"power_gpu_soc_mean_watts": 20.943, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.702, "power_watts_avg": 20.943, "energy_joules_est": 232.85, "duration_seconds": 11.118, "sample_count": 94}, "timestamp": "2026-01-25T17:52:26.852747"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7175.592, "latencies_ms": [7175.592], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "person: 1, hair: 1, phone: 1, tie: 1, shirt: 1, button: 1, pocket: 1, earring: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24433.5, "ram_available_mb": 38407.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24435.4, "ram_available_mb": 38405.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.721}, "power_stats": {"power_gpu_soc_mean_watts": 22.966, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 73.721, "power_watts_avg": 22.966, "energy_joules_est": 164.81, "duration_seconds": 7.176, "sample_count": 61}, "timestamp": "2026-01-25T17:52:36.063230"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10836.585, "latencies_ms": [10836.585], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The person with blue hair is in the foreground, taking a selfie with a smartphone held in their right hand. The phone is positioned in the right hand, slightly in front of the person's face, capturing the image. The background is a plain wall with a door handle visible on the left side, indicating the photo was taken indoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.4, "ram_available_mb": 38405.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24432.8, "ram_available_mb": 38408.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.293}, "power_stats": {"power_gpu_soc_mean_watts": 21.044, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 70.293, "power_watts_avg": 21.044, "energy_joules_est": 228.06, "duration_seconds": 10.837, "sample_count": 92}, "timestamp": "2026-01-25T17:52:48.947208"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4398.961, "latencies_ms": [4398.961], "images_per_second": 0.227, "prompt_tokens": 37, "response_tokens_est": 17, "n_tiles": 16, "output_text": "A person with blue hair is taking a selfie in a bathroom mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.8, "ram_available_mb": 38408.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24436.5, "ram_available_mb": 38404.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 81.649}, "power_stats": {"power_gpu_soc_mean_watts": 26.156, "power_cpu_cv_mean_watts": 1.104, "power_sys_5v0_mean_watts": 9.132, "gpu_utilization_percent_mean": 81.649, "power_watts_avg": 26.156, "energy_joules_est": 115.08, "duration_seconds": 4.4, "sample_count": 37}, "timestamp": "2026-01-25T17:52:55.390076"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8696.694, "latencies_ms": [8696.694], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The person in the image has vibrant blue hair and is wearing a dark blue shirt with a black tie. They are holding a smartphone in their right hand, taking a selfie in an indoor setting with a neutral-colored wall in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24436.5, "ram_available_mb": 38404.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.192}, "power_stats": {"power_gpu_soc_mean_watts": 21.827, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 71.192, "power_watts_avg": 21.827, "energy_joules_est": 189.84, "duration_seconds": 8.697, "sample_count": 73}, "timestamp": "2026-01-25T17:53:06.131980"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11109.246, "latencies_ms": [11109.246], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a room steeped in history, possibly a museum exhibit. Dominating the scene is a fireplace, its mantel adorned with a black and gold frame, housing a painting of a man and a woman. The fireplace, a symbol of warmth and comfort, is flanked by two lamps, their light casting a soft glow on the", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24418.9, "ram_available_mb": 38422.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.479}, "power_stats": {"power_gpu_soc_mean_watts": 20.952, "power_cpu_cv_mean_watts": 1.925, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 69.479, "power_watts_avg": 20.952, "energy_joules_est": 232.77, "duration_seconds": 11.11, "sample_count": 94}, "timestamp": "2026-01-25T17:53:19.276879"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6956.015, "latencies_ms": [6956.015], "images_per_second": 0.144, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.9, "ram_available_mb": 38422.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24425.0, "ram_available_mb": 38415.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.862}, "power_stats": {"power_gpu_soc_mean_watts": 22.839, "power_cpu_cv_mean_watts": 1.567, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 73.862, "power_watts_avg": 22.839, "energy_joules_est": 158.88, "duration_seconds": 6.957, "sample_count": 58}, "timestamp": "2026-01-25T17:53:28.265184"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10118.071, "latencies_ms": [10118.071], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground of the image, there is a fireplace with a white mantel and a fire burning inside. To the right of the fireplace, there is a wooden cabinet with a red curtain hanging in front of it. In the background, there are two paintings hanging on the wall, one above the other.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24425.0, "ram_available_mb": 38415.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24421.5, "ram_available_mb": 38419.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.535}, "power_stats": {"power_gpu_soc_mean_watts": 21.227, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 70.535, "power_watts_avg": 21.227, "energy_joules_est": 214.79, "duration_seconds": 10.119, "sample_count": 86}, "timestamp": "2026-01-25T17:53:40.440862"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8075.99, "latencies_ms": [8075.99], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image depicts a cozy room with a fireplace, a wooden cabinet, a chair, and a table. There are also two paintings hanging on the wall, one above the fireplace and the other above the cabinet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.5, "ram_available_mb": 38419.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24421.2, "ram_available_mb": 38419.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.368}, "power_stats": {"power_gpu_soc_mean_watts": 22.212, "power_cpu_cv_mean_watts": 1.695, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 72.368, "power_watts_avg": 22.212, "energy_joules_est": 179.4, "duration_seconds": 8.077, "sample_count": 68}, "timestamp": "2026-01-25T17:53:50.533760"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7748.184, "latencies_ms": [7748.184], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The room is dimly lit with natural light coming from the window, and the furniture is made of wood with a dark finish. The floor is covered with a patterned carpet in shades of green, red, and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.2, "ram_available_mb": 38419.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24415.1, "ram_available_mb": 38425.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.523}, "power_stats": {"power_gpu_soc_mean_watts": 22.217, "power_cpu_cv_mean_watts": 1.7, "power_sys_5v0_mean_watts": 9.033, "gpu_utilization_percent_mean": 71.523, "power_watts_avg": 22.217, "energy_joules_est": 172.16, "duration_seconds": 7.749, "sample_count": 65}, "timestamp": "2026-01-25T17:54:00.306648"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12284.884, "latencies_ms": [12284.884], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a brown dog is captured in mid-air, leaping energetically to catch a red frisbee. The dog's body is stretched out, and its front paws are extended forward, ready to grasp the frisbee. The frisbee is positioned above the dog's head, slightly to the right. The dog is surrounded", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24415.1, "ram_available_mb": 38425.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24417.4, "ram_available_mb": 38423.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.59}, "power_stats": {"power_gpu_soc_mean_watts": 22.871, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 9.197, "gpu_utilization_percent_mean": 74.59, "power_watts_avg": 22.871, "energy_joules_est": 280.98, "duration_seconds": 12.286, "sample_count": 105}, "timestamp": "2026-01-25T17:54:14.641211"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8599.218, "latencies_ms": [8599.218], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "tree: 1, frisbee: 1, dog: 1, car: 1, grass: 1, bush: 1, path: 1, leaf: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24417.4, "ram_available_mb": 38423.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24413.3, "ram_available_mb": 38427.6, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.411}, "power_stats": {"power_gpu_soc_mean_watts": 24.441, "power_cpu_cv_mean_watts": 1.426, "power_sys_5v0_mean_watts": 9.184, "gpu_utilization_percent_mean": 79.411, "power_watts_avg": 24.441, "energy_joules_est": 210.19, "duration_seconds": 8.6, "sample_count": 73}, "timestamp": "2026-01-25T17:54:25.259537"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12419.392, "latencies_ms": [12419.392], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a brown dog is captured in mid-air, leaping towards a red frisbee that is positioned near the top right corner of the image, suggesting the dog is attempting to catch it. The frisbee is closer to the camera than the dog, creating a sense of depth. In the background, there is a black car parked on the left", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24413.3, "ram_available_mb": 38427.6, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24421.1, "ram_available_mb": 38419.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.206}, "power_stats": {"power_gpu_soc_mean_watts": 22.87, "power_cpu_cv_mean_watts": 1.785, "power_sys_5v0_mean_watts": 9.205, "gpu_utilization_percent_mean": 74.206, "power_watts_avg": 22.87, "energy_joules_est": 284.05, "duration_seconds": 12.42, "sample_count": 107}, "timestamp": "2026-01-25T17:54:39.713122"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10241.222, "latencies_ms": [10241.222], "images_per_second": 0.098, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "A brown dog is in mid-air, leaping towards a red frisbee with its mouth open, attempting to catch it. The scene is set in a grassy area with a black car parked in the background and a tree to the left of the dog.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24421.1, "ram_available_mb": 38419.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24415.6, "ram_available_mb": 38425.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.759}, "power_stats": {"power_gpu_soc_mean_watts": 23.257, "power_cpu_cv_mean_watts": 1.583, "power_sys_5v0_mean_watts": 9.14, "gpu_utilization_percent_mean": 76.759, "power_watts_avg": 23.257, "energy_joules_est": 238.2, "duration_seconds": 10.242, "sample_count": 87}, "timestamp": "2026-01-25T17:54:51.978429"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8290.145, "latencies_ms": [8290.145], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image features a brown dog in mid-air, reaching for a red frisbee. The dog is positioned in a grassy area with a black car and a tree in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24415.6, "ram_available_mb": 38425.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24432.8, "ram_available_mb": 38408.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.829}, "power_stats": {"power_gpu_soc_mean_watts": 24.226, "power_cpu_cv_mean_watts": 1.447, "power_sys_5v0_mean_watts": 9.23, "gpu_utilization_percent_mean": 76.829, "power_watts_avg": 24.226, "energy_joules_est": 200.85, "duration_seconds": 8.291, "sample_count": 70}, "timestamp": "2026-01-25T17:55:02.287211"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11138.147, "latencies_ms": [11138.147], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a verdant landscape, a giraffe stands tall and majestic. Its long neck, a marvel of nature's design, stretches upwards towards the sky, while its large ears, adorned with black tips, are alert and attentive. The giraffe's coat is a beautiful mosaic of brown and white spots,", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24432.8, "ram_available_mb": 38408.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24425.8, "ram_available_mb": 38415.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.347}, "power_stats": {"power_gpu_soc_mean_watts": 20.873, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.93, "gpu_utilization_percent_mean": 69.347, "power_watts_avg": 20.873, "energy_joules_est": 232.5, "duration_seconds": 11.139, "sample_count": 95}, "timestamp": "2026-01-25T17:55:15.462279"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7189.62, "latencies_ms": [7189.62], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "ear: 2\neye: 2\nnose: 1\nmouth: 1\nfur: 1\near: 1\near: 1\near: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24425.8, "ram_available_mb": 38415.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24421.4, "ram_available_mb": 38419.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.467}, "power_stats": {"power_gpu_soc_mean_watts": 22.745, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 9.017, "gpu_utilization_percent_mean": 73.467, "power_watts_avg": 22.745, "energy_joules_est": 163.54, "duration_seconds": 7.19, "sample_count": 60}, "timestamp": "2026-01-25T17:55:24.694356"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8315.546, "latencies_ms": [8315.546], "images_per_second": 0.12, "prompt_tokens": 44, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The giraffe is in the foreground, standing near the center of the image, with trees and foliage in the background. The person's head is partially visible on the left side of the image, suggesting they are behind the giraffe.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24421.4, "ram_available_mb": 38419.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24422.7, "ram_available_mb": 38418.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.62}, "power_stats": {"power_gpu_soc_mean_watts": 21.928, "power_cpu_cv_mean_watts": 1.754, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 71.62, "power_watts_avg": 21.928, "energy_joules_est": 182.36, "duration_seconds": 8.316, "sample_count": 71}, "timestamp": "2026-01-25T17:55:35.027322"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9296.181, "latencies_ms": [9296.181], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 61, "n_tiles": 16, "output_text": "In the image, a giraffe is standing in a lush green environment, surrounded by trees and foliage. The giraffe is looking directly at the camera, with its head slightly tilted to the left, giving a sense of curiosity and engagement with the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.7, "ram_available_mb": 38418.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24420.0, "ram_available_mb": 38420.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.244}, "power_stats": {"power_gpu_soc_mean_watts": 21.655, "power_cpu_cv_mean_watts": 1.76, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 71.244, "power_watts_avg": 21.655, "energy_joules_est": 201.32, "duration_seconds": 9.297, "sample_count": 78}, "timestamp": "2026-01-25T17:55:46.353876"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6425.372, "latencies_ms": [6425.372], "images_per_second": 0.156, "prompt_tokens": 36, "response_tokens_est": 37, "n_tiles": 16, "output_text": "The giraffe has a light brown coat with darker brown spots. The background is a mix of green and yellow hues, indicating a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24420.0, "ram_available_mb": 38420.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24415.1, "ram_available_mb": 38425.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.704}, "power_stats": {"power_gpu_soc_mean_watts": 23.121, "power_cpu_cv_mean_watts": 1.535, "power_sys_5v0_mean_watts": 9.091, "gpu_utilization_percent_mean": 73.704, "power_watts_avg": 23.121, "energy_joules_est": 148.58, "duration_seconds": 6.426, "sample_count": 54}, "timestamp": "2026-01-25T17:55:54.835460"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11113.616, "latencies_ms": [11113.616], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two zebras standing in a grassy field. The zebras are facing away from the camera, showcasing their distinctive black and white stripes. They are positioned close to each other, with one zebra slightly ahead of the other. The background features a chain-link fence, indicating that they are in a protected area, possibly", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 24415.1, "ram_available_mb": 38425.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24432.5, "ram_available_mb": 38408.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.617}, "power_stats": {"power_gpu_soc_mean_watts": 20.937, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 69.617, "power_watts_avg": 20.937, "energy_joules_est": 232.7, "duration_seconds": 11.114, "sample_count": 94}, "timestamp": "2026-01-25T17:56:08.003465"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7104.189, "latencies_ms": [7104.189], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "zebra: 2, fence: 1, leaves: many, ground: dry, rocks: 1, sunlight: bright, trees: 1, sky: not visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.5, "ram_available_mb": 38408.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.283}, "power_stats": {"power_gpu_soc_mean_watts": 22.915, "power_cpu_cv_mean_watts": 1.561, "power_sys_5v0_mean_watts": 9.01, "gpu_utilization_percent_mean": 74.283, "power_watts_avg": 22.915, "energy_joules_est": 162.81, "duration_seconds": 7.105, "sample_count": 60}, "timestamp": "2026-01-25T17:56:17.163823"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11176.559, "latencies_ms": [11176.559], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The two zebras are standing close to each other, with one slightly in front of the other, creating a sense of depth in the image. They are positioned in the foreground, while the chain-link fence and the background of trees and foliage are in the background. The zebras are near the camera, making them the main focus of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24439.0, "ram_available_mb": 38401.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.695}, "power_stats": {"power_gpu_soc_mean_watts": 20.971, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 69.695, "power_watts_avg": 20.971, "energy_joules_est": 234.4, "duration_seconds": 11.177, "sample_count": 95}, "timestamp": "2026-01-25T17:56:30.371592"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7077.711, "latencies_ms": [7077.711], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "Two zebras are standing in a grassy area with a chain-link fence in the background. The zebras are facing away from the camera, and their tails are raised.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24439.0, "ram_available_mb": 38401.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24428.9, "ram_available_mb": 38412.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.883}, "power_stats": {"power_gpu_soc_mean_watts": 22.896, "power_cpu_cv_mean_watts": 1.568, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 74.883, "power_watts_avg": 22.896, "energy_joules_est": 162.07, "duration_seconds": 7.078, "sample_count": 60}, "timestamp": "2026-01-25T17:56:39.512791"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8208.86, "latencies_ms": [8208.86], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The zebras have distinct black and white stripes, and the image is taken in natural daylight with shadows cast on the ground. The background shows a wire fence, indicating the zebras are likely in a zoo or wildlife park.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24428.9, "ram_available_mb": 38412.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24432.2, "ram_available_mb": 38408.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.913}, "power_stats": {"power_gpu_soc_mean_watts": 22.275, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 9.059, "gpu_utilization_percent_mean": 71.913, "power_watts_avg": 22.275, "energy_joules_est": 182.87, "duration_seconds": 8.209, "sample_count": 69}, "timestamp": "2026-01-25T17:56:49.777214"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11124.633, "latencies_ms": [11124.633], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a group of horses is walking down a road, with some of them standing near the side of the road. There are at least four horses visible, with one horse standing closer to the left side of the road and the other three horses positioned more towards the center. \n\nA car is parked on the right side of the road, occupying a significant portion of", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24432.2, "ram_available_mb": 38408.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24406.8, "ram_available_mb": 38434.1, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.278}, "power_stats": {"power_gpu_soc_mean_watts": 20.841, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 69.278, "power_watts_avg": 20.841, "energy_joules_est": 231.86, "duration_seconds": 11.125, "sample_count": 97}, "timestamp": "2026-01-25T17:57:02.942295"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7625.091, "latencies_ms": [7625.091], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "fence: 1\nhorses: 3\ncar: 1\nroad: 1\ntrees: 1\nleaves: 1\ngrass: 1\ndirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24406.8, "ram_available_mb": 38434.1, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24422.0, "ram_available_mb": 38418.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.266}, "power_stats": {"power_gpu_soc_mean_watts": 22.437, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 72.266, "power_watts_avg": 22.437, "energy_joules_est": 171.1, "duration_seconds": 7.626, "sample_count": 64}, "timestamp": "2026-01-25T17:57:12.602466"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9997.787, "latencies_ms": [9997.787], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a silver car parked on the right side of the road. In the background, there are three horses walking on the road, with one horse closer to the camera and two others further away. The car is positioned near the middle of the road, while the horses are walking parallel to the car.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24422.0, "ram_available_mb": 38418.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24414.4, "ram_available_mb": 38426.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.729}, "power_stats": {"power_gpu_soc_mean_watts": 21.326, "power_cpu_cv_mean_watts": 1.86, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.729, "power_watts_avg": 21.326, "energy_joules_est": 213.23, "duration_seconds": 9.998, "sample_count": 85}, "timestamp": "2026-01-25T17:57:24.655831"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7933.681, "latencies_ms": [7933.681], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A group of horses are standing on the side of a road, with a silver car parked nearby. The scene appears to be in a rural or countryside area, with trees and a wooden fence visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24414.4, "ram_available_mb": 38426.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24421.8, "ram_available_mb": 38419.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.912}, "power_stats": {"power_gpu_soc_mean_watts": 22.412, "power_cpu_cv_mean_watts": 1.69, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 72.912, "power_watts_avg": 22.412, "energy_joules_est": 177.82, "duration_seconds": 7.934, "sample_count": 68}, "timestamp": "2026-01-25T17:57:34.642003"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7638.176, "latencies_ms": [7638.176], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image shows a sunny day with clear skies, casting natural light on the scene. A silver car is parked on the side of a road, and there is a wooden fence on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24421.8, "ram_available_mb": 38419.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24416.1, "ram_available_mb": 38424.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.391}, "power_stats": {"power_gpu_soc_mean_watts": 22.35, "power_cpu_cv_mean_watts": 1.701, "power_sys_5v0_mean_watts": 9.038, "gpu_utilization_percent_mean": 72.391, "power_watts_avg": 22.35, "energy_joules_est": 170.73, "duration_seconds": 7.639, "sample_count": 64}, "timestamp": "2026-01-25T17:57:44.295801"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11143.982, "latencies_ms": [11143.982], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a wooden desk with a chair positioned in front of it. On the desk, there is a stack of books, an apple, and a bottle. The chair is placed close to the desk, providing a comfortable seating area for someone to sit and work or study. The books are arranged in a pile, with the apple placed on top of the stack", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24416.1, "ram_available_mb": 38424.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24442.5, "ram_available_mb": 38398.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.063}, "power_stats": {"power_gpu_soc_mean_watts": 20.891, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.93, "gpu_utilization_percent_mean": 69.063, "power_watts_avg": 20.891, "energy_joules_est": 232.82, "duration_seconds": 11.145, "sample_count": 95}, "timestamp": "2026-01-25T17:57:57.507032"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8164.604, "latencies_ms": [8164.604], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "- Chair: 1\n- Desk: 1\n- Book: 3\n- Apple: 1\n- Desk lamp: 1\n- Chair: 1\n- Chair: 1\n- Chair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.5, "ram_available_mb": 38398.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24422.7, "ram_available_mb": 38418.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.957}, "power_stats": {"power_gpu_soc_mean_watts": 22.158, "power_cpu_cv_mean_watts": 1.706, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 72.957, "power_watts_avg": 22.158, "energy_joules_est": 180.93, "duration_seconds": 8.165, "sample_count": 69}, "timestamp": "2026-01-25T17:58:07.699653"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11095.507, "latencies_ms": [11095.507], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden desk with a stack of books leaning against it, and a red apple resting on the desk. To the left of the desk, there is a wooden chair with a curved backrest. In the background, there is a large blackboard mounted on the wall, and a small picture frame is hanging on the wall to the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.7, "ram_available_mb": 38418.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.632}, "power_stats": {"power_gpu_soc_mean_watts": 20.972, "power_cpu_cv_mean_watts": 1.934, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.632, "power_watts_avg": 20.972, "energy_joules_est": 232.71, "duration_seconds": 11.096, "sample_count": 95}, "timestamp": "2026-01-25T17:58:20.858553"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8522.566, "latencies_ms": [8522.566], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a classroom setting with a wooden desk in the foreground. On the desk, there is a stack of books, an apple, and a quill pen. The background features a chalkboard with some writing on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24418.1, "ram_available_mb": 38422.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.297}, "power_stats": {"power_gpu_soc_mean_watts": 21.953, "power_cpu_cv_mean_watts": 1.726, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 72.297, "power_watts_avg": 21.953, "energy_joules_est": 187.12, "duration_seconds": 8.523, "sample_count": 74}, "timestamp": "2026-01-25T17:58:31.404758"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9868.742, "latencies_ms": [9868.742], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image features a blackboard with a chalkboard texture, and the lighting appears to be coming from the left side, casting shadows to the right. The desk is made of wood with a dark finish, and there is a red apple and a blue book on it, adding a pop of color to the scene.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24418.1, "ram_available_mb": 38422.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24421.4, "ram_available_mb": 38419.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.262}, "power_stats": {"power_gpu_soc_mean_watts": 21.29, "power_cpu_cv_mean_watts": 1.868, "power_sys_5v0_mean_watts": 9.018, "gpu_utilization_percent_mean": 70.262, "power_watts_avg": 21.29, "energy_joules_est": 210.12, "duration_seconds": 9.869, "sample_count": 84}, "timestamp": "2026-01-25T17:58:43.327324"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11170.45, "latencies_ms": [11170.45], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment on a bustling street in India. Dominating the scene is a yellow and white bus, adorned with a red and white stripe running along its side. The bus is in motion, as indicated by the blurred background, suggesting it's speeding down the road. The license plate of the bus reads \"DL-10 3", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24421.4, "ram_available_mb": 38419.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24415.1, "ram_available_mb": 38425.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.135}, "power_stats": {"power_gpu_soc_mean_watts": 19.968, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 70.135, "power_watts_avg": 19.968, "energy_joules_est": 223.06, "duration_seconds": 11.171, "sample_count": 96}, "timestamp": "2026-01-25T17:58:56.536519"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7372.949, "latencies_ms": [7372.949], "images_per_second": 0.136, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "bus: 1, van: 1, motorcycle: 1, car: 1, license plate: 1, rear light: 2, side mirror: 1, window: 2", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24415.1, "ram_available_mb": 38425.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24422.4, "ram_available_mb": 38418.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.746}, "power_stats": {"power_gpu_soc_mean_watts": 22.603, "power_cpu_cv_mean_watts": 1.639, "power_sys_5v0_mean_watts": 9.007, "gpu_utilization_percent_mean": 72.746, "power_watts_avg": 22.603, "energy_joules_est": 166.66, "duration_seconds": 7.374, "sample_count": 63}, "timestamp": "2026-01-25T17:59:05.941907"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8890.488, "latencies_ms": [8890.488], "images_per_second": 0.112, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The bus is in the foreground on the left side of the image, moving towards the right. There is a white van on the right side of the image, further back in the scene. The background is less distinct but appears to be an urban street with other vehicles and possibly buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.4, "ram_available_mb": 38418.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24420.8, "ram_available_mb": 38420.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.973}, "power_stats": {"power_gpu_soc_mean_watts": 21.683, "power_cpu_cv_mean_watts": 1.793, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 70.973, "power_watts_avg": 21.683, "energy_joules_est": 192.79, "duration_seconds": 8.891, "sample_count": 75}, "timestamp": "2026-01-25T17:59:16.852782"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6845.034, "latencies_ms": [6845.034], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A yellow and white bus with the number 475 and some text in Hindi is driving on a busy street. There are other vehicles and a motorcycle visible in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24420.8, "ram_available_mb": 38420.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.259}, "power_stats": {"power_gpu_soc_mean_watts": 22.997, "power_cpu_cv_mean_watts": 1.574, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 74.259, "power_watts_avg": 22.997, "energy_joules_est": 157.43, "duration_seconds": 6.846, "sample_count": 58}, "timestamp": "2026-01-25T17:59:25.709682"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6659.738, "latencies_ms": [6659.738], "images_per_second": 0.15, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The bus in the image is predominantly yellow with red and white accents. The weather appears to be overcast, as the sky is grey and the overall lighting is dim.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24416.5, "ram_available_mb": 38424.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.357}, "power_stats": {"power_gpu_soc_mean_watts": 22.876, "power_cpu_cv_mean_watts": 1.573, "power_sys_5v0_mean_watts": 9.057, "gpu_utilization_percent_mean": 73.357, "power_watts_avg": 22.876, "energy_joules_est": 152.36, "duration_seconds": 6.66, "sample_count": 56}, "timestamp": "2026-01-25T17:59:34.431263"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11118.882, "latencies_ms": [11118.882], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a bathroom, where the primary focus is a television mounted on the wall. The television, which is the central object in the image, is turned on and broadcasting a football game. The game is in progress, with players actively engaged in the action.\n\nThe bathroom itself is well-equipped, featuring a sink and a toilet", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24416.5, "ram_available_mb": 38424.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24437.0, "ram_available_mb": 38403.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.564}, "power_stats": {"power_gpu_soc_mean_watts": 20.95, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 69.564, "power_watts_avg": 20.95, "energy_joules_est": 232.95, "duration_seconds": 11.12, "sample_count": 94}, "timestamp": "2026-01-25T17:59:47.574437"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10636.77, "latencies_ms": [10636.77], "images_per_second": 0.094, "prompt_tokens": 39, "response_tokens_est": 73, "n_tiles": 16, "output_text": "- Television: 1\n\n- Mirror: 1\n\n- Sink: 1\n\n- Faucet: 1\n\n- Tiles: Multiple (exact count not determinable)\n\n- Towel dispenser: 1\n\n- Trash can: 1\n\n- Door: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24437.0, "ram_available_mb": 38403.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24432.6, "ram_available_mb": 38408.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.011}, "power_stats": {"power_gpu_soc_mean_watts": 21.248, "power_cpu_cv_mean_watts": 1.85, "power_sys_5v0_mean_watts": 8.924, "gpu_utilization_percent_mean": 71.011, "power_watts_avg": 21.248, "energy_joules_est": 226.02, "duration_seconds": 10.637, "sample_count": 90}, "timestamp": "2026-01-25T18:00:00.236188"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10385.323, "latencies_ms": [10385.323], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground of the image, there is a bathroom sink with a white basin and a silver faucet. The television is mounted on the wall in the background, above the sink. The toilet paper dispenser is located to the right of the television, and the trash can is situated to the left of the television.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24432.6, "ram_available_mb": 38408.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24510.1, "ram_available_mb": 38330.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.193}, "power_stats": {"power_gpu_soc_mean_watts": 21.186, "power_cpu_cv_mean_watts": 1.874, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 70.193, "power_watts_avg": 21.186, "energy_joules_est": 220.04, "duration_seconds": 10.386, "sample_count": 88}, "timestamp": "2026-01-25T18:00:12.661131"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6843.894, "latencies_ms": [6843.894], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image shows a bathroom with a large mirror reflecting a television screen displaying a football game. The bathroom has beige tiles on the walls and a granite countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24510.1, "ram_available_mb": 38330.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24510.3, "ram_available_mb": 38330.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.655}, "power_stats": {"power_gpu_soc_mean_watts": 23.07, "power_cpu_cv_mean_watts": 1.546, "power_sys_5v0_mean_watts": 9.016, "gpu_utilization_percent_mean": 74.655, "power_watts_avg": 23.07, "energy_joules_est": 157.9, "duration_seconds": 6.845, "sample_count": 58}, "timestamp": "2026-01-25T18:00:21.526400"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7128.791, "latencies_ms": [7128.791], "images_per_second": 0.14, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image shows a bathroom with beige tiled walls and a dark brown door. A television is mounted on the wall, displaying a blue-toned image of what appears to be a football game.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24510.3, "ram_available_mb": 38330.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24552.3, "ram_available_mb": 38288.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.803}, "power_stats": {"power_gpu_soc_mean_watts": 22.658, "power_cpu_cv_mean_watts": 1.647, "power_sys_5v0_mean_watts": 9.036, "gpu_utilization_percent_mean": 71.803, "power_watts_avg": 22.658, "energy_joules_est": 161.54, "duration_seconds": 7.129, "sample_count": 61}, "timestamp": "2026-01-25T18:00:30.706609"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12342.698, "latencies_ms": [12342.698], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photograph, a solitary figure is captured in a moment of quiet contemplation. The person, dressed in a white shirt and dark pants, is seated on a wooden bench. Their gaze is directed downwards, suggesting a moment of introspection or deep thought.\n\nThe bench is situated in a park, surrounded by lush greenery", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24413.6, "ram_available_mb": 38427.3, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24439.6, "ram_available_mb": 38401.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.66}, "power_stats": {"power_gpu_soc_mean_watts": 22.79, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 9.188, "gpu_utilization_percent_mean": 73.66, "power_watts_avg": 22.79, "energy_joules_est": 281.3, "duration_seconds": 12.343, "sample_count": 106}, "timestamp": "2026-01-25T18:00:45.097517"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10542.263, "latencies_ms": [10542.263], "images_per_second": 0.095, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "1. Bench: 1\n2. Man: 1\n3. Tree: 1\n4. Street lamp: 1\n5. Bush: 1\n6. Hedge: 1\n7. Bush: 1\n8. Bush: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24439.6, "ram_available_mb": 38401.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24438.9, "ram_available_mb": 38402.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.527}, "power_stats": {"power_gpu_soc_mean_watts": 23.49, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 9.113, "gpu_utilization_percent_mean": 75.527, "power_watts_avg": 23.49, "energy_joules_est": 247.65, "duration_seconds": 10.543, "sample_count": 91}, "timestamp": "2026-01-25T18:00:57.662775"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12320.028, "latencies_ms": [12320.028], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "In the foreground, there is a person sitting on a bench, positioned on the left side of the image. The bench is located in a park-like setting with trees and bushes around it. In the background, there is a tall clock tower with a steeple, situated behind the trees and bushes, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.9, "ram_available_mb": 38402.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24429.3, "ram_available_mb": 38411.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.33}, "power_stats": {"power_gpu_soc_mean_watts": 22.889, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 9.207, "gpu_utilization_percent_mean": 74.33, "power_watts_avg": 22.889, "energy_joules_est": 282.01, "duration_seconds": 12.321, "sample_count": 106}, "timestamp": "2026-01-25T18:01:12.014534"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8340.996, "latencies_ms": [8340.996], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A man is sitting on a bench in a park-like setting with a tall clock tower in the background. The scene is in black and white, giving it a timeless and classic feel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.3, "ram_available_mb": 38411.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24449.8, "ram_available_mb": 38391.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.718}, "power_stats": {"power_gpu_soc_mean_watts": 24.529, "power_cpu_cv_mean_watts": 1.421, "power_sys_5v0_mean_watts": 9.171, "gpu_utilization_percent_mean": 78.718, "power_watts_avg": 24.529, "energy_joules_est": 204.61, "duration_seconds": 8.342, "sample_count": 71}, "timestamp": "2026-01-25T18:01:22.396411"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12394.491, "latencies_ms": [12394.491], "images_per_second": 0.081, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a black and white photograph, giving it a timeless and classic feel. The lighting is soft and natural, with the sunlight filtering through the trees and casting shadows on the ground. The weather appears to be overcast, with a cloudy sky and no visible sun. The photograph is taken in a park-like setting, with a man sitting on a bench in", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24449.8, "ram_available_mb": 38391.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24448.2, "ram_available_mb": 38392.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.745}, "power_stats": {"power_gpu_soc_mean_watts": 22.945, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 9.2, "gpu_utilization_percent_mean": 73.745, "power_watts_avg": 22.945, "energy_joules_est": 284.41, "duration_seconds": 12.395, "sample_count": 106}, "timestamp": "2026-01-25T18:01:36.825884"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11120.361, "latencies_ms": [11120.361], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a bustling street scene under a clear blue sky. A row of cars, each with its own unique color and model, are parked neatly along the side of the road. The cars are of various sizes and colors, including white, black, and silver. They are parked in an orderly fashion, with some closer to the foreground and others further in", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24448.2, "ram_available_mb": 38392.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24422.5, "ram_available_mb": 38418.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.67}, "power_stats": {"power_gpu_soc_mean_watts": 20.927, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.67, "power_watts_avg": 20.927, "energy_joules_est": 232.73, "duration_seconds": 11.121, "sample_count": 94}, "timestamp": "2026-01-25T18:01:49.976308"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7277.054, "latencies_ms": [7277.054], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "car: 8, truck: 2, person: 3, building: 1, sign: 2, tree: 1, street light: 1, bus stop: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.5, "ram_available_mb": 38418.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24419.9, "ram_available_mb": 38421.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.049}, "power_stats": {"power_gpu_soc_mean_watts": 22.705, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 9.01, "gpu_utilization_percent_mean": 73.049, "power_watts_avg": 22.705, "energy_joules_est": 165.24, "duration_seconds": 7.278, "sample_count": 61}, "timestamp": "2026-01-25T18:01:59.295383"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11148.255, "latencies_ms": [11148.255], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a busy street scene with multiple cars and a bus, indicating a high-traffic area. The cars are parked and in motion, with some near the bus stop and others further down the road. In the background, there is a large stone wall, which appears to be part of a historical or significant building, adding to the urban environment. The sky is", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.9, "ram_available_mb": 38421.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24417.5, "ram_available_mb": 38423.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.453}, "power_stats": {"power_gpu_soc_mean_watts": 20.893, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 69.453, "power_watts_avg": 20.893, "energy_joules_est": 232.93, "duration_seconds": 11.149, "sample_count": 95}, "timestamp": "2026-01-25T18:02:12.473161"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7300.426, "latencies_ms": [7300.426], "images_per_second": 0.137, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image depicts a busy street scene with multiple cars and a bus stopped at a bus stop. There are several people walking on the sidewalk, and a stone wall can be seen in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24417.5, "ram_available_mb": 38423.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24425.1, "ram_available_mb": 38415.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.129}, "power_stats": {"power_gpu_soc_mean_watts": 22.611, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 73.129, "power_watts_avg": 22.611, "energy_joules_est": 165.09, "duration_seconds": 7.301, "sample_count": 62}, "timestamp": "2026-01-25T18:02:21.796657"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8309.368, "latencies_ms": [8309.368], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a clear blue sky with sunlight casting shadows on the road, indicating it is a sunny day. The road is paved with asphalt, and there are multiple vehicles, including cars and a bus, indicating a busy traffic scenario.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.1, "ram_available_mb": 38415.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24421.3, "ram_available_mb": 38419.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.873}, "power_stats": {"power_gpu_soc_mean_watts": 21.928, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 9.023, "gpu_utilization_percent_mean": 71.873, "power_watts_avg": 21.928, "energy_joules_est": 182.22, "duration_seconds": 8.31, "sample_count": 71}, "timestamp": "2026-01-25T18:02:32.146183"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11142.428, "latencies_ms": [11142.428], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility, featuring a wooden table set for a meal. On the table, there's a white plate holding a slice of pizza, a bowl filled with an assortment of fruits including bananas, apples, and watermelon, and a white cup filled with a red liquid, possibly juice or wine. The", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24421.3, "ram_available_mb": 38419.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24423.6, "ram_available_mb": 38417.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.979}, "power_stats": {"power_gpu_soc_mean_watts": 20.861, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 68.979, "power_watts_avg": 20.861, "energy_joules_est": 232.46, "duration_seconds": 11.143, "sample_count": 95}, "timestamp": "2026-01-25T18:02:45.322508"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7757.853, "latencies_ms": [7757.853], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "plate: 2, cup: 2, spoon: 1, fork: 2, knife: 1, bowl: 1, watermelon: 1, banana: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24423.6, "ram_available_mb": 38417.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24422.0, "ram_available_mb": 38418.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.308}, "power_stats": {"power_gpu_soc_mean_watts": 22.539, "power_cpu_cv_mean_watts": 1.632, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 73.308, "power_watts_avg": 22.539, "energy_joules_est": 174.87, "duration_seconds": 7.758, "sample_count": 65}, "timestamp": "2026-01-25T18:02:55.128898"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11123.72, "latencies_ms": [11123.72], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The main objects are arranged on a wooden table. In the foreground, there is a white plate with a folded omelette and a slice being lifted, a bowl of fruit salad with a fork and knife, and a cup of tea in the background. The table is positioned on a tiled floor, and the shadows of the objects are cast on the floor,", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24422.0, "ram_available_mb": 38418.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24422.0, "ram_available_mb": 38418.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.894}, "power_stats": {"power_gpu_soc_mean_watts": 20.95, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 69.894, "power_watts_avg": 20.95, "energy_joules_est": 233.06, "duration_seconds": 11.124, "sample_count": 94}, "timestamp": "2026-01-25T18:03:08.269177"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7265.347, "latencies_ms": [7265.347], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image shows a wooden table with a plate of food and a cup of tea on it. The table is placed on a tiled floor, and there is a shadow of a person visible on the floor.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24422.0, "ram_available_mb": 38418.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24420.9, "ram_available_mb": 38420.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.274}, "power_stats": {"power_gpu_soc_mean_watts": 22.701, "power_cpu_cv_mean_watts": 1.621, "power_sys_5v0_mean_watts": 9.036, "gpu_utilization_percent_mean": 73.274, "power_watts_avg": 22.701, "energy_joules_est": 164.95, "duration_seconds": 7.266, "sample_count": 62}, "timestamp": "2026-01-25T18:03:17.583880"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7119.216, "latencies_ms": [7119.216], "images_per_second": 0.14, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image shows a wooden table with a plate of food and a cup of tea on it. The lighting is natural, coming from the top left corner, casting shadows on the table and the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.9, "ram_available_mb": 38420.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24418.3, "ram_available_mb": 38422.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.883}, "power_stats": {"power_gpu_soc_mean_watts": 22.509, "power_cpu_cv_mean_watts": 1.648, "power_sys_5v0_mean_watts": 9.056, "gpu_utilization_percent_mean": 72.883, "power_watts_avg": 22.509, "energy_joules_est": 160.26, "duration_seconds": 7.12, "sample_count": 60}, "timestamp": "2026-01-25T18:03:26.726924"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11099.019, "latencies_ms": [11099.019], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the image, an elderly woman is standing at a dining table, cutting dough with a knife. The table is filled with various food items, including rolls, cookies, and a pizza. There are also cups and a bowl on the table. The woman is wearing an apron, and the scene appears to be a kitchen or dining area.", "error": null, "sys_before": {"cpu_percent": 3.6, "ram_used_mb": 24418.3, "ram_available_mb": 38422.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24435.5, "ram_available_mb": 38405.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.766}, "power_stats": {"power_gpu_soc_mean_watts": 20.945, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.766, "power_watts_avg": 20.945, "energy_joules_est": 232.48, "duration_seconds": 11.1, "sample_count": 94}, "timestamp": "2026-01-25T18:03:39.902690"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10617.341, "latencies_ms": [10617.341], "images_per_second": 0.094, "prompt_tokens": 39, "response_tokens_est": 73, "n_tiles": 16, "output_text": "- Bread: 12\n\n- Cookie cutter: 1\n\n- Cookie dough: 1\n\n- Baking sheet: 1\n\n- Baking pan: 1\n\n- Baking powder: 1\n\n- Flour: 1\n\n- Rolling pin: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.5, "ram_available_mb": 38405.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24442.2, "ram_available_mb": 38398.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.725}, "power_stats": {"power_gpu_soc_mean_watts": 21.301, "power_cpu_cv_mean_watts": 1.856, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 71.725, "power_watts_avg": 21.301, "energy_joules_est": 226.17, "duration_seconds": 10.618, "sample_count": 91}, "timestamp": "2026-01-25T18:03:52.543844"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10033.836, "latencies_ms": [10033.836], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a table with various baked goods and items scattered across its surface. To the left, a person is standing and appears to be reaching for something on the table. In the background, there is a couch and a door, suggesting that the table is located in a living room or dining area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.2, "ram_available_mb": 38398.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24448.1, "ram_available_mb": 38392.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.624}, "power_stats": {"power_gpu_soc_mean_watts": 21.366, "power_cpu_cv_mean_watts": 1.837, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 70.624, "power_watts_avg": 21.366, "energy_joules_est": 214.4, "duration_seconds": 10.035, "sample_count": 85}, "timestamp": "2026-01-25T18:04:04.616069"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7961.639, "latencies_ms": [7961.639], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "An elderly woman is standing at a kitchen table, cutting dough with a knife. The table is covered with various baked goods, including rolls and cookies, and there are cups and a bowl on the table.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24448.1, "ram_available_mb": 38392.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24422.6, "ram_available_mb": 38418.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.353}, "power_stats": {"power_gpu_soc_mean_watts": 22.227, "power_cpu_cv_mean_watts": 1.678, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 71.353, "power_watts_avg": 22.227, "energy_joules_est": 176.98, "duration_seconds": 7.962, "sample_count": 68}, "timestamp": "2026-01-25T18:04:14.621406"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9862.131, "latencies_ms": [9862.131], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image shows an indoor setting with warm lighting, likely from artificial sources, as there are no visible windows or natural light. The table is covered with a patterned tablecloth, and the person is wearing a striped shirt with a pink apron, suggesting a casual, homey atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.6, "ram_available_mb": 38418.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24420.4, "ram_available_mb": 38420.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.212}, "power_stats": {"power_gpu_soc_mean_watts": 21.175, "power_cpu_cv_mean_watts": 1.851, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 70.212, "power_watts_avg": 21.175, "energy_joules_est": 208.84, "duration_seconds": 9.863, "sample_count": 85}, "timestamp": "2026-01-25T18:04:26.527749"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11104.023, "latencies_ms": [11104.023], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing in front of a traffic light, which is currently displaying a red light. The man is wearing a white shirt and shorts, and he appears to be smiling. The traffic light is positioned on the right side of the image.\n\nThe scene is set in a lush, green environment with various plants surrounding the man and the traffic", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24420.4, "ram_available_mb": 38420.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24427.9, "ram_available_mb": 38413.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.904}, "power_stats": {"power_gpu_soc_mean_watts": 20.938, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.904, "power_watts_avg": 20.938, "energy_joules_est": 232.51, "duration_seconds": 11.105, "sample_count": 94}, "timestamp": "2026-01-25T18:04:39.658897"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8627.554, "latencies_ms": [8627.554], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Man: 1\n- Traffic light: 1\n- Bushes: 1\n- Flowers: 1\n- Sign: 1\n- Gravel: 1\n- Sandals: 1\n- Pole: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.9, "ram_available_mb": 38413.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24431.3, "ram_available_mb": 38409.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.311}, "power_stats": {"power_gpu_soc_mean_watts": 21.913, "power_cpu_cv_mean_watts": 1.736, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 71.311, "power_watts_avg": 21.913, "energy_joules_est": 189.07, "duration_seconds": 8.628, "sample_count": 74}, "timestamp": "2026-01-25T18:04:50.328210"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11165.008, "latencies_ms": [11165.008], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "A man is standing in the foreground on the left side of the image, near a traffic light that is in the middle ground. The traffic light is positioned between the man and a sign that reads \"AUSTRALIA TRAFFIC LIGHT\" in the background. There are plants and shrubs surrounding the scene, with some in the background and others in the foreground.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24431.3, "ram_available_mb": 38409.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24427.9, "ram_available_mb": 38413.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.484}, "power_stats": {"power_gpu_soc_mean_watts": 20.944, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.484, "power_watts_avg": 20.944, "energy_joules_est": 233.86, "duration_seconds": 11.166, "sample_count": 95}, "timestamp": "2026-01-25T18:05:03.530867"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6741.747, "latencies_ms": [6741.747], "images_per_second": 0.148, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A man is standing in front of a traffic light that is showing a red signal. The traffic light is located in a tropical setting with lush green foliage in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24427.9, "ram_available_mb": 38413.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24445.0, "ram_available_mb": 38395.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_gpu_soc_mean_watts": 23.254, "power_cpu_cv_mean_watts": 1.501, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 74.0, "power_watts_avg": 23.254, "energy_joules_est": 156.79, "duration_seconds": 6.742, "sample_count": 56}, "timestamp": "2026-01-25T18:05:12.286385"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6709.864, "latencies_ms": [6709.864], "images_per_second": 0.149, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A man wearing a white t-shirt and khaki shorts stands in front of a traffic light. The traffic light is red and there are green plants in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24445.0, "ram_available_mb": 38395.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24440.4, "ram_available_mb": 38400.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.14}, "power_stats": {"power_gpu_soc_mean_watts": 22.916, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 9.045, "gpu_utilization_percent_mean": 73.14, "power_watts_avg": 22.916, "energy_joules_est": 153.78, "duration_seconds": 6.711, "sample_count": 57}, "timestamp": "2026-01-25T18:05:21.036941"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11115.375, "latencies_ms": [11115.375], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large group of colorful kites flying in the sky, creating a vibrant and lively scene. The kites are of various shapes and sizes, with some resembling fish and others resembling birds. They are spread across the sky, with some kites flying higher and others lower.\n\nThere are several people in the image, likely enjoying the", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24440.4, "ram_available_mb": 38400.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24421.7, "ram_available_mb": 38419.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.25}, "power_stats": {"power_gpu_soc_mean_watts": 20.877, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 69.25, "power_watts_avg": 20.877, "energy_joules_est": 232.07, "duration_seconds": 11.116, "sample_count": 96}, "timestamp": "2026-01-25T18:05:34.203529"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9416.417, "latencies_ms": [9416.417], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "Kites: 10\nFish-shaped kites: 5\nFlagpoles: 10\nGrassy field: 1\nBuilding in background: 1\nPeople: 10\nBags: 1\nCooler: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.7, "ram_available_mb": 38419.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24425.4, "ram_available_mb": 38415.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.85}, "power_stats": {"power_gpu_soc_mean_watts": 21.64, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 70.85, "power_watts_avg": 21.64, "energy_joules_est": 203.79, "duration_seconds": 9.417, "sample_count": 80}, "timestamp": "2026-01-25T18:05:45.641864"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9334.95, "latencies_ms": [9334.95], "images_per_second": 0.107, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The colorful kites are flying in the sky, with some positioned higher and others lower. The kites are spread out across the field, with some closer to the foreground and others further away in the background. The kites are flying in different directions, creating a dynamic and lively scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.4, "ram_available_mb": 38415.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24413.6, "ram_available_mb": 38427.3, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.825}, "power_stats": {"power_gpu_soc_mean_watts": 21.476, "power_cpu_cv_mean_watts": 1.821, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 70.825, "power_watts_avg": 21.476, "energy_joules_est": 200.49, "duration_seconds": 9.336, "sample_count": 80}, "timestamp": "2026-01-25T18:05:57.006106"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6941.101, "latencies_ms": [6941.101], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A group of colorful kites resembling fish are flying in the sky above a grassy field. People are gathered on the ground, enjoying the kite-flying event.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24413.6, "ram_available_mb": 38427.3, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.397}, "power_stats": {"power_gpu_soc_mean_watts": 22.954, "power_cpu_cv_mean_watts": 1.56, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 73.397, "power_watts_avg": 22.954, "energy_joules_est": 159.35, "duration_seconds": 6.942, "sample_count": 58}, "timestamp": "2026-01-25T18:06:05.975005"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6749.219, "latencies_ms": [6749.219], "images_per_second": 0.148, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The kites in the image are predominantly red, white, and purple, with some blue accents. They are flying in a cloudy sky, suggesting overcast weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24416.3, "ram_available_mb": 38424.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.018}, "power_stats": {"power_gpu_soc_mean_watts": 22.893, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 9.063, "gpu_utilization_percent_mean": 73.018, "power_watts_avg": 22.893, "energy_joules_est": 154.53, "duration_seconds": 6.75, "sample_count": 57}, "timestamp": "2026-01-25T18:06:14.784401"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11127.031, "latencies_ms": [11127.031], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man and a young boy are sitting on the floor, sharing a slice of pizza. The man is wearing a blue shirt and the boy is wearing a blue sweater. They are both focused on the pizza, with the man holding the slice and the boy looking at it. The pizza is placed on a red tray. In the background,", "error": null, "sys_before": {"cpu_percent": 11.5, "ram_used_mb": 24416.3, "ram_available_mb": 38424.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24425.7, "ram_available_mb": 38415.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.511}, "power_stats": {"power_gpu_soc_mean_watts": 20.916, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.511, "power_watts_avg": 20.916, "energy_joules_est": 232.75, "duration_seconds": 11.128, "sample_count": 94}, "timestamp": "2026-01-25T18:06:27.955739"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8404.881, "latencies_ms": [8404.881], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "- Man: 1\n- Child: 1\n- Pizza slice: 1\n- Bottle: 1\n- Toy: 1\n- Chair: 1\n- Couch: 1\n- Box: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.7, "ram_available_mb": 38415.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24424.7, "ram_available_mb": 38416.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.972}, "power_stats": {"power_gpu_soc_mean_watts": 22.151, "power_cpu_cv_mean_watts": 1.708, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 72.972, "power_watts_avg": 22.151, "energy_joules_est": 186.19, "duration_seconds": 8.406, "sample_count": 71}, "timestamp": "2026-01-25T18:06:38.400029"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11119.933, "latencies_ms": [11119.933], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a man in a blue patterned jacket is seated on the floor, holding a slice of pizza towards a young child who is standing in front of him. The child, wearing a blue sweatshirt, is looking down at the pizza with interest. In the background, there is a couch with a red blanket and a green object with", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24424.7, "ram_available_mb": 38416.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24416.1, "ram_available_mb": 38424.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.284}, "power_stats": {"power_gpu_soc_mean_watts": 20.939, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 69.284, "power_watts_avg": 20.939, "energy_joules_est": 232.85, "duration_seconds": 11.121, "sample_count": 95}, "timestamp": "2026-01-25T18:06:51.547985"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7404.047, "latencies_ms": [7404.047], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A man and a young child are sitting on the floor, sharing a slice of pizza. The man is wearing a blue patterned jacket and the child is wearing a blue sweatshirt.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24416.1, "ram_available_mb": 38424.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.302}, "power_stats": {"power_gpu_soc_mean_watts": 22.648, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 74.302, "power_watts_avg": 22.648, "energy_joules_est": 167.7, "duration_seconds": 7.405, "sample_count": 63}, "timestamp": "2026-01-25T18:07:00.980308"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7435.864, "latencies_ms": [7435.864], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a man and a child sitting indoors with a bottle of water on the table. The man is wearing a blue patterned jacket and the child is in a blue sweatshirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24414.0, "ram_available_mb": 38426.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.317}, "power_stats": {"power_gpu_soc_mean_watts": 22.385, "power_cpu_cv_mean_watts": 1.671, "power_sys_5v0_mean_watts": 9.041, "gpu_utilization_percent_mean": 72.317, "power_watts_avg": 22.385, "energy_joules_est": 166.47, "duration_seconds": 7.437, "sample_count": 63}, "timestamp": "2026-01-25T18:07:10.442860"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11137.392, "latencies_ms": [11137.392], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is sitting in a camping chair, enjoying a meal outdoors. She is holding a hot dog in her hand and has a plate of chips in front of her. The woman appears to be eating the hot dog while looking at the camera. The setting is a camping area, with a backpack placed nearby, indicating that she might be", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24414.0, "ram_available_mb": 38426.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24420.6, "ram_available_mb": 38420.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.653}, "power_stats": {"power_gpu_soc_mean_watts": 20.901, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 69.653, "power_watts_avg": 20.901, "energy_joules_est": 232.8, "duration_seconds": 11.138, "sample_count": 95}, "timestamp": "2026-01-25T18:07:23.645020"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7669.577, "latencies_ms": [7669.577], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "person: 1, plate: 1, tortilla chip: 1, bite of food: 1, chair: 1, backpack: 1, rocks: 5, branches: 2", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24420.6, "ram_available_mb": 38420.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24421.5, "ram_available_mb": 38419.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.485}, "power_stats": {"power_gpu_soc_mean_watts": 22.409, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 73.485, "power_watts_avg": 22.409, "energy_joules_est": 171.88, "duration_seconds": 7.67, "sample_count": 66}, "timestamp": "2026-01-25T18:07:33.374766"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9585.74, "latencies_ms": [9585.74], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The person is seated in the foreground on the left side of the image, holding a sandwich near their mouth. In the background, there is a plate with what appears to be tortilla chips on the right side, and the setting seems to be outdoors with rocks and foliage around.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24421.5, "ram_available_mb": 38419.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24421.1, "ram_available_mb": 38419.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.183}, "power_stats": {"power_gpu_soc_mean_watts": 21.39, "power_cpu_cv_mean_watts": 1.845, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 70.183, "power_watts_avg": 21.39, "energy_joules_est": 205.05, "duration_seconds": 9.586, "sample_count": 82}, "timestamp": "2026-01-25T18:07:45.019305"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7928.006, "latencies_ms": [7928.006], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A person is sitting in a camping chair outdoors at night, eating a hot dog and enjoying a plate of tortilla chips. The setting appears to be a campsite with a natural, outdoor environment.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24421.1, "ram_available_mb": 38419.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24426.0, "ram_available_mb": 38414.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.029}, "power_stats": {"power_gpu_soc_mean_watts": 22.332, "power_cpu_cv_mean_watts": 1.69, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 72.029, "power_watts_avg": 22.332, "energy_joules_est": 177.06, "duration_seconds": 7.929, "sample_count": 68}, "timestamp": "2026-01-25T18:07:54.970138"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7985.647, "latencies_ms": [7985.647], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a person sitting outdoors at night, illuminated by the light of a campfire. The person is wearing a striped, long-sleeved shirt and is seated on a blue camping chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.0, "ram_available_mb": 38414.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24426.9, "ram_available_mb": 38414.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.221}, "power_stats": {"power_gpu_soc_mean_watts": 22.093, "power_cpu_cv_mean_watts": 1.737, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 71.221, "power_watts_avg": 22.093, "energy_joules_est": 176.44, "duration_seconds": 7.986, "sample_count": 68}, "timestamp": "2026-01-25T18:08:04.993005"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11110.147, "latencies_ms": [11110.147], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large group of people gathered around a dining table, enjoying a meal together. There are at least 13 people in the scene, with some sitting and others standing. The table is filled with various food items, including multiple plates of food, bowls, and cups. \n\nThe dining table is covered with a purple tablecloth", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 24426.9, "ram_available_mb": 38414.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.547}, "power_stats": {"power_gpu_soc_mean_watts": 20.943, "power_cpu_cv_mean_watts": 1.925, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.547, "power_watts_avg": 20.943, "energy_joules_est": 232.69, "duration_seconds": 11.111, "sample_count": 95}, "timestamp": "2026-01-25T18:08:18.160482"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9734.945, "latencies_ms": [9734.945], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "- People: 15\n- Plates: 20\n- Glasses: 10\n- Wine glasses: 5\n- Wine bottles: 2\n- Silverware: 15\n- Dishes: 10\n- Fruits: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24418.0, "ram_available_mb": 38422.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.56}, "power_stats": {"power_gpu_soc_mean_watts": 21.544, "power_cpu_cv_mean_watts": 1.811, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 71.56, "power_watts_avg": 21.544, "energy_joules_est": 209.74, "duration_seconds": 9.736, "sample_count": 84}, "timestamp": "2026-01-25T18:08:29.920223"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11111.924, "latencies_ms": [11111.924], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a long dining table is filled with plates of food, glasses, and cutlery, indicating a family gathering. The people are seated around the table, with some standing in the background, suggesting a casual and intimate atmosphere. The table is the central focus of the image, with the people arranged around it, creating a sense of togeth", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.0, "ram_available_mb": 38422.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24418.8, "ram_available_mb": 38422.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.854}, "power_stats": {"power_gpu_soc_mean_watts": 20.928, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 69.854, "power_watts_avg": 20.928, "energy_joules_est": 232.56, "duration_seconds": 11.113, "sample_count": 96}, "timestamp": "2026-01-25T18:08:43.058706"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7829.546, "latencies_ms": [7829.546], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A large group of people are gathered around a long dining table, enjoying a meal together. The table is filled with various dishes, drinks, and utensils, indicating a festive or celebratory occasion.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24418.8, "ram_available_mb": 38422.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24412.1, "ram_available_mb": 38428.8, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.537}, "power_stats": {"power_gpu_soc_mean_watts": 22.459, "power_cpu_cv_mean_watts": 1.672, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 73.537, "power_watts_avg": 22.459, "energy_joules_est": 175.86, "duration_seconds": 7.83, "sample_count": 67}, "timestamp": "2026-01-25T18:08:52.915799"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11087.048, "latencies_ms": [11087.048], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a group of people gathered around a long dining table covered with a purple tablecloth. The table is set with various dishes, glasses, and utensils, indicating a meal is being shared. The lighting in the room is warm and natural, coming from the windows in the background, creating a cozy and inviting atmosphere. The materials used for", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24412.1, "ram_available_mb": 38428.8, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24424.1, "ram_available_mb": 38416.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.649}, "power_stats": {"power_gpu_soc_mean_watts": 20.963, "power_cpu_cv_mean_watts": 1.92, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 69.649, "power_watts_avg": 20.963, "energy_joules_est": 232.43, "duration_seconds": 11.088, "sample_count": 94}, "timestamp": "2026-01-25T18:09:06.042156"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11138.829, "latencies_ms": [11138.829], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment from a baseball game. The central focus is a baseball player, dressed in a black uniform, who is sliding into a base. The base is marked by a white line on the dirt ground. The player's body is parallel to the ground, and his legs are extended, indicating a swift movement. \n\nIn the background, there are other players", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24424.1, "ram_available_mb": 38416.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24420.5, "ram_available_mb": 38420.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.263}, "power_stats": {"power_gpu_soc_mean_watts": 20.924, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.934, "gpu_utilization_percent_mean": 69.263, "power_watts_avg": 20.924, "energy_joules_est": 233.08, "duration_seconds": 11.14, "sample_count": 95}, "timestamp": "2026-01-25T18:09:19.221892"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9041.511, "latencies_ms": [9041.511], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "players: 4, catcher: 1, umpire: 1, batter: 1, pitcher: 1, home plate: 1, base: 1, fence: 1, trash can: 1, bench: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24420.5, "ram_available_mb": 38420.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24424.4, "ram_available_mb": 38416.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.833}, "power_stats": {"power_gpu_soc_mean_watts": 21.742, "power_cpu_cv_mean_watts": 1.77, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 69.833, "power_watts_avg": 21.742, "energy_joules_est": 196.6, "duration_seconds": 9.042, "sample_count": 78}, "timestamp": "2026-01-25T18:09:30.304585"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11001.123, "latencies_ms": [11001.123], "images_per_second": 0.091, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "In the foreground, a baseball player is sliding into a base, with the base and home plate clearly visible. The catcher and umpire are in the middle ground, closely positioned to the action at the plate. In the background, spectators are seated on bleachers, and a blue trash can is visible on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24424.4, "ram_available_mb": 38416.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24425.3, "ram_available_mb": 38415.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.894}, "power_stats": {"power_gpu_soc_mean_watts": 20.984, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.894, "power_watts_avg": 20.984, "energy_joules_est": 230.86, "duration_seconds": 11.002, "sample_count": 94}, "timestamp": "2026-01-25T18:09:43.337247"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7936.602, "latencies_ms": [7936.602], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image captures a moment during a baseball game on a field with a dirt infield and a grass outfield. A player is sliding into home plate while another player, the catcher, is attempting to tag him out.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24425.3, "ram_available_mb": 38415.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24419.5, "ram_available_mb": 38421.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.612}, "power_stats": {"power_gpu_soc_mean_watts": 22.422, "power_cpu_cv_mean_watts": 1.661, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 72.612, "power_watts_avg": 22.422, "energy_joules_est": 177.97, "duration_seconds": 7.937, "sample_count": 67}, "timestamp": "2026-01-25T18:09:53.316020"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10313.753, "latencies_ms": [10313.753], "images_per_second": 0.097, "prompt_tokens": 36, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image captures a baseball game in progress under clear skies, with the sun casting shadows on the field, which is a vibrant green with a red dirt infield. The players are wearing a mix of dark and light-colored uniforms, and the stands in the background are mostly empty with a few spectators.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.5, "ram_available_mb": 38421.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24418.9, "ram_available_mb": 38422.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.195}, "power_stats": {"power_gpu_soc_mean_watts": 21.207, "power_cpu_cv_mean_watts": 1.873, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 70.195, "power_watts_avg": 21.207, "energy_joules_est": 218.74, "duration_seconds": 10.314, "sample_count": 87}, "timestamp": "2026-01-25T18:10:05.647188"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12336.823, "latencies_ms": [12336.823], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a dynamic scene at a skatepark. A skateboarder, clad in a black t-shirt and helmet, is in the midst of performing a trick on a concrete ramp. The skateboarder's body is angled towards the ground, with one foot on the skateboard and the other extended outwards, suggesting a moment of", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24418.9, "ram_available_mb": 38422.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24441.1, "ram_available_mb": 38399.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.505}, "power_stats": {"power_gpu_soc_mean_watts": 22.801, "power_cpu_cv_mean_watts": 1.784, "power_sys_5v0_mean_watts": 9.19, "gpu_utilization_percent_mean": 73.505, "power_watts_avg": 22.801, "energy_joules_est": 281.31, "duration_seconds": 12.338, "sample_count": 105}, "timestamp": "2026-01-25T18:10:20.032630"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8609.02, "latencies_ms": [8609.02], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "skateboard: 1, person: 1, fence: 2, trees: 3, grass: 2, buildings: 1, sky: 1, shadow: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24441.1, "ram_available_mb": 38399.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24434.4, "ram_available_mb": 38406.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.959}, "power_stats": {"power_gpu_soc_mean_watts": 24.347, "power_cpu_cv_mean_watts": 1.426, "power_sys_5v0_mean_watts": 9.137, "gpu_utilization_percent_mean": 78.959, "power_watts_avg": 24.347, "energy_joules_est": 209.62, "duration_seconds": 8.61, "sample_count": 73}, "timestamp": "2026-01-25T18:10:30.655421"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11668.281, "latencies_ms": [11668.281], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, performing a trick on a concrete ramp. The railing is in the middle ground, and the grassy area with trees is in the background. The shadow of the skateboarder is cast on the ramp, indicating the light source is coming from the upper left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.4, "ram_available_mb": 38406.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24439.4, "ram_available_mb": 38401.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.888}, "power_stats": {"power_gpu_soc_mean_watts": 23.031, "power_cpu_cv_mean_watts": 1.728, "power_sys_5v0_mean_watts": 9.183, "gpu_utilization_percent_mean": 74.888, "power_watts_avg": 23.031, "energy_joules_est": 268.75, "duration_seconds": 11.669, "sample_count": 98}, "timestamp": "2026-01-25T18:10:44.343952"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7608.806, "latencies_ms": [7608.806], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "A person is skateboarding at a skate park, performing a trick on a rail. The skate park is surrounded by trees and grassy areas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.4, "ram_available_mb": 38401.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24445.5, "ram_available_mb": 38395.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 80.406}, "power_stats": {"power_gpu_soc_mean_watts": 24.855, "power_cpu_cv_mean_watts": 1.282, "power_sys_5v0_mean_watts": 9.128, "gpu_utilization_percent_mean": 80.406, "power_watts_avg": 24.855, "energy_joules_est": 189.13, "duration_seconds": 7.609, "sample_count": 64}, "timestamp": "2026-01-25T18:10:53.974853"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11918.493, "latencies_ms": [11918.493], "images_per_second": 0.084, "prompt_tokens": 36, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The image captures a skateboarder in mid-air, performing a trick at a skatepark. The skateboarder is wearing a black helmet and t-shirt, and the skatepark has a concrete surface with some graffiti. The weather appears to be sunny and clear, with shadows cast on the ground.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24445.5, "ram_available_mb": 38395.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24440.3, "ram_available_mb": 38400.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.196}, "power_stats": {"power_gpu_soc_mean_watts": 23.001, "power_cpu_cv_mean_watts": 1.754, "power_sys_5v0_mean_watts": 9.199, "gpu_utilization_percent_mean": 74.196, "power_watts_avg": 23.001, "energy_joules_est": 274.15, "duration_seconds": 11.919, "sample_count": 102}, "timestamp": "2026-01-25T18:11:07.929097"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11118.666, "latencies_ms": [11118.666], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with a plate of food placed on it. The plate contains a variety of items, including a sandwich, fries, and a salad. The sandwich is accompanied by a side of fries, and the salad consists of a tomato and a pickle. There are also two bottles of condiments, possibly ketchup and mustard", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24440.3, "ram_available_mb": 38400.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24419.7, "ram_available_mb": 38421.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.632}, "power_stats": {"power_gpu_soc_mean_watts": 20.944, "power_cpu_cv_mean_watts": 1.925, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 69.632, "power_watts_avg": 20.944, "energy_joules_est": 232.88, "duration_seconds": 11.119, "sample_count": 95}, "timestamp": "2026-01-25T18:11:21.103655"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10194.972, "latencies_ms": [10194.972], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 69, "n_tiles": 16, "output_text": "- Fries: 12\n\n- Bread: 1\n\n- Meat patty: 1\n\n- Tomato: 1\n\n- Lettuce: 1\n\n- Pickles: 1\n\n- Mayonnaise: 2\n\n- Ketchup: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.7, "ram_available_mb": 38421.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24417.0, "ram_available_mb": 38423.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.264}, "power_stats": {"power_gpu_soc_mean_watts": 21.381, "power_cpu_cv_mean_watts": 1.822, "power_sys_5v0_mean_watts": 8.926, "gpu_utilization_percent_mean": 71.264, "power_watts_avg": 21.381, "energy_joules_est": 217.99, "duration_seconds": 10.196, "sample_count": 87}, "timestamp": "2026-01-25T18:11:33.338571"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11117.246, "latencies_ms": [11117.246], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "On the left side of the image, there is a plate with a burger and fries, which is positioned in the foreground and appears to be the main focus of the meal. In the background, there is a plate with a salad, containing lettuce, tomato, and pickles, which is slightly less prominent. The bottles of condiments are placed on the", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24417.0, "ram_available_mb": 38423.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24422.2, "ram_available_mb": 38418.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.926}, "power_stats": {"power_gpu_soc_mean_watts": 20.997, "power_cpu_cv_mean_watts": 1.903, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.926, "power_watts_avg": 20.997, "energy_joules_est": 233.44, "duration_seconds": 11.118, "sample_count": 94}, "timestamp": "2026-01-25T18:11:46.490573"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8621.825, "latencies_ms": [8621.825], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows a meal consisting of a hamburger, French fries, and a side salad with a slice of lemon on a table. There are also condiment bottles and a glass of water on the table, suggesting a dining setting.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24422.2, "ram_available_mb": 38418.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24416.6, "ram_available_mb": 38424.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.027}, "power_stats": {"power_gpu_soc_mean_watts": 21.945, "power_cpu_cv_mean_watts": 1.722, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 71.027, "power_watts_avg": 21.945, "energy_joules_est": 189.22, "duration_seconds": 8.623, "sample_count": 73}, "timestamp": "2026-01-25T18:11:57.143607"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9547.775, "latencies_ms": [9547.775], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows a meal on a table with a white plate containing a hamburger, fries, and a side of pickles and tomato, accompanied by a glass of water and condiment bottles. The lighting appears to be artificial, and the table has a textured white tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24416.6, "ram_available_mb": 38424.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24418.8, "ram_available_mb": 38422.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.21}, "power_stats": {"power_gpu_soc_mean_watts": 21.355, "power_cpu_cv_mean_watts": 1.838, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 70.21, "power_watts_avg": 21.355, "energy_joules_est": 203.91, "duration_seconds": 9.548, "sample_count": 81}, "timestamp": "2026-01-25T18:12:08.745767"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 8269.244, "latencies_ms": [8269.244], "images_per_second": 0.121, "prompt_tokens": 24, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a red Harley Davidson motorcycle parked on a paved area with a sandy beach in the background. There are palm trees and a wooden fence visible. The sky is clear and blue, suggesting a sunny day.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24418.8, "ram_available_mb": 38422.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24422.7, "ram_available_mb": 38418.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.812}, "power_stats": {"power_gpu_soc_mean_watts": 21.553, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 71.812, "power_watts_avg": 21.553, "energy_joules_est": 178.24, "duration_seconds": 8.27, "sample_count": 69}, "timestamp": "2026-01-25T18:12:19.043332"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7863.714, "latencies_ms": [7863.714], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "motorcycle: 1, palm tree: 3, fence post: 1, fence: 1, sand dune: 1, grass: 1, sky: 1, shadow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.7, "ram_available_mb": 38418.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24432.2, "ram_available_mb": 38408.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.806}, "power_stats": {"power_gpu_soc_mean_watts": 22.172, "power_cpu_cv_mean_watts": 1.691, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 71.806, "power_watts_avg": 22.172, "energy_joules_est": 174.37, "duration_seconds": 7.864, "sample_count": 67}, "timestamp": "2026-01-25T18:12:28.941826"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8111.663, "latencies_ms": [8111.663], "images_per_second": 0.123, "prompt_tokens": 44, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The motorcycle is parked in the foreground on the right side of the image, near the center. The palm trees are in the background, behind the motorcycle, and there is a wooden fence to the right of the motorcycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.2, "ram_available_mb": 38408.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24430.9, "ram_available_mb": 38410.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.015}, "power_stats": {"power_gpu_soc_mean_watts": 22.016, "power_cpu_cv_mean_watts": 1.742, "power_sys_5v0_mean_watts": 9.052, "gpu_utilization_percent_mean": 72.015, "power_watts_avg": 22.016, "energy_joules_est": 178.6, "duration_seconds": 8.112, "sample_count": 68}, "timestamp": "2026-01-25T18:12:39.103914"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8271.45, "latencies_ms": [8271.45], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A red motorcycle with a sidecar is parked on a paved area near a wooden fence, with palm trees and a sandy beach in the background. The sky is clear and blue, suggesting a sunny day at the beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24430.9, "ram_available_mb": 38410.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24425.9, "ram_available_mb": 38415.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.829}, "power_stats": {"power_gpu_soc_mean_watts": 22.215, "power_cpu_cv_mean_watts": 1.716, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 72.829, "power_watts_avg": 22.215, "energy_joules_est": 183.76, "duration_seconds": 8.272, "sample_count": 70}, "timestamp": "2026-01-25T18:12:49.393047"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6636.616, "latencies_ms": [6636.616], "images_per_second": 0.151, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The motorcycle is red and black, parked on a sunny day with clear blue skies. The sand is light beige, and there are palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.9, "ram_available_mb": 38415.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24428.1, "ram_available_mb": 38412.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.246}, "power_stats": {"power_gpu_soc_mean_watts": 22.822, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 9.066, "gpu_utilization_percent_mean": 73.246, "power_watts_avg": 22.822, "energy_joules_est": 151.48, "duration_seconds": 6.637, "sample_count": 57}, "timestamp": "2026-01-25T18:12:58.083776"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11125.081, "latencies_ms": [11125.081], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of quiet introspection. A man, dressed in a formal black suit and tie, stands against a stark white wall. His face is turned away from the camera, suggesting a moment of contemplation or perhaps a desire for privacy. The lighting in the room is dim, casting a soft glow on his suit and creating a sense of solitude. The", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24428.1, "ram_available_mb": 38412.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24419.4, "ram_available_mb": 38421.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.606}, "power_stats": {"power_gpu_soc_mean_watts": 20.931, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.606, "power_watts_avg": 20.931, "energy_joules_est": 232.87, "duration_seconds": 11.126, "sample_count": 94}, "timestamp": "2026-01-25T18:13:11.237166"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6942.951, "latencies_ms": [6942.951], "images_per_second": 0.144, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.4, "ram_available_mb": 38421.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24423.0, "ram_available_mb": 38417.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.39}, "power_stats": {"power_gpu_soc_mean_watts": 22.987, "power_cpu_cv_mean_watts": 1.547, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 74.39, "power_watts_avg": 22.987, "energy_joules_est": 159.61, "duration_seconds": 6.944, "sample_count": 59}, "timestamp": "2026-01-25T18:13:20.193683"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11100.458, "latencies_ms": [11100.458], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The man is positioned in the foreground of the image, with his upper body and attire in clear focus. The electrical outlet is located on the wall in the background, slightly to the left of the man's body. The man is standing close to the wall, with a small gap between his suit jacket and the wall, indicating he is not too far from the background", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.0, "ram_available_mb": 38417.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24421.9, "ram_available_mb": 38419.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.758}, "power_stats": {"power_gpu_soc_mean_watts": 20.966, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.758, "power_watts_avg": 20.966, "energy_joules_est": 232.75, "duration_seconds": 11.101, "sample_count": 95}, "timestamp": "2026-01-25T18:13:33.320172"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6596.179, "latencies_ms": [6596.179], "images_per_second": 0.152, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A man in a dark suit with a white shirt and a patterned tie is standing in a dimly lit room. The light switch is visible on the wall behind him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.9, "ram_available_mb": 38419.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24431.9, "ram_available_mb": 38409.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.964}, "power_stats": {"power_gpu_soc_mean_watts": 23.282, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 9.005, "gpu_utilization_percent_mean": 74.964, "power_watts_avg": 23.282, "energy_joules_est": 153.59, "duration_seconds": 6.597, "sample_count": 56}, "timestamp": "2026-01-25T18:13:41.976326"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7090.003, "latencies_ms": [7090.003], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image shows a person wearing a dark suit with a white shirt and a patterned tie. The lighting is dim, highlighting the person's attire and creating a moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.9, "ram_available_mb": 38409.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24448.0, "ram_available_mb": 38392.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.383}, "power_stats": {"power_gpu_soc_mean_watts": 22.626, "power_cpu_cv_mean_watts": 1.641, "power_sys_5v0_mean_watts": 9.066, "gpu_utilization_percent_mean": 72.383, "power_watts_avg": 22.626, "energy_joules_est": 160.43, "duration_seconds": 7.091, "sample_count": 60}, "timestamp": "2026-01-25T18:13:51.094664"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11140.588, "latencies_ms": [11140.588], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a gray and white cat is peacefully sleeping on a pair of shoes. The cat is curled up, with its head resting on the shoes, and its eyes closed. The shoes are placed on a wooden floor, and the cat is positioned in the center of the image. The background features a white wall, providing a neutral backdrop that contrast", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24448.0, "ram_available_mb": 38392.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24540.9, "ram_available_mb": 38300.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.137}, "power_stats": {"power_gpu_soc_mean_watts": 20.873, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 69.137, "power_watts_avg": 20.873, "energy_joules_est": 232.55, "duration_seconds": 11.141, "sample_count": 95}, "timestamp": "2026-01-25T18:14:04.264580"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4407.385, "latencies_ms": [4407.385], "images_per_second": 0.227, "prompt_tokens": 39, "response_tokens_est": 17, "n_tiles": 16, "output_text": "shoe: 2\ncat: 1\nwall: 1\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.1, "ram_available_mb": 38421.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 80.568}, "power_stats": {"power_gpu_soc_mean_watts": 25.692, "power_cpu_cv_mean_watts": 1.071, "power_sys_5v0_mean_watts": 9.143, "gpu_utilization_percent_mean": 80.568, "power_watts_avg": 25.692, "energy_joules_est": 113.25, "duration_seconds": 4.408, "sample_count": 37}, "timestamp": "2026-01-25T18:14:10.702457"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11120.108, "latencies_ms": [11120.108], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The cat is resting on top of a shoe, which is placed on the ground in the foreground of the image. The shoe is positioned to the left of the image, and there is a white wall in the background. The cat is near the front edge of the image, and the shoe is in the immediate foreground, making the cat appear to be closer to the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24423.8, "ram_available_mb": 38417.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.701}, "power_stats": {"power_gpu_soc_mean_watts": 20.835, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 69.701, "power_watts_avg": 20.835, "energy_joules_est": 231.71, "duration_seconds": 11.121, "sample_count": 97}, "timestamp": "2026-01-25T18:14:23.840204"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6274.517, "latencies_ms": [6274.517], "images_per_second": 0.159, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "A cat is sleeping peacefully on a pair of shoes. The shoes are placed on a wooden surface, and the background is a plain white wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.8, "ram_available_mb": 38417.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24422.6, "ram_available_mb": 38418.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.788}, "power_stats": {"power_gpu_soc_mean_watts": 23.502, "power_cpu_cv_mean_watts": 1.463, "power_sys_5v0_mean_watts": 9.055, "gpu_utilization_percent_mean": 75.788, "power_watts_avg": 23.502, "energy_joules_est": 147.48, "duration_seconds": 6.275, "sample_count": 52}, "timestamp": "2026-01-25T18:14:32.136238"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7786.025, "latencies_ms": [7786.025], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a gray and white cat with a white chest and paws, sleeping on a pair of blue and white sneakers. The cat is resting on a wooden surface, with a white wall in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24422.6, "ram_available_mb": 38418.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24418.4, "ram_available_mb": 38422.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.697}, "power_stats": {"power_gpu_soc_mean_watts": 22.164, "power_cpu_cv_mean_watts": 1.71, "power_sys_5v0_mean_watts": 9.011, "gpu_utilization_percent_mean": 71.697, "power_watts_avg": 22.164, "energy_joules_est": 172.58, "duration_seconds": 7.787, "sample_count": 66}, "timestamp": "2026-01-25T18:14:41.979733"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11147.773, "latencies_ms": [11147.773], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene on a city street. Dominating the frame is a large green dump truck, its body adorned with a red and white striped pattern on the front bumper. The truck is in motion, driving on the right side of the road, as indicated by the white line on the left side of the image. \n\nOn", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24418.4, "ram_available_mb": 38422.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24421.1, "ram_available_mb": 38419.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.745}, "power_stats": {"power_gpu_soc_mean_watts": 20.878, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 69.745, "power_watts_avg": 20.878, "energy_joules_est": 232.76, "duration_seconds": 11.148, "sample_count": 94}, "timestamp": "2026-01-25T18:14:55.177357"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7513.602, "latencies_ms": [7513.602], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "truck: 1, people: 3, buildings: 2, trees: 2, vehicles: 2, traffic lights: 1, signs: 2, pedestrians: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.1, "ram_available_mb": 38419.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24417.3, "ram_available_mb": 38423.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.672}, "power_stats": {"power_gpu_soc_mean_watts": 22.559, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 73.672, "power_watts_avg": 22.559, "energy_joules_est": 169.51, "duration_seconds": 7.514, "sample_count": 64}, "timestamp": "2026-01-25T18:15:04.725069"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11122.226, "latencies_ms": [11122.226], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The green truck is in the foreground of the image, occupying the lower half of the frame, and is positioned on the right side of the road. There are two individuals on the truck's bed, with one seated on the left side and the other on the right, both facing forward. In the background, there is a red bus on the left side and a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24417.3, "ram_available_mb": 38423.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24442.7, "ram_available_mb": 38398.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.137}, "power_stats": {"power_gpu_soc_mean_watts": 20.94, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 69.137, "power_watts_avg": 20.94, "energy_joules_est": 232.91, "duration_seconds": 11.123, "sample_count": 95}, "timestamp": "2026-01-25T18:15:17.874477"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8501.8, "latencies_ms": [8501.8], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "A green Isuzu truck with a red and white striped front bumper is on a city street. There are two people sitting on the back of the truck, one wearing a yellow hat and the other wearing a green jacket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.7, "ram_available_mb": 38398.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24430.0, "ram_available_mb": 38410.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.597}, "power_stats": {"power_gpu_soc_mean_watts": 21.885, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 71.597, "power_watts_avg": 21.885, "energy_joules_est": 186.08, "duration_seconds": 8.503, "sample_count": 72}, "timestamp": "2026-01-25T18:15:28.389548"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8459.864, "latencies_ms": [8459.864], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "A green Isuzu FTR truck with a red and white striped front bumper is on the road. The truck has a license plate that reads \"77-6921\" and is marked with the logo of \"Thailand\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24430.0, "ram_available_mb": 38410.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24449.6, "ram_available_mb": 38391.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.282}, "power_stats": {"power_gpu_soc_mean_watts": 21.833, "power_cpu_cv_mean_watts": 1.748, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 71.282, "power_watts_avg": 21.833, "energy_joules_est": 184.72, "duration_seconds": 8.461, "sample_count": 71}, "timestamp": "2026-01-25T18:15:38.877160"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12142.247, "latencies_ms": [12142.247], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a bird is standing on a rocky shore near a river. The bird is positioned in the center of the scene, surrounded by various rocks and pebbles. The river appears to be shallow, with the bird standing on the rocks close to the water's edge.\n\nIn the background, there is a bridge spanning across the river, with multiple cars", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 24449.6, "ram_available_mb": 38391.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24422.9, "ram_available_mb": 38418.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.602}, "power_stats": {"power_gpu_soc_mean_watts": 22.199, "power_cpu_cv_mean_watts": 1.788, "power_sys_5v0_mean_watts": 9.132, "gpu_utilization_percent_mean": 72.602, "power_watts_avg": 22.199, "energy_joules_est": 269.56, "duration_seconds": 12.143, "sample_count": 103}, "timestamp": "2026-01-25T18:15:53.045142"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7429.714, "latencies_ms": [7429.714], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 35, "n_tiles": 16, "output_text": "rocks: numerous\nwater: river\ntrees: greenery\nbridge: overpass\ncars: on bridge\nbird: standing on rocks\nfish: in water", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24422.9, "ram_available_mb": 38418.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24423.8, "ram_available_mb": 38417.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.46}, "power_stats": {"power_gpu_soc_mean_watts": 24.605, "power_cpu_cv_mean_watts": 1.315, "power_sys_5v0_mean_watts": 9.114, "gpu_utilization_percent_mean": 79.46, "power_watts_avg": 24.605, "energy_joules_est": 182.82, "duration_seconds": 7.43, "sample_count": 63}, "timestamp": "2026-01-25T18:16:02.528787"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12067.575, "latencies_ms": [12067.575], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a rocky riverbed with various sized rocks scattered across the water's surface. Further back, on the riverbank, there is a bridge spanning across the river, with vegetation growing on the banks. The bird is standing on the rocks in the middle ground, closer to the camera than the bridge, but further away from the camera than the rocks", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.8, "ram_available_mb": 38417.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24432.1, "ram_available_mb": 38408.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.961}, "power_stats": {"power_gpu_soc_mean_watts": 22.701, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 9.151, "gpu_utilization_percent_mean": 72.961, "power_watts_avg": 22.701, "energy_joules_est": 273.96, "duration_seconds": 12.068, "sample_count": 102}, "timestamp": "2026-01-25T18:16:16.623193"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7928.765, "latencies_ms": [7928.765], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image depicts a serene river scene with a bridge in the background and a bird standing on the rocky shore. The bird appears to be searching for food among the rocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.1, "ram_available_mb": 38408.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24424.2, "ram_available_mb": 38416.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.235}, "power_stats": {"power_gpu_soc_mean_watts": 23.833, "power_cpu_cv_mean_watts": 1.407, "power_sys_5v0_mean_watts": 9.103, "gpu_utilization_percent_mean": 79.235, "power_watts_avg": 23.833, "energy_joules_est": 188.98, "duration_seconds": 7.929, "sample_count": 68}, "timestamp": "2026-01-25T18:16:26.602688"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7915.09, "latencies_ms": [7915.09], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image depicts a serene river scene with a bridge in the background and a clear sky above. The river is filled with rocks and boulders, and the water appears calm and still.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24424.2, "ram_available_mb": 38416.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24423.8, "ram_available_mb": 38417.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.731}, "power_stats": {"power_gpu_soc_mean_watts": 24.093, "power_cpu_cv_mean_watts": 1.476, "power_sys_5v0_mean_watts": 9.21, "gpu_utilization_percent_mean": 76.731, "power_watts_avg": 24.093, "energy_joules_est": 190.71, "duration_seconds": 7.916, "sample_count": 67}, "timestamp": "2026-01-25T18:16:36.575429"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11135.746, "latencies_ms": [11135.746], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a row of parked motor scooters lined up along a street. There are at least 13 motor scooters visible, with some positioned closer to the camera and others further away. The scooters are parked in a neat row, occupying a significant portion of the street.\n\nIn addition to the motor scooters, there are several", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24423.8, "ram_available_mb": 38417.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24420.2, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.573}, "power_stats": {"power_gpu_soc_mean_watts": 20.856, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.573, "power_watts_avg": 20.856, "energy_joules_est": 232.26, "duration_seconds": 11.136, "sample_count": 96}, "timestamp": "2026-01-25T18:16:49.761105"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8294.465, "latencies_ms": [8294.465], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "- scooters: 10\n- people: 5\n- buildings: 2\n- windows: 14\n- doors: 2\n- signs: 3\n- trees: 1\n- plants: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24420.2, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24427.4, "ram_available_mb": 38413.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.643}, "power_stats": {"power_gpu_soc_mean_watts": 22.19, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 71.643, "power_watts_avg": 22.19, "energy_joules_est": 184.07, "duration_seconds": 8.295, "sample_count": 70}, "timestamp": "2026-01-25T18:17:00.085948"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11205.223, "latencies_ms": [11205.223], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a row of scooters parked on the side of the street, with the closest scooters being nearest to the viewer and the farthest ones being the last in the row. The background features a building with a red awning and a sign that reads \"BAR BRASERIE.\" There are also several people walking on the sidewalk", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24427.4, "ram_available_mb": 38413.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24430.6, "ram_available_mb": 38410.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.531}, "power_stats": {"power_gpu_soc_mean_watts": 20.014, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 70.531, "power_watts_avg": 20.014, "energy_joules_est": 224.27, "duration_seconds": 11.206, "sample_count": 96}, "timestamp": "2026-01-25T18:17:13.305479"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8372.582, "latencies_ms": [8372.582], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image depicts a street scene with a row of parked scooters and motorcycles in front of a building with a red awning. There are several people walking on the sidewalk and a few individuals standing near the parked vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24430.6, "ram_available_mb": 38410.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24425.7, "ram_available_mb": 38415.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.718}, "power_stats": {"power_gpu_soc_mean_watts": 22.06, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 71.718, "power_watts_avg": 22.06, "energy_joules_est": 184.71, "duration_seconds": 8.373, "sample_count": 71}, "timestamp": "2026-01-25T18:17:23.694545"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7550.553, "latencies_ms": [7550.553], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a row of scooters parked on a street, with a variety of colors including black, silver, and green. The weather appears to be overcast, with no direct sunlight visible in the scene.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24425.7, "ram_available_mb": 38415.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24421.7, "ram_available_mb": 38419.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.062}, "power_stats": {"power_gpu_soc_mean_watts": 22.262, "power_cpu_cv_mean_watts": 1.683, "power_sys_5v0_mean_watts": 9.018, "gpu_utilization_percent_mean": 72.062, "power_watts_avg": 22.262, "energy_joules_est": 168.1, "duration_seconds": 7.551, "sample_count": 64}, "timestamp": "2026-01-25T18:17:33.296507"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10135.971, "latencies_ms": [10135.971], "images_per_second": 0.099, "prompt_tokens": 24, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image shows a close-up of a person's hand holding a piece of broccoli. The broccoli is green with a few brown spots, indicating it may be slightly overripe or cooked. The background is blurred, but it appears to be a kitchen setting with a tiled backsplash.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24421.7, "ram_available_mb": 38419.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24429.0, "ram_available_mb": 38411.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.419}, "power_stats": {"power_gpu_soc_mean_watts": 21.142, "power_cpu_cv_mean_watts": 1.848, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.419, "power_watts_avg": 21.142, "energy_joules_est": 214.31, "duration_seconds": 10.137, "sample_count": 86}, "timestamp": "2026-01-25T18:17:45.503195"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6967.57, "latencies_ms": [6967.57], "images_per_second": 0.144, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.0, "ram_available_mb": 38411.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24418.9, "ram_available_mb": 38422.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.695}, "power_stats": {"power_gpu_soc_mean_watts": 22.991, "power_cpu_cv_mean_watts": 1.567, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 74.695, "power_watts_avg": 22.991, "energy_joules_est": 160.21, "duration_seconds": 6.968, "sample_count": 59}, "timestamp": "2026-01-25T18:17:54.495249"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10233.89, "latencies_ms": [10233.89], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The main object, which is a piece of broccoli, is held in the foreground of the image, appearing large and in focus. It is positioned in the center of the image, with a blurred background that suggests a kitchen setting. The broccoli is held up, indicating it is the primary subject of the photo.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24418.9, "ram_available_mb": 38422.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.36}, "power_stats": {"power_gpu_soc_mean_watts": 21.182, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 70.36, "power_watts_avg": 21.182, "energy_joules_est": 216.79, "duration_seconds": 10.235, "sample_count": 86}, "timestamp": "2026-01-25T18:18:06.741712"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6660.888, "latencies_ms": [6660.888], "images_per_second": 0.15, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A person is holding a piece of broccoli with a mushroom attached to it, against a backdrop of a kitchen with a black pot and a tiled wall.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24416.3, "ram_available_mb": 38424.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.912}, "power_stats": {"power_gpu_soc_mean_watts": 23.054, "power_cpu_cv_mean_watts": 1.524, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 73.912, "power_watts_avg": 23.054, "energy_joules_est": 153.58, "duration_seconds": 6.662, "sample_count": 57}, "timestamp": "2026-01-25T18:18:15.454189"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9344.613, "latencies_ms": [9344.613], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image shows a close-up of a hand holding a piece of broccoli with a dark, possibly roasted, mushroom on top. The lighting is bright and appears to be coming from the upper left, casting a shadow to the right of the broccoli and mushroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24416.3, "ram_available_mb": 38424.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24411.7, "ram_available_mb": 38429.2, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.425}, "power_stats": {"power_gpu_soc_mean_watts": 21.364, "power_cpu_cv_mean_watts": 1.831, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 70.425, "power_watts_avg": 21.364, "energy_joules_est": 199.65, "duration_seconds": 9.345, "sample_count": 80}, "timestamp": "2026-01-25T18:18:26.858856"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11140.863, "latencies_ms": [11140.863], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two individuals standing close to each other. The person on the left is wearing a black jacket and has their mouth wide open, as if they are shouting or laughing. The person on the right is wearing a green jacket with a fur hood, and they are looking upwards. The background is blurred, but it appears to be", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24411.7, "ram_available_mb": 38429.2, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24448.4, "ram_available_mb": 38392.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.263}, "power_stats": {"power_gpu_soc_mean_watts": 20.886, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 69.263, "power_watts_avg": 20.886, "energy_joules_est": 232.7, "duration_seconds": 11.141, "sample_count": 95}, "timestamp": "2026-01-25T18:18:40.049209"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6971.101, "latencies_ms": [6971.101], "images_per_second": 0.143, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24448.4, "ram_available_mb": 38392.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24415.0, "ram_available_mb": 38425.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.119}, "power_stats": {"power_gpu_soc_mean_watts": 22.935, "power_cpu_cv_mean_watts": 1.574, "power_sys_5v0_mean_watts": 9.019, "gpu_utilization_percent_mean": 73.119, "power_watts_avg": 22.935, "energy_joules_est": 159.9, "duration_seconds": 6.972, "sample_count": 59}, "timestamp": "2026-01-25T18:18:49.044293"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7530.881, "latencies_ms": [7530.881], "images_per_second": 0.133, "prompt_tokens": 44, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The person on the left is in the foreground and appears to be facing the camera, while the person on the right is slightly behind and to the right of the first person, both are in the middle ground of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24415.0, "ram_available_mb": 38425.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.266}, "power_stats": {"power_gpu_soc_mean_watts": 22.418, "power_cpu_cv_mean_watts": 1.695, "power_sys_5v0_mean_watts": 9.049, "gpu_utilization_percent_mean": 72.266, "power_watts_avg": 22.418, "energy_joules_est": 168.84, "duration_seconds": 7.532, "sample_count": 64}, "timestamp": "2026-01-25T18:18:58.634381"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8198.992, "latencies_ms": [8198.992], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows two individuals in a blurred motion, suggesting movement or a quick capture of the moment. They appear to be outdoors, possibly in a public space, as indicated by the presence of other people and structures in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24427.3, "ram_available_mb": 38413.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.609}, "power_stats": {"power_gpu_soc_mean_watts": 22.076, "power_cpu_cv_mean_watts": 1.688, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 71.609, "power_watts_avg": 22.076, "energy_joules_est": 181.01, "duration_seconds": 8.2, "sample_count": 69}, "timestamp": "2026-01-25T18:19:08.888680"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8999.153, "latencies_ms": [8999.153], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image features a person wearing a dark jacket with a fur-lined hood, and the lighting appears to be artificial, possibly from an indoor source. The background is blurred, but there seems to be a hint of a snowy environment, suggesting cold weather.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24427.3, "ram_available_mb": 38413.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24429.6, "ram_available_mb": 38411.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.645}, "power_stats": {"power_gpu_soc_mean_watts": 21.65, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 9.014, "gpu_utilization_percent_mean": 70.645, "power_watts_avg": 21.65, "energy_joules_est": 194.85, "duration_seconds": 9.0, "sample_count": 76}, "timestamp": "2026-01-25T18:19:19.949104"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11147.393, "latencies_ms": [11147.393], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a man standing on a tennis court, holding a tennis racket and preparing to hit a tennis ball. He is positioned near the center of the court, and the ball is located slightly to his right. The man appears to be focused on the ball, getting ready to make a shot.\n\nThe court is surrounded by numerous chairs, with some placed closer to the", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24429.6, "ram_available_mb": 38411.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24425.8, "ram_available_mb": 38415.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.691}, "power_stats": {"power_gpu_soc_mean_watts": 20.89, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 69.691, "power_watts_avg": 20.89, "energy_joules_est": 232.88, "duration_seconds": 11.148, "sample_count": 94}, "timestamp": "2026-01-25T18:19:33.137183"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9129.464, "latencies_ms": [9129.464], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Chair: 11\n- Tennis ball: 1\n- Tennis racket: 1\n- Tennis player: 1\n- Shoe: 1\n- Sock: 1\n- Wristband: 1\n- Wristband: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.8, "ram_available_mb": 38415.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24419.3, "ram_available_mb": 38421.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.675}, "power_stats": {"power_gpu_soc_mean_watts": 21.511, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 70.675, "power_watts_avg": 21.511, "energy_joules_est": 196.4, "duration_seconds": 9.13, "sample_count": 77}, "timestamp": "2026-01-25T18:19:44.304907"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11154.998, "latencies_ms": [11154.998], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a tennis player is positioned on the left side of the image, holding a tennis racket and preparing to hit a tennis ball that is near the center of the image. The background consists of multiple rows of white stadium seats, which are arranged in a pattern that recedes towards the right side of the image. The tennis court itself is a vibrant blue", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.3, "ram_available_mb": 38421.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.594}, "power_stats": {"power_gpu_soc_mean_watts": 20.845, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 69.594, "power_watts_avg": 20.845, "energy_joules_est": 232.54, "duration_seconds": 11.156, "sample_count": 96}, "timestamp": "2026-01-25T18:19:57.482053"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6727.046, "latencies_ms": [6727.046], "images_per_second": 0.149, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A tennis player is on a blue tennis court, holding a racket and preparing to hit a yellow tennis ball. The court is surrounded by rows of white folding chairs.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24437.9, "ram_available_mb": 38403.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.589}, "power_stats": {"power_gpu_soc_mean_watts": 23.07, "power_cpu_cv_mean_watts": 1.544, "power_sys_5v0_mean_watts": 9.008, "gpu_utilization_percent_mean": 73.589, "power_watts_avg": 23.07, "energy_joules_est": 155.21, "duration_seconds": 6.728, "sample_count": 56}, "timestamp": "2026-01-25T18:20:06.250209"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5879.706, "latencies_ms": [5879.706], "images_per_second": 0.17, "prompt_tokens": 36, "response_tokens_est": 32, "n_tiles": 16, "output_text": "The tennis player is wearing a white sleeveless shirt and black shorts. The court is blue with white lines marking the boundaries.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24437.9, "ram_available_mb": 38403.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.837}, "power_stats": {"power_gpu_soc_mean_watts": 23.586, "power_cpu_cv_mean_watts": 1.471, "power_sys_5v0_mean_watts": 9.1, "gpu_utilization_percent_mean": 74.837, "power_watts_avg": 23.586, "energy_joules_est": 138.7, "duration_seconds": 5.881, "sample_count": 49}, "timestamp": "2026-01-25T18:20:14.171437"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11140.83, "latencies_ms": [11140.83], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a serene indoor setting. At the center of the scene is a **red glass vase** with a unique design, featuring a **floral pattern**. The vase is placed on a **green glass coaster**, which is positioned on a **wooden table**. The table is adorned with a **white candle**, adding a touch", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24416.9, "ram_available_mb": 38424.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.611}, "power_stats": {"power_gpu_soc_mean_watts": 20.909, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.92, "gpu_utilization_percent_mean": 69.611, "power_watts_avg": 20.909, "energy_joules_est": 232.96, "duration_seconds": 11.141, "sample_count": 95}, "timestamp": "2026-01-25T18:20:27.367808"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7509.045, "latencies_ms": [7509.045], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "candle: 1, vase: 1, plate: 1, string lights: multiple, wooden frame: 1, wall: 1, table: 1, candle holder: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24416.9, "ram_available_mb": 38424.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24416.6, "ram_available_mb": 38424.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.19}, "power_stats": {"power_gpu_soc_mean_watts": 22.49, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 72.19, "power_watts_avg": 22.49, "energy_joules_est": 168.89, "duration_seconds": 7.51, "sample_count": 63}, "timestamp": "2026-01-25T18:20:36.917417"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7883.154, "latencies_ms": [7883.154], "images_per_second": 0.127, "prompt_tokens": 44, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The glass vase is placed in the foreground on the right side of the image, while the white candle is in the foreground on the left side. The candle is positioned closer to the viewer than the vase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24416.6, "ram_available_mb": 38424.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24421.9, "ram_available_mb": 38419.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.448}, "power_stats": {"power_gpu_soc_mean_watts": 22.168, "power_cpu_cv_mean_watts": 1.715, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 71.448, "power_watts_avg": 22.168, "energy_joules_est": 174.77, "duration_seconds": 7.884, "sample_count": 67}, "timestamp": "2026-01-25T18:20:46.835751"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11303.054, "latencies_ms": [11303.054], "images_per_second": 0.088, "prompt_tokens": 37, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The image shows a decorative setting with a large, ornate glass vase placed on a glass coaster, surrounded by a string of warm white lights that create a cozy ambiance. The vase is situated on a wooden surface, possibly a table, and there is a white candle to the left of the vase, suggesting a tranquil and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.9, "ram_available_mb": 38419.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24423.6, "ram_available_mb": 38417.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.76}, "power_stats": {"power_gpu_soc_mean_watts": 21.072, "power_cpu_cv_mean_watts": 1.881, "power_sys_5v0_mean_watts": 8.929, "gpu_utilization_percent_mean": 70.76, "power_watts_avg": 21.072, "energy_joules_est": 238.19, "duration_seconds": 11.304, "sample_count": 96}, "timestamp": "2026-01-25T18:21:00.189322"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6856.75, "latencies_ms": [6856.75], "images_per_second": 0.146, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image features a clear glass vase with a red interior, placed on a green coaster. The vase contains orange flowers, and the setting is illuminated by warm white string lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.6, "ram_available_mb": 38417.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24418.8, "ram_available_mb": 38422.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.517}, "power_stats": {"power_gpu_soc_mean_watts": 22.78, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.06, "gpu_utilization_percent_mean": 72.517, "power_watts_avg": 22.78, "energy_joules_est": 156.21, "duration_seconds": 6.857, "sample_count": 58}, "timestamp": "2026-01-25T18:21:09.097365"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11116.362, "latencies_ms": [11116.362], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a man in a room with a brown couch. He is wearing a black shirt and is bending over, possibly adjusting something on the couch. The room is equipped with a tripod and a light, suggesting that the man might be setting up for a photoshoot or a video recording. The man's face is blurred", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 24418.8, "ram_available_mb": 38422.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24424.9, "ram_available_mb": 38416.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.394}, "power_stats": {"power_gpu_soc_mean_watts": 20.971, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 69.394, "power_watts_avg": 20.971, "energy_joules_est": 233.13, "duration_seconds": 11.117, "sample_count": 94}, "timestamp": "2026-01-25T18:21:22.264152"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9643.862, "latencies_ms": [9643.862], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "- Couch: 1\n\n- Jacket: 1\n\n- Microphone stand: 1\n\n- Cable: 1\n\n- Suitcase: 1\n\n- Lighting equipment: 1\n\n- Chair: 1\n\n- Table: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24424.9, "ram_available_mb": 38416.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24421.3, "ram_available_mb": 38419.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.573}, "power_stats": {"power_gpu_soc_mean_watts": 21.436, "power_cpu_cv_mean_watts": 1.811, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 70.573, "power_watts_avg": 21.436, "energy_joules_est": 206.74, "duration_seconds": 9.644, "sample_count": 82}, "timestamp": "2026-01-25T18:21:33.941428"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9434.246, "latencies_ms": [9434.246], "images_per_second": 0.106, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "In the foreground, there is a person bending over, seemingly interacting with the equipment. Behind them, another person is standing and appears to be operating a lighting setup. The lighting equipment is positioned to the left of the scene, while the person operating it is to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.3, "ram_available_mb": 38419.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24408.9, "ram_available_mb": 38432.0, "ram_percent": 38.8}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.825}, "power_stats": {"power_gpu_soc_mean_watts": 21.498, "power_cpu_cv_mean_watts": 1.831, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 70.825, "power_watts_avg": 21.498, "energy_joules_est": 202.83, "duration_seconds": 9.435, "sample_count": 80}, "timestamp": "2026-01-25T18:21:45.387957"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7933.989, "latencies_ms": [7933.989], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "In a room with a beige carpet, a man is bending over a couch while another man stands behind him, holding a white umbrella. There is a suitcase and a coat hanging on the couch.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 24408.9, "ram_available_mb": 38432.0, "ram_percent": 38.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24419.4, "ram_available_mb": 38421.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.397}, "power_stats": {"power_gpu_soc_mean_watts": 22.266, "power_cpu_cv_mean_watts": 1.684, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 72.397, "power_watts_avg": 22.266, "energy_joules_est": 176.67, "duration_seconds": 7.935, "sample_count": 68}, "timestamp": "2026-01-25T18:21:55.356725"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6872.894, "latencies_ms": [6872.894], "images_per_second": 0.145, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image shows a person in a room with a wooden floor and a beige carpet. There is a white umbrella in the background and a brown jacket hanging on a chair.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24419.4, "ram_available_mb": 38421.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24449.5, "ram_available_mb": 38391.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.5}, "power_stats": {"power_gpu_soc_mean_watts": 22.851, "power_cpu_cv_mean_watts": 1.85, "power_sys_5v0_mean_watts": 9.114, "gpu_utilization_percent_mean": 72.5, "power_watts_avg": 22.851, "energy_joules_est": 157.07, "duration_seconds": 6.873, "sample_count": 58}, "timestamp": "2026-01-25T18:22:04.271087"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12332.271, "latencies_ms": [12332.271], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photo, a woman is the central figure. She is adorned in a wide-brimmed hat that casts a shadow over her face, adding an air of mystery. Her eyes are closed, and a smile graces her lips, suggesting she is savoring the moment. In her hand, she holds a cigarette, its smoke curling upwards", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24449.5, "ram_available_mb": 38391.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24444.6, "ram_available_mb": 38396.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.267}, "power_stats": {"power_gpu_soc_mean_watts": 22.82, "power_cpu_cv_mean_watts": 1.788, "power_sys_5v0_mean_watts": 9.191, "gpu_utilization_percent_mean": 74.267, "power_watts_avg": 22.82, "energy_joules_est": 281.44, "duration_seconds": 12.333, "sample_count": 105}, "timestamp": "2026-01-25T18:22:18.629079"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9157.293, "latencies_ms": [9157.293], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "hat: 1, earring: 1, necklace: 1, striped top: 1, cigarette: 1, hand: 1, bracelet: 1, smile: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24444.6, "ram_available_mb": 38396.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24451.6, "ram_available_mb": 38389.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.064}, "power_stats": {"power_gpu_soc_mean_watts": 24.129, "power_cpu_cv_mean_watts": 1.488, "power_sys_5v0_mean_watts": 9.152, "gpu_utilization_percent_mean": 78.064, "power_watts_avg": 24.129, "energy_joules_est": 220.97, "duration_seconds": 9.158, "sample_count": 78}, "timestamp": "2026-01-25T18:22:29.838790"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12422.436, "latencies_ms": [12422.436], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The woman is positioned in the foreground, wearing a wide-brimmed hat that extends to the left side of the frame, and a striped top that is visible in the mid-ground. She is holding a cigarette to her mouth, which is near the center of the image. The background is plain and unadorned, providing no additional context or objects to", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24451.6, "ram_available_mb": 38389.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24456.0, "ram_available_mb": 38384.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.505}, "power_stats": {"power_gpu_soc_mean_watts": 22.909, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 9.221, "gpu_utilization_percent_mean": 74.505, "power_watts_avg": 22.909, "energy_joules_est": 284.6, "duration_seconds": 12.423, "sample_count": 105}, "timestamp": "2026-01-25T18:22:44.278028"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10952.885, "latencies_ms": [10952.885], "images_per_second": 0.091, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "A woman is seen wearing a wide-brimmed hat and a striped tank top, with a necklace and a bracelet on her wrist, smiling and holding a cigarette in her hand. The image is in black and white, giving it a timeless and classic feel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24456.0, "ram_available_mb": 38384.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24433.4, "ram_available_mb": 38407.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.255}, "power_stats": {"power_gpu_soc_mean_watts": 23.409, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 9.134, "gpu_utilization_percent_mean": 76.255, "power_watts_avg": 23.409, "energy_joules_est": 256.41, "duration_seconds": 10.954, "sample_count": 94}, "timestamp": "2026-01-25T18:22:57.282504"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9028.729, "latencies_ms": [9028.729], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image is in black and white, featuring a person wearing a wide-brimmed hat and a striped tank top. The lighting appears to be natural, casting soft shadows on the person's face and hat.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24433.4, "ram_available_mb": 38407.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24430.9, "ram_available_mb": 38410.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.403}, "power_stats": {"power_gpu_soc_mean_watts": 23.986, "power_cpu_cv_mean_watts": 1.539, "power_sys_5v0_mean_watts": 9.209, "gpu_utilization_percent_mean": 76.403, "power_watts_avg": 23.986, "energy_joules_est": 216.58, "duration_seconds": 9.029, "sample_count": 77}, "timestamp": "2026-01-25T18:23:08.356641"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11145.414, "latencies_ms": [11145.414], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two zebras grazing in a grassy field. One zebra is located on the left side of the field, while the other is on the right side. They are both eating grass and appear to be enjoying their meal. The field is surrounded by a rock wall, providing a natural boundary for the zebras. The scene captures", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24430.9, "ram_available_mb": 38410.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24428.3, "ram_available_mb": 38412.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.135}, "power_stats": {"power_gpu_soc_mean_watts": 20.811, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.931, "gpu_utilization_percent_mean": 69.135, "power_watts_avg": 20.811, "energy_joules_est": 231.96, "duration_seconds": 11.146, "sample_count": 96}, "timestamp": "2026-01-25T18:23:21.570051"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7421.292, "latencies_ms": [7421.292], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "zebra: 2, rock: 1, tree: 1, grass: many, dirt path: 1, stone wall: 1, building: 1, shadow: 2", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24428.3, "ram_available_mb": 38412.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24424.7, "ram_available_mb": 38416.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.508}, "power_stats": {"power_gpu_soc_mean_watts": 22.64, "power_cpu_cv_mean_watts": 1.62, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 73.508, "power_watts_avg": 22.64, "energy_joules_est": 168.03, "duration_seconds": 7.422, "sample_count": 63}, "timestamp": "2026-01-25T18:23:31.036192"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10099.757, "latencies_ms": [10099.757], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground of the image, there are two zebras grazing on the grass. One zebra is positioned slightly to the left of the other, and both are facing the same direction. In the background, there is a rocky wall and a few trees, providing a natural habitat setting for the zebras.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24424.7, "ram_available_mb": 38416.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24431.1, "ram_available_mb": 38409.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.337}, "power_stats": {"power_gpu_soc_mean_watts": 21.252, "power_cpu_cv_mean_watts": 1.876, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 70.337, "power_watts_avg": 21.252, "energy_joules_est": 214.66, "duration_seconds": 10.1, "sample_count": 86}, "timestamp": "2026-01-25T18:23:43.149954"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6744.235, "latencies_ms": [6744.235], "images_per_second": 0.148, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "Two zebras are grazing in a grassy enclosure with a rocky wall and trees in the background. The sun is shining, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24431.1, "ram_available_mb": 38409.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24429.9, "ram_available_mb": 38411.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.754}, "power_stats": {"power_gpu_soc_mean_watts": 23.002, "power_cpu_cv_mean_watts": 1.559, "power_sys_5v0_mean_watts": 9.003, "gpu_utilization_percent_mean": 73.754, "power_watts_avg": 23.002, "energy_joules_est": 155.14, "duration_seconds": 6.745, "sample_count": 57}, "timestamp": "2026-01-25T18:23:51.908501"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7102.439, "latencies_ms": [7102.439], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image features two zebras grazing in a grassy field with a backdrop of trees and a rocky wall. The lighting is natural and bright, suggesting it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.9, "ram_available_mb": 38411.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24428.2, "ram_available_mb": 38412.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.667}, "power_stats": {"power_gpu_soc_mean_watts": 22.505, "power_cpu_cv_mean_watts": 1.648, "power_sys_5v0_mean_watts": 9.062, "gpu_utilization_percent_mean": 72.667, "power_watts_avg": 22.505, "energy_joules_est": 159.85, "duration_seconds": 7.103, "sample_count": 60}, "timestamp": "2026-01-25T18:24:01.041153"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11197.853, "latencies_ms": [11197.853], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of urban decay and neglect. Dominating the frame is a fire hydrant, its once vibrant orange color now faded to a dull brown, a testament to the passage of time. The hydrant, showing signs of rust and wear, stands on a sidewalk, its once gleaming surface now marred by the elements. \n\nA", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24428.2, "ram_available_mb": 38412.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24428.8, "ram_available_mb": 38412.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.25}, "power_stats": {"power_gpu_soc_mean_watts": 20.72, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.25, "power_watts_avg": 20.72, "energy_joules_est": 232.03, "duration_seconds": 11.198, "sample_count": 96}, "timestamp": "2026-01-25T18:24:14.299939"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7184.059, "latencies_ms": [7184.059], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "Fire hydrant: 1, chain: 2, cap: 1, step: 1, stone: 1, plant: 1, leaf: 1, flower: 1", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24428.8, "ram_available_mb": 38412.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24421.7, "ram_available_mb": 38419.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.41}, "power_stats": {"power_gpu_soc_mean_watts": 22.787, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 74.41, "power_watts_avg": 22.787, "energy_joules_est": 163.72, "duration_seconds": 7.185, "sample_count": 61}, "timestamp": "2026-01-25T18:24:23.512644"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10588.95, "latencies_ms": [10588.95], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The fire hydrant is positioned in the foreground of the image, appearing large and detailed. It is situated on the left side of the frame, with a blurred background that includes a stone wall and some greenery. The hydrant is also the main focus, with other elements like the chain and cap being secondary and placed on its right side.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24421.7, "ram_available_mb": 38419.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24425.8, "ram_available_mb": 38415.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.178}, "power_stats": {"power_gpu_soc_mean_watts": 21.132, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 70.178, "power_watts_avg": 21.132, "energy_joules_est": 223.78, "duration_seconds": 10.59, "sample_count": 90}, "timestamp": "2026-01-25T18:24:36.136748"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6076.083, "latencies_ms": [6076.083], "images_per_second": 0.165, "prompt_tokens": 37, "response_tokens_est": 32, "n_tiles": 16, "output_text": "The image shows an old, rusted fire hydrant on a sidewalk. It is located next to a wall with a painted green hedge design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.8, "ram_available_mb": 38415.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24417.5, "ram_available_mb": 38423.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.353}, "power_stats": {"power_gpu_soc_mean_watts": 23.702, "power_cpu_cv_mean_watts": 1.429, "power_sys_5v0_mean_watts": 9.05, "gpu_utilization_percent_mean": 74.353, "power_watts_avg": 23.702, "energy_joules_est": 144.03, "duration_seconds": 6.077, "sample_count": 51}, "timestamp": "2026-01-25T18:24:44.225271"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7920.189, "latencies_ms": [7920.189], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The fire hydrant is a faded orange color with a black top and is situated on a concrete step. It appears to be an older model, with visible rust and wear, indicating it has been in use for a considerable amount of time.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24417.5, "ram_available_mb": 38423.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24422.7, "ram_available_mb": 38418.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.224}, "power_stats": {"power_gpu_soc_mean_watts": 22.23, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 9.021, "gpu_utilization_percent_mean": 72.224, "power_watts_avg": 22.23, "energy_joules_est": 176.08, "duration_seconds": 7.921, "sample_count": 67}, "timestamp": "2026-01-25T18:24:54.193303"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11210.999, "latencies_ms": [11210.999], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two brown bears walking on a dirt road. The bear in the foreground is walking towards the camera, while the other bear is walking away from the camera. The road they are walking on is surrounded by dry grass and shrubs. The bears appear to be in a natural habitat, possibly a forest or a wildlife reserve. The image does not contain", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24422.7, "ram_available_mb": 38418.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24422.6, "ram_available_mb": 38418.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.74}, "power_stats": {"power_gpu_soc_mean_watts": 20.002, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.931, "gpu_utilization_percent_mean": 69.74, "power_watts_avg": 20.002, "energy_joules_est": 224.26, "duration_seconds": 11.212, "sample_count": 96}, "timestamp": "2026-01-25T18:25:07.437041"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7074.058, "latencies_ms": [7074.058], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "bear: 2, road: 1, vegetation: 1, ground: 1, sky: 1, sun: 1, shadow: 1, rocks: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24422.6, "ram_available_mb": 38418.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24446.4, "ram_available_mb": 38394.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.661}, "power_stats": {"power_gpu_soc_mean_watts": 22.868, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 72.661, "power_watts_avg": 22.868, "energy_joules_est": 161.78, "duration_seconds": 7.075, "sample_count": 59}, "timestamp": "2026-01-25T18:25:16.531263"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9997.684, "latencies_ms": [9997.684], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a large brown bear walking towards the right side of the image, while a smaller bear is seen in the background, slightly to the left and behind the first bear. The background consists of a barren landscape with dry grass and a dirt road, which appears to be quite far from the bears.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24446.4, "ram_available_mb": 38394.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24414.9, "ram_available_mb": 38426.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.105}, "power_stats": {"power_gpu_soc_mean_watts": 21.253, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 70.105, "power_watts_avg": 21.253, "energy_joules_est": 212.49, "duration_seconds": 9.998, "sample_count": 86}, "timestamp": "2026-01-25T18:25:28.585444"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6824.271, "latencies_ms": [6824.271], "images_per_second": 0.147, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "Two brown bears are walking on a rocky terrain with dry grass in the background. The bears appear to be in a natural habitat, possibly a forest or wilderness area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24414.9, "ram_available_mb": 38426.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24427.1, "ram_available_mb": 38413.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.912}, "power_stats": {"power_gpu_soc_mean_watts": 23.002, "power_cpu_cv_mean_watts": 1.545, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 73.912, "power_watts_avg": 23.002, "energy_joules_est": 156.99, "duration_seconds": 6.825, "sample_count": 57}, "timestamp": "2026-01-25T18:25:37.438581"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7202.506, "latencies_ms": [7202.506], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image features two brown bears in a natural setting with a clear sky. The bears are walking on a dirt ground with sparse vegetation, and the lighting suggests it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24427.1, "ram_available_mb": 38413.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24416.9, "ram_available_mb": 38424.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.468}, "power_stats": {"power_gpu_soc_mean_watts": 22.441, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 9.058, "gpu_utilization_percent_mean": 72.468, "power_watts_avg": 22.441, "energy_joules_est": 161.65, "duration_seconds": 7.203, "sample_count": 62}, "timestamp": "2026-01-25T18:25:46.698724"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11137.558, "latencies_ms": [11137.558], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young child is playing in a garden. The child is wearing a white shirt and a colorful tie, and is squatting down to interact with the ground. The child is holding a shovel and digging into a large metal tub filled with dirt. The child appears to be enjoying the outdoor activity, possibly exploring or engaging in a", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24416.9, "ram_available_mb": 38424.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24447.0, "ram_available_mb": 38393.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.135}, "power_stats": {"power_gpu_soc_mean_watts": 20.796, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 69.135, "power_watts_avg": 20.796, "energy_joules_est": 231.63, "duration_seconds": 11.138, "sample_count": 96}, "timestamp": "2026-01-25T18:25:59.863981"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7177.738, "latencies_ms": [7177.738], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "bucket: 1, shovel: 1, child: 1, tie: 1, leaves: many, ground: 1, sunlight: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24447.0, "ram_available_mb": 38393.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.639}, "power_stats": {"power_gpu_soc_mean_watts": 22.881, "power_cpu_cv_mean_watts": 1.621, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 73.639, "power_watts_avg": 22.881, "energy_joules_est": 164.25, "duration_seconds": 7.178, "sample_count": 61}, "timestamp": "2026-01-25T18:26:09.090214"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11164.782, "latencies_ms": [11164.782], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a young child is kneeling on the ground, reaching into a metal tub filled with dark material, likely soil or sand. The child is positioned to the left of the image, with the tub in the lower left quadrant. In the background, there is a dense arrangement of dark leaves, possibly a hedge or shrubbery, which occupies the upper", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24441.4, "ram_available_mb": 38399.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.281}, "power_stats": {"power_gpu_soc_mean_watts": 20.986, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 70.281, "power_watts_avg": 20.986, "energy_joules_est": 234.32, "duration_seconds": 11.165, "sample_count": 96}, "timestamp": "2026-01-25T18:26:22.296359"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8322.297, "latencies_ms": [8322.297], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A young child is playing in a large metal tub filled with black sand, surrounded by green foliage. The child is wearing a white shirt and a colorful tie, and appears to be enjoying the sensory experience of the sand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24441.4, "ram_available_mb": 38399.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24444.7, "ram_available_mb": 38396.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.141}, "power_stats": {"power_gpu_soc_mean_watts": 22.092, "power_cpu_cv_mean_watts": 1.714, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 71.141, "power_watts_avg": 22.092, "energy_joules_est": 183.87, "duration_seconds": 8.323, "sample_count": 71}, "timestamp": "2026-01-25T18:26:32.662014"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8674.589, "latencies_ms": [8674.589], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image features a child with light-colored hair, wearing a white shirt and a colorful tie with a pattern of leaves and the letter \"B\". The child is crouched down and appears to be playing with a black substance in a metal tub.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24444.7, "ram_available_mb": 38396.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24442.5, "ram_available_mb": 38398.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.959}, "power_stats": {"power_gpu_soc_mean_watts": 21.835, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 70.959, "power_watts_avg": 21.835, "energy_joules_est": 189.42, "duration_seconds": 8.675, "sample_count": 74}, "timestamp": "2026-01-25T18:26:43.351515"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11111.623, "latencies_ms": [11111.623], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a desert landscape, where a small table is set up in the middle of the sandy terrain. The table is adorned with various items, including a bottle of beer, a can of soda, and a bouquet of yellow flowers. A teddy bear, dressed in a red shirt and a blue hat, is seated on the", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24442.5, "ram_available_mb": 38398.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24426.0, "ram_available_mb": 38414.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.442}, "power_stats": {"power_gpu_soc_mean_watts": 20.917, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.442, "power_watts_avg": 20.917, "energy_joules_est": 232.44, "duration_seconds": 11.112, "sample_count": 95}, "timestamp": "2026-01-25T18:26:56.485895"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11078.284, "latencies_ms": [11078.284], "images_per_second": 0.09, "prompt_tokens": 39, "response_tokens_est": 77, "n_tiles": 16, "output_text": "object: teddy bear, count: 2\nobject: bottle, count: 2\nobject: can, count: 1\nobject: cardboard box, count: 1\nobject: table, count: 1\nobject: chair, count: 1\nobject: bag, count: 1\nobject: flower, count: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24426.0, "ram_available_mb": 38414.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.719}, "power_stats": {"power_gpu_soc_mean_watts": 21.084, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 70.719, "power_watts_avg": 21.084, "energy_joules_est": 233.59, "duration_seconds": 11.079, "sample_count": 96}, "timestamp": "2026-01-25T18:27:09.622285"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10217.427, "latencies_ms": [10217.427], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "In the foreground, there is a table with various items on it, positioned slightly to the right of the center of the image. The background features a vast, open desert landscape that stretches out to the horizon. The table and its contents are in the middle ground, creating a sense of depth against the expansive backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24419.7, "ram_available_mb": 38421.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.379}, "power_stats": {"power_gpu_soc_mean_watts": 21.201, "power_cpu_cv_mean_watts": 1.877, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 70.379, "power_watts_avg": 21.201, "energy_joules_est": 216.63, "duration_seconds": 10.218, "sample_count": 87}, "timestamp": "2026-01-25T18:27:21.872801"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9930.824, "latencies_ms": [9930.824], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image depicts a desert-like setting with a small table holding various items, including a bottle of beer and a can of soda. The table is surrounded by a few stuffed animals and a couch with a red cross on it, suggesting a makeshift outdoor gathering or picnic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.7, "ram_available_mb": 38421.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24425.4, "ram_available_mb": 38415.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.894}, "power_stats": {"power_gpu_soc_mean_watts": 21.514, "power_cpu_cv_mean_watts": 1.823, "power_sys_5v0_mean_watts": 8.93, "gpu_utilization_percent_mean": 71.894, "power_watts_avg": 21.514, "energy_joules_est": 213.67, "duration_seconds": 9.931, "sample_count": 85}, "timestamp": "2026-01-25T18:27:33.847588"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8228.436, "latencies_ms": [8228.436], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image depicts a desert landscape with a table set up in the middle, featuring a variety of items. The table is covered with a green cloth and has a red can, a bottle, and a small vase with yellow flowers on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.4, "ram_available_mb": 38415.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24429.1, "ram_available_mb": 38411.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.304}, "power_stats": {"power_gpu_soc_mean_watts": 21.972, "power_cpu_cv_mean_watts": 1.741, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 71.304, "power_watts_avg": 21.972, "energy_joules_est": 180.81, "duration_seconds": 8.229, "sample_count": 69}, "timestamp": "2026-01-25T18:27:44.086589"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12339.881, "latencies_ms": [12339.881], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a harbor with several boats docked at a pier. There are two boats prominently visible in the foreground, one on the left and another on the right. The boats are of different sizes and are moored to the pier. \n\nIn the background, there are more boats scattered across the water, with some closer to the shore and others further away. The", "error": null, "sys_before": {"cpu_percent": 10.3, "ram_used_mb": 24429.1, "ram_available_mb": 38411.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24444.0, "ram_available_mb": 38396.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.131}, "power_stats": {"power_gpu_soc_mean_watts": 22.725, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 9.17, "gpu_utilization_percent_mean": 74.131, "power_watts_avg": 22.725, "energy_joules_est": 280.44, "duration_seconds": 12.341, "sample_count": 107}, "timestamp": "2026-01-25T18:27:58.501334"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8950.044, "latencies_ms": [8950.044], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "boat: 2\nfish: 0\nperson: 2\nmountain: 1\nhouse: 0\nboat: 1\nlife preserver: 1\npaddle: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24444.0, "ram_available_mb": 38396.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24451.5, "ram_available_mb": 38389.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.697}, "power_stats": {"power_gpu_soc_mean_watts": 24.186, "power_cpu_cv_mean_watts": 1.47, "power_sys_5v0_mean_watts": 9.163, "gpu_utilization_percent_mean": 77.697, "power_watts_avg": 24.186, "energy_joules_est": 216.48, "duration_seconds": 8.951, "sample_count": 76}, "timestamp": "2026-01-25T18:28:09.479645"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11953.27, "latencies_ms": [11953.27], "images_per_second": 0.084, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there is a green boat with the number 9 on it, docked at a pier. To the left of this boat, there is another boat with a yellow structure on top, and further back, there are more boats in the water. In the background, there are hills covered in greenery and a few people standing on the pier.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24451.5, "ram_available_mb": 38389.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24473.9, "ram_available_mb": 38367.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.534}, "power_stats": {"power_gpu_soc_mean_watts": 22.986, "power_cpu_cv_mean_watts": 1.749, "power_sys_5v0_mean_watts": 9.201, "gpu_utilization_percent_mean": 74.534, "power_watts_avg": 22.986, "energy_joules_est": 274.77, "duration_seconds": 11.954, "sample_count": 103}, "timestamp": "2026-01-25T18:28:23.473056"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9743.527, "latencies_ms": [9743.527], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image depicts a serene harbor scene with several boats docked at a pier. The boats are moored to the pier, and there are a few people visible on the pier, possibly attending to their boats or enjoying the view.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24473.9, "ram_available_mb": 38367.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24491.6, "ram_available_mb": 38349.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.878}, "power_stats": {"power_gpu_soc_mean_watts": 23.866, "power_cpu_cv_mean_watts": 1.548, "power_sys_5v0_mean_watts": 9.178, "gpu_utilization_percent_mean": 76.878, "power_watts_avg": 23.866, "energy_joules_est": 232.55, "duration_seconds": 9.744, "sample_count": 82}, "timestamp": "2026-01-25T18:28:35.228674"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9817.878, "latencies_ms": [9817.878], "images_per_second": 0.102, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image features a serene harbor scene with boats docked at a pier. The boats are primarily green and white, with some wooden structures and ropes visible. The weather appears to be overcast, with a cloudy sky casting a soft light over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24491.6, "ram_available_mb": 38349.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24574.7, "ram_available_mb": 38266.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.952}, "power_stats": {"power_gpu_soc_mean_watts": 23.684, "power_cpu_cv_mean_watts": 1.592, "power_sys_5v0_mean_watts": 9.248, "gpu_utilization_percent_mean": 75.952, "power_watts_avg": 23.684, "energy_joules_est": 232.54, "duration_seconds": 9.819, "sample_count": 83}, "timestamp": "2026-01-25T18:28:47.092298"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11138.332, "latencies_ms": [11138.332], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is captured in the act of taking a bite out of a hot dog. The individual is wearing a black jacket, and their face is partially visible, with their mouth open wide as they bite into the hot dog. The hot dog itself is golden brown and appears to be freshly cooked. The background of the image is blurred, but", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24492.8, "ram_available_mb": 38348.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24438.9, "ram_available_mb": 38402.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.337}, "power_stats": {"power_gpu_soc_mean_watts": 20.917, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 69.337, "power_watts_avg": 20.917, "energy_joules_est": 232.99, "duration_seconds": 11.139, "sample_count": 95}, "timestamp": "2026-01-25T18:29:00.262797"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7166.951, "latencies_ms": [7166.951], "images_per_second": 0.14, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "face: 1, mouth: 1, tongue: 1, chin: 1, nose: 1, ear: 1, eye: 1, cheek: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.9, "ram_available_mb": 38402.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24427.2, "ram_available_mb": 38413.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.183}, "power_stats": {"power_gpu_soc_mean_watts": 22.833, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 9.011, "gpu_utilization_percent_mean": 74.183, "power_watts_avg": 22.833, "energy_joules_est": 163.66, "duration_seconds": 7.168, "sample_count": 60}, "timestamp": "2026-01-25T18:29:09.472728"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8129.211, "latencies_ms": [8129.211], "images_per_second": 0.123, "prompt_tokens": 44, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The person is in the foreground, holding a hot dog with their right hand, which is near the front of the image. The background is blurred but appears to be an outdoor setting with lights that could suggest a street or public area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.2, "ram_available_mb": 38413.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24428.3, "ram_available_mb": 38412.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.968, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 9.009, "gpu_utilization_percent_mean": 71.0, "power_watts_avg": 21.968, "energy_joules_est": 178.6, "duration_seconds": 8.13, "sample_count": 70}, "timestamp": "2026-01-25T18:29:19.620209"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7690.412, "latencies_ms": [7690.412], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A person is seen holding a hot dog with their mouth wide open, as if they are about to take a bite. The background is blurred, but it appears to be an outdoor setting with some lights visible.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24428.3, "ram_available_mb": 38412.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24431.1, "ram_available_mb": 38409.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.182}, "power_stats": {"power_gpu_soc_mean_watts": 22.389, "power_cpu_cv_mean_watts": 1.686, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 72.182, "power_watts_avg": 22.389, "energy_joules_est": 172.19, "duration_seconds": 7.691, "sample_count": 66}, "timestamp": "2026-01-25T18:29:29.368849"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7623.783, "latencies_ms": [7623.783], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image features a person with short hair, wearing a dark jacket, holding a hot dog with a bite taken out of it. The lighting is dim with a warm tone, suggesting an evening or nighttime setting.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24431.1, "ram_available_mb": 38409.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24438.4, "ram_available_mb": 38402.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.531}, "power_stats": {"power_gpu_soc_mean_watts": 22.432, "power_cpu_cv_mean_watts": 1.72, "power_sys_5v0_mean_watts": 9.062, "gpu_utilization_percent_mean": 72.531, "power_watts_avg": 22.432, "energy_joules_est": 171.03, "duration_seconds": 7.624, "sample_count": 64}, "timestamp": "2026-01-25T18:29:39.028502"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11121.715, "latencies_ms": [11121.715], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man and a woman are standing side by side in a room. The man is dressed in a black suit with a white shirt and a black tie, and he is holding a martini glass in his right hand. The woman is wearing a gray dress and has blonde hair. They are both looking to the left, and the man is holding a black purse", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24438.4, "ram_available_mb": 38402.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 24444.3, "ram_available_mb": 38396.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.851}, "power_stats": {"power_gpu_soc_mean_watts": 20.901, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 69.851, "power_watts_avg": 20.901, "energy_joules_est": 232.47, "duration_seconds": 11.122, "sample_count": 94}, "timestamp": "2026-01-25T18:29:52.210039"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7294.859, "latencies_ms": [7294.859], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "woman: 1, man: 1, dress: 1, wine glass: 1, door: 1, wall: 1, curtain: 1, shelf: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24444.3, "ram_available_mb": 38396.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24447.3, "ram_available_mb": 38393.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.984}, "power_stats": {"power_gpu_soc_mean_watts": 22.747, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 72.984, "power_watts_avg": 22.747, "energy_joules_est": 165.95, "duration_seconds": 7.295, "sample_count": 61}, "timestamp": "2026-01-25T18:30:01.530696"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9044.225, "latencies_ms": [9044.225], "images_per_second": 0.111, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The man is standing to the right of the woman, slightly closer to the camera, creating a sense of depth in the image. The woman is positioned to the left of the man, and both are standing in the foreground with a blurred background that suggests an indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24447.3, "ram_available_mb": 38393.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24438.1, "ram_available_mb": 38402.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.949}, "power_stats": {"power_gpu_soc_mean_watts": 21.634, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 70.949, "power_watts_avg": 21.634, "energy_joules_est": 195.68, "duration_seconds": 9.045, "sample_count": 78}, "timestamp": "2026-01-25T18:30:12.591223"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7984.14, "latencies_ms": [7984.14], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A man and a woman are standing in a room, with the man holding a glass of wine. The woman is wearing a grey dress and the man is wearing a black suit with a white shirt and a maroon tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.1, "ram_available_mb": 38402.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.985}, "power_stats": {"power_gpu_soc_mean_watts": 22.32, "power_cpu_cv_mean_watts": 1.707, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 72.985, "power_watts_avg": 22.32, "energy_joules_est": 178.22, "duration_seconds": 7.985, "sample_count": 68}, "timestamp": "2026-01-25T18:30:22.625157"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7803.156, "latencies_ms": [7803.156], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a man and a woman in an indoor setting with soft, warm lighting. The man is wearing a dark suit with a white shirt and a dark tie, while the woman is dressed in a grey dress.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.925}, "power_stats": {"power_gpu_soc_mean_watts": 22.205, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 9.024, "gpu_utilization_percent_mean": 71.925, "power_watts_avg": 22.205, "energy_joules_est": 173.28, "duration_seconds": 7.804, "sample_count": 67}, "timestamp": "2026-01-25T18:30:32.481290"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12159.12, "latencies_ms": [12159.12], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a blue shelf with various objects placed on it. There are several cups and bottles of different sizes and shapes, some of which are placed on the top of the shelf. A spoon is also visible on the shelf, resting near the cups. \n\nIn addition to the shelf, there is a dining table in the scene, with", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24431.6, "ram_available_mb": 38409.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.019}, "power_stats": {"power_gpu_soc_mean_watts": 22.12, "power_cpu_cv_mean_watts": 1.798, "power_sys_5v0_mean_watts": 9.133, "gpu_utilization_percent_mean": 73.019, "power_watts_avg": 22.12, "energy_joules_est": 268.97, "duration_seconds": 12.16, "sample_count": 104}, "timestamp": "2026-01-25T18:30:46.677832"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7983.117, "latencies_ms": [7983.117], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "object: 1, object: 2, object: 3, object: 4, object: 5, object: 6, object: 7, object: 8", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24431.6, "ram_available_mb": 38409.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24417.5, "ram_available_mb": 38423.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.206}, "power_stats": {"power_gpu_soc_mean_watts": 24.26, "power_cpu_cv_mean_watts": 1.407, "power_sys_5v0_mean_watts": 9.125, "gpu_utilization_percent_mean": 78.206, "power_watts_avg": 24.26, "energy_joules_est": 193.68, "duration_seconds": 7.984, "sample_count": 68}, "timestamp": "2026-01-25T18:30:56.700031"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12050.368, "latencies_ms": [12050.368], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a blue shelf with various objects placed on it, including a silver teapot and a small blue box. Behind the shelf, there is a yellow chair and a green table with a wooden base. In the background, there is a white chair and a brown table with a green base. The objects are arranged in a way that creates a sense of", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24417.5, "ram_available_mb": 38423.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24418.5, "ram_available_mb": 38422.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.196}, "power_stats": {"power_gpu_soc_mean_watts": 22.705, "power_cpu_cv_mean_watts": 1.782, "power_sys_5v0_mean_watts": 9.174, "gpu_utilization_percent_mean": 73.196, "power_watts_avg": 22.705, "energy_joules_est": 273.62, "duration_seconds": 12.051, "sample_count": 102}, "timestamp": "2026-01-25T18:31:10.790525"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8106.436, "latencies_ms": [8106.436], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image shows a blue wooden cabinet with a green top, placed on a concrete floor. It has a few items on top, including a vase, a small box, and some cups.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.5, "ram_available_mb": 38422.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24421.4, "ram_available_mb": 38419.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.279}, "power_stats": {"power_gpu_soc_mean_watts": 24.255, "power_cpu_cv_mean_watts": 1.425, "power_sys_5v0_mean_watts": 9.119, "gpu_utilization_percent_mean": 77.279, "power_watts_avg": 24.255, "energy_joules_est": 196.64, "duration_seconds": 8.107, "sample_count": 68}, "timestamp": "2026-01-25T18:31:20.952265"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8707.917, "latencies_ms": [8707.917], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image shows a blue wooden cabinet with a green top, placed outdoors under natural lighting. There are various items on the cabinet, including a silver teapot, a yellow chair, and a basket with a red cloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.4, "ram_available_mb": 38419.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24431.8, "ram_available_mb": 38409.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.649}, "power_stats": {"power_gpu_soc_mean_watts": 23.591, "power_cpu_cv_mean_watts": 1.553, "power_sys_5v0_mean_watts": 9.155, "gpu_utilization_percent_mean": 75.649, "power_watts_avg": 23.591, "energy_joules_est": 205.44, "duration_seconds": 8.709, "sample_count": 74}, "timestamp": "2026-01-25T18:31:31.713118"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11127.374, "latencies_ms": [11127.374], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a white plate with several pieces of bread covered in cheese, placed on a dining table. The bread appears to be toasted, and the cheese is melted on top of it. The table also has a keyboard and a mouse, suggesting that the setting might be a workspace or a casual dining area. The focus of the image is on the che", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24431.8, "ram_available_mb": 38409.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24420.5, "ram_available_mb": 38420.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.66}, "power_stats": {"power_gpu_soc_mean_watts": 20.946, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.924, "gpu_utilization_percent_mean": 69.66, "power_watts_avg": 20.946, "energy_joules_est": 233.09, "duration_seconds": 11.128, "sample_count": 94}, "timestamp": "2026-01-25T18:31:44.882181"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7510.232, "latencies_ms": [7510.232], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "cheese: 5\ncracker: 5\nplate: 1\nkeyboard: 1\nmouse: 1\nbowl: 1\ndrink: 1\nbook: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.5, "ram_available_mb": 38420.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24419.9, "ram_available_mb": 38421.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.562}, "power_stats": {"power_gpu_soc_mean_watts": 22.549, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 73.562, "power_watts_avg": 22.549, "energy_joules_est": 169.36, "duration_seconds": 7.511, "sample_count": 64}, "timestamp": "2026-01-25T18:31:54.419288"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9248.608, "latencies_ms": [9248.608], "images_per_second": 0.108, "prompt_tokens": 44, "response_tokens_est": 62, "n_tiles": 16, "output_text": "In the foreground, there are four pieces of bread with cheese on top, arranged in a square pattern on a white plate. In the background, there is a blurred image of a keyboard and a mouse, suggesting that the bread is on a desk or table near a computer setup.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.9, "ram_available_mb": 38421.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24426.1, "ram_available_mb": 38414.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.321}, "power_stats": {"power_gpu_soc_mean_watts": 21.463, "power_cpu_cv_mean_watts": 1.817, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 70.321, "power_watts_avg": 21.463, "energy_joules_est": 198.52, "duration_seconds": 9.249, "sample_count": 78}, "timestamp": "2026-01-25T18:32:05.720254"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8248.283, "latencies_ms": [8248.283], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a white plate with four pieces of bread topped with cheese, placed on a desk. In the background, there is a keyboard and a mouse, suggesting that the setting is likely a workspace or a computer desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.1, "ram_available_mb": 38414.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24414.0, "ram_available_mb": 38426.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.286}, "power_stats": {"power_gpu_soc_mean_watts": 21.674, "power_cpu_cv_mean_watts": 1.704, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 73.286, "power_watts_avg": 21.674, "energy_joules_est": 178.79, "duration_seconds": 8.249, "sample_count": 70}, "timestamp": "2026-01-25T18:32:16.007580"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10005.443, "latencies_ms": [10005.443], "images_per_second": 0.1, "prompt_tokens": 36, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image shows a white plate with four pieces of food, each topped with a white, creamy substance that could be cheese or a sauce. The lighting in the image is warm and artificial, coming from a source that is not visible in the frame, casting a soft glow on the plate and its contents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24414.0, "ram_available_mb": 38426.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24421.2, "ram_available_mb": 38419.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.024}, "power_stats": {"power_gpu_soc_mean_watts": 21.209, "power_cpu_cv_mean_watts": 1.865, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 70.024, "power_watts_avg": 21.209, "energy_joules_est": 212.22, "duration_seconds": 10.006, "sample_count": 85}, "timestamp": "2026-01-25T18:32:28.036986"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12308.304, "latencies_ms": [12308.304], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing against a dark gray background. He is dressed in a black suit and tie, and he is wearing glasses. His hair is dark and styled in a messy manner. The man is adjusting his tie, which is adorned with a string of colorful lights. The lights on the tie are arranged in a vertical line, with each", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24421.2, "ram_available_mb": 38419.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.314}, "power_stats": {"power_gpu_soc_mean_watts": 22.8, "power_cpu_cv_mean_watts": 1.792, "power_sys_5v0_mean_watts": 9.2, "gpu_utilization_percent_mean": 74.314, "power_watts_avg": 22.8, "energy_joules_est": 280.64, "duration_seconds": 12.309, "sample_count": 105}, "timestamp": "2026-01-25T18:32:42.404832"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11404.87, "latencies_ms": [11404.87], "images_per_second": 0.088, "prompt_tokens": 39, "response_tokens_est": 68, "n_tiles": 16, "output_text": "- Man: 1\n\n- Glasses: 1\n\n- Tie: 1\n\n- Jacket: 1\n\n- Pocket: 1\n\n- Ring: 1\n\n- Beads on tie: 1\n\n- Colorful lights on tie: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24514.6, "ram_available_mb": 38326.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.357}, "power_stats": {"power_gpu_soc_mean_watts": 23.3, "power_cpu_cv_mean_watts": 1.679, "power_sys_5v0_mean_watts": 9.168, "gpu_utilization_percent_mean": 75.357, "power_watts_avg": 23.3, "energy_joules_est": 265.75, "duration_seconds": 11.405, "sample_count": 98}, "timestamp": "2026-01-25T18:32:55.866305"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12415.148, "latencies_ms": [12415.148], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The man is positioned in the foreground, standing against a dark background. He is wearing a black suit jacket and a white shirt, and is adjusting a red and green LED light strip on his tie, which is located in the mid-ground of the image. The lights on the tie are arranged in a vertical line, with the red lights at the top and the green", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24514.6, "ram_available_mb": 38326.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24516.4, "ram_available_mb": 38324.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.67}, "power_stats": {"power_gpu_soc_mean_watts": 22.871, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 9.221, "gpu_utilization_percent_mean": 73.67, "power_watts_avg": 22.871, "energy_joules_est": 283.96, "duration_seconds": 12.416, "sample_count": 106}, "timestamp": "2026-01-25T18:33:10.302496"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7707.044, "latencies_ms": [7707.044], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A man in a suit is adjusting a tie with a unique design of red and green lights. The background is dark, making the man and his tie stand out.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24444.5, "ram_available_mb": 38396.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24559.6, "ram_available_mb": 38281.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 80.662}, "power_stats": {"power_gpu_soc_mean_watts": 24.926, "power_cpu_cv_mean_watts": 1.324, "power_sys_5v0_mean_watts": 9.171, "gpu_utilization_percent_mean": 80.662, "power_watts_avg": 24.926, "energy_joules_est": 192.12, "duration_seconds": 7.708, "sample_count": 65}, "timestamp": "2026-01-25T18:33:20.065573"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11271.492, "latencies_ms": [11271.492], "images_per_second": 0.089, "prompt_tokens": 36, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image features a man in a dark suit with a white shirt and a tie that has a series of lights along its length, emitting a spectrum of colors from red to green. The lighting is dramatic, with a spotlight effect highlighting the man and the illuminated tie against a dark, shadowy background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24559.6, "ram_available_mb": 38281.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24593.3, "ram_available_mb": 38247.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.421}, "power_stats": {"power_gpu_soc_mean_watts": 23.142, "power_cpu_cv_mean_watts": 1.707, "power_sys_5v0_mean_watts": 9.215, "gpu_utilization_percent_mean": 74.421, "power_watts_avg": 23.142, "energy_joules_est": 260.86, "duration_seconds": 11.272, "sample_count": 95}, "timestamp": "2026-01-25T18:33:33.356103"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11108.365, "latencies_ms": [11108.365], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a woman crossing the street at a crosswalk during the evening. She is wearing a brown jacket and carrying a handbag. The scene is set in a city environment, with a traffic light visible in the background. There are several other people in the scene, some of whom are also carrying handbags.\n\nIn the background, there is a building with a", "error": null, "sys_before": {"cpu_percent": 8.6, "ram_used_mb": 24429.4, "ram_available_mb": 38411.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 24426.3, "ram_available_mb": 38414.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.457}, "power_stats": {"power_gpu_soc_mean_watts": 20.912, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 69.457, "power_watts_avg": 20.912, "energy_joules_est": 232.31, "duration_seconds": 11.109, "sample_count": 94}, "timestamp": "2026-01-25T18:33:46.500311"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7314.633, "latencies_ms": [7314.633], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "pedestrian: 1, traffic light: 1, building: 2, window: multiple, streetlight: 1, sign: 1, restaurant: 1, car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.3, "ram_available_mb": 38414.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24442.7, "ram_available_mb": 38398.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.226}, "power_stats": {"power_gpu_soc_mean_watts": 22.714, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 74.226, "power_watts_avg": 22.714, "energy_joules_est": 166.16, "duration_seconds": 7.315, "sample_count": 62}, "timestamp": "2026-01-25T18:33:55.839327"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11162.633, "latencies_ms": [11162.633], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person walking across the street, positioned near the center of the image. The background features a large, ornate building with a sign that reads \"TADURIA,\" which is situated on the right side of the image. The sky is visible in the upper part of the image, indicating that the scene is set outdoors during the evening or night", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.7, "ram_available_mb": 38398.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24420.7, "ram_available_mb": 38420.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.072}, "power_stats": {"power_gpu_soc_mean_watts": 21.008, "power_cpu_cv_mean_watts": 1.915, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 70.072, "power_watts_avg": 21.008, "energy_joules_est": 234.52, "duration_seconds": 11.163, "sample_count": 97}, "timestamp": "2026-01-25T18:34:09.020788"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8968.364, "latencies_ms": [8968.364], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image depicts a street scene at night with a person crossing the road. The person is wearing a brown jacket and jeans, and is carrying a black bag. The street is illuminated by streetlights and the buildings in the background have lit windows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.7, "ram_available_mb": 38420.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24444.6, "ram_available_mb": 38396.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.684}, "power_stats": {"power_gpu_soc_mean_watts": 21.844, "power_cpu_cv_mean_watts": 1.77, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 72.684, "power_watts_avg": 21.844, "energy_joules_est": 195.92, "duration_seconds": 8.969, "sample_count": 76}, "timestamp": "2026-01-25T18:34:20.050829"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7461.833, "latencies_ms": [7461.833], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image depicts a night scene with artificial lighting, including street lamps and building lights, casting a warm glow on the scene. The sky is dark blue, indicating it is likely evening or night time.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24444.6, "ram_available_mb": 38396.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24418.4, "ram_available_mb": 38422.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.683}, "power_stats": {"power_gpu_soc_mean_watts": 22.506, "power_cpu_cv_mean_watts": 1.671, "power_sys_5v0_mean_watts": 9.041, "gpu_utilization_percent_mean": 72.683, "power_watts_avg": 22.506, "energy_joules_est": 167.95, "duration_seconds": 7.463, "sample_count": 63}, "timestamp": "2026-01-25T18:34:29.528583"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11110.617, "latencies_ms": [11110.617], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a woman in a bikini riding a surfboard on a wave in the ocean. She is skillfully balancing on the surfboard as the wave carries her. There are several other people in the water, some of them also on surfboards, while others are swimming or floating. The scene captures the excitement and enjoyment of surfing", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24418.4, "ram_available_mb": 38422.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24424.3, "ram_available_mb": 38416.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.809}, "power_stats": {"power_gpu_soc_mean_watts": 20.958, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 69.809, "power_watts_avg": 20.958, "energy_joules_est": 232.87, "duration_seconds": 11.111, "sample_count": 94}, "timestamp": "2026-01-25T18:34:42.666091"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7399.678, "latencies_ms": [7399.678], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "girl: 1, surfboard: 1, wave: 1, water: 1, person: 3, swimsuit: 2, arm: 1, leg: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24424.3, "ram_available_mb": 38416.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24420.8, "ram_available_mb": 38420.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.635}, "power_stats": {"power_gpu_soc_mean_watts": 22.551, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 73.635, "power_watts_avg": 22.551, "energy_joules_est": 166.88, "duration_seconds": 7.4, "sample_count": 63}, "timestamp": "2026-01-25T18:34:52.109542"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11099.198, "latencies_ms": [11099.198], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is surfing on a blue surfboard, riding a wave towards the right side of the image. In the background, there are two other individuals; one is lying on a surfboard further out at sea, and the other is standing on a surfboard closer to the shore, holding a blue surfboard. The main surfer is", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.8, "ram_available_mb": 38420.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24417.7, "ram_available_mb": 38423.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.766}, "power_stats": {"power_gpu_soc_mean_watts": 20.983, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 69.766, "power_watts_avg": 20.983, "energy_joules_est": 232.91, "duration_seconds": 11.1, "sample_count": 94}, "timestamp": "2026-01-25T18:35:05.223381"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8754.769, "latencies_ms": [8754.769], "images_per_second": 0.114, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "A young girl is surfing on a wave in the ocean, with two other people in the background, one of whom is holding a surfboard. The girl is wearing a bikini and appears to be enjoying herself as she rides the wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24417.7, "ram_available_mb": 38423.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24426.3, "ram_available_mb": 38414.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.676}, "power_stats": {"power_gpu_soc_mean_watts": 21.95, "power_cpu_cv_mean_watts": 1.742, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 72.676, "power_watts_avg": 21.95, "energy_joules_est": 192.18, "duration_seconds": 8.755, "sample_count": 74}, "timestamp": "2026-01-25T18:35:16.013051"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7192.396, "latencies_ms": [7192.396], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image captures a vibrant scene at the beach with a person surfing on a blue surfboard. The water is a mix of light blue and green hues, indicating clear weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.3, "ram_available_mb": 38414.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24418.6, "ram_available_mb": 38422.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.18}, "power_stats": {"power_gpu_soc_mean_watts": 22.502, "power_cpu_cv_mean_watts": 1.647, "power_sys_5v0_mean_watts": 9.094, "gpu_utilization_percent_mean": 72.18, "power_watts_avg": 22.502, "energy_joules_est": 161.86, "duration_seconds": 7.193, "sample_count": 61}, "timestamp": "2026-01-25T18:35:25.264419"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11144.704, "latencies_ms": [11144.704], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing in a fenced area, reaching out to pet an elephant. The elephant is standing on a concrete platform, and the man is wearing a white shirt and a black belt. There are two other people in the background, one of whom is wearing a hat. The scene takes place in a lush green environment, with", "error": null, "sys_before": {"cpu_percent": 11.5, "ram_used_mb": 24418.6, "ram_available_mb": 38422.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24424.0, "ram_available_mb": 38416.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.484}, "power_stats": {"power_gpu_soc_mean_watts": 20.927, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 69.484, "power_watts_avg": 20.927, "energy_joules_est": 233.24, "duration_seconds": 11.145, "sample_count": 95}, "timestamp": "2026-01-25T18:35:38.435430"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7526.817, "latencies_ms": [7526.817], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "elephant: 1, fence: 2, hand: 1, trunk: 1, watch: 1, shirt: 1, belt: 1, trees: multiple", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24424.0, "ram_available_mb": 38416.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24415.5, "ram_available_mb": 38425.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.094}, "power_stats": {"power_gpu_soc_mean_watts": 22.571, "power_cpu_cv_mean_watts": 1.651, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 73.094, "power_watts_avg": 22.571, "energy_joules_est": 169.9, "duration_seconds": 7.527, "sample_count": 64}, "timestamp": "2026-01-25T18:35:48.018683"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11109.661, "latencies_ms": [11109.661], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is standing to the right of a fence, reaching out to touch an elephant on the left side of the image. The elephant is positioned behind the fence, in the middle ground of the image, and is facing the person. The background is filled with greenery and another person is visible behind the fence, slightly to the right", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24415.5, "ram_available_mb": 38425.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24420.4, "ram_available_mb": 38420.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.628}, "power_stats": {"power_gpu_soc_mean_watts": 20.945, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 69.628, "power_watts_avg": 20.945, "energy_joules_est": 232.71, "duration_seconds": 11.11, "sample_count": 94}, "timestamp": "2026-01-25T18:36:01.170011"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9622.644, "latencies_ms": [9622.644], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "In the image, a person is seen feeding an elephant with a stick, while another person watches from behind a fence. The scene takes place in a zoo or wildlife sanctuary, where the elephant is standing on a concrete platform and the person is interacting with it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.4, "ram_available_mb": 38420.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.061}, "power_stats": {"power_gpu_soc_mean_watts": 21.572, "power_cpu_cv_mean_watts": 1.792, "power_sys_5v0_mean_watts": 8.931, "gpu_utilization_percent_mean": 72.061, "power_watts_avg": 21.572, "energy_joules_est": 207.59, "duration_seconds": 9.623, "sample_count": 82}, "timestamp": "2026-01-25T18:36:12.822992"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8997.909, "latencies_ms": [8997.909], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image shows a scene with an elephant and a human interacting. The elephant is grey, and the human is wearing a white shirt and dark pants. The lighting appears to be natural daylight, and the weather seems to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24420.3, "ram_available_mb": 38420.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24417.2, "ram_available_mb": 38423.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.816}, "power_stats": {"power_gpu_soc_mean_watts": 21.606, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 70.816, "power_watts_avg": 21.606, "energy_joules_est": 194.42, "duration_seconds": 8.999, "sample_count": 76}, "timestamp": "2026-01-25T18:36:23.878220"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11150.214, "latencies_ms": [11150.214], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a brown dog with a white chest is sitting on a bed, surrounded by a mess of clothes and various items. The dog appears to be looking directly at the camera, giving the impression that it is posing for the photo. The bed is covered with clothes, and there are multiple items scattered around, including a pillow and a jacket. The scene", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24417.2, "ram_available_mb": 38423.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24417.7, "ram_available_mb": 38423.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.221}, "power_stats": {"power_gpu_soc_mean_watts": 20.881, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.221, "power_watts_avg": 20.881, "energy_joules_est": 232.84, "duration_seconds": 11.151, "sample_count": 95}, "timestamp": "2026-01-25T18:36:37.070674"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9022.916, "latencies_ms": [9022.916], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 58, "n_tiles": 16, "output_text": "- Bed: 1\n- Blanket: 1\n- Pillow: 1\n- Pillowcase: 1\n- Clothes: 1\n- Bags: 1\n- Box: 1\n- Dog: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24417.7, "ram_available_mb": 38423.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24420.1, "ram_available_mb": 38420.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.026}, "power_stats": {"power_gpu_soc_mean_watts": 21.557, "power_cpu_cv_mean_watts": 1.757, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 71.026, "power_watts_avg": 21.557, "energy_joules_est": 194.52, "duration_seconds": 9.024, "sample_count": 77}, "timestamp": "2026-01-25T18:36:48.131985"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7335.159, "latencies_ms": [7335.159], "images_per_second": 0.136, "prompt_tokens": 44, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The dog is sitting in the foreground on the left side of the image, near the center. The bed is in the background, with various items scattered on it, including a brown pillow on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.1, "ram_available_mb": 38420.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24416.9, "ram_available_mb": 38424.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.19}, "power_stats": {"power_gpu_soc_mean_watts": 22.378, "power_cpu_cv_mean_watts": 1.671, "power_sys_5v0_mean_watts": 9.047, "gpu_utilization_percent_mean": 72.19, "power_watts_avg": 22.378, "energy_joules_est": 164.16, "duration_seconds": 7.336, "sample_count": 63}, "timestamp": "2026-01-25T18:36:57.491991"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6063.164, "latencies_ms": [6063.164], "images_per_second": 0.165, "prompt_tokens": 37, "response_tokens_est": 32, "n_tiles": 16, "output_text": "A dog is sitting on a bed surrounded by a pile of clothes and a pillow. The room appears to be messy and disorganized.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24416.9, "ram_available_mb": 38424.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.961}, "power_stats": {"power_gpu_soc_mean_watts": 23.538, "power_cpu_cv_mean_watts": 1.476, "power_sys_5v0_mean_watts": 9.04, "gpu_utilization_percent_mean": 75.961, "power_watts_avg": 23.538, "energy_joules_est": 142.73, "duration_seconds": 6.064, "sample_count": 51}, "timestamp": "2026-01-25T18:37:05.579016"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11095.82, "latencies_ms": [11095.82], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dog sitting amidst a pile of clothes and bags on a bed. The dog appears to be a large breed with a dark coat and white markings on its face. The bed is covered with a white sheet, and the background shows a window with white curtains. The lighting in the room is natural, coming from the window. The clothes and b", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24420.2, "ram_available_mb": 38420.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24424.2, "ram_available_mb": 38416.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.284}, "power_stats": {"power_gpu_soc_mean_watts": 20.946, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 69.284, "power_watts_avg": 20.946, "energy_joules_est": 232.42, "duration_seconds": 11.096, "sample_count": 95}, "timestamp": "2026-01-25T18:37:18.686730"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11116.636, "latencies_ms": [11116.636], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seated at a desk in an office environment. He is dressed in a white shirt and a blue tie, and he is holding a pen in his right hand, poised to write on a piece of paper that is placed on the desk. His left hand is raised to his chin, suggesting a moment of contemplation or deep thought.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24424.2, "ram_available_mb": 38416.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24425.0, "ram_available_mb": 38415.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.468}, "power_stats": {"power_gpu_soc_mean_watts": 20.907, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 69.468, "power_watts_avg": 20.907, "energy_joules_est": 232.43, "duration_seconds": 11.117, "sample_count": 94}, "timestamp": "2026-01-25T18:37:31.830443"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7257.008, "latencies_ms": [7257.008], "images_per_second": 0.138, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "laptop: 1, pen: 1, paper: 1, clipboard: 1, glasses: 1, shirt: 1, tie: 1, background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.0, "ram_available_mb": 38415.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.484}, "power_stats": {"power_gpu_soc_mean_watts": 22.824, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 9.038, "gpu_utilization_percent_mean": 72.484, "power_watts_avg": 22.824, "energy_joules_est": 165.65, "duration_seconds": 7.258, "sample_count": 62}, "timestamp": "2026-01-25T18:37:41.134538"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10768.972, "latencies_ms": [10768.972], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The laptop is positioned to the left of the image, placed on a desk in the foreground. A person is seated at the desk, with their right hand resting on their chin, suggesting a thoughtful or contemplative pose. The background is blurred, indicating that the focus is on the person and the immediate surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24419.0, "ram_available_mb": 38421.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24423.1, "ram_available_mb": 38417.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.207}, "power_stats": {"power_gpu_soc_mean_watts": 21.062, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 70.207, "power_watts_avg": 21.062, "energy_joules_est": 226.83, "duration_seconds": 10.77, "sample_count": 92}, "timestamp": "2026-01-25T18:37:53.919223"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7705.287, "latencies_ms": [7705.287], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A man in a white shirt and blue tie is sitting at a desk with a laptop and papers in front of him, looking thoughtful. He is holding a pen in his hand and appears to be deep in thought.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.1, "ram_available_mb": 38417.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24418.4, "ram_available_mb": 38422.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.138}, "power_stats": {"power_gpu_soc_mean_watts": 22.484, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 9.015, "gpu_utilization_percent_mean": 73.138, "power_watts_avg": 22.484, "energy_joules_est": 173.26, "duration_seconds": 7.706, "sample_count": 65}, "timestamp": "2026-01-25T18:38:03.677755"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8196.15, "latencies_ms": [8196.15], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a person wearing a white shirt and a blue tie, sitting at a desk with a laptop open in front of them. The desk appears to be made of wood, and there is a pen in the person's hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24418.4, "ram_available_mb": 38422.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24420.7, "ram_available_mb": 38420.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.314}, "power_stats": {"power_gpu_soc_mean_watts": 21.944, "power_cpu_cv_mean_watts": 1.75, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 71.314, "power_watts_avg": 21.944, "energy_joules_est": 179.87, "duration_seconds": 8.197, "sample_count": 70}, "timestamp": "2026-01-25T18:38:13.887157"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12343.944, "latencies_ms": [12343.944], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a clear blue sky, where a large, full moon is visible in the lower left corner, casting a soft glow. A commercial airplane, painted in white with a red and blue tail, is seen flying from the right to the left of the frame. The airplane's wings are fully extended, indicating it is in the process of taking", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24420.7, "ram_available_mb": 38420.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.208}, "power_stats": {"power_gpu_soc_mean_watts": 22.833, "power_cpu_cv_mean_watts": 1.79, "power_sys_5v0_mean_watts": 9.186, "gpu_utilization_percent_mean": 74.208, "power_watts_avg": 22.833, "energy_joules_est": 281.86, "duration_seconds": 12.345, "sample_count": 106}, "timestamp": "2026-01-25T18:38:28.277987"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8706.26, "latencies_ms": [8706.26], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "airplane: 1, moon: 1, sky: 1, day: 1, flight: 1, aircraft: 1, celestial body: 1, horizon: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24424.6, "ram_available_mb": 38416.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24426.7, "ram_available_mb": 38414.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.986}, "power_stats": {"power_gpu_soc_mean_watts": 24.309, "power_cpu_cv_mean_watts": 1.444, "power_sys_5v0_mean_watts": 9.183, "gpu_utilization_percent_mean": 78.986, "power_watts_avg": 24.309, "energy_joules_est": 211.66, "duration_seconds": 8.707, "sample_count": 74}, "timestamp": "2026-01-25T18:38:39.010788"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10491.029, "latencies_ms": [10491.029], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The airplane is flying in the background, higher up in the sky compared to the moon, which is closer to the viewer in the foreground. The moon appears to be partially obscured by the airplane's shadow, indicating that the airplane is between the moon and the viewer.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24426.7, "ram_available_mb": 38414.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24421.0, "ram_available_mb": 38419.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.427}, "power_stats": {"power_gpu_soc_mean_watts": 23.382, "power_cpu_cv_mean_watts": 1.669, "power_sys_5v0_mean_watts": 9.225, "gpu_utilization_percent_mean": 75.427, "power_watts_avg": 23.382, "energy_joules_est": 245.32, "duration_seconds": 10.492, "sample_count": 89}, "timestamp": "2026-01-25T18:38:51.558784"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9849.268, "latencies_ms": [9849.268], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image captures a serene scene of a clear blue sky with a large, pale orange moon visible in the bottom left corner. A commercial airplane with a red and white tail is seen flying from left to right in the upper right corner of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24421.0, "ram_available_mb": 38419.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24423.7, "ram_available_mb": 38417.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.217}, "power_stats": {"power_gpu_soc_mean_watts": 23.86, "power_cpu_cv_mean_watts": 1.538, "power_sys_5v0_mean_watts": 9.149, "gpu_utilization_percent_mean": 77.217, "power_watts_avg": 23.86, "energy_joules_est": 235.03, "duration_seconds": 9.85, "sample_count": 83}, "timestamp": "2026-01-25T18:39:03.420904"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9809.127, "latencies_ms": [9809.127], "images_per_second": 0.102, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image features a clear blue sky with a large, pale orange moon visible in the lower left corner. An airplane with a predominantly white body and blue tail is captured in flight, with its landing gear extended, suggesting it is either taking off or landing.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24423.7, "ram_available_mb": 38417.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24422.7, "ram_available_mb": 38418.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.94}, "power_stats": {"power_gpu_soc_mean_watts": 23.62, "power_cpu_cv_mean_watts": 1.611, "power_sys_5v0_mean_watts": 9.207, "gpu_utilization_percent_mean": 75.94, "power_watts_avg": 23.62, "energy_joules_est": 231.71, "duration_seconds": 9.81, "sample_count": 84}, "timestamp": "2026-01-25T18:39:15.267629"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12324.976, "latencies_ms": [12324.976], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is captured in the midst of performing a skateboard trick at a skate park. He is wearing a tie-dye shirt and black pants, and his skateboard is adorned with a vibrant design. The skate park itself is a concrete ramp, and the background is filled with palm trees and a play", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24422.7, "ram_available_mb": 38418.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24419.5, "ram_available_mb": 38421.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.314}, "power_stats": {"power_gpu_soc_mean_watts": 22.862, "power_cpu_cv_mean_watts": 1.784, "power_sys_5v0_mean_watts": 9.196, "gpu_utilization_percent_mean": 74.314, "power_watts_avg": 22.862, "energy_joules_est": 281.79, "duration_seconds": 12.326, "sample_count": 105}, "timestamp": "2026-01-25T18:39:29.662529"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8939.919, "latencies_ms": [8939.919], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "palm tree: 3, skateboard: 1, person: 1, earphones: 1, building: 1, tree: 2, slide: 1, wall: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24419.5, "ram_available_mb": 38421.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24428.0, "ram_available_mb": 38412.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.171}, "power_stats": {"power_gpu_soc_mean_watts": 24.221, "power_cpu_cv_mean_watts": 1.459, "power_sys_5v0_mean_watts": 9.171, "gpu_utilization_percent_mean": 78.171, "power_watts_avg": 24.221, "energy_joules_est": 216.55, "duration_seconds": 8.941, "sample_count": 76}, "timestamp": "2026-01-25T18:39:40.618157"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12080.05, "latencies_ms": [12080.05], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, performing a trick on a ramp. In the background, there are palm trees and a playground structure, indicating the skate park is located in a park-like setting. The skateboarder is near the edge of the ramp, suggesting they are in the process of executing a jump or trick.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24428.0, "ram_available_mb": 38412.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24427.9, "ram_available_mb": 38413.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.81}, "power_stats": {"power_gpu_soc_mean_watts": 22.92, "power_cpu_cv_mean_watts": 1.761, "power_sys_5v0_mean_watts": 9.225, "gpu_utilization_percent_mean": 74.81, "power_watts_avg": 22.92, "energy_joules_est": 276.89, "duration_seconds": 12.081, "sample_count": 105}, "timestamp": "2026-01-25T18:39:54.755421"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8268.07, "latencies_ms": [8268.07], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A person is performing a skateboard trick on a ramp at a skate park. The skateboarder is wearing a tie-dye shirt and black pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.9, "ram_available_mb": 38413.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24426.4, "ram_available_mb": 38414.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.254}, "power_stats": {"power_gpu_soc_mean_watts": 24.55, "power_cpu_cv_mean_watts": 1.398, "power_sys_5v0_mean_watts": 9.191, "gpu_utilization_percent_mean": 79.254, "power_watts_avg": 24.55, "energy_joules_est": 203.0, "duration_seconds": 8.269, "sample_count": 71}, "timestamp": "2026-01-25T18:40:05.083172"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9231.332, "latencies_ms": [9231.332], "images_per_second": 0.108, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The skateboarder is wearing a tie-dye shirt with a mix of purple, blue, and white colors. The lighting is natural, suggesting it is daytime, and the weather appears to be partly cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.4, "ram_available_mb": 38414.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.975}, "power_stats": {"power_gpu_soc_mean_watts": 23.878, "power_cpu_cv_mean_watts": 1.571, "power_sys_5v0_mean_watts": 9.256, "gpu_utilization_percent_mean": 75.975, "power_watts_avg": 23.878, "energy_joules_est": 220.44, "duration_seconds": 9.232, "sample_count": 79}, "timestamp": "2026-01-25T18:40:16.364243"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11105.615, "latencies_ms": [11105.615], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a sheep with a thick, shaggy coat of wool, standing behind a wire fence. The sheep is facing the camera, and its expression is calm and attentive. The fence is made of metal wires, and the sheep is positioned behind it, with its body partially obscured by the fence. The background of the image features", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24433.5, "ram_available_mb": 38407.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.723}, "power_stats": {"power_gpu_soc_mean_watts": 20.961, "power_cpu_cv_mean_watts": 1.925, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 69.723, "power_watts_avg": 20.961, "energy_joules_est": 232.8, "duration_seconds": 11.106, "sample_count": 94}, "timestamp": "2026-01-25T18:40:29.505979"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7522.392, "latencies_ms": [7522.392], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "wire: 10, sheep: 1, wool: 1, grass: 1, trees: 10, sky: 1, rocks: 1, fence post: 2", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24433.5, "ram_available_mb": 38407.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.905}, "power_stats": {"power_gpu_soc_mean_watts": 22.663, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 73.905, "power_watts_avg": 22.663, "energy_joules_est": 170.49, "duration_seconds": 7.523, "sample_count": 63}, "timestamp": "2026-01-25T18:40:39.048221"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7108.383, "latencies_ms": [7108.383], "images_per_second": 0.141, "prompt_tokens": 44, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The sheep is positioned in the foreground of the image, behind a wire fence that is in the middle ground. The background consists of a lush green field with trees and a clear sky above.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.1, "ram_available_mb": 38418.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24432.4, "ram_available_mb": 38408.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.65}, "power_stats": {"power_gpu_soc_mean_watts": 22.715, "power_cpu_cv_mean_watts": 1.634, "power_sys_5v0_mean_watts": 9.044, "gpu_utilization_percent_mean": 72.65, "power_watts_avg": 22.715, "energy_joules_est": 161.48, "duration_seconds": 7.109, "sample_count": 60}, "timestamp": "2026-01-25T18:40:48.215578"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7829.222, "latencies_ms": [7829.222], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A sheep is lying down in a wire fenced area with a lush green background of trees and grass. The sheep appears to be resting or possibly sleeping, with its woolly body partially covered by the fence.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24432.4, "ram_available_mb": 38408.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24427.7, "ram_available_mb": 38413.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.667}, "power_stats": {"power_gpu_soc_mean_watts": 22.482, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 73.667, "power_watts_avg": 22.482, "energy_joules_est": 176.03, "duration_seconds": 7.83, "sample_count": 66}, "timestamp": "2026-01-25T18:40:58.079404"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9770.993, "latencies_ms": [9770.993], "images_per_second": 0.102, "prompt_tokens": 36, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image features a sheep with a thick, shaggy coat of wool, predominantly in a light brown color, with some white areas. The lighting is natural and soft, suggesting an overcast day, and the sheep is behind a wire fence, indicating it is likely in a farm or rural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.7, "ram_available_mb": 38413.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24430.7, "ram_available_mb": 38410.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.651}, "power_stats": {"power_gpu_soc_mean_watts": 21.352, "power_cpu_cv_mean_watts": 1.852, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 69.651, "power_watts_avg": 21.352, "energy_joules_est": 208.64, "duration_seconds": 9.772, "sample_count": 83}, "timestamp": "2026-01-25T18:41:09.905693"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11148.449, "latencies_ms": [11148.449], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a close-up view of a Nokia phone, bathed in a soft, golden hue. The phone's back cover, a pristine white, contrasts with the black buttons that are visible. The camera lens, a silver circle, is positioned on the top right corner of the phone, ready to capture moments. The volume rocker,", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24430.7, "ram_available_mb": 38410.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24434.0, "ram_available_mb": 38406.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.316}, "power_stats": {"power_gpu_soc_mean_watts": 20.869, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 69.316, "power_watts_avg": 20.869, "energy_joules_est": 232.67, "duration_seconds": 11.149, "sample_count": 95}, "timestamp": "2026-01-25T18:41:23.099029"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7729.462, "latencies_ms": [7729.462], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "camera: 1, flash: 1, volume rocker: 1, power button: 1, camera lens: 1, camera button: 1, camera icon: 1, logo: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.0, "ram_available_mb": 38406.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.908}, "power_stats": {"power_gpu_soc_mean_watts": 22.553, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 72.908, "power_watts_avg": 22.553, "energy_joules_est": 174.34, "duration_seconds": 7.73, "sample_count": 65}, "timestamp": "2026-01-25T18:41:32.842960"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11131.643, "latencies_ms": [11131.643], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The camera is positioned in the foreground of the image, with its buttons and lens clearly visible. In the background, there is a blurred image of what appears to be a colorful object, possibly a poster or a screen displaying an image. The main object, the camera, is in sharp focus and occupies the central space of the image, while the background is out of", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.8, "ram_available_mb": 38418.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24425.5, "ram_available_mb": 38415.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.484}, "power_stats": {"power_gpu_soc_mean_watts": 20.95, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 9.003, "gpu_utilization_percent_mean": 69.484, "power_watts_avg": 20.95, "energy_joules_est": 233.22, "duration_seconds": 11.132, "sample_count": 95}, "timestamp": "2026-01-25T18:41:46.019829"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9749.646, "latencies_ms": [9749.646], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows a close-up of a Huawei smartphone with its power button illuminated, indicating that the device is either turned on or about to be turned on. The focus is on the power button and the camera lens, with the Huawei logo visible on the side of the phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.5, "ram_available_mb": 38415.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24437.4, "ram_available_mb": 38403.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.253}, "power_stats": {"power_gpu_soc_mean_watts": 21.536, "power_cpu_cv_mean_watts": 1.828, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 71.253, "power_watts_avg": 21.536, "energy_joules_est": 209.98, "duration_seconds": 9.75, "sample_count": 83}, "timestamp": "2026-01-25T18:41:57.781939"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8571.591, "latencies_ms": [8571.591], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image showcases a close-up of a device with a metallic finish, featuring a prominent circular button with a red and white logo in the center. The device is illuminated by a soft light, highlighting its sleek design and reflective surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.4, "ram_available_mb": 38403.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24427.0, "ram_available_mb": 38413.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.041}, "power_stats": {"power_gpu_soc_mean_watts": 21.654, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 71.041, "power_watts_avg": 21.654, "energy_joules_est": 185.62, "duration_seconds": 8.572, "sample_count": 73}, "timestamp": "2026-01-25T18:42:08.378745"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12301.382, "latencies_ms": [12301.382], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing in a kitchen, holding a wine glass and smiling. She is wearing a black dress and black heels. The kitchen features a refrigerator, a sink, and a counter. There are also several bottles on the counter, and a bowl is visible in the background. The woman appears to be enjoying her time in the kitchen", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24427.0, "ram_available_mb": 38413.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24447.0, "ram_available_mb": 38393.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.143}, "power_stats": {"power_gpu_soc_mean_watts": 22.861, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 9.213, "gpu_utilization_percent_mean": 74.143, "power_watts_avg": 22.861, "energy_joules_est": 281.24, "duration_seconds": 12.302, "sample_count": 105}, "timestamp": "2026-01-25T18:42:22.699267"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9461.362, "latencies_ms": [9461.362], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "refrigerator: 1, woman: 1, wine bottle: 1, wine glass: 1, bowl: 1, knife block: 1, tiles: 1, drawer: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24447.0, "ram_available_mb": 38393.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24451.1, "ram_available_mb": 38389.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.768}, "power_stats": {"power_gpu_soc_mean_watts": 23.964, "power_cpu_cv_mean_watts": 1.538, "power_sys_5v0_mean_watts": 9.176, "gpu_utilization_percent_mean": 77.768, "power_watts_avg": 23.964, "energy_joules_est": 226.75, "duration_seconds": 9.462, "sample_count": 82}, "timestamp": "2026-01-25T18:42:34.179189"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12422.804, "latencies_ms": [12422.804], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The woman is standing in the foreground on a kitchen floor, with a refrigerator behind her to her right. The refrigerator is in the background, and there is a countertop to the left of the refrigerator with various items on it. The woman is positioned near the center of the image, with the refrigerator to her right and the counter", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24451.1, "ram_available_mb": 38389.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24446.9, "ram_available_mb": 38394.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.642}, "power_stats": {"power_gpu_soc_mean_watts": 22.911, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 9.212, "gpu_utilization_percent_mean": 74.642, "power_watts_avg": 22.911, "energy_joules_est": 284.63, "duration_seconds": 12.423, "sample_count": 106}, "timestamp": "2026-01-25T18:42:48.661989"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8622.924, "latencies_ms": [8622.924], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A woman in a black dress is standing in a kitchen, holding a glass of champagne in front of an open refrigerator. The kitchen has wooden cabinets and a tiled backsplash.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24446.9, "ram_available_mb": 38394.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24464.3, "ram_available_mb": 38376.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.562}, "power_stats": {"power_gpu_soc_mean_watts": 24.376, "power_cpu_cv_mean_watts": 1.42, "power_sys_5v0_mean_watts": 9.17, "gpu_utilization_percent_mean": 78.562, "power_watts_avg": 24.376, "energy_joules_est": 210.21, "duration_seconds": 8.624, "sample_count": 73}, "timestamp": "2026-01-25T18:42:59.333239"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9264.304, "latencies_ms": [9264.304], "images_per_second": 0.108, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a person standing in a kitchen with wooden cabinets and a stainless steel refrigerator. The person is wearing a black dress with sparkles and black heels, and is holding a glass with a yellow drink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24464.3, "ram_available_mb": 38376.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24453.6, "ram_available_mb": 38387.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.615}, "power_stats": {"power_gpu_soc_mean_watts": 23.924, "power_cpu_cv_mean_watts": 1.534, "power_sys_5v0_mean_watts": 9.24, "gpu_utilization_percent_mean": 76.615, "power_watts_avg": 23.924, "energy_joules_est": 221.65, "duration_seconds": 9.265, "sample_count": 78}, "timestamp": "2026-01-25T18:43:10.627099"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11146.649, "latencies_ms": [11146.649], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment on a road, viewed through the reflection of a round mirror. The mirror, attached to a black pole, is the main focus of the image. It reflects a yellow school bus, a white car, and a blue sky. The bus and car are in motion, driving on the road. The pole holding the mirror is positioned in front of a building,", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24453.6, "ram_available_mb": 38387.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24431.0, "ram_available_mb": 38409.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.479}, "power_stats": {"power_gpu_soc_mean_watts": 20.851, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.479, "power_watts_avg": 20.851, "energy_joules_est": 232.43, "duration_seconds": 11.147, "sample_count": 96}, "timestamp": "2026-01-25T18:43:23.796827"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7069.197, "latencies_ms": [7069.197], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "bus: 1, car: 1, traffic light: 2, building: 1, sky: 1, clouds: 1, road: 1, mirror: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24431.0, "ram_available_mb": 38409.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24437.3, "ram_available_mb": 38403.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.25}, "power_stats": {"power_gpu_soc_mean_watts": 22.909, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 74.25, "power_watts_avg": 22.909, "energy_joules_est": 161.96, "duration_seconds": 7.07, "sample_count": 60}, "timestamp": "2026-01-25T18:43:32.925621"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11110.749, "latencies_ms": [11110.749], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a close-up of a rearview mirror reflecting a yellow school bus on the road behind it. The bus is positioned in the middle ground of the reflection, with other vehicles and a building visible in the background. The mirror is attached to a pole on the left side of the image, and the reflection shows the bus moving away from the mirror's", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24437.3, "ram_available_mb": 38403.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24434.2, "ram_available_mb": 38406.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.851}, "power_stats": {"power_gpu_soc_mean_watts": 20.941, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 69.851, "power_watts_avg": 20.941, "energy_joules_est": 232.68, "duration_seconds": 11.111, "sample_count": 94}, "timestamp": "2026-01-25T18:43:46.096641"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8619.05, "latencies_ms": [8619.05], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows a rearview mirror of a vehicle, likely a car or a bus, reflecting the road ahead. The mirror captures the image of a yellow school bus and other vehicles on the road, indicating that the vehicle is on a busy road or highway.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24434.2, "ram_available_mb": 38406.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24430.5, "ram_available_mb": 38410.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.918}, "power_stats": {"power_gpu_soc_mean_watts": 22.035, "power_cpu_cv_mean_watts": 1.716, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 72.918, "power_watts_avg": 22.035, "energy_joules_est": 189.93, "duration_seconds": 8.62, "sample_count": 73}, "timestamp": "2026-01-25T18:43:56.737257"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7106.496, "latencies_ms": [7106.496], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The mirror shows a yellow school bus with a reflection of a car and a building in the background. The sky is cloudy and the lighting is dim, suggesting it might be early morning or late afternoon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24430.5, "ram_available_mb": 38410.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24428.0, "ram_available_mb": 38412.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.4}, "power_stats": {"power_gpu_soc_mean_watts": 22.504, "power_cpu_cv_mean_watts": 1.641, "power_sys_5v0_mean_watts": 9.049, "gpu_utilization_percent_mean": 72.4, "power_watts_avg": 22.504, "energy_joules_est": 159.94, "duration_seconds": 7.107, "sample_count": 60}, "timestamp": "2026-01-25T18:44:05.893788"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11093.661, "latencies_ms": [11093.661], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a gray cat and a brown dog are sitting on a wooden table, looking at a potted plant. The cat is positioned on the right side of the table, while the dog is on the left side. The potted plant is placed in the center of the table, capturing the attention of both the cat and the dog. The scene appears to be set in a", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24428.0, "ram_available_mb": 38412.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24433.5, "ram_available_mb": 38407.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.745}, "power_stats": {"power_gpu_soc_mean_watts": 20.926, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.745, "power_watts_avg": 20.926, "energy_joules_est": 232.16, "duration_seconds": 11.094, "sample_count": 94}, "timestamp": "2026-01-25T18:44:19.010189"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7080.038, "latencies_ms": [7080.038], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "cat: 1, dog: 1, window: 1, plant: 1, pot: 1, soil: 1, label: 1, sunlight: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24433.5, "ram_available_mb": 38407.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.117}, "power_stats": {"power_gpu_soc_mean_watts": 22.812, "power_cpu_cv_mean_watts": 1.568, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 74.117, "power_watts_avg": 22.812, "energy_joules_est": 161.53, "duration_seconds": 7.081, "sample_count": 60}, "timestamp": "2026-01-25T18:44:28.104375"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10652.248, "latencies_ms": [10652.248], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there is a potted plant on the right side of the image, which is near the cat that is on the right side of the image. The dog is in the background, behind the cat, and appears to be looking towards the plant. The plant is closer to the camera than the dog, making it appear larger in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24425.1, "ram_available_mb": 38415.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.696}, "power_stats": {"power_gpu_soc_mean_watts": 21.045, "power_cpu_cv_mean_watts": 1.902, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 69.696, "power_watts_avg": 21.045, "energy_joules_est": 224.19, "duration_seconds": 10.653, "sample_count": 92}, "timestamp": "2026-01-25T18:44:40.815416"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7197.69, "latencies_ms": [7197.69], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A gray cat and a brown dog are sitting on a windowsill, looking at a potted plant. The cat is on the right side of the image, while the dog is on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.1, "ram_available_mb": 38415.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24432.5, "ram_available_mb": 38408.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.167}, "power_stats": {"power_gpu_soc_mean_watts": 22.803, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 9.005, "gpu_utilization_percent_mean": 73.167, "power_watts_avg": 22.803, "energy_joules_est": 164.14, "duration_seconds": 7.198, "sample_count": 60}, "timestamp": "2026-01-25T18:44:50.051758"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7313.355, "latencies_ms": [7313.355], "images_per_second": 0.137, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image features a gray cat and a brown dog standing on a wooden surface, with a potted plant in front of them. The lighting in the room is natural, coming from the large window in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.5, "ram_available_mb": 38408.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24429.4, "ram_available_mb": 38411.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.306}, "power_stats": {"power_gpu_soc_mean_watts": 22.456, "power_cpu_cv_mean_watts": 1.659, "power_sys_5v0_mean_watts": 9.045, "gpu_utilization_percent_mean": 72.306, "power_watts_avg": 22.456, "energy_joules_est": 164.24, "duration_seconds": 7.314, "sample_count": 62}, "timestamp": "2026-01-25T18:44:59.380821"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11147.855, "latencies_ms": [11147.855], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young woman is the main focus, dressed in a blue sports jersey and shorts, actively engaged in a game of soccer. She is skillfully maneuvering a soccer ball with her right hand, showcasing her athletic prowess. The soccer ball is positioned in the lower left corner of the image, indicating that she", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24429.4, "ram_available_mb": 38411.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24422.6, "ram_available_mb": 38418.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.362}, "power_stats": {"power_gpu_soc_mean_watts": 20.924, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.935, "gpu_utilization_percent_mean": 69.362, "power_watts_avg": 20.924, "energy_joules_est": 233.27, "duration_seconds": 11.149, "sample_count": 94}, "timestamp": "2026-01-25T18:45:12.584759"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7195.589, "latencies_ms": [7195.589], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "ball: 1, player: 2, jersey: 2, hand: 2, foot: 2, grass: 1, field: 1, team: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24422.6, "ram_available_mb": 38418.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24436.4, "ram_available_mb": 38404.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.852}, "power_stats": {"power_gpu_soc_mean_watts": 22.798, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 73.852, "power_watts_avg": 22.798, "energy_joules_est": 164.06, "duration_seconds": 7.196, "sample_count": 61}, "timestamp": "2026-01-25T18:45:21.807357"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11220.178, "latencies_ms": [11220.178], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The main object, a person in a blue sports jersey, is in the foreground and appears to be running towards the left side of the image, holding a soccer ball. Another person, wearing a yellow jersey, is in the background and seems to be slightly behind and to the right of the main object. The background is out of focus, emphasizing the main object", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.4, "ram_available_mb": 38404.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 24427.3, "ram_available_mb": 38413.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.358}, "power_stats": {"power_gpu_soc_mean_watts": 20.343, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 70.358, "power_watts_avg": 20.343, "energy_joules_est": 228.27, "duration_seconds": 11.221, "sample_count": 95}, "timestamp": "2026-01-25T18:45:35.043364"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7875.549, "latencies_ms": [7875.549], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A female soccer player in a blue jersey is in possession of the ball and appears to be running with it. Another player in a yellow jersey is in the background, possibly preparing to challenge for the ball.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24427.3, "ram_available_mb": 38413.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24418.0, "ram_available_mb": 38422.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.09}, "power_stats": {"power_gpu_soc_mean_watts": 22.292, "power_cpu_cv_mean_watts": 1.649, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 73.09, "power_watts_avg": 22.292, "energy_joules_est": 175.58, "duration_seconds": 7.876, "sample_count": 67}, "timestamp": "2026-01-25T18:45:44.938409"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11215.29, "latencies_ms": [11215.29], "images_per_second": 0.089, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a person in a blue sports jersey with the word \"Acronis\" prominently displayed across the chest, indicating a sponsorship or branding element. The jersey has a V-neck design and is paired with shorts, suggesting a casual or athletic setting. The person is holding a soccer ball, which is white with", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24418.0, "ram_available_mb": 38422.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 24428.3, "ram_available_mb": 38412.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.284}, "power_stats": {"power_gpu_soc_mean_watts": 20.246, "power_cpu_cv_mean_watts": 1.938, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 70.284, "power_watts_avg": 20.246, "energy_joules_est": 227.08, "duration_seconds": 11.216, "sample_count": 95}, "timestamp": "2026-01-25T18:45:58.190691"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11132.935, "latencies_ms": [11132.935], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two giraffes in a grassy enclosure. The giraffe on the left is bending down to eat some grass, while the one on the right is standing tall and looking around. The enclosure is surrounded by a wooden fence, and there are trees in the background. The giraffes are the main focus of the image, and", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24428.3, "ram_available_mb": 38412.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24430.4, "ram_available_mb": 38410.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.798}, "power_stats": {"power_gpu_soc_mean_watts": 20.891, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.798, "power_watts_avg": 20.891, "energy_joules_est": 232.59, "duration_seconds": 11.134, "sample_count": 94}, "timestamp": "2026-01-25T18:46:11.358595"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7310.799, "latencies_ms": [7310.799], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "giraffe: 2, fence: 1, tree: multiple, grass: large area, path: small dirt area, enclosure: large, wall: wooden, building: not visible", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24430.4, "ram_available_mb": 38410.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24425.1, "ram_available_mb": 38415.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.016}, "power_stats": {"power_gpu_soc_mean_watts": 22.693, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 74.016, "power_watts_avg": 22.693, "energy_joules_est": 165.93, "duration_seconds": 7.312, "sample_count": 62}, "timestamp": "2026-01-25T18:46:20.711439"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9451.372, "latencies_ms": [9451.372], "images_per_second": 0.106, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "In the foreground, there is a giraffe standing on the left side of a wooden fence, while another giraffe is standing on the right side of the same fence. The background is filled with lush green trees, creating a natural and serene environment for the giraffes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.1, "ram_available_mb": 38415.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24427.0, "ram_available_mb": 38413.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.9}, "power_stats": {"power_gpu_soc_mean_watts": 21.432, "power_cpu_cv_mean_watts": 1.821, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 69.9, "power_watts_avg": 21.432, "energy_joules_est": 202.57, "duration_seconds": 9.452, "sample_count": 80}, "timestamp": "2026-01-25T18:46:32.213513"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7515.618, "latencies_ms": [7515.618], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "Two giraffes are standing in a grassy enclosure with a wooden fence, surrounded by trees. One giraffe is bending down to eat grass, while the other stands tall and looks around.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24427.0, "ram_available_mb": 38413.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24424.7, "ram_available_mb": 38416.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.141}, "power_stats": {"power_gpu_soc_mean_watts": 22.617, "power_cpu_cv_mean_watts": 1.633, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 73.141, "power_watts_avg": 22.617, "energy_joules_est": 170.0, "duration_seconds": 7.516, "sample_count": 64}, "timestamp": "2026-01-25T18:46:41.742765"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6987.504, "latencies_ms": [6987.504], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image shows two giraffes in a grassy enclosure with a wooden fence. The lighting appears to be natural daylight, and the weather seems to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24424.7, "ram_available_mb": 38416.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24435.1, "ram_available_mb": 38405.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.847}, "power_stats": {"power_gpu_soc_mean_watts": 22.669, "power_cpu_cv_mean_watts": 1.622, "power_sys_5v0_mean_watts": 9.065, "gpu_utilization_percent_mean": 72.847, "power_watts_avg": 22.669, "energy_joules_est": 158.41, "duration_seconds": 6.988, "sample_count": 59}, "timestamp": "2026-01-25T18:46:50.784521"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12421.305, "latencies_ms": [12421.305], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of travel preparation. A gray suitcase, equipped with a handle and wheels, sits on a carpeted floor, ready for a journey. It's accompanied by a black trash bag, perhaps filled with discarded items, and a black bag, possibly containing personal belongings. The backdrop is a white curtain, adding a touch", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24435.1, "ram_available_mb": 38405.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24427.9, "ram_available_mb": 38413.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.067}, "power_stats": {"power_gpu_soc_mean_watts": 22.203, "power_cpu_cv_mean_watts": 1.788, "power_sys_5v0_mean_watts": 9.166, "gpu_utilization_percent_mean": 74.067, "power_watts_avg": 22.203, "energy_joules_est": 275.8, "duration_seconds": 12.422, "sample_count": 105}, "timestamp": "2026-01-25T18:47:05.268660"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6938.316, "latencies_ms": [6938.316], "images_per_second": 0.144, "prompt_tokens": 39, "response_tokens_est": 28, "n_tiles": 16, "output_text": "bag: 2\nsuitcase: 1\nbox: 1\ncarpet: 1\ncurtain: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.9, "ram_available_mb": 38413.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24433.0, "ram_available_mb": 38407.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 82.431}, "power_stats": {"power_gpu_soc_mean_watts": 25.475, "power_cpu_cv_mean_watts": 1.173, "power_sys_5v0_mean_watts": 9.206, "gpu_utilization_percent_mean": 82.431, "power_watts_avg": 25.475, "energy_joules_est": 176.77, "duration_seconds": 6.939, "sample_count": 58}, "timestamp": "2026-01-25T18:47:14.228323"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12437.597, "latencies_ms": [12437.597], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a suitcase standing upright with its handle extended to the right, suggesting it is ready to be picked up. Behind the suitcase, there is a trash bin to its left, partially obscured by the suitcase. The trash bin appears to be on the same floor level as the suitcase. The background consists of a", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24433.0, "ram_available_mb": 38407.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.302}, "power_stats": {"power_gpu_soc_mean_watts": 22.87, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 9.195, "gpu_utilization_percent_mean": 74.302, "power_watts_avg": 22.87, "energy_joules_est": 284.46, "duration_seconds": 12.438, "sample_count": 106}, "timestamp": "2026-01-25T18:47:28.698105"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11080.047, "latencies_ms": [11080.047], "images_per_second": 0.09, "prompt_tokens": 37, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows a collection of luggage items, including a suitcase and a duffel bag, placed on a carpeted floor against a backdrop of sheer curtains. It appears to be a hotel room or a similar setting where someone is packing or unpacking their belongings.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24434.3, "ram_available_mb": 38406.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.106}, "power_stats": {"power_gpu_soc_mean_watts": 23.402, "power_cpu_cv_mean_watts": 1.652, "power_sys_5v0_mean_watts": 9.127, "gpu_utilization_percent_mean": 76.106, "power_watts_avg": 23.402, "energy_joules_est": 259.31, "duration_seconds": 11.081, "sample_count": 94}, "timestamp": "2026-01-25T18:47:41.811605"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10513.951, "latencies_ms": [10513.951], "images_per_second": 0.095, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image is in black and white, featuring a suitcase, a bag, and a trash can against a backdrop of sheer curtains. The materials appear to be typical for travel items, with the suitcase and bag made of fabric and the trash bag made of plastic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.3, "ram_available_mb": 38406.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.478}, "power_stats": {"power_gpu_soc_mean_watts": 23.338, "power_cpu_cv_mean_watts": 1.641, "power_sys_5v0_mean_watts": 9.218, "gpu_utilization_percent_mean": 75.478, "power_watts_avg": 23.338, "energy_joules_est": 245.39, "duration_seconds": 10.515, "sample_count": 90}, "timestamp": "2026-01-25T18:47:54.363533"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11170.445, "latencies_ms": [11170.445], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man wearing a red bandana and a blue shirt is standing in a forest, looking at two horses that are walking down a rocky path. The man appears to be observing the horses as they make their way through the forest. The horses are positioned in the middle of the scene, with one horse closer to the left side and the other horse closer to", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24428.0, "ram_available_mb": 38412.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.232}, "power_stats": {"power_gpu_soc_mean_watts": 20.824, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.232, "power_watts_avg": 20.824, "energy_joules_est": 232.63, "duration_seconds": 11.171, "sample_count": 95}, "timestamp": "2026-01-25T18:48:07.561329"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9404.309, "latencies_ms": [9404.309], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "- Trees: numerous\n\n- Rocks: numerous\n\n- Horses: 2\n\n- Person: 1\n\n- Backpack: 1\n\n- Hat: 1\n\n- Grass: patches of green\n\n- Dirt path: visible", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24428.0, "ram_available_mb": 38412.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24426.4, "ram_available_mb": 38414.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.062}, "power_stats": {"power_gpu_soc_mean_watts": 21.668, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 72.062, "power_watts_avg": 21.668, "energy_joules_est": 203.79, "duration_seconds": 9.405, "sample_count": 80}, "timestamp": "2026-01-25T18:48:19.026224"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11089.762, "latencies_ms": [11089.762], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person wearing a blue shirt and a red bandana on their head, with a backpack on their back, looking towards the left side of the image. In the background, there are two individuals on horseback, one of whom is wearing a hat, and they are situated between the person in the foreground and the trees. The person on", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24426.4, "ram_available_mb": 38414.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24433.2, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.642}, "power_stats": {"power_gpu_soc_mean_watts": 20.912, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 69.642, "power_watts_avg": 20.912, "energy_joules_est": 231.92, "duration_seconds": 11.09, "sample_count": 95}, "timestamp": "2026-01-25T18:48:32.135088"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8284.085, "latencies_ms": [8284.085], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "In a forested area with a rocky path, a person is riding a horse, while another person with a backpack is watching. The scene suggests a leisurely outdoor activity, possibly hiking or horseback riding.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.2, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24428.6, "ram_available_mb": 38412.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.057}, "power_stats": {"power_gpu_soc_mean_watts": 22.197, "power_cpu_cv_mean_watts": 1.698, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 73.057, "power_watts_avg": 22.197, "energy_joules_est": 183.89, "duration_seconds": 8.285, "sample_count": 70}, "timestamp": "2026-01-25T18:48:42.475472"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8240.456, "latencies_ms": [8240.456], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image features a bright and sunny day with clear skies, casting natural light that illuminates the scene. The colors are vibrant, with the green of the trees contrasting against the blue of the sky and the brown of the horses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24428.6, "ram_available_mb": 38412.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24425.1, "ram_available_mb": 38415.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.623}, "power_stats": {"power_gpu_soc_mean_watts": 21.94, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 9.025, "gpu_utilization_percent_mean": 71.623, "power_watts_avg": 21.94, "energy_joules_est": 180.81, "duration_seconds": 8.241, "sample_count": 69}, "timestamp": "2026-01-25T18:48:52.754532"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11147.108, "latencies_ms": [11147.108], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photo, a man is riding a horse on a bridge. The man is wearing a jacket and is holding the reins of the horse, guiding it as it moves. The horse is positioned in the center of the image, with the man standing on its back. The bridge they are on is made of wood and has a railing on the side", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24425.1, "ram_available_mb": 38415.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24452.4, "ram_available_mb": 38388.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.537}, "power_stats": {"power_gpu_soc_mean_watts": 20.88, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.537, "power_watts_avg": 20.88, "energy_joules_est": 232.76, "duration_seconds": 11.148, "sample_count": 95}, "timestamp": "2026-01-25T18:49:05.963783"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9836.281, "latencies_ms": [9836.281], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "1. Man: 1\n2. Horse: 1\n3. Stick: 1\n4. Sweater: 1\n5. Rider: 1\n6. Blanket: 1\n7. Saddle: 1\n8. Grass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24452.4, "ram_available_mb": 38388.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24440.6, "ram_available_mb": 38400.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.106}, "power_stats": {"power_gpu_soc_mean_watts": 21.516, "power_cpu_cv_mean_watts": 1.827, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 71.106, "power_watts_avg": 21.516, "energy_joules_est": 211.66, "duration_seconds": 9.837, "sample_count": 85}, "timestamp": "2026-01-25T18:49:17.859546"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10026.846, "latencies_ms": [10026.846], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The person is standing on the back of the horse, which is in the foreground of the image. The background is blurred, but it appears to be a flat landscape with no other objects in the immediate vicinity. The horse and rider are the main focus of the image, with the background serving as a backdrop.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24440.6, "ram_available_mb": 38400.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24427.5, "ram_available_mb": 38413.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.294}, "power_stats": {"power_gpu_soc_mean_watts": 21.284, "power_cpu_cv_mean_watts": 1.856, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 70.294, "power_watts_avg": 21.284, "energy_joules_est": 213.42, "duration_seconds": 10.027, "sample_count": 85}, "timestamp": "2026-01-25T18:49:29.901445"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6606.51, "latencies_ms": [6606.51], "images_per_second": 0.151, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A man is riding a horse, and the horse is moving at a fast pace. The man is wearing a jacket with the word \"Rio\" on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.5, "ram_available_mb": 38413.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24428.6, "ram_available_mb": 38412.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.164}, "power_stats": {"power_gpu_soc_mean_watts": 23.268, "power_cpu_cv_mean_watts": 1.514, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 73.164, "power_watts_avg": 23.268, "energy_joules_est": 153.73, "duration_seconds": 6.607, "sample_count": 55}, "timestamp": "2026-01-25T18:49:38.529211"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7435.389, "latencies_ms": [7435.389], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image is in black and white, with a high contrast between the light and dark areas. The weather appears to be overcast, as the sky is filled with clouds and the overall lighting is soft and diffused.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24428.6, "ram_available_mb": 38412.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24432.2, "ram_available_mb": 38408.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.355}, "power_stats": {"power_gpu_soc_mean_watts": 22.41, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 9.032, "gpu_utilization_percent_mean": 72.355, "power_watts_avg": 22.41, "energy_joules_est": 166.64, "duration_seconds": 7.436, "sample_count": 62}, "timestamp": "2026-01-25T18:49:47.981688"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11145.749, "latencies_ms": [11145.749], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a group of geese swimming in a calm body of water. There are at least six geese visible in the water, with some of them closer to the foreground and others further away. The geese are floating peacefully, enjoying their time in the water.\n\nThe water appears to be a lake or a pond, surrounded", "error": null, "sys_before": {"cpu_percent": 7.9, "ram_used_mb": 24432.2, "ram_available_mb": 38408.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 24428.4, "ram_available_mb": 38412.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.158}, "power_stats": {"power_gpu_soc_mean_watts": 20.832, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 69.158, "power_watts_avg": 20.832, "energy_joules_est": 232.21, "duration_seconds": 11.147, "sample_count": 95}, "timestamp": "2026-01-25T18:50:01.162965"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4171.514, "latencies_ms": [4171.514], "images_per_second": 0.24, "prompt_tokens": 39, "response_tokens_est": 15, "n_tiles": 16, "output_text": "geese: 5, trees: numerous, grasses: dense", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24428.4, "ram_available_mb": 38412.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24432.6, "ram_available_mb": 38408.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 82.265}, "power_stats": {"power_gpu_soc_mean_watts": 26.64, "power_cpu_cv_mean_watts": 0.977, "power_sys_5v0_mean_watts": 9.11, "gpu_utilization_percent_mean": 82.265, "power_watts_avg": 26.64, "energy_joules_est": 111.15, "duration_seconds": 4.172, "sample_count": 34}, "timestamp": "2026-01-25T18:50:07.348808"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11105.809, "latencies_ms": [11105.809], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are three ducks swimming in the water, with one duck slightly closer to the viewer than the others. In the background, there are trees and grasses surrounding the body of water, creating a natural and serene environment. The ducks are positioned near the center of the image, with the trees and grasses appearing further away, creating a sense", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24432.6, "ram_available_mb": 38408.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24436.1, "ram_available_mb": 38404.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.653}, "power_stats": {"power_gpu_soc_mean_watts": 20.963, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.653, "power_watts_avg": 20.963, "energy_joules_est": 232.83, "duration_seconds": 11.107, "sample_count": 95}, "timestamp": "2026-01-25T18:50:20.473392"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7077.077, "latencies_ms": [7077.077], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A group of geese is swimming in a calm body of water surrounded by lush greenery and tall grasses. The tranquil scene depicts a peaceful moment in nature.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.1, "ram_available_mb": 38404.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24433.7, "ram_available_mb": 38407.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.033}, "power_stats": {"power_gpu_soc_mean_watts": 22.876, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 74.033, "power_watts_avg": 22.876, "energy_joules_est": 161.91, "duration_seconds": 7.078, "sample_count": 60}, "timestamp": "2026-01-25T18:50:29.594878"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6773.867, "latencies_ms": [6773.867], "images_per_second": 0.148, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image features a serene body of water with a group of geese swimming in it. The water is reflecting the surrounding trees and grass, creating a calm and peaceful atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.7, "ram_available_mb": 38407.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24428.5, "ram_available_mb": 38412.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.724}, "power_stats": {"power_gpu_soc_mean_watts": 22.735, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.067, "gpu_utilization_percent_mean": 72.724, "power_watts_avg": 22.735, "energy_joules_est": 154.02, "duration_seconds": 6.775, "sample_count": 58}, "timestamp": "2026-01-25T18:50:38.413182"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11175.923, "latencies_ms": [11175.923], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a cat with a coat of white and orange fur is perched on the hood of a black Mercedes-Benz car. The car is parked in front of a brick building, and a green fence can be seen in the background. The cat appears to be looking down at the car, perhaps curious about its surroundings or simply enjoying the view from", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24428.5, "ram_available_mb": 38412.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24430.0, "ram_available_mb": 38410.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.208}, "power_stats": {"power_gpu_soc_mean_watts": 20.073, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.928, "gpu_utilization_percent_mean": 70.208, "power_watts_avg": 20.073, "energy_joules_est": 224.35, "duration_seconds": 11.177, "sample_count": 96}, "timestamp": "2026-01-25T18:50:51.634064"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7392.515, "latencies_ms": [7392.515], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "car: 1\nmercedes logo: 1\nwindow: 4\ngrill: 1\ncat: 1\nbuilding: 1\nplant: 1\nflower: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24430.0, "ram_available_mb": 38410.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.794}, "power_stats": {"power_gpu_soc_mean_watts": 22.595, "power_cpu_cv_mean_watts": 1.62, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 72.794, "power_watts_avg": 22.595, "energy_joules_est": 167.05, "duration_seconds": 7.393, "sample_count": 63}, "timestamp": "2026-01-25T18:51:01.068913"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7208.78, "latencies_ms": [7208.78], "images_per_second": 0.139, "prompt_tokens": 44, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The cat is sitting on the hood of a black Mercedes car, which is in the foreground of the image. In the background, there is a building with windows and a fence with green plants.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24433.2, "ram_available_mb": 38407.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.656}, "power_stats": {"power_gpu_soc_mean_watts": 22.575, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 9.051, "gpu_utilization_percent_mean": 72.656, "power_watts_avg": 22.575, "energy_joules_est": 162.77, "duration_seconds": 7.21, "sample_count": 61}, "timestamp": "2026-01-25T18:51:10.300243"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8624.166, "latencies_ms": [8624.166], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "A cat is sitting on the hood of a black Mercedes-Benz car, with its reflection visible in the glossy surface. The car is parked in front of a building with a brick facade and a green fence with plants behind it.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24433.2, "ram_available_mb": 38407.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24425.7, "ram_available_mb": 38415.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.671}, "power_stats": {"power_gpu_soc_mean_watts": 21.989, "power_cpu_cv_mean_watts": 1.716, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 72.671, "power_watts_avg": 21.989, "energy_joules_est": 189.65, "duration_seconds": 8.625, "sample_count": 73}, "timestamp": "2026-01-25T18:51:20.980208"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7430.041, "latencies_ms": [7430.041], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image features a black Mercedes-Benz car with a cat sitting on the hood. The car is parked in front of a building with a brick facade and a green fence with plants behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.7, "ram_available_mb": 38415.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24433.2, "ram_available_mb": 38407.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.127}, "power_stats": {"power_gpu_soc_mean_watts": 22.384, "power_cpu_cv_mean_watts": 1.665, "power_sys_5v0_mean_watts": 9.075, "gpu_utilization_percent_mean": 72.127, "power_watts_avg": 22.384, "energy_joules_est": 166.33, "duration_seconds": 7.431, "sample_count": 63}, "timestamp": "2026-01-25T18:51:30.442954"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11100.426, "latencies_ms": [11100.426], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a snowboarder is captured in mid-air, performing a daring trick on a snowy mountain. The snowboarder is wearing a brown jacket and yellow pants, and is holding onto a snowboard with both hands. The snowboarder is suspended in the air, with the snowboard angled upwards, indicating a high jump. The background", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 24433.2, "ram_available_mb": 38407.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24433.1, "ram_available_mb": 38407.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.653}, "power_stats": {"power_gpu_soc_mean_watts": 21.006, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.653, "power_watts_avg": 21.006, "energy_joules_est": 233.19, "duration_seconds": 11.101, "sample_count": 95}, "timestamp": "2026-01-25T18:51:43.578107"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9150.988, "latencies_ms": [9150.988], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "sky: 1\n\nsnowboard: 1\n\nsnowflakes: numerous\n\nsnow: 1\n\nsnowboarder: 1\n\nsweater: 1\n\npants: 1\n\njacket: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24433.1, "ram_available_mb": 38407.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24430.5, "ram_available_mb": 38410.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.526}, "power_stats": {"power_gpu_soc_mean_watts": 21.826, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 72.526, "power_watts_avg": 21.826, "energy_joules_est": 199.74, "duration_seconds": 9.152, "sample_count": 78}, "timestamp": "2026-01-25T18:51:54.787085"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8656.757, "latencies_ms": [8656.757], "images_per_second": 0.116, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The snowboarder is in the foreground, performing a trick in the air above a snowy hill. The clear blue sky forms the background, and the snowflakes are scattered throughout the image, indicating that the photo was taken on a cold, snowy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24430.5, "ram_available_mb": 38410.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24435.7, "ram_available_mb": 38405.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.853}, "power_stats": {"power_gpu_soc_mean_watts": 21.722, "power_cpu_cv_mean_watts": 1.788, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 70.853, "power_watts_avg": 21.722, "energy_joules_est": 188.05, "duration_seconds": 8.657, "sample_count": 75}, "timestamp": "2026-01-25T18:52:05.466697"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8369.821, "latencies_ms": [8369.821], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "A snowboarder is captured in mid-air, performing a trick against a clear blue sky backdrop. The snowboarder is wearing a brown jacket and yellow pants, and is in the process of jumping off a snowy hill.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.7, "ram_available_mb": 38405.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24428.4, "ram_available_mb": 38412.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.507}, "power_stats": {"power_gpu_soc_mean_watts": 22.151, "power_cpu_cv_mean_watts": 1.708, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 72.507, "power_watts_avg": 22.151, "energy_joules_est": 185.41, "duration_seconds": 8.37, "sample_count": 71}, "timestamp": "2026-01-25T18:52:15.896837"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7655.887, "latencies_ms": [7655.887], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The snowboarder is wearing a brown jacket and bright yellow pants, and is performing a trick in the air. The sky is a clear blue, and there are snowflakes falling around the snowboarder.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24428.4, "ram_available_mb": 38412.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24457.5, "ram_available_mb": 38383.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.078}, "power_stats": {"power_gpu_soc_mean_watts": 22.274, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 9.067, "gpu_utilization_percent_mean": 72.078, "power_watts_avg": 22.274, "energy_joules_est": 170.54, "duration_seconds": 7.657, "sample_count": 64}, "timestamp": "2026-01-25T18:52:25.572744"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11121.588, "latencies_ms": [11121.588], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a quaint, small bathroom bathed in soft light. Dominating the scene is a pristine white toilet, its lid closed, standing on a wooden floor. The toilet is positioned against a white wall, which is adorned with a single light switch. Above the toilet, a white pipe runs along the wall, its", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24457.5, "ram_available_mb": 38383.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24542.4, "ram_available_mb": 38298.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.872}, "power_stats": {"power_gpu_soc_mean_watts": 20.904, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 69.872, "power_watts_avg": 20.904, "energy_joules_est": 232.5, "duration_seconds": 11.122, "sample_count": 94}, "timestamp": "2026-01-25T18:52:38.749888"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10090.904, "latencies_ms": [10090.904], "images_per_second": 0.099, "prompt_tokens": 39, "response_tokens_est": 68, "n_tiles": 16, "output_text": "- Toilet: 1\n\n- Bathtub: 1\n\n- Pipes: 10\n\n- Chain: 1\n\n- Tiles: 1\n\n- Door: 1\n\n- Light fixture: 1\n\n- Graffiti: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.2, "ram_available_mb": 38403.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24436.6, "ram_available_mb": 38404.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.071}, "power_stats": {"power_gpu_soc_mean_watts": 21.389, "power_cpu_cv_mean_watts": 1.823, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 70.071, "power_watts_avg": 21.389, "energy_joules_est": 215.85, "duration_seconds": 10.091, "sample_count": 85}, "timestamp": "2026-01-25T18:52:50.877175"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11164.173, "latencies_ms": [11164.173], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a toilet positioned on the left side, which is relatively close to the viewer. Behind the toilet, there is a white bathtub on the right side, which is further away from the viewer. The pipes and tanks are mounted on the wall in the background, creating a sense of depth in the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.6, "ram_available_mb": 38404.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24430.0, "ram_available_mb": 38410.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.242}, "power_stats": {"power_gpu_soc_mean_watts": 20.992, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 70.242, "power_watts_avg": 20.992, "energy_joules_est": 234.37, "duration_seconds": 11.165, "sample_count": 95}, "timestamp": "2026-01-25T18:53:04.074886"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11312.995, "latencies_ms": [11312.995], "images_per_second": 0.088, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a small, old-fashioned bathroom with a toilet and a bathtub. The toilet has a unique design with a wooden seat and a chain hanging from the wall above it. The bathtub is white and appears to be made of porcelain. The walls are painted white and there are several pipes running along the wall", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24430.0, "ram_available_mb": 38410.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24425.3, "ram_available_mb": 38415.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.115}, "power_stats": {"power_gpu_soc_mean_watts": 20.993, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.93, "gpu_utilization_percent_mean": 70.115, "power_watts_avg": 20.993, "energy_joules_est": 237.51, "duration_seconds": 11.314, "sample_count": 96}, "timestamp": "2026-01-25T18:53:17.416310"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11088.365, "latencies_ms": [11088.365], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a bathroom with a vintage or industrial aesthetic, featuring a white toilet with a wooden seat and lid, a white bathtub with metal fixtures, and a complex network of white pipes and chains hanging from the ceiling. The lighting is dim, and the walls are painted in a light color, possibly white or cre", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24425.3, "ram_available_mb": 38415.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24427.5, "ram_available_mb": 38413.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.83}, "power_stats": {"power_gpu_soc_mean_watts": 21.098, "power_cpu_cv_mean_watts": 1.929, "power_sys_5v0_mean_watts": 9.018, "gpu_utilization_percent_mean": 69.83, "power_watts_avg": 21.098, "energy_joules_est": 233.96, "duration_seconds": 11.089, "sample_count": 94}, "timestamp": "2026-01-25T18:53:30.550396"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11204.943, "latencies_ms": [11204.943], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a statue of a man is depicted flying a kite. The statue is positioned on a pedestal, and the kite is soaring high in the sky. The man appears to be enjoying the activity, as he is holding the kite string with both hands. The kite itself is multicolored, adding a vibrant touch to the scene", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24427.5, "ram_available_mb": 38413.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24432.5, "ram_available_mb": 38408.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.253}, "power_stats": {"power_gpu_soc_mean_watts": 20.35, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.932, "gpu_utilization_percent_mean": 70.253, "power_watts_avg": 20.35, "energy_joules_est": 228.03, "duration_seconds": 11.206, "sample_count": 95}, "timestamp": "2026-01-25T18:53:43.783436"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10531.586, "latencies_ms": [10531.586], "images_per_second": 0.095, "prompt_tokens": 39, "response_tokens_est": 72, "n_tiles": 16, "output_text": "1. Kite: 1\n2. Statue: 1\n3. Building: 1\n4. Skyscraper: 1\n5. Clouds: 1\n6. Sculpture: 1\n7. Statue base: 1\n8. Sculpture base: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.5, "ram_available_mb": 38408.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24417.9, "ram_available_mb": 38423.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.382}, "power_stats": {"power_gpu_soc_mean_watts": 21.291, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 70.382, "power_watts_avg": 21.291, "energy_joules_est": 224.24, "duration_seconds": 10.532, "sample_count": 89}, "timestamp": "2026-01-25T18:53:56.352001"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11164.678, "latencies_ms": [11164.678], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a statue of a person holding a kite, which is positioned near the top right corner of the image. The kite is flying in the air, slightly above the statue's head. In the background, there is a tall building with a glass facade, and the sky is visible above it. The statue appears to be in front of the building", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24417.9, "ram_available_mb": 38423.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 24429.9, "ram_available_mb": 38411.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.316}, "power_stats": {"power_gpu_soc_mean_watts": 21.016, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 70.316, "power_watts_avg": 21.016, "energy_joules_est": 234.65, "duration_seconds": 11.165, "sample_count": 95}, "timestamp": "2026-01-25T18:54:09.536544"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7292.021, "latencies_ms": [7292.021], "images_per_second": 0.137, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A statue of a person holding a kite is located in front of a modern building with glass windows. The kite is flying high in the sky, and the person appears to be enjoying the activity.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24429.9, "ram_available_mb": 38411.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24428.1, "ram_available_mb": 38412.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.246}, "power_stats": {"power_gpu_soc_mean_watts": 22.705, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 9.018, "gpu_utilization_percent_mean": 72.246, "power_watts_avg": 22.705, "energy_joules_est": 165.58, "duration_seconds": 7.293, "sample_count": 61}, "timestamp": "2026-01-25T18:54:18.857912"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7821.409, "latencies_ms": [7821.409], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a statue of a person holding a kite, with the kite displaying a colorful pattern. The statue is located in front of a modern building with a glass facade, and the sky appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24428.1, "ram_available_mb": 38412.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24448.7, "ram_available_mb": 38392.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.667}, "power_stats": {"power_gpu_soc_mean_watts": 22.265, "power_cpu_cv_mean_watts": 1.692, "power_sys_5v0_mean_watts": 9.02, "gpu_utilization_percent_mean": 72.667, "power_watts_avg": 22.265, "energy_joules_est": 174.16, "duration_seconds": 7.822, "sample_count": 66}, "timestamp": "2026-01-25T18:54:28.729317"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11112.242, "latencies_ms": [11112.242], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a table filled with a variety of fresh vegetables and fruits. There are several bowls and baskets containing different types of produce. The table is covered with a diverse assortment of vegetables, including carrots, broccoli, and potatoes. \n\nIn addition to the vegetables, there are also some fruits present, such as st", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24448.7, "ram_available_mb": 38392.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24454.7, "ram_available_mb": 38386.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.511}, "power_stats": {"power_gpu_soc_mean_watts": 20.926, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 69.511, "power_watts_avg": 20.926, "energy_joules_est": 232.55, "duration_seconds": 11.113, "sample_count": 94}, "timestamp": "2026-01-25T18:54:41.865032"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9063.213, "latencies_ms": [9063.213], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "strawberries: 20, broccoli: 1, cucumber: 1, radishes: 12, carrots: 5, potatoes: 8, green beans: 10, asparagus: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24454.7, "ram_available_mb": 38386.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.623}, "power_stats": {"power_gpu_soc_mean_watts": 21.821, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 71.623, "power_watts_avg": 21.821, "energy_joules_est": 197.78, "duration_seconds": 9.064, "sample_count": 77}, "timestamp": "2026-01-25T18:54:52.966661"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10143.13, "latencies_ms": [10143.13], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there are bright red strawberries in a wooden bowl on the left side of the image. Behind them, in the middle ground, are green beans in a white plastic bag. In the background, there are various vegetables including radishes, carrots, and asparagus.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24433.3, "ram_available_mb": 38407.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24442.0, "ram_available_mb": 38398.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.407}, "power_stats": {"power_gpu_soc_mean_watts": 21.238, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 70.407, "power_watts_avg": 21.238, "energy_joules_est": 215.43, "duration_seconds": 10.144, "sample_count": 86}, "timestamp": "2026-01-25T18:55:05.126138"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9942.003, "latencies_ms": [9942.003], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image showcases a variety of fresh vegetables and fruits arranged on a table, including strawberries, broccoli, carrots, and potatoes. The setting appears to be a market or a display of produce, with the vegetables and fruits presented in an appealing and colorful manner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.0, "ram_available_mb": 38398.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24431.3, "ram_available_mb": 38409.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.595}, "power_stats": {"power_gpu_soc_mean_watts": 21.395, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 70.595, "power_watts_avg": 21.395, "energy_joules_est": 212.72, "duration_seconds": 9.943, "sample_count": 84}, "timestamp": "2026-01-25T18:55:17.097054"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9781.709, "latencies_ms": [9781.709], "images_per_second": 0.102, "prompt_tokens": 36, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image showcases a variety of fresh produce, including vibrant red strawberries, green asparagus, and pink radishes, all laid out on a wooden surface. The lighting is bright and natural, suggesting the photo was taken outdoors or in a well-lit indoor area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.3, "ram_available_mb": 38409.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24448.9, "ram_available_mb": 38392.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.301}, "power_stats": {"power_gpu_soc_mean_watts": 21.277, "power_cpu_cv_mean_watts": 1.847, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 70.301, "power_watts_avg": 21.277, "energy_joules_est": 208.14, "duration_seconds": 9.782, "sample_count": 83}, "timestamp": "2026-01-25T18:55:28.920609"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11122.15, "latencies_ms": [11122.15], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of joy and camaraderie among three individuals, likely friends, as they engage in a shared activity of playing video games. The room is dimly lit, with the primary source of light coming from a television screen that is visible in the background. The television is turned on, and the screen is displaying a game, although the specific game is not discern", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24448.9, "ram_available_mb": 38392.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24425.5, "ram_available_mb": 38415.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.547}, "power_stats": {"power_gpu_soc_mean_watts": 20.912, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 69.547, "power_watts_avg": 20.912, "energy_joules_est": 232.6, "duration_seconds": 11.123, "sample_count": 95}, "timestamp": "2026-01-25T18:55:42.091733"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7936.516, "latencies_ms": [7936.516], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "person: 3\ngame controller: 2\ncouch: 1\ntv: 1\nremote control: 2\nwii: 1\nwii remote: 2\nwii remote holder: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24425.5, "ram_available_mb": 38415.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24427.8, "ram_available_mb": 38413.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.574}, "power_stats": {"power_gpu_soc_mean_watts": 22.248, "power_cpu_cv_mean_watts": 1.695, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 71.574, "power_watts_avg": 22.248, "energy_joules_est": 176.59, "duration_seconds": 7.937, "sample_count": 68}, "timestamp": "2026-01-25T18:55:52.063461"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11094.248, "latencies_ms": [11094.248], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person on the left side of the image, holding a gaming controller and facing towards the right side of the image where another person is seated on a couch. The third person is on the far right, also holding a controller and facing the left. The couch is positioned in the middle ground, and there is a bed with a backpack", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.8, "ram_available_mb": 38413.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24426.6, "ram_available_mb": 38414.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.354}, "power_stats": {"power_gpu_soc_mean_watts": 20.989, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.354, "power_watts_avg": 20.989, "energy_joules_est": 232.87, "duration_seconds": 11.095, "sample_count": 96}, "timestamp": "2026-01-25T18:56:05.217444"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6939.735, "latencies_ms": [6939.735], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "Three people are sitting on a couch in a dimly lit room, laughing and playing video games together. The room appears to be a living space with a comfortable and casual atmosphere.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24426.6, "ram_available_mb": 38414.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24430.3, "ram_available_mb": 38410.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.644}, "power_stats": {"power_gpu_soc_mean_watts": 23.055, "power_cpu_cv_mean_watts": 1.574, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 74.644, "power_watts_avg": 23.055, "energy_joules_est": 160.01, "duration_seconds": 6.94, "sample_count": 59}, "timestamp": "2026-01-25T18:56:14.200528"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7870.399, "latencies_ms": [7870.399], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image is dimly lit with a focus on the three individuals who are seated on a patterned couch. The lighting appears to be coming from a television screen in front of them, casting a blue hue on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24430.3, "ram_available_mb": 38410.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24420.6, "ram_available_mb": 38420.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.303}, "power_stats": {"power_gpu_soc_mean_watts": 22.104, "power_cpu_cv_mean_watts": 1.71, "power_sys_5v0_mean_watts": 9.037, "gpu_utilization_percent_mean": 72.303, "power_watts_avg": 22.104, "energy_joules_est": 173.98, "duration_seconds": 7.871, "sample_count": 66}, "timestamp": "2026-01-25T18:56:24.098179"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11131.975, "latencies_ms": [11131.975], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene pastoral scene. In the foreground, a lone white cow is lying down on a lush green field, its body relaxed and at ease. A little distance away, a group of black and white cows are also resting, their bodies stretched out in a similar fashion. The field they are in is a vibrant green, indic", "error": null, "sys_before": {"cpu_percent": 14.8, "ram_used_mb": 24420.6, "ram_available_mb": 38420.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24434.8, "ram_available_mb": 38406.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.811}, "power_stats": {"power_gpu_soc_mean_watts": 20.888, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.811, "power_watts_avg": 20.888, "energy_joules_est": 232.54, "duration_seconds": 11.133, "sample_count": 95}, "timestamp": "2026-01-25T18:56:37.260620"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7276.594, "latencies_ms": [7276.594], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "tree: 1\ncows: 5\ngrass: many\nfield: 1\nsky: 1\nleaves: many\nstones: 0\nbirds: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.8, "ram_available_mb": 38406.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24432.1, "ram_available_mb": 38408.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.161}, "power_stats": {"power_gpu_soc_mean_watts": 22.768, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 74.161, "power_watts_avg": 22.768, "energy_joules_est": 165.69, "duration_seconds": 7.277, "sample_count": 62}, "timestamp": "2026-01-25T18:56:46.559292"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11128.493, "latencies_ms": [11128.493], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a lone cow lying on the grass, closer to the viewer than the other cows. The cows are spread out in the background, with some lying down and others standing, all at a distance from the viewer. The tree trunk is in the foreground on the right side of the image, while the cows are scattered in the middle", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.1, "ram_available_mb": 38408.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24433.5, "ram_available_mb": 38407.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.67}, "power_stats": {"power_gpu_soc_mean_watts": 20.931, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.67, "power_watts_avg": 20.931, "energy_joules_est": 232.94, "duration_seconds": 11.129, "sample_count": 94}, "timestamp": "2026-01-25T18:56:59.707146"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7399.555, "latencies_ms": [7399.555], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A group of cows is resting in a lush green field, with one cow lying down close to a tree trunk. The cows appear to be at ease, enjoying the peaceful environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.5, "ram_available_mb": 38407.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24422.2, "ram_available_mb": 38418.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.73}, "power_stats": {"power_gpu_soc_mean_watts": 22.641, "power_cpu_cv_mean_watts": 1.62, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 73.73, "power_watts_avg": 22.641, "energy_joules_est": 167.55, "duration_seconds": 7.4, "sample_count": 63}, "timestamp": "2026-01-25T18:57:09.143202"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6878.758, "latencies_ms": [6878.758], "images_per_second": 0.145, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image features a lush green field with a clear sky, indicating a sunny day. A tree with a rough bark texture is prominently visible on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24422.2, "ram_available_mb": 38418.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24420.7, "ram_available_mb": 38420.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.879}, "power_stats": {"power_gpu_soc_mean_watts": 22.342, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.047, "gpu_utilization_percent_mean": 73.879, "power_watts_avg": 22.342, "energy_joules_est": 153.7, "duration_seconds": 6.879, "sample_count": 58}, "timestamp": "2026-01-25T18:57:18.083323"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11178.989, "latencies_ms": [11178.989], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a black and white photograph of a large group of young boys, likely students, posing for a group photo. They are all dressed in formal attire, with some wearing ties. The boys are arranged in rows, with some sitting on the ground and others standing. The photograph appears to be from the early 20th century, as indicated by the style of cl", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 24420.7, "ram_available_mb": 38420.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24426.6, "ram_available_mb": 38414.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.884}, "power_stats": {"power_gpu_soc_mean_watts": 20.31, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 69.884, "power_watts_avg": 20.31, "energy_joules_est": 227.06, "duration_seconds": 11.18, "sample_count": 95}, "timestamp": "2026-01-25T18:57:31.313400"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11177.974, "latencies_ms": [11177.974], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 78, "n_tiles": 16, "output_text": "group of boys: 50, boys wearing ties: 30, boys wearing suits: 10, boys wearing sweaters: 10, boys wearing shorts: 10, boys wearing socks: 10, boys wearing shoes: 10, boys wearing hats: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.6, "ram_available_mb": 38414.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24442.3, "ram_available_mb": 38398.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.646}, "power_stats": {"power_gpu_soc_mean_watts": 21.023, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 8.917, "gpu_utilization_percent_mean": 69.646, "power_watts_avg": 21.023, "energy_joules_est": 235.01, "duration_seconds": 11.179, "sample_count": 96}, "timestamp": "2026-01-25T18:57:44.522046"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9785.473, "latencies_ms": [9785.473], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "In the image, the group of boys is arranged in two distinct formations. The larger group is seated in the foreground, with boys spaced out evenly across the ground, while the smaller group is standing in the background, positioned behind the seated boys, creating a clear spatial separation between the two groups.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24442.3, "ram_available_mb": 38398.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24460.1, "ram_available_mb": 38380.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.25}, "power_stats": {"power_gpu_soc_mean_watts": 21.252, "power_cpu_cv_mean_watts": 1.854, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 70.25, "power_watts_avg": 21.252, "energy_joules_est": 207.97, "duration_seconds": 9.786, "sample_count": 84}, "timestamp": "2026-01-25T18:57:56.334428"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8721.125, "latencies_ms": [8721.125], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image is a black and white photograph of a large group of students at Goodmayes Boys' School, taken in April 1929. The students are arranged in rows, with some standing and others sitting, and they are all dressed in formal attire.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24460.1, "ram_available_mb": 38380.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24432.7, "ram_available_mb": 38408.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.987}, "power_stats": {"power_gpu_soc_mean_watts": 21.848, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 70.987, "power_watts_avg": 21.848, "energy_joules_est": 190.55, "duration_seconds": 8.722, "sample_count": 75}, "timestamp": "2026-01-25T18:58:07.116704"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7990.301, "latencies_ms": [7990.301], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image is a black and white photograph, indicating it was taken in an era before color photography was common. The lighting is even, suggesting it was taken on a clear day, and the attire of the individuals suggests a formal occasion.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24432.7, "ram_available_mb": 38408.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24453.3, "ram_available_mb": 38387.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.299}, "power_stats": {"power_gpu_soc_mean_watts": 22.095, "power_cpu_cv_mean_watts": 1.727, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 71.299, "power_watts_avg": 22.095, "energy_joules_est": 176.56, "duration_seconds": 7.991, "sample_count": 67}, "timestamp": "2026-01-25T18:58:17.125518"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11167.737, "latencies_ms": [11167.737], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene of a kite soaring high in the sky. The kite, a striking combination of orange and blue, is tilted slightly to the left, adding a dynamic element to the composition. It's flying over a lush green field, which is dotted with trees and buildings in the background. The sky above is a clear blue,", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24453.3, "ram_available_mb": 38387.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24431.5, "ram_available_mb": 38409.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.232}, "power_stats": {"power_gpu_soc_mean_watts": 20.207, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.916, "gpu_utilization_percent_mean": 70.232, "power_watts_avg": 20.207, "energy_joules_est": 225.68, "duration_seconds": 11.168, "sample_count": 95}, "timestamp": "2026-01-25T18:58:30.333780"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6416.124, "latencies_ms": [6416.124], "images_per_second": 0.156, "prompt_tokens": 39, "response_tokens_est": 35, "n_tiles": 16, "output_text": "kite: 1, cloud: multiple, tree: multiple, building: multiple, grass: multiple, sky: multiple, person: 1, car: multiple", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.5, "ram_available_mb": 38409.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24426.2, "ram_available_mb": 38414.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.759}, "power_stats": {"power_gpu_soc_mean_watts": 23.372, "power_cpu_cv_mean_watts": 1.475, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 75.759, "power_watts_avg": 23.372, "energy_joules_est": 149.97, "duration_seconds": 6.417, "sample_count": 54}, "timestamp": "2026-01-25T18:58:38.788421"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11113.062, "latencies_ms": [11113.062], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The kite is flying in the sky, positioned in the upper right quadrant of the image, while the buildings are in the background, located in the lower left quadrant. The trees are situated between the buildings and the kite, creating a sense of depth in the image. The kite appears to be flying closer to the viewer, while the buildings seem further away.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24426.2, "ram_available_mb": 38414.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24431.0, "ram_available_mb": 38409.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.642}, "power_stats": {"power_gpu_soc_mean_watts": 20.979, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 69.642, "power_watts_avg": 20.979, "energy_joules_est": 233.15, "duration_seconds": 11.114, "sample_count": 95}, "timestamp": "2026-01-25T18:58:51.932986"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7446.157, "latencies_ms": [7446.157], "images_per_second": 0.134, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A colorful kite is flying high in the sky above a park with trees and buildings in the background. The kite appears to be a rainbow-colored butterfly or dragonfly design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.0, "ram_available_mb": 38409.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24431.3, "ram_available_mb": 38409.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.159}, "power_stats": {"power_gpu_soc_mean_watts": 22.082, "power_cpu_cv_mean_watts": 1.62, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 74.159, "power_watts_avg": 22.082, "energy_joules_est": 164.44, "duration_seconds": 7.447, "sample_count": 63}, "timestamp": "2026-01-25T18:59:01.394787"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8998.045, "latencies_ms": [8998.045], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The kite in the image has a gradient of colors, transitioning from red at the tip to blue at the tail, with a hint of orange near the tail. It is flying high in the sky with a backdrop of a partly cloudy blue sky and fluffy white clouds.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24431.3, "ram_available_mb": 38409.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24432.7, "ram_available_mb": 38408.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.5}, "power_stats": {"power_gpu_soc_mean_watts": 21.64, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 70.5, "power_watts_avg": 21.64, "energy_joules_est": 194.73, "duration_seconds": 8.999, "sample_count": 76}, "timestamp": "2026-01-25T18:59:12.429523"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11103.672, "latencies_ms": [11103.672], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of anticipation, just before the first slice of a freshly baked pizza is taken. The pizza, with its golden brown crust, sits in a cardboard box, its surface adorned with a generous layer of melted cheese that has turned a light golden color. The cheese is speckled with red pepper", "error": null, "sys_before": {"cpu_percent": 10.8, "ram_used_mb": 24432.7, "ram_available_mb": 38408.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24439.5, "ram_available_mb": 38401.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.66}, "power_stats": {"power_gpu_soc_mean_watts": 20.969, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 69.66, "power_watts_avg": 20.969, "energy_joules_est": 232.85, "duration_seconds": 11.104, "sample_count": 94}, "timestamp": "2026-01-25T18:59:25.584253"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8187.038, "latencies_ms": [8187.038], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "pizza: 1\nbox: 1\ncheese: 1\ntomato sauce: 1\npepperoni: 1\nmushroom: 1\nolive: 1\nbasil: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.5, "ram_available_mb": 38401.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24427.8, "ram_available_mb": 38413.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.87}, "power_stats": {"power_gpu_soc_mean_watts": 22.06, "power_cpu_cv_mean_watts": 1.688, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 71.87, "power_watts_avg": 22.06, "energy_joules_est": 180.62, "duration_seconds": 8.188, "sample_count": 69}, "timestamp": "2026-01-25T18:59:35.785874"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11118.909, "latencies_ms": [11118.909], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The pizza is centrally located in the image, occupying the majority of the space. It is placed within a cardboard pizza box, which is positioned on a flat surface that appears to be a table or countertop. The pizza is in the foreground, making it the main focus of the image, while the background is less distinct but seems to be the edge of", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24427.8, "ram_available_mb": 38413.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24432.3, "ram_available_mb": 38408.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.505}, "power_stats": {"power_gpu_soc_mean_watts": 20.959, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.505, "power_watts_avg": 20.959, "energy_joules_est": 233.05, "duration_seconds": 11.12, "sample_count": 95}, "timestamp": "2026-01-25T18:59:48.933815"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7179.689, "latencies_ms": [7179.689], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A large pizza with melted cheese and tomato sauce is placed in a cardboard pizza box. The pizza appears to be freshly baked and ready to be served.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24432.3, "ram_available_mb": 38408.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24432.6, "ram_available_mb": 38408.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.8}, "power_stats": {"power_gpu_soc_mean_watts": 22.726, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 9.004, "gpu_utilization_percent_mean": 73.8, "power_watts_avg": 22.726, "energy_joules_est": 163.18, "duration_seconds": 7.18, "sample_count": 60}, "timestamp": "2026-01-25T18:59:58.155466"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9999.452, "latencies_ms": [9999.452], "images_per_second": 0.1, "prompt_tokens": 36, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The pizza in the image has a golden-brown crust with a generous amount of melted cheese on top, which is slightly browned in spots. It is placed in a cardboard pizza box, and the lighting in the image highlights the texture of the cheese and the crust.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.6, "ram_available_mb": 38408.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24433.2, "ram_available_mb": 38407.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.918}, "power_stats": {"power_gpu_soc_mean_watts": 21.233, "power_cpu_cv_mean_watts": 1.866, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 69.918, "power_watts_avg": 21.233, "energy_joules_est": 212.33, "duration_seconds": 10.0, "sample_count": 85}, "timestamp": "2026-01-25T19:00:10.176604"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11127.961, "latencies_ms": [11127.961], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two women are sitting on a refrigerator placed on the sidewalk. One woman is sitting on the left side of the refrigerator, while the other woman is sitting on the right side. They both appear to be enjoying their time together, possibly having a conversation or sharing a drink.\n\nThere are two cups visible in the scene, one near", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24433.2, "ram_available_mb": 38407.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24439.9, "ram_available_mb": 38401.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.309}, "power_stats": {"power_gpu_soc_mean_watts": 20.946, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 69.309, "power_watts_avg": 20.946, "energy_joules_est": 233.1, "duration_seconds": 11.129, "sample_count": 94}, "timestamp": "2026-01-25T19:00:23.355148"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8520.612, "latencies_ms": [8520.612], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Woman: 2\n- Refrigerator: 1\n- Beer: 2\n- Cup: 2\n- Street: 1\n- Sidewalk: 1\n- Pavement: 1\n- Chair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.9, "ram_available_mb": 38401.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24436.9, "ram_available_mb": 38404.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.194}, "power_stats": {"power_gpu_soc_mean_watts": 22.078, "power_cpu_cv_mean_watts": 1.696, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 72.194, "power_watts_avg": 22.078, "energy_joules_est": 188.14, "duration_seconds": 8.521, "sample_count": 72}, "timestamp": "2026-01-25T19:00:33.934638"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9217.967, "latencies_ms": [9217.967], "images_per_second": 0.108, "prompt_tokens": 44, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The woman sitting on the left is positioned in the foreground and is closer to the camera than the woman sitting in the fridge. The fridge is located on the right side of the image, near the curb, and is further away from the camera than the woman sitting on the left.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24436.9, "ram_available_mb": 38404.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.269}, "power_stats": {"power_gpu_soc_mean_watts": 21.595, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 71.269, "power_watts_avg": 21.595, "energy_joules_est": 199.08, "duration_seconds": 9.219, "sample_count": 78}, "timestamp": "2026-01-25T19:00:45.174371"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6892.068, "latencies_ms": [6892.068], "images_per_second": 0.145, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "Two women are sitting on a small refrigerator placed on the sidewalk. One woman is talking on her cell phone while the other woman is sitting inside the refrigerator.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24439.8, "ram_available_mb": 38401.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.814}, "power_stats": {"power_gpu_soc_mean_watts": 22.748, "power_cpu_cv_mean_watts": 1.568, "power_sys_5v0_mean_watts": 9.009, "gpu_utilization_percent_mean": 73.814, "power_watts_avg": 22.748, "energy_joules_est": 156.8, "duration_seconds": 6.893, "sample_count": 59}, "timestamp": "2026-01-25T19:00:54.081038"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7007.511, "latencies_ms": [7007.511], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image shows a sunny day with clear skies, as indicated by the bright lighting and shadows cast on the ground. The weather appears to be mild, suitable for outdoor activities.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 24439.8, "ram_available_mb": 38401.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24432.3, "ram_available_mb": 38408.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.153}, "power_stats": {"power_gpu_soc_mean_watts": 22.616, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 9.059, "gpu_utilization_percent_mean": 73.153, "power_watts_avg": 22.616, "energy_joules_est": 158.5, "duration_seconds": 7.008, "sample_count": 59}, "timestamp": "2026-01-25T19:01:03.120064"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11119.67, "latencies_ms": [11119.67], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man wearing a hat is sitting at a dining table, enjoying a meal of hot dogs. There are several hot dogs on the table, with some placed in buns and others laid out on a tray. The man appears to be focused on the food, possibly taking a bite or preparing to eat. The scene suggests a casual outdoor", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24432.3, "ram_available_mb": 38408.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24457.1, "ram_available_mb": 38383.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.32}, "power_stats": {"power_gpu_soc_mean_watts": 20.832, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.32, "power_watts_avg": 20.832, "energy_joules_est": 231.66, "duration_seconds": 11.12, "sample_count": 97}, "timestamp": "2026-01-25T19:01:16.296775"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8502.87, "latencies_ms": [8502.87], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Hot dog: 10\n- Bun: 10\n- Man: 1\n- Hat: 1\n- Chair: 1\n- Tray: 1\n- Grass: 1\n- Grill: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24457.1, "ram_available_mb": 38383.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24427.6, "ram_available_mb": 38413.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.597}, "power_stats": {"power_gpu_soc_mean_watts": 21.998, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 71.597, "power_watts_avg": 21.998, "energy_joules_est": 187.06, "duration_seconds": 8.504, "sample_count": 72}, "timestamp": "2026-01-25T19:01:26.817522"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9191.404, "latencies_ms": [9191.404], "images_per_second": 0.109, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "In the foreground, there is a tray of hot dogs with one being held by a person. The person is seated to the right of the tray, wearing a green shirt and a straw hat. The background shows a grassy area, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24427.6, "ram_available_mb": 38413.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24447.3, "ram_available_mb": 38393.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.269}, "power_stats": {"power_gpu_soc_mean_watts": 21.676, "power_cpu_cv_mean_watts": 1.776, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 71.269, "power_watts_avg": 21.676, "energy_joules_est": 199.25, "duration_seconds": 9.192, "sample_count": 78}, "timestamp": "2026-01-25T19:01:38.050357"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8868.965, "latencies_ms": [8868.965], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "A man wearing a straw hat and a green polo shirt is sitting at a table with a tray of hot dogs in front of him. He appears to be enjoying a meal outdoors, possibly at a picnic or a barbecue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24447.3, "ram_available_mb": 38393.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24571.4, "ram_available_mb": 38269.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.44}, "power_stats": {"power_gpu_soc_mean_watts": 21.817, "power_cpu_cv_mean_watts": 1.751, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 71.44, "power_watts_avg": 21.817, "energy_joules_est": 193.51, "duration_seconds": 8.87, "sample_count": 75}, "timestamp": "2026-01-25T19:01:48.967260"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9259.592, "latencies_ms": [9259.592], "images_per_second": 0.108, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image shows a person wearing a straw hat and a green polo shirt, sitting in a white chair outdoors. The person is holding a tray with several hot dogs, some of which have red and some with black toppings, on a foil-lined tray.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24571.4, "ram_available_mb": 38269.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24433.1, "ram_available_mb": 38407.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.228}, "power_stats": {"power_gpu_soc_mean_watts": 21.569, "power_cpu_cv_mean_watts": 1.804, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 71.228, "power_watts_avg": 21.569, "energy_joules_est": 199.73, "duration_seconds": 9.26, "sample_count": 79}, "timestamp": "2026-01-25T19:02:00.244864"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11133.216, "latencies_ms": [11133.216], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a small, cluttered room with a desk and a chair. On the desk, there is a laptop computer, a book, and a mouse. The chair is positioned in front of the desk, providing a comfortable seating area for the user. \n\nIn the room, there are numerous books scattered around, with some placed on the des", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24433.1, "ram_available_mb": 38407.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 24435.6, "ram_available_mb": 38405.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.372}, "power_stats": {"power_gpu_soc_mean_watts": 20.865, "power_cpu_cv_mean_watts": 1.955, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 69.372, "power_watts_avg": 20.865, "energy_joules_est": 232.31, "duration_seconds": 11.134, "sample_count": 94}, "timestamp": "2026-01-25T19:02:13.405189"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9833.496, "latencies_ms": [9833.496], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "- Chair: 1\n- Book: multiple (exact count not possible)\n- Laptop: 1\n- Star decoration: 1\n- Cords: multiple (exact count not possible)\n- Fan: 1\n- Couch: 1\n- Bed: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24435.6, "ram_available_mb": 38405.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24423.6, "ram_available_mb": 38417.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.274}, "power_stats": {"power_gpu_soc_mean_watts": 21.396, "power_cpu_cv_mean_watts": 1.83, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 70.274, "power_watts_avg": 21.396, "energy_joules_est": 210.41, "duration_seconds": 9.834, "sample_count": 84}, "timestamp": "2026-01-25T19:02:25.298571"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11127.31, "latencies_ms": [11127.31], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden desk with a laptop and a chair positioned to the left side of the desk. The chair is in front of the desk, suggesting it is used for sitting while working at the desk. In the background, there is a bookshelf filled with books, and a white radiator is located to the right of the bookshelf", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.6, "ram_available_mb": 38417.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24431.0, "ram_available_mb": 38409.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.191}, "power_stats": {"power_gpu_soc_mean_watts": 20.943, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.191, "power_watts_avg": 20.943, "energy_joules_est": 233.05, "duration_seconds": 11.128, "sample_count": 94}, "timestamp": "2026-01-25T19:02:38.441218"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8573.663, "latencies_ms": [8573.663], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a cluttered room with a desk, a chair, and a bookshelf filled with books. There is a laptop on the desk, and various items scattered around the room, including a backpack and a radiator.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24431.0, "ram_available_mb": 38409.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24423.7, "ram_available_mb": 38417.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.431}, "power_stats": {"power_gpu_soc_mean_watts": 21.273, "power_cpu_cv_mean_watts": 1.712, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 73.431, "power_watts_avg": 21.273, "energy_joules_est": 182.4, "duration_seconds": 8.574, "sample_count": 72}, "timestamp": "2026-01-25T19:02:49.039992"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7671.032, "latencies_ms": [7671.032], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The room is dimly lit with natural light coming from the window on the left. The walls are painted in a light beige color, and the furniture includes a wooden desk and a bookshelf filled with various books.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 24423.7, "ram_available_mb": 38417.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24433.9, "ram_available_mb": 38407.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.266}, "power_stats": {"power_gpu_soc_mean_watts": 22.236, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 72.266, "power_watts_avg": 22.236, "energy_joules_est": 170.59, "duration_seconds": 7.672, "sample_count": 64}, "timestamp": "2026-01-25T19:02:58.729404"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11143.372, "latencies_ms": [11143.372], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a verdant landscape, two majestic elephants stand in a field of tall grass. The elephant on the left, with its dark brown skin, is facing the camera, its trunk extended towards the ground as if exploring the terrain. Its companion, on the right, is facing away from the camera, its trunk raised high in the air, perhaps", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24433.9, "ram_available_mb": 38407.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24445.5, "ram_available_mb": 38395.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.426}, "power_stats": {"power_gpu_soc_mean_watts": 20.863, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.426, "power_watts_avg": 20.863, "energy_joules_est": 232.5, "duration_seconds": 11.144, "sample_count": 94}, "timestamp": "2026-01-25T19:03:11.910548"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6818.112, "latencies_ms": [6818.112], "images_per_second": 0.147, "prompt_tokens": 39, "response_tokens_est": 39, "n_tiles": 16, "output_text": "elephant: 2, grass: numerous, trees: scattered, sky: hazy, clouds: visible, sun: not visible, water: not visible, birds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24445.5, "ram_available_mb": 38395.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24450.0, "ram_available_mb": 38390.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.155}, "power_stats": {"power_gpu_soc_mean_watts": 22.944, "power_cpu_cv_mean_watts": 1.56, "power_sys_5v0_mean_watts": 9.015, "gpu_utilization_percent_mean": 74.155, "power_watts_avg": 22.944, "energy_joules_est": 156.45, "duration_seconds": 6.819, "sample_count": 58}, "timestamp": "2026-01-25T19:03:20.764374"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9857.115, "latencies_ms": [9857.115], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The two elephants are positioned in the background of the image, standing close to each other in a grassy field. They appear to be near the center of the image, with trees and shrubs in the foreground. The elephants are farther away from the camera compared to the vegetation in the foreground.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24450.0, "ram_available_mb": 38390.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24450.0, "ram_available_mb": 38390.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.224}, "power_stats": {"power_gpu_soc_mean_watts": 21.37, "power_cpu_cv_mean_watts": 1.856, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 70.224, "power_watts_avg": 21.37, "energy_joules_est": 210.66, "duration_seconds": 9.858, "sample_count": 85}, "timestamp": "2026-01-25T19:03:32.636407"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6267.132, "latencies_ms": [6267.132], "images_per_second": 0.16, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "Two elephants are standing in a grassy field with trees in the background. They appear to be interacting with each other, possibly playing or communicating.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24450.0, "ram_available_mb": 38390.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24455.6, "ram_available_mb": 38385.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.151}, "power_stats": {"power_gpu_soc_mean_watts": 23.468, "power_cpu_cv_mean_watts": 1.495, "power_sys_5v0_mean_watts": 9.054, "gpu_utilization_percent_mean": 74.151, "power_watts_avg": 23.468, "energy_joules_est": 147.09, "duration_seconds": 6.268, "sample_count": 53}, "timestamp": "2026-01-25T19:03:40.919153"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7012.13, "latencies_ms": [7012.13], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "Two elephants are standing in a grassy field with green vegetation around them. The sky is overcast, and the lighting is soft, giving the scene a calm and serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24455.6, "ram_available_mb": 38385.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24436.5, "ram_available_mb": 38404.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.407}, "power_stats": {"power_gpu_soc_mean_watts": 22.513, "power_cpu_cv_mean_watts": 1.628, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 72.407, "power_watts_avg": 22.513, "energy_joules_est": 157.88, "duration_seconds": 7.013, "sample_count": 59}, "timestamp": "2026-01-25T19:03:49.958126"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11135.314, "latencies_ms": [11135.314], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing in a grassy field, holding a Frisbee in his right hand. He is wearing a baseball cap and appears to be preparing to throw the Frisbee. Another person is visible in the background, standing further away from the main subject. The scene suggests that the man is participating in a game of Frisbee or", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24436.5, "ram_available_mb": 38404.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24429.5, "ram_available_mb": 38411.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.926}, "power_stats": {"power_gpu_soc_mean_watts": 20.9, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.927, "gpu_utilization_percent_mean": 68.926, "power_watts_avg": 20.9, "energy_joules_est": 232.74, "duration_seconds": 11.136, "sample_count": 95}, "timestamp": "2026-01-25T19:04:03.149089"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7173.665, "latencies_ms": [7173.665], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "person: 2, frisbee: 1, bottle: 1, trees: many, grass: field, sky: clear, sun: visible, shirtless: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24429.5, "ram_available_mb": 38411.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24432.4, "ram_available_mb": 38408.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.35}, "power_stats": {"power_gpu_soc_mean_watts": 22.913, "power_cpu_cv_mean_watts": 1.561, "power_sys_5v0_mean_watts": 9.014, "gpu_utilization_percent_mean": 74.35, "power_watts_avg": 22.913, "energy_joules_est": 164.38, "duration_seconds": 7.174, "sample_count": 60}, "timestamp": "2026-01-25T19:04:12.334103"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11128.081, "latencies_ms": [11128.081], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, there is a man standing with his left arm extended, holding a frisbee in his right hand. He is wearing a cap, sunglasses, and shorts. In the background, there is another person who appears to be running or walking towards the left side of the image. The background is filled with trees and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24432.4, "ram_available_mb": 38408.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24433.4, "ram_available_mb": 38407.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.863}, "power_stats": {"power_gpu_soc_mean_watts": 20.907, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.863, "power_watts_avg": 20.907, "energy_joules_est": 232.67, "duration_seconds": 11.129, "sample_count": 95}, "timestamp": "2026-01-25T19:04:25.474458"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6732.636, "latencies_ms": [6732.636], "images_per_second": 0.149, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A man is standing in a grassy field, throwing a frisbee while another person is in the background. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.4, "ram_available_mb": 38407.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24423.6, "ram_available_mb": 38417.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.018}, "power_stats": {"power_gpu_soc_mean_watts": 23.026, "power_cpu_cv_mean_watts": 1.53, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 74.018, "power_watts_avg": 23.026, "energy_joules_est": 155.04, "duration_seconds": 6.733, "sample_count": 56}, "timestamp": "2026-01-25T19:04:34.220458"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8656.104, "latencies_ms": [8656.104], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image shows a sunny day with clear blue skies and bright sunlight casting shadows on the ground. The man in the foreground is wearing a white cap and sunglasses, and he is shirtless, revealing a muscular torso.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24423.6, "ram_available_mb": 38417.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24426.2, "ram_available_mb": 38414.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.932}, "power_stats": {"power_gpu_soc_mean_watts": 21.773, "power_cpu_cv_mean_watts": 1.766, "power_sys_5v0_mean_watts": 9.029, "gpu_utilization_percent_mean": 70.932, "power_watts_avg": 21.773, "energy_joules_est": 188.48, "duration_seconds": 8.657, "sample_count": 73}, "timestamp": "2026-01-25T19:04:44.919701"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11164.125, "latencies_ms": [11164.125], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young boy is sitting at a dining table, cutting a cake with a knife. The cake is placed on a plate, and the boy is focused on cutting it. The table is covered with a dining tablecloth, and there are several cups and a knife on the table. The boy is wearing a blue shirt, and he", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24426.2, "ram_available_mb": 38414.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24451.4, "ram_available_mb": 38389.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.713}, "power_stats": {"power_gpu_soc_mean_watts": 20.872, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.713, "power_watts_avg": 20.872, "energy_joules_est": 233.03, "duration_seconds": 11.165, "sample_count": 94}, "timestamp": "2026-01-25T19:04:58.103988"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10357.794, "latencies_ms": [10357.794], "images_per_second": 0.097, "prompt_tokens": 39, "response_tokens_est": 70, "n_tiles": 16, "output_text": "- Cake: 1\n\n- Knife: 1\n\n- Frosting: 1\n\n- Plates: 2\n\n- Tablecloth: 1\n\n- Cake pan: 1\n\n- Cake decorations: 1\n\n- Cake topper: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24451.4, "ram_available_mb": 38389.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24444.5, "ram_available_mb": 38396.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.034}, "power_stats": {"power_gpu_soc_mean_watts": 21.229, "power_cpu_cv_mean_watts": 1.849, "power_sys_5v0_mean_watts": 8.897, "gpu_utilization_percent_mean": 70.034, "power_watts_avg": 21.229, "energy_joules_est": 219.9, "duration_seconds": 10.358, "sample_count": 89}, "timestamp": "2026-01-25T19:05:10.498968"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9477.89, "latencies_ms": [9477.89], "images_per_second": 0.106, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "In the foreground, a child in a blue shirt is cutting a chocolate cake with a knife. The cake is placed on a colorful tablecloth that covers the table. The child is seated at the table, and the cake is positioned in front of them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24444.5, "ram_available_mb": 38396.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24453.4, "ram_available_mb": 38387.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.725}, "power_stats": {"power_gpu_soc_mean_watts": 21.56, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 71.725, "power_watts_avg": 21.56, "energy_joules_est": 204.36, "duration_seconds": 9.478, "sample_count": 80}, "timestamp": "2026-01-25T19:05:22.011051"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7210.102, "latencies_ms": [7210.102], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A young boy in a blue sports jersey is cutting a chocolate cake with a knife. The cake is decorated with a baseball theme, featuring a baseball bat and ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24453.4, "ram_available_mb": 38387.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24447.3, "ram_available_mb": 38393.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.016}, "power_stats": {"power_gpu_soc_mean_watts": 22.779, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 73.016, "power_watts_avg": 22.779, "energy_joules_est": 164.26, "duration_seconds": 7.211, "sample_count": 61}, "timestamp": "2026-01-25T19:05:31.251490"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9360.285, "latencies_ms": [9360.285], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image shows a person wearing a blue sports jersey with a white logo, cutting a chocolate cake on a table covered with a colorful tablecloth. The cake is decorated to look like a chocolate bar with pieces of chocolate and other decorations.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24447.3, "ram_available_mb": 38393.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24452.4, "ram_available_mb": 38388.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.595}, "power_stats": {"power_gpu_soc_mean_watts": 21.584, "power_cpu_cv_mean_watts": 1.809, "power_sys_5v0_mean_watts": 8.986, "gpu_utilization_percent_mean": 71.595, "power_watts_avg": 21.584, "energy_joules_est": 202.05, "duration_seconds": 9.361, "sample_count": 79}, "timestamp": "2026-01-25T19:05:42.630423"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11114.139, "latencies_ms": [11114.139], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two zebras standing close to each other, with one zebra partially visible on the left side and the other zebra fully visible on the right side. They are standing near a metal fence, which is likely part of their enclosure. The zebras appear to be in a zoo or wildlife park setting, as they are in a conf", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 24452.4, "ram_available_mb": 38388.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24441.7, "ram_available_mb": 38399.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.628}, "power_stats": {"power_gpu_soc_mean_watts": 20.981, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.924, "gpu_utilization_percent_mean": 69.628, "power_watts_avg": 20.981, "energy_joules_est": 233.2, "duration_seconds": 11.115, "sample_count": 94}, "timestamp": "2026-01-25T19:05:55.771842"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9011.821, "latencies_ms": [9011.821], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 58, "n_tiles": 16, "output_text": "zebra: 2, pipe: 1, rock: 1, leaves: 1, background: 1, zebra's mane: 1, zebra's eye: 1, zebra's ear: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24441.7, "ram_available_mb": 38399.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 24432.0, "ram_available_mb": 38408.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.908}, "power_stats": {"power_gpu_soc_mean_watts": 21.408, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 72.908, "power_watts_avg": 21.408, "energy_joules_est": 192.94, "duration_seconds": 9.012, "sample_count": 76}, "timestamp": "2026-01-25T19:06:06.801502"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9889.65, "latencies_ms": [9889.65], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The zebra on the left is in the foreground and appears to be facing the camera, while the zebra on the right is slightly behind and to the right, partially obscured by the zebra in the foreground. The background is blurred but seems to be a natural environment with trees and rocks.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24432.0, "ram_available_mb": 38408.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24432.8, "ram_available_mb": 38408.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.19}, "power_stats": {"power_gpu_soc_mean_watts": 21.357, "power_cpu_cv_mean_watts": 1.849, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 70.19, "power_watts_avg": 21.357, "energy_joules_est": 211.23, "duration_seconds": 9.89, "sample_count": 84}, "timestamp": "2026-01-25T19:06:18.740686"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7876.395, "latencies_ms": [7876.395], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "Two zebras are standing close to each other, with one facing the camera and the other partially visible in the background. They appear to be in a zoo enclosure, as there is a metal fence visible in the image.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24432.8, "ram_available_mb": 38408.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24431.7, "ram_available_mb": 38409.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.075}, "power_stats": {"power_gpu_soc_mean_watts": 22.216, "power_cpu_cv_mean_watts": 1.667, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 72.075, "power_watts_avg": 22.216, "energy_joules_est": 175.0, "duration_seconds": 7.877, "sample_count": 67}, "timestamp": "2026-01-25T19:06:28.658275"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7440.709, "latencies_ms": [7440.709], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The zebra's stripes are black and white, and the lighting appears to be natural sunlight. The zebra is standing next to a metal pole, and there is a rock in the background.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 24431.7, "ram_available_mb": 38409.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24427.8, "ram_available_mb": 38413.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.381}, "power_stats": {"power_gpu_soc_mean_watts": 22.341, "power_cpu_cv_mean_watts": 1.671, "power_sys_5v0_mean_watts": 9.023, "gpu_utilization_percent_mean": 72.381, "power_watts_avg": 22.341, "energy_joules_est": 166.25, "duration_seconds": 7.441, "sample_count": 63}, "timestamp": "2026-01-25T19:06:38.152055"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11129.199, "latencies_ms": [11129.199], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment at the La Spezia Centrale train station in Italy. The station, bathed in the soft glow of daylight, is a symphony of grayscale hues. The platform, constructed of brick, stretches out in the foreground, leading the eye towards the train tracks that disappear into the distance. \n\nTwo trains, one on each", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24427.8, "ram_available_mb": 38413.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24437.2, "ram_available_mb": 38403.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.326}, "power_stats": {"power_gpu_soc_mean_watts": 20.917, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 69.326, "power_watts_avg": 20.917, "energy_joules_est": 232.8, "duration_seconds": 11.13, "sample_count": 95}, "timestamp": "2026-01-25T19:06:51.307708"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7288.783, "latencies_ms": [7288.783], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "platform: 1, bench: 1, train: 1, train tracks: 2, train car: 1, building: 1, sign: 1, mountains: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24437.2, "ram_available_mb": 38403.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24435.5, "ram_available_mb": 38405.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.377}, "power_stats": {"power_gpu_soc_mean_watts": 22.629, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 73.377, "power_watts_avg": 22.629, "energy_joules_est": 164.95, "duration_seconds": 7.289, "sample_count": 61}, "timestamp": "2026-01-25T19:07:00.634593"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11122.091, "latencies_ms": [11122.091], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The train tracks extend from the foreground into the background, converging towards the center of the image where a train is positioned on the right side, suggesting it is moving away from the viewer. A bench is placed in the foreground on the left side, indicating it is closer to the viewer than the train. The overhead structure spans across the image, connecting the two ends", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24435.5, "ram_available_mb": 38405.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24444.3, "ram_available_mb": 38396.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.851}, "power_stats": {"power_gpu_soc_mean_watts": 20.986, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 69.851, "power_watts_avg": 20.986, "energy_joules_est": 233.43, "duration_seconds": 11.123, "sample_count": 94}, "timestamp": "2026-01-25T19:07:13.774313"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7253.138, "latencies_ms": [7253.138], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image depicts a train station named \"La Spezia Centrale\" with a train on the tracks and a bench on the platform. The station appears to be quiet with no visible passengers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24444.3, "ram_available_mb": 38396.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24435.5, "ram_available_mb": 38405.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.033}, "power_stats": {"power_gpu_soc_mean_watts": 21.986, "power_cpu_cv_mean_watts": 1.575, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 73.033, "power_watts_avg": 21.986, "energy_joules_est": 159.48, "duration_seconds": 7.254, "sample_count": 60}, "timestamp": "2026-01-25T19:07:23.063811"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5215.473, "latencies_ms": [5215.473], "images_per_second": 0.192, "prompt_tokens": 36, "response_tokens_est": 26, "n_tiles": 16, "output_text": "The image is a black and white photograph of a train station. The station has a brick platform and a metal roof structure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.5, "ram_available_mb": 38405.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24429.4, "ram_available_mb": 38411.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.341}, "power_stats": {"power_gpu_soc_mean_watts": 24.328, "power_cpu_cv_mean_watts": 1.356, "power_sys_5v0_mean_watts": 9.143, "gpu_utilization_percent_mean": 75.341, "power_watts_avg": 24.328, "energy_joules_est": 126.9, "duration_seconds": 5.216, "sample_count": 44}, "timestamp": "2026-01-25T19:07:30.327338"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11133.809, "latencies_ms": [11133.809], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is sitting on a red surfboard in the middle of the ocean. The surfer is wearing a black wetsuit and is facing away from the camera, looking out at the horizon. The sky is filled with dark clouds, suggesting an impending storm. The ocean is calm, with a few small waves visible. The surfer appears to be waiting for", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24429.4, "ram_available_mb": 38411.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24436.3, "ram_available_mb": 38404.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.274}, "power_stats": {"power_gpu_soc_mean_watts": 20.874, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 69.274, "power_watts_avg": 20.874, "energy_joules_est": 232.42, "duration_seconds": 11.134, "sample_count": 95}, "timestamp": "2026-01-25T19:07:43.513987"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6728.772, "latencies_ms": [6728.772], "images_per_second": 0.149, "prompt_tokens": 39, "response_tokens_est": 38, "n_tiles": 16, "output_text": "person: 1, surfboard: 1, ocean: multiple, waves: multiple, sky: multiple, clouds: multiple, sun: 1, horizon: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.3, "ram_available_mb": 38404.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24432.4, "ram_available_mb": 38408.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.807}, "power_stats": {"power_gpu_soc_mean_watts": 23.155, "power_cpu_cv_mean_watts": 1.531, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 74.807, "power_watts_avg": 23.155, "energy_joules_est": 155.82, "duration_seconds": 6.729, "sample_count": 57}, "timestamp": "2026-01-25T19:07:52.296546"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9911.473, "latencies_ms": [9911.473], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The person is seated on a surfboard in the foreground, positioned near the water's edge. The ocean extends towards the horizon, with the sky above and the beach in the far background. The clouds are closer to the viewer, creating a sense of depth as they appear to be floating above the water.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24432.4, "ram_available_mb": 38408.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24429.2, "ram_available_mb": 38411.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.047}, "power_stats": {"power_gpu_soc_mean_watts": 21.28, "power_cpu_cv_mean_watts": 1.86, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 70.047, "power_watts_avg": 21.28, "energy_joules_est": 210.93, "duration_seconds": 9.912, "sample_count": 85}, "timestamp": "2026-01-25T19:08:04.257281"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8305.515, "latencies_ms": [8305.515], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A person is sitting on a red surfboard in the ocean, facing away from the camera, with a dramatic cloudy sky in the background. The sun is setting, casting a warm glow on the horizon and creating a serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.2, "ram_available_mb": 38411.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24431.4, "ram_available_mb": 38409.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.139, "power_cpu_cv_mean_watts": 1.727, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 72.0, "power_watts_avg": 22.139, "energy_joules_est": 183.89, "duration_seconds": 8.306, "sample_count": 70}, "timestamp": "2026-01-25T19:08:14.576287"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9084.394, "latencies_ms": [9084.394], "images_per_second": 0.11, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image features a surfer in a black wetsuit sitting on a red surfboard, with the ocean in the background under a cloudy sky. The lighting is dim, with the sun setting or rising, casting a warm glow on the horizon and creating a moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24431.4, "ram_available_mb": 38409.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24434.2, "ram_available_mb": 38406.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.909}, "power_stats": {"power_gpu_soc_mean_watts": 21.559, "power_cpu_cv_mean_watts": 1.82, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 70.909, "power_watts_avg": 21.559, "energy_joules_est": 195.86, "duration_seconds": 9.085, "sample_count": 77}, "timestamp": "2026-01-25T19:08:25.698641"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11121.309, "latencies_ms": [11121.309], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man and a woman are sitting together on a train, enjoying a meal. The man is holding chopsticks and a plate of food, while the woman is also holding a plate of food. They are both smiling and appear to be having a pleasant time.\n\nThe train has several chairs, with one near the man and woman, and another", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 24434.2, "ram_available_mb": 38406.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24448.6, "ram_available_mb": 38392.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.904}, "power_stats": {"power_gpu_soc_mean_watts": 20.935, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 69.904, "power_watts_avg": 20.935, "energy_joules_est": 232.84, "duration_seconds": 11.122, "sample_count": 94}, "timestamp": "2026-01-25T19:08:38.854488"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9650.318, "latencies_ms": [9650.318], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "1. Chair: 2\n2. Tray: 1\n3. Chopsticks: 2\n4. Food items: 5\n5. Train car: 1\n6. Window: 1\n7. Curtain: 1\n8. Sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24448.6, "ram_available_mb": 38392.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24574.7, "ram_available_mb": 38266.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.512}, "power_stats": {"power_gpu_soc_mean_watts": 21.62, "power_cpu_cv_mean_watts": 1.792, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 71.512, "power_watts_avg": 21.62, "energy_joules_est": 208.65, "duration_seconds": 9.651, "sample_count": 82}, "timestamp": "2026-01-25T19:08:50.561907"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 6781.138, "latencies_ms": [6781.138], "images_per_second": 0.147, "prompt_tokens": 44, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The man is seated to the left of the woman, and they are both in the foreground of the image. In the background, there is a train station with various signs and structures.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24574.7, "ram_available_mb": 38266.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24522.5, "ram_available_mb": 38318.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.421}, "power_stats": {"power_gpu_soc_mean_watts": 23.113, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 9.077, "gpu_utilization_percent_mean": 74.421, "power_watts_avg": 23.113, "energy_joules_est": 156.75, "duration_seconds": 6.782, "sample_count": 57}, "timestamp": "2026-01-25T19:08:59.382903"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6973.413, "latencies_ms": [6973.413], "images_per_second": 0.143, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A man and a woman are sitting in a train, enjoying a meal together. The woman is holding chopsticks and the man is holding a tray with various food items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.5, "ram_available_mb": 38398.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24443.3, "ram_available_mb": 38397.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.441}, "power_stats": {"power_gpu_soc_mean_watts": 22.98, "power_cpu_cv_mean_watts": 1.561, "power_sys_5v0_mean_watts": 9.018, "gpu_utilization_percent_mean": 74.441, "power_watts_avg": 22.98, "energy_joules_est": 160.26, "duration_seconds": 6.974, "sample_count": 59}, "timestamp": "2026-01-25T19:09:08.372661"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11173.34, "latencies_ms": [11173.34], "images_per_second": 0.089, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a man and a woman seated in a train, with the man wearing a plaid shirt and the woman in a pink top. They are both holding chopsticks and appear to be eating, with a tray of food in front of them that includes sushi and other items. The train interior is well-lit, and the seats are u", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24443.3, "ram_available_mb": 38397.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24428.5, "ram_available_mb": 38412.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.789}, "power_stats": {"power_gpu_soc_mean_watts": 20.958, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.789, "power_watts_avg": 20.958, "energy_joules_est": 234.18, "duration_seconds": 11.174, "sample_count": 95}, "timestamp": "2026-01-25T19:09:21.589683"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11147.03, "latencies_ms": [11147.03], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two men are walking down a sidewalk at night. One man is wearing a white shirt and black pants, while the other is wearing a pink shirt and black pants. They are both dressed in business attire, with ties and dress shoes. The sidewalk is lined with metal poles and there are cars parked on the", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24428.5, "ram_available_mb": 38412.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24434.3, "ram_available_mb": 38406.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.421}, "power_stats": {"power_gpu_soc_mean_watts": 20.852, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.421, "power_watts_avg": 20.852, "energy_joules_est": 232.45, "duration_seconds": 11.148, "sample_count": 95}, "timestamp": "2026-01-25T19:09:34.806358"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9078.604, "latencies_ms": [9078.604], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- People: 2\n\n- Pole: 2\n\n- Building: 1\n\n- Sign: 1\n\n- Car: 1\n\n- Street light: 1\n\n- Sidewalk: 1\n\n- Stairs: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24434.3, "ram_available_mb": 38406.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24434.3, "ram_available_mb": 38406.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.519}, "power_stats": {"power_gpu_soc_mean_watts": 21.821, "power_cpu_cv_mean_watts": 1.752, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 72.519, "power_watts_avg": 21.821, "energy_joules_est": 198.12, "duration_seconds": 9.079, "sample_count": 77}, "timestamp": "2026-01-25T19:09:45.898281"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7763.489, "latencies_ms": [7763.489], "images_per_second": 0.129, "prompt_tokens": 44, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The two men are walking on a sidewalk in the foreground of the image. The building with the sign 'Hierro Y Albero' is in the background, and there are street lamps and cars visible in the distance.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24434.3, "ram_available_mb": 38406.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24429.8, "ram_available_mb": 38411.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.561}, "power_stats": {"power_gpu_soc_mean_watts": 22.218, "power_cpu_cv_mean_watts": 1.698, "power_sys_5v0_mean_watts": 9.037, "gpu_utilization_percent_mean": 71.561, "power_watts_avg": 22.218, "energy_joules_est": 172.5, "duration_seconds": 7.764, "sample_count": 66}, "timestamp": "2026-01-25T19:09:55.719265"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8741.443, "latencies_ms": [8741.443], "images_per_second": 0.114, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "Two men are walking on a sidewalk at night, one wearing a white shirt and black pants, the other wearing a pink shirt and black pants. They appear to be in a city setting with buildings and a street lamp in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24429.8, "ram_available_mb": 38411.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24437.2, "ram_available_mb": 38403.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.662}, "power_stats": {"power_gpu_soc_mean_watts": 21.936, "power_cpu_cv_mean_watts": 1.726, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 72.662, "power_watts_avg": 21.936, "energy_joules_est": 191.77, "duration_seconds": 8.742, "sample_count": 74}, "timestamp": "2026-01-25T19:10:06.493283"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8445.138, "latencies_ms": [8445.138], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image depicts two individuals in business attire walking on a street at night. The lighting is artificial, with street lamps casting a warm glow on the scene, and the weather appears to be clear as there are no signs of rain or snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.2, "ram_available_mb": 38403.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24507.2, "ram_available_mb": 38333.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.169}, "power_stats": {"power_gpu_soc_mean_watts": 21.878, "power_cpu_cv_mean_watts": 1.759, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 71.169, "power_watts_avg": 21.878, "energy_joules_est": 184.78, "duration_seconds": 8.446, "sample_count": 71}, "timestamp": "2026-01-25T19:10:16.964253"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11143.66, "latencies_ms": [11143.66], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a man standing in a wine cellar, holding a wine bottle and a cloth. He is surrounded by numerous wine bottles, some of which are placed on shelves and others are scattered around the room. There are also wine glasses placed on the table, with one near the man and another further away. The man appears to be cleaning the wine bottle with", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24507.2, "ram_available_mb": 38333.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24532.7, "ram_available_mb": 38308.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.979}, "power_stats": {"power_gpu_soc_mean_watts": 20.052, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 70.979, "power_watts_avg": 20.052, "energy_joules_est": 223.47, "duration_seconds": 11.144, "sample_count": 95}, "timestamp": "2026-01-25T19:10:30.146293"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9290.418, "latencies_ms": [9290.418], "images_per_second": 0.108, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Wine bottles: 15\n- Wine glasses: 3\n- Glass: 1\n- Table: 1\n- Menu: 1\n- Chair: 1\n- Wine bottle: 1\n- Wine glass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24532.7, "ram_available_mb": 38308.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24549.6, "ram_available_mb": 38291.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.987}, "power_stats": {"power_gpu_soc_mean_watts": 21.653, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 70.987, "power_watts_avg": 21.653, "energy_joules_est": 201.18, "duration_seconds": 9.291, "sample_count": 80}, "timestamp": "2026-01-25T19:10:41.500376"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11155.968, "latencies_ms": [11155.968], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a man is standing and holding a wine bottle with a cloth, positioned near a large glass and a smaller wine glass on the table in front of him. In the background, there are multiple wine bottles on shelves, indicating a wine cellar or a wine tasting room. The man is closer to the camera than the wine bottles on the", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24427.6, "ram_available_mb": 38413.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24433.4, "ram_available_mb": 38407.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.479}, "power_stats": {"power_gpu_soc_mean_watts": 21.017, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 70.479, "power_watts_avg": 21.017, "energy_joules_est": 234.49, "duration_seconds": 11.157, "sample_count": 94}, "timestamp": "2026-01-25T19:10:54.687610"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7434.75, "latencies_ms": [7434.75], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A man is standing in a wine cellar, holding a bottle of wine and cleaning a glass with a cloth. The cellar is filled with numerous wine bottles on shelves and racks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.4, "ram_available_mb": 38407.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24426.9, "ram_available_mb": 38414.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.222}, "power_stats": {"power_gpu_soc_mean_watts": 22.696, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 74.222, "power_watts_avg": 22.696, "energy_joules_est": 168.75, "duration_seconds": 7.435, "sample_count": 63}, "timestamp": "2026-01-25T19:11:04.156821"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8922.344, "latencies_ms": [8922.344], "images_per_second": 0.112, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a man in a dimly lit room with wooden furnishings, holding a bottle of wine and a cloth. There are multiple wine bottles on the shelves in the background, and the room has a cozy, intimate atmosphere with warm lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24426.9, "ram_available_mb": 38414.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24430.6, "ram_available_mb": 38410.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.539}, "power_stats": {"power_gpu_soc_mean_watts": 21.724, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 9.005, "gpu_utilization_percent_mean": 71.539, "power_watts_avg": 21.724, "energy_joules_est": 193.84, "duration_seconds": 8.923, "sample_count": 76}, "timestamp": "2026-01-25T19:11:15.135080"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11098.532, "latencies_ms": [11098.532], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a tennis player is captured in the midst of a powerful swing. The player, dressed in a crisp white shirt and shorts, is poised on a vibrant green tennis court. The player's right arm is extended upwards, holding a blue and green tennis racket, ready to strike the yellow tennis ball that hovers in the air above. The", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24430.6, "ram_available_mb": 38410.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24435.2, "ram_available_mb": 38405.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.766}, "power_stats": {"power_gpu_soc_mean_watts": 20.937, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 69.766, "power_watts_avg": 20.937, "energy_joules_est": 232.39, "duration_seconds": 11.099, "sample_count": 94}, "timestamp": "2026-01-25T19:11:28.287126"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7411.18, "latencies_ms": [7411.18], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "grass: numerous\nwhite lines: numerous\nracket: 1\nball: 1\nperson: 1\nshirt: 1\nshorts: 1\nwristband: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.2, "ram_available_mb": 38405.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24453.1, "ram_available_mb": 38387.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.254}, "power_stats": {"power_gpu_soc_mean_watts": 22.702, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 74.254, "power_watts_avg": 22.702, "energy_joules_est": 168.26, "duration_seconds": 7.412, "sample_count": 63}, "timestamp": "2026-01-25T19:11:37.731243"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9824.422, "latencies_ms": [9824.422], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The tennis player is positioned in the foreground, appearing large and in focus, while the tennis ball is in the background, slightly out of focus and higher in the air. The lines on the tennis court create a sense of depth, with the player closer to the viewer and the lines leading the eye towards the background.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24453.1, "ram_available_mb": 38387.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24435.5, "ram_available_mb": 38405.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.393}, "power_stats": {"power_gpu_soc_mean_watts": 21.38, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 70.393, "power_watts_avg": 21.38, "energy_joules_est": 210.06, "duration_seconds": 9.825, "sample_count": 84}, "timestamp": "2026-01-25T19:11:49.571347"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8100.342, "latencies_ms": [8100.342], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A tennis player is captured in the midst of a powerful serve on a grass court, with a tennis ball suspended in the air above his racket. The player is dressed in white attire, and the court is marked with white lines.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24435.5, "ram_available_mb": 38405.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24460.0, "ram_available_mb": 38380.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.838}, "power_stats": {"power_gpu_soc_mean_watts": 22.313, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 72.838, "power_watts_avg": 22.313, "energy_joules_est": 180.76, "duration_seconds": 8.101, "sample_count": 68}, "timestamp": "2026-01-25T19:11:59.710689"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8948.092, "latencies_ms": [8948.092], "images_per_second": 0.112, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image captures a moment of intense action on a vibrant green tennis court, bathed in natural daylight. The player, dressed in white, is in the midst of a powerful serve, with a blue and green tennis racket poised to strike the yellow ball.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24460.0, "ram_available_mb": 38380.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24452.6, "ram_available_mb": 38388.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.487}, "power_stats": {"power_gpu_soc_mean_watts": 21.707, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 71.487, "power_watts_avg": 21.707, "energy_joules_est": 194.25, "duration_seconds": 8.949, "sample_count": 76}, "timestamp": "2026-01-25T19:12:10.673736"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11135.784, "latencies_ms": [11135.784], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a white and orange cat is the main subject. The cat is standing on a wooden shelf, its body facing the television screen. The television, which is turned on, displays a man in a suit. The man appears to be in a state of surprise or shock. The shelf is positioned against a beige wall, and a white mug is", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24452.6, "ram_available_mb": 38388.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24435.3, "ram_available_mb": 38405.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.484}, "power_stats": {"power_gpu_soc_mean_watts": 20.886, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 69.484, "power_watts_avg": 20.886, "energy_joules_est": 232.6, "duration_seconds": 11.136, "sample_count": 95}, "timestamp": "2026-01-25T19:12:23.878130"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8182.541, "latencies_ms": [8182.541], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "Television: 1\nCat: 1\nTv stand: 1\nDvd player: 1\nCable box: 1\nRemote control: 1\nCoffee cup: 1\nWall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.3, "ram_available_mb": 38405.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24435.1, "ram_available_mb": 38405.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.565}, "power_stats": {"power_gpu_soc_mean_watts": 22.205, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 71.565, "power_watts_avg": 22.205, "energy_joules_est": 181.71, "duration_seconds": 8.183, "sample_count": 69}, "timestamp": "2026-01-25T19:12:34.086316"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8898.481, "latencies_ms": [8898.481], "images_per_second": 0.112, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "In the foreground, there is a cat standing on a wooden shelf. The shelf is positioned in front of a television that is mounted on the wall. The cat is closer to the camera than the television, and the television is further away from the camera than the cat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.1, "ram_available_mb": 38405.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24443.6, "ram_available_mb": 38397.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.427}, "power_stats": {"power_gpu_soc_mean_watts": 21.748, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 70.427, "power_watts_avg": 21.748, "energy_joules_est": 193.54, "duration_seconds": 8.899, "sample_count": 75}, "timestamp": "2026-01-25T19:12:44.997038"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6816.953, "latencies_ms": [6816.953], "images_per_second": 0.147, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A cat is standing on a wooden entertainment center, with a television displaying a man in a suit in the background. The entertainment center has a DVD player and a cup on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24443.6, "ram_available_mb": 38397.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24441.0, "ram_available_mb": 38399.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.491}, "power_stats": {"power_gpu_soc_mean_watts": 23.156, "power_cpu_cv_mean_watts": 1.559, "power_sys_5v0_mean_watts": 9.035, "gpu_utilization_percent_mean": 75.491, "power_watts_avg": 23.156, "energy_joules_est": 157.87, "duration_seconds": 6.818, "sample_count": 57}, "timestamp": "2026-01-25T19:12:53.857721"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6092.259, "latencies_ms": [6092.259], "images_per_second": 0.164, "prompt_tokens": 36, "response_tokens_est": 34, "n_tiles": 16, "output_text": "The television is placed on a wooden stand with a black remote control in front of it. The cat is white with orange patches and is standing on the stand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24441.0, "ram_available_mb": 38399.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24432.2, "ram_available_mb": 38408.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.451}, "power_stats": {"power_gpu_soc_mean_watts": 23.446, "power_cpu_cv_mean_watts": 1.515, "power_sys_5v0_mean_watts": 9.102, "gpu_utilization_percent_mean": 74.451, "power_watts_avg": 23.446, "energy_joules_est": 142.85, "duration_seconds": 6.093, "sample_count": 51}, "timestamp": "2026-01-25T19:13:01.979319"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11138.549, "latencies_ms": [11138.549], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a blue circular sign with a white border, prominently displayed against a backdrop of a clear blue sky. The sign is affixed to a metal pole, which is adorned with a white rectangular sign bearing Chinese characters. The pole is situated in a verdant area, surrounded by lush green trees that add a touch of nature to the urban", "error": null, "sys_before": {"cpu_percent": 6.1, "ram_used_mb": 24432.2, "ram_available_mb": 38408.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24440.3, "ram_available_mb": 38400.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.149}, "power_stats": {"power_gpu_soc_mean_watts": 20.896, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 69.149, "power_watts_avg": 20.896, "energy_joules_est": 232.77, "duration_seconds": 11.139, "sample_count": 94}, "timestamp": "2026-01-25T19:13:15.162531"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9847.934, "latencies_ms": [9847.934], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "1. Sign: 1\n2. Bicycle symbol: 1\n3. Pedestrian symbol: 1\n4. Traffic light: 1\n5. Street light: 1\n6. Tree: 1\n7. Sky: 1\n8. Sun: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24440.3, "ram_available_mb": 38400.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.435}, "power_stats": {"power_gpu_soc_mean_watts": 21.511, "power_cpu_cv_mean_watts": 1.823, "power_sys_5v0_mean_watts": 8.932, "gpu_utilization_percent_mean": 71.435, "power_watts_avg": 21.511, "energy_joules_est": 211.85, "duration_seconds": 9.849, "sample_count": 85}, "timestamp": "2026-01-25T19:13:27.051111"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10111.294, "latencies_ms": [10111.294], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The sign with the pedestrian and bicycle symbol is in the background, mounted on a pole that is positioned in the foreground. The sign with the Chinese characters is in the foreground, attached to the same pole. The background is a clear blue sky with some green foliage from trees partially visible behind the pole.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24432.5, "ram_available_mb": 38408.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.221}, "power_stats": {"power_gpu_soc_mean_watts": 21.275, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 70.221, "power_watts_avg": 21.275, "energy_joules_est": 215.13, "duration_seconds": 10.112, "sample_count": 86}, "timestamp": "2026-01-25T19:13:39.195899"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9080.573, "latencies_ms": [9080.573], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a blue circular sign with a white pictogram of a person and a bicycle, indicating a shared path for pedestrians and cyclists. Below it, there is a rectangular sign with Japanese characters, likely providing additional information or instructions for the path.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24432.5, "ram_available_mb": 38408.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.701}, "power_stats": {"power_gpu_soc_mean_watts": 21.781, "power_cpu_cv_mean_watts": 1.762, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 72.701, "power_watts_avg": 21.781, "energy_joules_est": 197.8, "duration_seconds": 9.081, "sample_count": 77}, "timestamp": "2026-01-25T19:13:50.296362"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6993.645, "latencies_ms": [6993.645], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The sign is circular with a blue background and features symbols of a pedestrian and a bicycle. It is mounted on a metal pole with a rectangular sign below it that has Chinese characters.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24438.6, "ram_available_mb": 38402.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.78}, "power_stats": {"power_gpu_soc_mean_watts": 22.648, "power_cpu_cv_mean_watts": 1.622, "power_sys_5v0_mean_watts": 9.062, "gpu_utilization_percent_mean": 72.78, "power_watts_avg": 22.648, "energy_joules_est": 158.41, "duration_seconds": 6.994, "sample_count": 59}, "timestamp": "2026-01-25T19:13:59.305320"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11131.501, "latencies_ms": [11131.501], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a woman sitting at a dining table in a restaurant. She is holding a slice of pizza in her hand, and there are two other pizza slices on the table in front of her. The table is surrounded by chairs, and there are a few more chairs visible in the background. \n\nIn addition to the pizza, there are two cups", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24438.6, "ram_available_mb": 38402.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 24423.3, "ram_available_mb": 38417.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.421}, "power_stats": {"power_gpu_soc_mean_watts": 20.896, "power_cpu_cv_mean_watts": 1.956, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.421, "power_watts_avg": 20.896, "energy_joules_est": 232.62, "duration_seconds": 11.132, "sample_count": 95}, "timestamp": "2026-01-25T19:14:12.478435"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9856.221, "latencies_ms": [9856.221], "images_per_second": 0.101, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "- Chairs: 10\n\n- Tables: 3\n\n- Glasses: 2\n\n- Pizza slices: 1\n\n- Napkin: 1\n\n- Book: 1\n\n- Water glass: 1\n\n- Paper: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24423.3, "ram_available_mb": 38417.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24433.1, "ram_available_mb": 38407.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.571}, "power_stats": {"power_gpu_soc_mean_watts": 21.509, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 71.571, "power_watts_avg": 21.509, "energy_joules_est": 212.01, "duration_seconds": 9.857, "sample_count": 84}, "timestamp": "2026-01-25T19:14:24.348036"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10569.293, "latencies_ms": [10569.293], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, there is a table with a glass of water and a slice of pizza, indicating a meal in progress. The person is seated at the table, which is in the middle ground of the image. In the background, there are other tables and chairs, suggesting this is a dining area with multiple seating options.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.1, "ram_available_mb": 38407.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24431.2, "ram_available_mb": 38409.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.225}, "power_stats": {"power_gpu_soc_mean_watts": 20.184, "power_cpu_cv_mean_watts": 1.898, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 71.225, "power_watts_avg": 20.184, "energy_joules_est": 213.34, "duration_seconds": 10.57, "sample_count": 89}, "timestamp": "2026-01-25T19:14:36.948894"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6848.733, "latencies_ms": [6848.733], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A person is sitting at a table in a restaurant, with a slice of pizza on the plate in front of them. The table has a glass of water and a book on it.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24431.2, "ram_available_mb": 38409.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24432.3, "ram_available_mb": 38408.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.386}, "power_stats": {"power_gpu_soc_mean_watts": 22.875, "power_cpu_cv_mean_watts": 1.566, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 74.386, "power_watts_avg": 22.875, "energy_joules_est": 156.68, "duration_seconds": 6.849, "sample_count": 57}, "timestamp": "2026-01-25T19:14:45.839893"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7773.633, "latencies_ms": [7773.633], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image shows an indoor setting with warm lighting, likely from hanging lamps or ceiling lights. The walls are adorned with various framed pictures and a notice board, and the flooring appears to be wooden.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24432.3, "ram_available_mb": 38408.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24428.9, "ram_available_mb": 38412.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.652}, "power_stats": {"power_gpu_soc_mean_watts": 22.256, "power_cpu_cv_mean_watts": 1.698, "power_sys_5v0_mean_watts": 9.046, "gpu_utilization_percent_mean": 71.652, "power_watts_avg": 22.256, "energy_joules_est": 173.02, "duration_seconds": 7.774, "sample_count": 66}, "timestamp": "2026-01-25T19:14:55.630579"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11132.909, "latencies_ms": [11132.909], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with a variety of food items spread across its surface. There are several bowls containing different dishes, including a plate of broccoli and cauliflower, a plate of rice, and a plate of meat. A spoon is also present on the table, likely used for serving or eating the food. \n\nIn addition to the main", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24428.9, "ram_available_mb": 38412.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24434.4, "ram_available_mb": 38406.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.511}, "power_stats": {"power_gpu_soc_mean_watts": 20.897, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.934, "gpu_utilization_percent_mean": 69.511, "power_watts_avg": 20.897, "energy_joules_est": 232.66, "duration_seconds": 11.134, "sample_count": 94}, "timestamp": "2026-01-25T19:15:08.800584"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11285.25, "latencies_ms": [11285.25], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "- bowl of broccoli: 1\n\n- bowl of cauliflower: 1\n\n- bowl of dumplings: 1\n\n- bowl of rice: 1\n\n- bowl of mixed vegetables: 1\n\n- bowl of butter: 1\n\n- glass of water: 1\n\n", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24434.4, "ram_available_mb": 38406.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.99}, "power_stats": {"power_gpu_soc_mean_watts": 20.958, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 69.99, "power_watts_avg": 20.958, "energy_joules_est": 236.53, "duration_seconds": 11.286, "sample_count": 96}, "timestamp": "2026-01-25T19:15:22.106466"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11128.481, "latencies_ms": [11128.481], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a plate of food with a red sauce and rice, and a glass of water to its right. Behind it, there is a bowl of broccoli and cauliflower, and further back, there are two plates of food, one with doughnuts and the other with a yellow sauce. The food items are arranged on", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24429.6, "ram_available_mb": 38411.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24429.3, "ram_available_mb": 38411.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.798}, "power_stats": {"power_gpu_soc_mean_watts": 20.94, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 69.798, "power_watts_avg": 20.94, "energy_joules_est": 233.04, "duration_seconds": 11.129, "sample_count": 94}, "timestamp": "2026-01-25T19:15:35.270859"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8741.543, "latencies_ms": [8741.543], "images_per_second": 0.114, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a dining table with a variety of food items including a bowl of broccoli, a plate of rice with vegetables, and a plate of dumplings. There is also a glass of water and a box of matches on the table.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24429.3, "ram_available_mb": 38411.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24431.2, "ram_available_mb": 38409.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.581}, "power_stats": {"power_gpu_soc_mean_watts": 21.959, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 72.581, "power_watts_avg": 21.959, "energy_joules_est": 191.97, "duration_seconds": 8.742, "sample_count": 74}, "timestamp": "2026-01-25T19:15:46.063057"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10096.412, "latencies_ms": [10096.412], "images_per_second": 0.099, "prompt_tokens": 36, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image shows a meal spread on a wooden table with a mix of warm and cool colors, such as the green of the broccoli, the white of the cauliflower, and the orange of the tomato-based dish. The lighting appears to be natural daylight, casting soft shadows on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.2, "ram_available_mb": 38409.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24433.2, "ram_available_mb": 38407.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.279}, "power_stats": {"power_gpu_soc_mean_watts": 21.238, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 70.279, "power_watts_avg": 21.238, "energy_joules_est": 214.44, "duration_seconds": 10.097, "sample_count": 86}, "timestamp": "2026-01-25T19:15:58.197963"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11154.774, "latencies_ms": [11154.774], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a busy city street scene with a mix of vehicles and pedestrians. There are several cars, a bus, and a truck on the street, with some cars stopped at a traffic light. The traffic light is located on the left side of the scene, and a person can be seen standing near it.\n\nThere are multiple pedestrians walking along the", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24433.2, "ram_available_mb": 38407.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24433.4, "ram_available_mb": 38407.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.454}, "power_stats": {"power_gpu_soc_mean_watts": 20.804, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 69.454, "power_watts_avg": 20.804, "energy_joules_est": 232.08, "duration_seconds": 11.155, "sample_count": 97}, "timestamp": "2026-01-25T19:16:11.403040"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9065.391, "latencies_ms": [9065.391], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Cars: 6\n- Buses: 1\n- Trucks: 1\n- Trees: 4\n- Buildings: 3\n- Traffic lights: 1\n- Signs: 3\n- Pedestrians: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.4, "ram_available_mb": 38407.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24435.5, "ram_available_mb": 38405.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.117}, "power_stats": {"power_gpu_soc_mean_watts": 21.792, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 8.921, "gpu_utilization_percent_mean": 71.117, "power_watts_avg": 21.792, "energy_joules_est": 197.57, "duration_seconds": 9.066, "sample_count": 77}, "timestamp": "2026-01-25T19:16:22.522737"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10567.283, "latencies_ms": [10567.283], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "In the foreground, there is a red car and a black car positioned near the center of the image, with a bus in the background slightly to the right. The red car is in the foreground on the right side, while the black car is in the center foreground. The bus is in the background, to the right of the black car.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24435.5, "ram_available_mb": 38405.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.045}, "power_stats": {"power_gpu_soc_mean_watts": 21.122, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 70.045, "power_watts_avg": 21.122, "energy_joules_est": 223.21, "duration_seconds": 10.568, "sample_count": 89}, "timestamp": "2026-01-25T19:16:35.111429"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7614.538, "latencies_ms": [7614.538], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image depicts a busy urban street scene with multiple vehicles, including a bus and cars, and a pedestrian crossing the street. Buildings line the street, and there are various signs and traffic lights visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.484}, "power_stats": {"power_gpu_soc_mean_watts": 22.545, "power_cpu_cv_mean_watts": 1.651, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 72.484, "power_watts_avg": 22.545, "energy_joules_est": 171.68, "duration_seconds": 7.615, "sample_count": 64}, "timestamp": "2026-01-25T19:16:44.758004"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9738.187, "latencies_ms": [9738.187], "images_per_second": 0.103, "prompt_tokens": 36, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image shows a busy urban street scene with a mix of vehicles, including a green and yellow bus, cars, and a white van with a \"GOLD SEAL PLUMBING\" sign. The weather appears to be overcast, and the lighting is natural, suggesting it might be a cloudy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24436.3, "ram_available_mb": 38404.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.361}, "power_stats": {"power_gpu_soc_mean_watts": 21.368, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 70.361, "power_watts_avg": 21.368, "energy_joules_est": 208.1, "duration_seconds": 9.739, "sample_count": 83}, "timestamp": "2026-01-25T19:16:56.525957"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 9676.284, "latencies_ms": [9676.284], "images_per_second": 0.103, "prompt_tokens": 24, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image shows a black Toshiba laptop open on a white table. Next to the laptop, there is a silver pen and a black smartphone. A person in a red shirt is standing behind the table, partially visible. The laptop screen displays a desktop wallpaper with a blue background and a white star.", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 24436.3, "ram_available_mb": 38404.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24452.5, "ram_available_mb": 38388.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.159}, "power_stats": {"power_gpu_soc_mean_watts": 21.331, "power_cpu_cv_mean_watts": 1.846, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 70.159, "power_watts_avg": 21.331, "energy_joules_est": 206.42, "duration_seconds": 9.677, "sample_count": 82}, "timestamp": "2026-01-25T19:17:08.258213"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7494.403, "latencies_ms": [7494.403], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "table: 1, laptop: 1, pen: 1, cell phone: 1, mouse: 1, screwdriver: 1, flashlight: 1, keypad: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24452.5, "ram_available_mb": 38388.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24456.6, "ram_available_mb": 38384.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.111}, "power_stats": {"power_gpu_soc_mean_watts": 22.611, "power_cpu_cv_mean_watts": 1.621, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 73.111, "power_watts_avg": 22.611, "energy_joules_est": 169.47, "duration_seconds": 7.495, "sample_count": 63}, "timestamp": "2026-01-25T19:17:17.773540"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11115.326, "latencies_ms": [11115.326], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The laptop is positioned in the foreground on a white surface, with a smartphone placed to its right side. A pen is lying vertically on the left side of the laptop, and a small device, possibly a digital camera or a remote control, is placed to the left of the pen. The background is less distinct but appears to be an indoor setting with a partial view of a", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24456.6, "ram_available_mb": 38384.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24463.1, "ram_available_mb": 38377.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.851}, "power_stats": {"power_gpu_soc_mean_watts": 20.965, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.851, "power_watts_avg": 20.965, "energy_joules_est": 233.05, "duration_seconds": 11.116, "sample_count": 94}, "timestamp": "2026-01-25T19:17:30.907088"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7043.747, "latencies_ms": [7043.747], "images_per_second": 0.142, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A person is standing behind a table with a Toshiba laptop open, displaying a desktop wallpaper. Next to the laptop, there is a pen and a smartphone lying on the table.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24463.1, "ram_available_mb": 38377.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24431.9, "ram_available_mb": 38409.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.467}, "power_stats": {"power_gpu_soc_mean_watts": 22.86, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 73.467, "power_watts_avg": 22.86, "energy_joules_est": 161.03, "duration_seconds": 7.044, "sample_count": 60}, "timestamp": "2026-01-25T19:17:40.009637"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7875.73, "latencies_ms": [7875.73], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a black Toshiba laptop on a white surface with a bright light reflecting off its screen. There is a silver pen lying next to the laptop, and a black smartphone is placed to the right of the laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.9, "ram_available_mb": 38409.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24432.9, "ram_available_mb": 38408.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.515}, "power_stats": {"power_gpu_soc_mean_watts": 22.036, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 9.046, "gpu_utilization_percent_mean": 71.515, "power_watts_avg": 22.036, "energy_joules_est": 173.57, "duration_seconds": 7.877, "sample_count": 68}, "timestamp": "2026-01-25T19:17:49.930148"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11110.718, "latencies_ms": [11110.718], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a desk with a computer setup, including a desktop computer and a laptop. The desktop computer is positioned on the left side of the desk, while the laptop is placed on the right side. A keyboard and a mouse are also present on the desk, with the keyboard located in front of the desktop computer and the mouse in front of the laptop.\n\nThere are", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24432.9, "ram_available_mb": 38408.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24429.3, "ram_available_mb": 38411.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.766}, "power_stats": {"power_gpu_soc_mean_watts": 20.951, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 69.766, "power_watts_avg": 20.951, "energy_joules_est": 232.79, "duration_seconds": 11.111, "sample_count": 94}, "timestamp": "2026-01-25T19:18:03.064024"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8720.252, "latencies_ms": [8720.252], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Computer monitor: 1\n- Keyboard: 1\n- Laptop: 1\n- Books: 10\n- Cable: 1\n- Computer mouse: 1\n- Water bottle: 1\n- Wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.3, "ram_available_mb": 38411.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24432.5, "ram_available_mb": 38408.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.432}, "power_stats": {"power_gpu_soc_mean_watts": 21.949, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 71.432, "power_watts_avg": 21.949, "energy_joules_est": 191.42, "duration_seconds": 8.721, "sample_count": 74}, "timestamp": "2026-01-25T19:18:13.800184"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10657.53, "latencies_ms": [10657.53], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The computer monitor is positioned in the center of the desk, with the keyboard in front of it and the laptop to its right. The books are stacked on the left side of the desk, and the mouse is located in the foreground on the right side. The window with blinds is in the background, providing natural light to the workspace.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24432.5, "ram_available_mb": 38408.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24434.6, "ram_available_mb": 38406.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.089}, "power_stats": {"power_gpu_soc_mean_watts": 21.094, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 70.089, "power_watts_avg": 21.094, "energy_joules_est": 224.82, "duration_seconds": 10.658, "sample_count": 90}, "timestamp": "2026-01-25T19:18:26.476821"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8297.969, "latencies_ms": [8297.969], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a home office setup with a desk containing a desktop computer with a monitor, keyboard, and mouse. There is also a laptop and a water bottle on the desk, and a bookshelf with several books in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.6, "ram_available_mb": 38406.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24458.2, "ram_available_mb": 38382.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.771}, "power_stats": {"power_gpu_soc_mean_watts": 22.189, "power_cpu_cv_mean_watts": 1.687, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 72.771, "power_watts_avg": 22.189, "energy_joules_est": 184.14, "duration_seconds": 8.299, "sample_count": 70}, "timestamp": "2026-01-25T19:18:36.815873"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7298.3, "latencies_ms": [7298.3], "images_per_second": 0.137, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows an indoor setting with a desk that has a computer monitor, a keyboard, a mouse, and a laptop. The room has a window with blinds, and the walls are a light color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24458.2, "ram_available_mb": 38382.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24537.3, "ram_available_mb": 38303.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.902}, "power_stats": {"power_gpu_soc_mean_watts": 22.582, "power_cpu_cv_mean_watts": 1.647, "power_sys_5v0_mean_watts": 9.058, "gpu_utilization_percent_mean": 72.902, "power_watts_avg": 22.582, "energy_joules_est": 164.82, "duration_seconds": 7.299, "sample_count": 61}, "timestamp": "2026-01-25T19:18:46.130962"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11081.064, "latencies_ms": [11081.064], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a thrilling moment of a skateboarder performing a trick in mid-air. The skateboarder is in the center of the scene, skillfully balancing on his skateboard while flying through the air. \n\nThere are several people around the skateboarder, some of them holding cell phones to capture the exciting moment. A few", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24475.6, "ram_available_mb": 38365.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24479.8, "ram_available_mb": 38361.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.516}, "power_stats": {"power_gpu_soc_mean_watts": 20.946, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.516, "power_watts_avg": 20.946, "energy_joules_est": 232.12, "duration_seconds": 11.082, "sample_count": 95}, "timestamp": "2026-01-25T19:18:59.267870"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7944.613, "latencies_ms": [7944.613], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "skateboarder: 1, skateboard: 1, camera: 3, spectators: 10, barrier: 1, ramp: 1, logo: 2, logo: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24479.8, "ram_available_mb": 38361.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24453.4, "ram_available_mb": 38387.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.559}, "power_stats": {"power_gpu_soc_mean_watts": 22.26, "power_cpu_cv_mean_watts": 1.701, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 72.559, "power_watts_avg": 22.26, "energy_joules_est": 176.86, "duration_seconds": 7.945, "sample_count": 68}, "timestamp": "2026-01-25T19:19:09.258372"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11125.999, "latencies_ms": [11125.999], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a skateboarder is captured in mid-air, performing a trick above a rail, with a crowd of spectators and photographers in the background. The skateboarder is positioned near the rail, indicating proximity to the obstacle being used for the trick. The photographers are standing at various distances from the rail, with some closer to the for", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24453.4, "ram_available_mb": 38387.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24452.6, "ram_available_mb": 38388.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.969}, "power_stats": {"power_gpu_soc_mean_watts": 20.974, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 69.969, "power_watts_avg": 20.974, "energy_joules_est": 233.37, "duration_seconds": 11.127, "sample_count": 96}, "timestamp": "2026-01-25T19:19:22.418844"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9498.54, "latencies_ms": [9498.54], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 63, "n_tiles": 16, "output_text": "A skateboarder is performing a trick in mid-air above a ramp, with a crowd of spectators and photographers capturing the moment. The setting appears to be an indoor skatepark or a similar venue, with a high ceiling and a large audience watching the performance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24452.6, "ram_available_mb": 38388.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24578.2, "ram_available_mb": 38262.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.605}, "power_stats": {"power_gpu_soc_mean_watts": 21.585, "power_cpu_cv_mean_watts": 1.819, "power_sys_5v0_mean_watts": 8.935, "gpu_utilization_percent_mean": 70.605, "power_watts_avg": 21.585, "energy_joules_est": 205.04, "duration_seconds": 9.499, "sample_count": 81}, "timestamp": "2026-01-25T19:19:33.969743"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11159.591, "latencies_ms": [11159.591], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a skateboarder performing a trick in mid-air with a dark background and a pattern of red and white lines. The skateboarder is wearing a black shirt and white pants, and the skateboard is green and white. The audience is visible in the foreground, with some people holding up their phones to take pictures. The lighting in", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24578.2, "ram_available_mb": 38262.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24578.9, "ram_available_mb": 38262.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.895}, "power_stats": {"power_gpu_soc_mean_watts": 20.98, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.895, "power_watts_avg": 20.98, "energy_joules_est": 234.14, "duration_seconds": 11.16, "sample_count": 95}, "timestamp": "2026-01-25T19:19:47.179507"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11067.466, "latencies_ms": [11067.466], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a red fire hydrant with a smiley face painted on it, located on the side of a street. The fire hydrant is positioned near a tree, adding a touch of nature to the urban setting. In the background, there are several cars parked along the street, indicating that this is a populated area. The fire hydrant's cheerful face adds a play", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24433.0, "ram_available_mb": 38407.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.547}, "power_stats": {"power_gpu_soc_mean_watts": 20.887, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.547, "power_watts_avg": 20.887, "energy_joules_est": 231.18, "duration_seconds": 11.068, "sample_count": 95}, "timestamp": "2026-01-25T19:20:00.318581"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8958.791, "latencies_ms": [8958.791], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 58, "n_tiles": 16, "output_text": "- Fire hydrant: 1\n- Tree: 1\n- Cars: 3\n- Buildings: 2\n- Sidewalk: 1\n- Road: 1\n- Bike lane: 1\n- Parking meter: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24433.0, "ram_available_mb": 38407.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24461.3, "ram_available_mb": 38379.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.697}, "power_stats": {"power_gpu_soc_mean_watts": 21.857, "power_cpu_cv_mean_watts": 1.744, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 71.697, "power_watts_avg": 21.857, "energy_joules_est": 195.83, "duration_seconds": 8.96, "sample_count": 76}, "timestamp": "2026-01-25T19:20:11.325625"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9155.759, "latencies_ms": [9155.759], "images_per_second": 0.109, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The fire hydrant is located in the foreground of the image, positioned on the right side. It is near the curb of a street, with a yellow line running parallel to it. In the background, there are cars parked along the street and trees lining the sidewalk.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24461.3, "ram_available_mb": 38379.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24542.7, "ram_available_mb": 38298.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.936}, "power_stats": {"power_gpu_soc_mean_watts": 21.567, "power_cpu_cv_mean_watts": 1.822, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 70.936, "power_watts_avg": 21.567, "energy_joules_est": 197.48, "duration_seconds": 9.156, "sample_count": 78}, "timestamp": "2026-01-25T19:20:22.540791"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7522.282, "latencies_ms": [7522.282], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A red fire hydrant with a smiley face painted on it is located on the side of a street. The hydrant is positioned near a tree and there are cars parked on the street in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24542.7, "ram_available_mb": 38298.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.75}, "power_stats": {"power_gpu_soc_mean_watts": 22.569, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 73.75, "power_watts_avg": 22.569, "energy_joules_est": 169.78, "duration_seconds": 7.523, "sample_count": 64}, "timestamp": "2026-01-25T19:20:32.117733"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7957.258, "latencies_ms": [7957.258], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The fire hydrant in the image is bright red with a black top and has a smiley face drawn on it. It is located on the side of a street with a yellow curb and is surrounded by trees and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.1, "ram_available_mb": 38403.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24453.3, "ram_available_mb": 38387.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.403}, "power_stats": {"power_gpu_soc_mean_watts": 22.096, "power_cpu_cv_mean_watts": 1.697, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 71.403, "power_watts_avg": 22.096, "energy_joules_est": 175.84, "duration_seconds": 7.958, "sample_count": 67}, "timestamp": "2026-01-25T19:20:42.092412"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11094.945, "latencies_ms": [11094.945], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a green luggage cart loaded with various suitcases and trunks. The cart is positioned in front of a green door, and the suitcases are stacked on top of each other, occupying most of the cart's space. The suitcases come in different sizes and colors, creating a visually interesting scene.\n\nIn addition to the suitcases, there", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24453.3, "ram_available_mb": 38387.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24572.4, "ram_available_mb": 38268.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.904}, "power_stats": {"power_gpu_soc_mean_watts": 20.978, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 69.904, "power_watts_avg": 20.978, "energy_joules_est": 232.76, "duration_seconds": 11.096, "sample_count": 94}, "timestamp": "2026-01-25T19:20:55.219943"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7749.732, "latencies_ms": [7749.732], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "cart: 1\ntrunk: 1\nsuitcase: 1\nsuitcase: 1\nsuitcase: 1\nsuitcase: 1\nsuitcase: 1\nsuitcase: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.4, "ram_available_mb": 38405.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24421.7, "ram_available_mb": 38419.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.379}, "power_stats": {"power_gpu_soc_mean_watts": 22.298, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 72.379, "power_watts_avg": 22.298, "energy_joules_est": 172.82, "duration_seconds": 7.75, "sample_count": 66}, "timestamp": "2026-01-25T19:21:04.989773"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9673.649, "latencies_ms": [9673.649], "images_per_second": 0.103, "prompt_tokens": 44, "response_tokens_est": 66, "n_tiles": 16, "output_text": "In the foreground, there is a green cart with a stack of old, worn-out suitcases on top of it. The suitcases are arranged in a pile, with the largest at the bottom and the smallest at the top. In the background, there is a green door and a poster on the wall.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24421.7, "ram_available_mb": 38419.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24430.9, "ram_available_mb": 38410.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.256}, "power_stats": {"power_gpu_soc_mean_watts": 21.406, "power_cpu_cv_mean_watts": 1.836, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 70.256, "power_watts_avg": 21.406, "energy_joules_est": 207.09, "duration_seconds": 9.674, "sample_count": 82}, "timestamp": "2026-01-25T19:21:16.720715"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9405.401, "latencies_ms": [9405.401], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image depicts a green luggage cart loaded with various old and worn suitcases in front of a green door and a green wall with a poster. The suitcases appear to be in a state of disrepair, suggesting they may have been abandoned or unused for a long time.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24430.9, "ram_available_mb": 38410.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24429.5, "ram_available_mb": 38411.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.475}, "power_stats": {"power_gpu_soc_mean_watts": 21.51, "power_cpu_cv_mean_watts": 1.782, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 70.475, "power_watts_avg": 21.51, "energy_joules_est": 202.32, "duration_seconds": 9.406, "sample_count": 80}, "timestamp": "2026-01-25T19:21:28.187273"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9197.836, "latencies_ms": [9197.836], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image features a collection of old, worn suitcases in various colors such as brown, blue, and green, stacked on a green cart. The lighting appears to be natural daylight, casting soft shadows on the ground, which suggests the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.5, "ram_available_mb": 38411.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24437.5, "ram_available_mb": 38403.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.628}, "power_stats": {"power_gpu_soc_mean_watts": 21.543, "power_cpu_cv_mean_watts": 1.822, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 70.628, "power_watts_avg": 21.543, "energy_joules_est": 198.16, "duration_seconds": 9.198, "sample_count": 78}, "timestamp": "2026-01-25T19:21:39.442625"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11115.939, "latencies_ms": [11115.939], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young girl is standing in front of a couch, holding a Wii remote in her hand. She appears to be playing a video game, possibly on a Nintendo Wii console. The girl is wearing a pink dress, and her focus is on the game she is playing. The couch is positioned behind her, and the girl is standing close to", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24437.5, "ram_available_mb": 38403.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24436.1, "ram_available_mb": 38404.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.737}, "power_stats": {"power_gpu_soc_mean_watts": 20.95, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 69.737, "power_watts_avg": 20.95, "energy_joules_est": 232.89, "duration_seconds": 11.117, "sample_count": 95}, "timestamp": "2026-01-25T19:21:52.594616"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7765.357, "latencies_ms": [7765.357], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "couch: 1\ndress: 1\nflowers: 5\ncontroller: 1\nwindows: 2\nblinds: 2\nred curtain: 1\ngirl: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.1, "ram_available_mb": 38404.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24439.6, "ram_available_mb": 38401.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.879}, "power_stats": {"power_gpu_soc_mean_watts": 22.341, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 71.879, "power_watts_avg": 22.341, "energy_joules_est": 173.5, "duration_seconds": 7.766, "sample_count": 66}, "timestamp": "2026-01-25T19:22:02.386391"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8792.841, "latencies_ms": [8792.841], "images_per_second": 0.114, "prompt_tokens": 44, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The girl is standing in the foreground, holding a Wii controller in her right hand, which is positioned near her body. The couch is in the background, behind her, and the window with blinds is further back, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24439.6, "ram_available_mb": 38401.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 24444.8, "ram_available_mb": 38396.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.838}, "power_stats": {"power_gpu_soc_mean_watts": 21.739, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 9.003, "gpu_utilization_percent_mean": 70.838, "power_watts_avg": 21.739, "energy_joules_est": 191.16, "duration_seconds": 8.793, "sample_count": 74}, "timestamp": "2026-01-25T19:22:13.197847"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7395.489, "latencies_ms": [7395.489], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A young girl in a pink dress is standing in front of a couch, holding a Wii remote control in her hands. She appears to be playing a video game or possibly preparing to play one.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24444.8, "ram_available_mb": 38396.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24441.5, "ram_available_mb": 38399.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.742}, "power_stats": {"power_gpu_soc_mean_watts": 22.568, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 71.742, "power_watts_avg": 22.568, "energy_joules_est": 166.91, "duration_seconds": 7.396, "sample_count": 62}, "timestamp": "2026-01-25T19:22:22.615930"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8326.709, "latencies_ms": [8326.709], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a young girl in a pink dress with a floral pattern, standing in a room with a beige couch. The room has white blinds on the window, and the lighting appears to be natural daylight coming from the window.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24441.5, "ram_available_mb": 38399.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24441.0, "ram_available_mb": 38399.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.042}, "power_stats": {"power_gpu_soc_mean_watts": 21.884, "power_cpu_cv_mean_watts": 1.748, "power_sys_5v0_mean_watts": 9.016, "gpu_utilization_percent_mean": 71.042, "power_watts_avg": 21.884, "energy_joules_est": 182.23, "duration_seconds": 8.327, "sample_count": 71}, "timestamp": "2026-01-25T19:22:32.982385"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11137.355, "latencies_ms": [11137.355], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a home office setup. Dominating the space is a glass desk, which houses a laptop and a keyboard, suggesting a workspace. A black office chair is positioned in front of the desk, ready for use. The desk is situated against a white wall, and a window with blinds is visible in the background, allowing natural light to filter", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24441.0, "ram_available_mb": 38399.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24443.9, "ram_available_mb": 38397.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.968}, "power_stats": {"power_gpu_soc_mean_watts": 20.939, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.968, "power_watts_avg": 20.939, "energy_joules_est": 233.22, "duration_seconds": 11.138, "sample_count": 94}, "timestamp": "2026-01-25T19:22:46.154078"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10530.561, "latencies_ms": [10530.561], "images_per_second": 0.095, "prompt_tokens": 39, "response_tokens_est": 72, "n_tiles": 16, "output_text": "- Computer monitor: 1\n\n- Keyboard: 1\n\n- Computer mouse: 1\n\n- Headphones: 1\n\n- Glass desk: 1\n\n- Glass desk surface: 1\n\n- Glass desk edge: 1\n\n- Wire: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24443.9, "ram_available_mb": 38397.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24447.0, "ram_available_mb": 38393.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.133}, "power_stats": {"power_gpu_soc_mean_watts": 21.26, "power_cpu_cv_mean_watts": 1.846, "power_sys_5v0_mean_watts": 8.918, "gpu_utilization_percent_mean": 71.133, "power_watts_avg": 21.26, "energy_joules_est": 223.89, "duration_seconds": 10.531, "sample_count": 90}, "timestamp": "2026-01-25T19:22:58.740374"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10284.417, "latencies_ms": [10284.417], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The laptop is positioned on the left side of the glass desk, which is in the foreground of the image. The chair is placed in front of the desk, occupying the right side of the desk space. The computer tower on the floor is in the background, behind the desk and to the left of the chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24447.0, "ram_available_mb": 38393.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24434.4, "ram_available_mb": 38406.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.138}, "power_stats": {"power_gpu_soc_mean_watts": 21.058, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 70.138, "power_watts_avg": 21.058, "energy_joules_est": 216.58, "duration_seconds": 10.285, "sample_count": 87}, "timestamp": "2026-01-25T19:23:11.038355"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8522.546, "latencies_ms": [8522.546], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a home office setting with a glass desk, a laptop, a keyboard, and a pair of headphones. There is a chair in front of the desk and a trash can with a plastic bag on the floor.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24434.4, "ram_available_mb": 38406.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24438.8, "ram_available_mb": 38402.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.847}, "power_stats": {"power_gpu_soc_mean_watts": 22.018, "power_cpu_cv_mean_watts": 1.713, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 72.847, "power_watts_avg": 22.018, "energy_joules_est": 187.66, "duration_seconds": 8.523, "sample_count": 72}, "timestamp": "2026-01-25T19:23:21.575326"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7339.911, "latencies_ms": [7339.911], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows an indoor setting with a laptop on a glass desk, a keyboard, and headphones placed beside it. The room has white walls, a window with blinds, and a wooden floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.8, "ram_available_mb": 38402.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24434.0, "ram_available_mb": 38406.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.452, "power_cpu_cv_mean_watts": 1.634, "power_sys_5v0_mean_watts": 9.019, "gpu_utilization_percent_mean": 72.0, "power_watts_avg": 22.452, "energy_joules_est": 164.81, "duration_seconds": 7.341, "sample_count": 62}, "timestamp": "2026-01-25T19:23:30.940188"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11082.497, "latencies_ms": [11082.497], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a close-up of a pizza on a white plate, which is placed on a table with a red and white checkered tablecloth. The pizza has a golden-brown crust and is topped with red tomato sauce, melted cheese, and slices of yellow bell pepper. The bell pepper slices are arranged in a", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24434.0, "ram_available_mb": 38406.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24441.4, "ram_available_mb": 38399.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.553}, "power_stats": {"power_gpu_soc_mean_watts": 20.969, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 69.553, "power_watts_avg": 20.969, "energy_joules_est": 232.4, "duration_seconds": 11.083, "sample_count": 94}, "timestamp": "2026-01-25T19:23:44.088188"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7966.675, "latencies_ms": [7966.675], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "pizza: 1\npepperoni: 1\nmushrooms: 1\nonions: 2\ntomatoes: 1\ncheese: 1\nfork: 1\nplate: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24441.4, "ram_available_mb": 38399.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.716}, "power_stats": {"power_gpu_soc_mean_watts": 22.258, "power_cpu_cv_mean_watts": 1.679, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 72.716, "power_watts_avg": 22.258, "energy_joules_est": 177.34, "duration_seconds": 7.967, "sample_count": 67}, "timestamp": "2026-01-25T19:23:54.094299"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10126.898, "latencies_ms": [10126.898], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The pizza is positioned in the foreground of the image, occupying the majority of the frame. It is placed on a white plate which is on a tablecloth with a red and white checkered pattern. A fork is held in the upper right corner, suggesting the action of eating or about to eat the pizza.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24441.0, "ram_available_mb": 38399.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.64}, "power_stats": {"power_gpu_soc_mean_watts": 21.275, "power_cpu_cv_mean_watts": 1.871, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.64, "power_watts_avg": 21.275, "energy_joules_est": 215.46, "duration_seconds": 10.127, "sample_count": 86}, "timestamp": "2026-01-25T19:24:06.250347"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8303.0, "latencies_ms": [8303.0], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a pizza with various toppings on a white plate, which is placed on a table with a red and white checkered tablecloth. A person's hand is visible, holding a fork, ready to eat the pizza.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24441.0, "ram_available_mb": 38399.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24433.0, "ram_available_mb": 38407.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.986}, "power_stats": {"power_gpu_soc_mean_watts": 22.201, "power_cpu_cv_mean_watts": 1.687, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 72.986, "power_watts_avg": 22.201, "energy_joules_est": 184.35, "duration_seconds": 8.304, "sample_count": 70}, "timestamp": "2026-01-25T19:24:16.586614"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10333.884, "latencies_ms": [10333.884], "images_per_second": 0.097, "prompt_tokens": 36, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The pizza is topped with red tomato sauce, melted cheese, slices of yellow bell pepper, and mushrooms, all resting on a thin, golden crust. The pizza is served on a white plate, which is placed on a table covered with a red and white checkered tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.0, "ram_available_mb": 38407.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24439.0, "ram_available_mb": 38401.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.682}, "power_stats": {"power_gpu_soc_mean_watts": 21.143, "power_cpu_cv_mean_watts": 1.879, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 69.682, "power_watts_avg": 21.143, "energy_joules_est": 218.5, "duration_seconds": 10.335, "sample_count": 88}, "timestamp": "2026-01-25T19:24:28.979170"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12321.467, "latencies_ms": [12321.467], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a white and red bus parked on the side of a street. The bus is a part of the Metropolitan Transit System, as indicated by the sign on the front. Several people are visible inside the bus, waiting for their ride or preparing to board. \n\nIn addition to the bus, there are a few cars parked along the street, with one car located", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24439.0, "ram_available_mb": 38401.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24458.2, "ram_available_mb": 38382.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.314}, "power_stats": {"power_gpu_soc_mean_watts": 22.876, "power_cpu_cv_mean_watts": 1.769, "power_sys_5v0_mean_watts": 9.196, "gpu_utilization_percent_mean": 74.314, "power_watts_avg": 22.876, "energy_joules_est": 281.88, "duration_seconds": 12.322, "sample_count": 105}, "timestamp": "2026-01-25T19:24:43.342895"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8746.641, "latencies_ms": [8746.641], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "bus: 1\nwindow: 6\nseat: 0\npassenger: 4\nwheel: 1\ntrunk: 1\nflag: 1\ntree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24458.2, "ram_available_mb": 38382.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24464.7, "ram_available_mb": 38376.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.946}, "power_stats": {"power_gpu_soc_mean_watts": 24.265, "power_cpu_cv_mean_watts": 1.428, "power_sys_5v0_mean_watts": 9.161, "gpu_utilization_percent_mean": 77.946, "power_watts_avg": 24.265, "energy_joules_est": 212.25, "duration_seconds": 8.747, "sample_count": 74}, "timestamp": "2026-01-25T19:24:54.114150"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11714.696, "latencies_ms": [11714.696], "images_per_second": 0.085, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The bus is in the foreground of the image, positioned on the left side, and appears to be in motion. It is a large vehicle with a red and white color scheme, and it is part of the Metropolitan Transit System. In the background, there are other vehicles and buildings, indicating that the bus is in an urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24464.7, "ram_available_mb": 38376.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24465.0, "ram_available_mb": 38375.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.367}, "power_stats": {"power_gpu_soc_mean_watts": 22.434, "power_cpu_cv_mean_watts": 1.724, "power_sys_5v0_mean_watts": 9.159, "gpu_utilization_percent_mean": 74.367, "power_watts_avg": 22.434, "energy_joules_est": 262.82, "duration_seconds": 11.715, "sample_count": 98}, "timestamp": "2026-01-25T19:25:07.853532"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9180.151, "latencies_ms": [9180.151], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A white and red Metropolitan Transit System bus is parked on the side of a street with trees in the background. The bus has the number 805 and the destination \"DOWNTOWN\" displayed on the front.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24465.0, "ram_available_mb": 38375.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24483.9, "ram_available_mb": 38357.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.192}, "power_stats": {"power_gpu_soc_mean_watts": 24.033, "power_cpu_cv_mean_watts": 1.478, "power_sys_5v0_mean_watts": 9.158, "gpu_utilization_percent_mean": 77.192, "power_watts_avg": 24.033, "energy_joules_est": 220.64, "duration_seconds": 9.181, "sample_count": 78}, "timestamp": "2026-01-25T19:25:19.080595"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6773.733, "latencies_ms": [6773.733], "images_per_second": 0.148, "prompt_tokens": 36, "response_tokens_est": 29, "n_tiles": 16, "output_text": "The bus is predominantly white with red and blue accents. The sky is clear and it appears to be a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24483.9, "ram_available_mb": 38357.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24463.5, "ram_available_mb": 38377.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 80.328}, "power_stats": {"power_gpu_soc_mean_watts": 25.243, "power_cpu_cv_mean_watts": 1.249, "power_sys_5v0_mean_watts": 9.293, "gpu_utilization_percent_mean": 80.328, "power_watts_avg": 25.243, "energy_joules_est": 171.01, "duration_seconds": 6.774, "sample_count": 58}, "timestamp": "2026-01-25T19:25:27.873739"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12301.654, "latencies_ms": [12301.654], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility on a baseball field. A black and gold baseball glove, with a white logo on the back, is resting on the ground. The glove is positioned in such a way that it partially covers a blue baseball cap with a white logo. The cap is leaning against a green pole, suggesting a pause in action. The background is", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 24463.5, "ram_available_mb": 38377.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24471.3, "ram_available_mb": 38369.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.333}, "power_stats": {"power_gpu_soc_mean_watts": 22.906, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 9.213, "gpu_utilization_percent_mean": 73.333, "power_watts_avg": 22.906, "energy_joules_est": 281.8, "duration_seconds": 12.302, "sample_count": 105}, "timestamp": "2026-01-25T19:25:42.231368"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9502.169, "latencies_ms": [9502.169], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "baseball glove: 1, baseball cap: 1, baseball cleat: 1, baseball bat: 1, baseball: 1, baseball glove: 1, baseball cap: 1, baseball cleat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24471.3, "ram_available_mb": 38369.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24462.2, "ram_available_mb": 38378.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.469}, "power_stats": {"power_gpu_soc_mean_watts": 23.953, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 9.172, "gpu_utilization_percent_mean": 76.469, "power_watts_avg": 23.953, "energy_joules_est": 227.62, "duration_seconds": 9.503, "sample_count": 81}, "timestamp": "2026-01-25T19:25:53.764361"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11397.84, "latencies_ms": [11397.84], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The baseball cap is positioned in the foreground, resting on top of the baseball glove which is lying on the ground. The glove is near the base of the metal pole, suggesting it is placed close to the pole. The background is less distinct but appears to be an outdoor setting with some gravel or dirt.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 24462.2, "ram_available_mb": 38378.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24475.1, "ram_available_mb": 38365.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.051}, "power_stats": {"power_gpu_soc_mean_watts": 23.137, "power_cpu_cv_mean_watts": 1.728, "power_sys_5v0_mean_watts": 9.205, "gpu_utilization_percent_mean": 75.051, "power_watts_avg": 23.137, "energy_joules_est": 263.73, "duration_seconds": 11.398, "sample_count": 98}, "timestamp": "2026-01-25T19:26:07.195485"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9277.588, "latencies_ms": [9277.588], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A baseball cap and a glove are placed on the ground, suggesting a casual or relaxed setting, possibly after a game or practice. The cap has a logo on it, indicating it might be from a specific team or brand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24475.1, "ram_available_mb": 38365.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24438.4, "ram_available_mb": 38402.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.899}, "power_stats": {"power_gpu_soc_mean_watts": 24.063, "power_cpu_cv_mean_watts": 1.576, "power_sys_5v0_mean_watts": 9.17, "gpu_utilization_percent_mean": 76.899, "power_watts_avg": 24.063, "energy_joules_est": 223.26, "duration_seconds": 9.278, "sample_count": 79}, "timestamp": "2026-01-25T19:26:18.517274"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8681.739, "latencies_ms": [8681.739], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The baseball cap is navy blue with a white logo, and the glove is black with gold accents. The image has a warm, natural lighting, likely from the sun, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.4, "ram_available_mb": 38402.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24544.2, "ram_available_mb": 38296.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.219}, "power_stats": {"power_gpu_soc_mean_watts": 24.109, "power_cpu_cv_mean_watts": 1.519, "power_sys_5v0_mean_watts": 9.247, "gpu_utilization_percent_mean": 77.219, "power_watts_avg": 24.109, "energy_joules_est": 209.32, "duration_seconds": 8.682, "sample_count": 73}, "timestamp": "2026-01-25T19:26:29.258029"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11108.987, "latencies_ms": [11108.987], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a surfer is skillfully riding a wave on a sunny day. The surfer is wearing a red shirt and is positioned on a white surfboard. The wave, a beautiful shade of blue-green, is breaking to the right of the surfer. The surfer is leaning into the wave, demonstrating their control and balance.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24449.1, "ram_available_mb": 38391.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24439.6, "ram_available_mb": 38401.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.511}, "power_stats": {"power_gpu_soc_mean_watts": 20.95, "power_cpu_cv_mean_watts": 1.925, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.511, "power_watts_avg": 20.95, "energy_joules_est": 232.75, "duration_seconds": 11.11, "sample_count": 94}, "timestamp": "2026-01-25T19:26:42.435900"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8529.429, "latencies_ms": [8529.429], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "wave: 1\nsurfboard: 1\nsurfer: 1\nred shirt: 1\nblack shorts: 1\nnumber 7: 1\ngreen stripe: 1\nwhite surfboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.6, "ram_available_mb": 38401.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24434.1, "ram_available_mb": 38406.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.403}, "power_stats": {"power_gpu_soc_mean_watts": 22.079, "power_cpu_cv_mean_watts": 1.718, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 72.403, "power_watts_avg": 22.079, "energy_joules_est": 188.33, "duration_seconds": 8.53, "sample_count": 72}, "timestamp": "2026-01-25T19:26:52.991951"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8673.107, "latencies_ms": [8673.107], "images_per_second": 0.115, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is breaking to the right of the frame. The wave originates in the background and extends towards the left side of the image, creating a dynamic spatial relationship between the surfer and the moving water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.1, "ram_available_mb": 38406.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24443.1, "ram_available_mb": 38397.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.162}, "power_stats": {"power_gpu_soc_mean_watts": 21.76, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 9.005, "gpu_utilization_percent_mean": 71.162, "power_watts_avg": 21.76, "energy_joules_est": 188.74, "duration_seconds": 8.674, "sample_count": 74}, "timestamp": "2026-01-25T19:27:03.705221"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6722.689, "latencies_ms": [6722.689], "images_per_second": 0.149, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A surfer in a red shirt is riding a wave on a sunny day. The ocean is a beautiful shade of blue and the wave is breaking to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24443.1, "ram_available_mb": 38397.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.569}, "power_stats": {"power_gpu_soc_mean_watts": 23.099, "power_cpu_cv_mean_watts": 1.56, "power_sys_5v0_mean_watts": 9.029, "gpu_utilization_percent_mean": 74.569, "power_watts_avg": 23.099, "energy_joules_est": 155.3, "duration_seconds": 6.723, "sample_count": 58}, "timestamp": "2026-01-25T19:27:12.466400"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7884.135, "latencies_ms": [7884.135], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The surfer is wearing a red shirt and is riding a wave that is a vibrant shade of blue-green. The water is splashing around the surfer, indicating active movement and possibly windy conditions.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24436.3, "ram_available_mb": 38404.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.162}, "power_stats": {"power_gpu_soc_mean_watts": 22.025, "power_cpu_cv_mean_watts": 1.725, "power_sys_5v0_mean_watts": 9.037, "gpu_utilization_percent_mean": 71.162, "power_watts_avg": 22.025, "energy_joules_est": 173.66, "duration_seconds": 7.885, "sample_count": 68}, "timestamp": "2026-01-25T19:27:22.409286"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11123.61, "latencies_ms": [11123.61], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "This image captures a quaint, small bathroom bathed in the soft glow of a black and white filter. The floor, a dark expanse of tiles, contrasts with the lighter walls. On the left, a pristine white toilet stands, its lid closed, exuding an air of cleanliness. To the right, a white sink is", "error": null, "sys_before": {"cpu_percent": 11.8, "ram_used_mb": 24436.3, "ram_available_mb": 38404.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24434.1, "ram_available_mb": 38406.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.863}, "power_stats": {"power_gpu_soc_mean_watts": 20.857, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 68.863, "power_watts_avg": 20.857, "energy_joules_est": 232.02, "duration_seconds": 11.124, "sample_count": 95}, "timestamp": "2026-01-25T19:27:35.557335"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9152.043, "latencies_ms": [9152.043], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "toilet: 1, sink: 1, toilet paper: 6, toilet brush: 1, garbage can: 1, trash bag: 1, toilet paper roll: 1, toilet paper holder: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.1, "ram_available_mb": 38406.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24459.2, "ram_available_mb": 38381.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.026}, "power_stats": {"power_gpu_soc_mean_watts": 21.701, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 71.026, "power_watts_avg": 21.701, "energy_joules_est": 198.62, "duration_seconds": 9.153, "sample_count": 78}, "timestamp": "2026-01-25T19:27:46.741850"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8426.389, "latencies_ms": [8426.389], "images_per_second": 0.119, "prompt_tokens": 44, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The toilet is positioned in the foreground on the left side of the image, while the sink is in the background on the right side. The toilet is closer to the viewer than the sink, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24459.2, "ram_available_mb": 38381.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24443.8, "ram_available_mb": 38397.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.917}, "power_stats": {"power_gpu_soc_mean_watts": 22.02, "power_cpu_cv_mean_watts": 1.785, "power_sys_5v0_mean_watts": 9.044, "gpu_utilization_percent_mean": 71.917, "power_watts_avg": 22.02, "energy_joules_est": 185.57, "duration_seconds": 8.427, "sample_count": 72}, "timestamp": "2026-01-25T19:27:57.218648"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7152.265, "latencies_ms": [7152.265], "images_per_second": 0.14, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image depicts a bathroom with a toilet and a sink. The toilet is positioned on the left side of the image, while the sink is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24443.8, "ram_available_mb": 38397.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24456.8, "ram_available_mb": 38384.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.65}, "power_stats": {"power_gpu_soc_mean_watts": 22.857, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 73.65, "power_watts_avg": 22.857, "energy_joules_est": 163.49, "duration_seconds": 7.153, "sample_count": 60}, "timestamp": "2026-01-25T19:28:06.429527"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7497.971, "latencies_ms": [7497.971], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image is in black and white, featuring a toilet and a sink in a bathroom. The tiles on the wall are made of marble, and there is a picture frame hanging above the sink.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24456.8, "ram_available_mb": 38384.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24437.5, "ram_available_mb": 38403.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.785}, "power_stats": {"power_gpu_soc_mean_watts": 21.753, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 9.059, "gpu_utilization_percent_mean": 72.785, "power_watts_avg": 21.753, "energy_joules_est": 163.12, "duration_seconds": 7.499, "sample_count": 65}, "timestamp": "2026-01-25T19:28:15.953523"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11134.949, "latencies_ms": [11134.949], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a white clock tower standing tall against a clear blue sky. The tower, adorned with a green dome, is the centerpiece of the image. It's surrounded by a white fence that adds a touch of elegance to the structure. The tower is nestled amidst lush green trees, creating a harmonious bl", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24437.5, "ram_available_mb": 38403.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24444.7, "ram_available_mb": 38396.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.432}, "power_stats": {"power_gpu_soc_mean_watts": 20.882, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 69.432, "power_watts_avg": 20.882, "energy_joules_est": 232.53, "duration_seconds": 11.136, "sample_count": 95}, "timestamp": "2026-01-25T19:28:29.143696"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10299.9, "latencies_ms": [10299.9], "images_per_second": 0.097, "prompt_tokens": 39, "response_tokens_est": 70, "n_tiles": 16, "output_text": "1. Dome: 1\n2. Clock: 1\n3. Bell: 1\n4. Roof tiles: 100\n5. Trees: 100\n6. Fence: 1\n7. Pole: 1\n8. Canopy: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24444.7, "ram_available_mb": 38396.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24429.9, "ram_available_mb": 38411.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.161}, "power_stats": {"power_gpu_soc_mean_watts": 21.358, "power_cpu_cv_mean_watts": 1.827, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 71.161, "power_watts_avg": 21.358, "energy_joules_est": 220.0, "duration_seconds": 10.301, "sample_count": 87}, "timestamp": "2026-01-25T19:28:41.478509"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11085.898, "latencies_ms": [11085.898], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The ornate structure on the left is in the foreground and appears to be closer to the viewer than the white tower with a blue dome in the background. The tower is situated on a higher elevation, as indicated by the perspective, and is surrounded by a clear blue sky. The roof tiles in the foreground are at a lower elevation compared to the tower, creating a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.9, "ram_available_mb": 38411.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24434.5, "ram_available_mb": 38406.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.905}, "power_stats": {"power_gpu_soc_mean_watts": 20.983, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 69.905, "power_watts_avg": 20.983, "energy_joules_est": 232.63, "duration_seconds": 11.087, "sample_count": 95}, "timestamp": "2026-01-25T19:28:54.622480"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11300.722, "latencies_ms": [11300.722], "images_per_second": 0.088, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a white clock tower with a green dome, standing majestically against a clear blue sky. The tower is adorned with a clock face, and a weather vane on top, indicating the direction of the wind. The perspective of the image is from a low angle, looking up at the tower, giving it a grand and imposing appearance", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.5, "ram_available_mb": 38406.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24438.9, "ram_available_mb": 38402.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.781}, "power_stats": {"power_gpu_soc_mean_watts": 21.002, "power_cpu_cv_mean_watts": 1.876, "power_sys_5v0_mean_watts": 8.921, "gpu_utilization_percent_mean": 70.781, "power_watts_avg": 21.002, "energy_joules_est": 237.35, "duration_seconds": 11.301, "sample_count": 96}, "timestamp": "2026-01-25T19:29:07.958861"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5990.39, "latencies_ms": [5990.39], "images_per_second": 0.167, "prompt_tokens": 36, "response_tokens_est": 33, "n_tiles": 16, "output_text": "The image features a clear blue sky and a white clock tower with a green dome. The roof of the building is covered in terracotta tiles.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24438.9, "ram_available_mb": 38402.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24436.2, "ram_available_mb": 38404.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.51}, "power_stats": {"power_gpu_soc_mean_watts": 23.402, "power_cpu_cv_mean_watts": 1.499, "power_sys_5v0_mean_watts": 9.114, "gpu_utilization_percent_mean": 74.51, "power_watts_avg": 23.402, "energy_joules_est": 140.2, "duration_seconds": 5.991, "sample_count": 51}, "timestamp": "2026-01-25T19:29:15.998681"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11118.914, "latencies_ms": [11118.914], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene moment in the wild, featuring a group of elephants in their natural habitat. The elephant in the foreground, a majestic creature with a dark gray skin, is the focal point of the image. Its trunk, a symbol of strength and dexterity, is extended towards the ground, perhaps in search of food or simply exploring", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24436.2, "ram_available_mb": 38404.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24441.8, "ram_available_mb": 38399.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.411}, "power_stats": {"power_gpu_soc_mean_watts": 20.948, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.411, "power_watts_avg": 20.948, "energy_joules_est": 232.93, "duration_seconds": 11.12, "sample_count": 95}, "timestamp": "2026-01-25T19:29:29.154744"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7628.38, "latencies_ms": [7628.38], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "elephant: 5, tree: 10, grass: 15, dirt: 20, path: 1, water: 0, rock: 2, bush: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24441.8, "ram_available_mb": 38399.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24447.4, "ram_available_mb": 38393.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.781}, "power_stats": {"power_gpu_soc_mean_watts": 22.579, "power_cpu_cv_mean_watts": 1.639, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 72.781, "power_watts_avg": 22.579, "energy_joules_est": 172.26, "duration_seconds": 7.629, "sample_count": 64}, "timestamp": "2026-01-25T19:29:38.816569"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10446.441, "latencies_ms": [10446.441], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, there is a single elephant standing on the left side of the image, facing towards the right. Behind it, there is a group of elephants, with one partially visible on the right side and others in the background. The elephants are positioned in a natural setting with trees and vegetation surrounding them.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24447.4, "ram_available_mb": 38393.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24437.9, "ram_available_mb": 38403.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.618}, "power_stats": {"power_gpu_soc_mean_watts": 21.128, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 69.618, "power_watts_avg": 21.128, "energy_joules_est": 220.73, "duration_seconds": 10.447, "sample_count": 89}, "timestamp": "2026-01-25T19:29:51.314396"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8731.377, "latencies_ms": [8731.377], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "A herd of elephants is gathered in a natural setting, possibly a forest or a grassland, with trees and vegetation in the background. The elephants are standing close together, with one elephant in the foreground looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24437.9, "ram_available_mb": 38403.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.081}, "power_stats": {"power_gpu_soc_mean_watts": 21.861, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 71.081, "power_watts_avg": 21.861, "energy_joules_est": 190.89, "duration_seconds": 8.732, "sample_count": 74}, "timestamp": "2026-01-25T19:30:02.067461"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9878.95, "latencies_ms": [9878.95], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image features a group of elephants with a prominent one in the foreground that has its trunk curled up towards its mouth, showcasing its tusks. The lighting is natural and soft, suggesting the photo was taken during the day in a shaded area, possibly under the canopy of trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24447.1, "ram_available_mb": 38393.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.169}, "power_stats": {"power_gpu_soc_mean_watts": 21.289, "power_cpu_cv_mean_watts": 1.857, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 70.169, "power_watts_avg": 21.289, "energy_joules_est": 210.33, "duration_seconds": 9.88, "sample_count": 83}, "timestamp": "2026-01-25T19:30:13.965865"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11118.908, "latencies_ms": [11118.908], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility in a kitchen, where a white refrigerator stands as the centerpiece. The refrigerator, pristine and clean, is open, revealing its empty shelves and drawers. The top shelf is devoid of any items, while the bottom shelf houses a single carton of eggs. The re", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 24447.1, "ram_available_mb": 38393.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 24466.3, "ram_available_mb": 38374.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.245}, "power_stats": {"power_gpu_soc_mean_watts": 20.88, "power_cpu_cv_mean_watts": 2.006, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 70.245, "power_watts_avg": 20.88, "energy_joules_est": 232.18, "duration_seconds": 11.12, "sample_count": 94}, "timestamp": "2026-01-25T19:30:27.117630"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10310.558, "latencies_ms": [10310.558], "images_per_second": 0.097, "prompt_tokens": 39, "response_tokens_est": 70, "n_tiles": 16, "output_text": "1. Refrigerator: 1\n2. Drawer: 1\n3. Shelf: 1\n4. Egg carton: 1\n5. Bottle: 1\n6. Door: 1\n7. Shelf: 1\n8. Drawer: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24466.3, "ram_available_mb": 38374.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24464.3, "ram_available_mb": 38376.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.761}, "power_stats": {"power_gpu_soc_mean_watts": 21.342, "power_cpu_cv_mean_watts": 1.838, "power_sys_5v0_mean_watts": 8.931, "gpu_utilization_percent_mean": 71.761, "power_watts_avg": 21.342, "energy_joules_est": 220.06, "duration_seconds": 10.311, "sample_count": 88}, "timestamp": "2026-01-25T19:30:39.441195"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10715.23, "latencies_ms": [10715.23], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The refrigerator is the main object in the foreground, with its door open revealing the interior. The shelves and drawers are empty, with the top shelf containing a carton of eggs and a bottle of condiment. The background is less distinct, but it appears to be the interior of a kitchen with a tiled floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24464.3, "ram_available_mb": 38376.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24466.1, "ram_available_mb": 38374.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.101, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 71.0, "power_watts_avg": 21.101, "energy_joules_est": 226.11, "duration_seconds": 10.716, "sample_count": 91}, "timestamp": "2026-01-25T19:30:52.172738"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8651.524, "latencies_ms": [8651.524], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows an open refrigerator with its door wide open, revealing the empty shelves and drawers inside. The refrigerator is placed in a room with a tiled floor, and the lighting suggests it might be nighttime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24466.1, "ram_available_mb": 38374.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24460.8, "ram_available_mb": 38380.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.216}, "power_stats": {"power_gpu_soc_mean_watts": 21.984, "power_cpu_cv_mean_watts": 1.715, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 72.216, "power_watts_avg": 21.984, "energy_joules_est": 190.21, "duration_seconds": 8.652, "sample_count": 74}, "timestamp": "2026-01-25T19:31:02.858558"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7142.89, "latencies_ms": [7142.89], "images_per_second": 0.14, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image shows an open refrigerator with a light interior, possibly white or light gray. The shelves are empty, and there is a bottle of beer on the top shelf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24460.8, "ram_available_mb": 38380.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24455.1, "ram_available_mb": 38385.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.443}, "power_stats": {"power_gpu_soc_mean_watts": 22.585, "power_cpu_cv_mean_watts": 1.64, "power_sys_5v0_mean_watts": 9.028, "gpu_utilization_percent_mean": 72.443, "power_watts_avg": 22.585, "energy_joules_est": 161.34, "duration_seconds": 7.144, "sample_count": 61}, "timestamp": "2026-01-25T19:31:12.021737"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12333.551, "latencies_ms": [12333.551], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a close-up view of a bunch of bananas resting on a shelf. The bananas are yellow, indicating they are ripe and ready to eat. They are arranged in a neat row, with each banana slightly overlapping the one next to it. The shelf they are on is metallic, reflecting a soft purple light that ill", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24455.1, "ram_available_mb": 38385.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24525.4, "ram_available_mb": 38315.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.733}, "power_stats": {"power_gpu_soc_mean_watts": 22.814, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 9.187, "gpu_utilization_percent_mean": 73.733, "power_watts_avg": 22.814, "energy_joules_est": 281.39, "duration_seconds": 12.334, "sample_count": 105}, "timestamp": "2026-01-25T19:31:26.424616"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10875.463, "latencies_ms": [10875.463], "images_per_second": 0.092, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "banana: 8\n\nbananas: 8\n\nbanana bunch: 1\n\nbanana peel: 0\n\nbanana sticker: 2\n\nbanana label: 2\n\nbanana skin: 0\n\nbanana stem: 0", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24445.2, "ram_available_mb": 38395.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24457.9, "ram_available_mb": 38383.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.674}, "power_stats": {"power_gpu_soc_mean_watts": 23.474, "power_cpu_cv_mean_watts": 1.636, "power_sys_5v0_mean_watts": 9.152, "gpu_utilization_percent_mean": 76.674, "power_watts_avg": 23.474, "energy_joules_est": 255.31, "duration_seconds": 10.876, "sample_count": 92}, "timestamp": "2026-01-25T19:31:39.343441"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12431.449, "latencies_ms": [12431.449], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The bananas are arranged in a diagonal line from the top left to the bottom right of the image, creating a sense of depth. The left side of the image is blurred, making it difficult to discern any details, while the right side is in focus, showing the individual bananas and their sticker labels clearly. The bananas in the foreground appear larger and more detailed than", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24457.9, "ram_available_mb": 38383.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24467.5, "ram_available_mb": 38373.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.047}, "power_stats": {"power_gpu_soc_mean_watts": 22.889, "power_cpu_cv_mean_watts": 1.764, "power_sys_5v0_mean_watts": 9.2, "gpu_utilization_percent_mean": 74.047, "power_watts_avg": 22.889, "energy_joules_est": 284.56, "duration_seconds": 12.432, "sample_count": 106}, "timestamp": "2026-01-25T19:31:53.796089"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10203.838, "latencies_ms": [10203.838], "images_per_second": 0.098, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image shows a close-up of a bunch of bananas with a sticker on them, placed on a surface with a blurred background. The focus is on the bananas, and the background is out of focus, making it difficult to determine the exact setting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24467.5, "ram_available_mb": 38373.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24541.4, "ram_available_mb": 38299.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.368}, "power_stats": {"power_gpu_soc_mean_watts": 23.634, "power_cpu_cv_mean_watts": 1.56, "power_sys_5v0_mean_watts": 9.14, "gpu_utilization_percent_mean": 76.368, "power_watts_avg": 23.634, "energy_joules_est": 241.17, "duration_seconds": 10.204, "sample_count": 87}, "timestamp": "2026-01-25T19:32:06.022000"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12432.363, "latencies_ms": [12432.363], "images_per_second": 0.08, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a close-up of a bunch of bananas with a focus on their yellow skin, which has a slight greenish tint at the top, indicating they are ripe. The lighting in the image is soft and diffused, casting a gentle shadow on the bananas, which are placed against a blurred background with a purple hue that suggests a cooler", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24445.6, "ram_available_mb": 38395.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24531.8, "ram_available_mb": 38309.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.262}, "power_stats": {"power_gpu_soc_mean_watts": 22.824, "power_cpu_cv_mean_watts": 1.766, "power_sys_5v0_mean_watts": 9.186, "gpu_utilization_percent_mean": 74.262, "power_watts_avg": 22.824, "energy_joules_est": 283.77, "duration_seconds": 12.433, "sample_count": 107}, "timestamp": "2026-01-25T19:32:20.516019"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12180.046, "latencies_ms": [12180.046], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The image captures a city street scene with three large, orange, cylindrical objects that resemble fire hydrants. They are placed on a sidewalk, and the street is wet, suggesting recent rain or snow. In the background, there are buildings with a mix of architectural styles, and the sky is overcast. The overall atmosphere is quiet and serene.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24531.8, "ram_available_mb": 38309.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24460.4, "ram_available_mb": 38380.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.24}, "power_stats": {"power_gpu_soc_mean_watts": 22.843, "power_cpu_cv_mean_watts": 1.778, "power_sys_5v0_mean_watts": 9.199, "gpu_utilization_percent_mean": 74.24, "power_watts_avg": 22.843, "energy_joules_est": 278.25, "duration_seconds": 12.181, "sample_count": 104}, "timestamp": "2026-01-25T19:32:34.745449"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 12332.751, "latencies_ms": [12332.751], "images_per_second": 0.081, "prompt_tokens": 39, "response_tokens_est": 76, "n_tiles": 16, "output_text": "object: 3, count: 3\nobject: trees, count: 4\nobject: buildings, count: 3\nobject: street, count: 1\nobject: snow, count: 1\nobject: traffic lights, count: 2\nobject: sidewalk, count: 1\nobject: bench, count: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24460.4, "ram_available_mb": 38380.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24460.2, "ram_available_mb": 38380.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.972}, "power_stats": {"power_gpu_soc_mean_watts": 23.037, "power_cpu_cv_mean_watts": 1.737, "power_sys_5v0_mean_watts": 9.161, "gpu_utilization_percent_mean": 74.972, "power_watts_avg": 23.037, "energy_joules_est": 284.13, "duration_seconds": 12.334, "sample_count": 106}, "timestamp": "2026-01-25T19:32:49.123795"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12433.111, "latencies_ms": [12433.111], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are three large, copper-colored bollards with circular designs on top, positioned close to the viewer. They are situated on a sidewalk that leads the eye towards the background, where a large, ornate building with many windows and a sign that reads 'IGN' can be seen. The bollards are in the near foreground,", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24460.2, "ram_available_mb": 38380.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24465.4, "ram_available_mb": 38375.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.72}, "power_stats": {"power_gpu_soc_mean_watts": 22.882, "power_cpu_cv_mean_watts": 1.77, "power_sys_5v0_mean_watts": 9.212, "gpu_utilization_percent_mean": 74.72, "power_watts_avg": 22.882, "energy_joules_est": 284.51, "duration_seconds": 12.434, "sample_count": 107}, "timestamp": "2026-01-25T19:33:03.612984"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11975.924, "latencies_ms": [11975.924], "images_per_second": 0.084, "prompt_tokens": 37, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The image depicts a city street with a row of large, ornate, copper-colored fire hydrants in the foreground, each with a unique design on top. The background features a mix of modern and older buildings, and the ground is covered with a layer of snow, suggesting a cold, winter day in an urban environment.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24465.4, "ram_available_mb": 38375.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24467.9, "ram_available_mb": 38373.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.118}, "power_stats": {"power_gpu_soc_mean_watts": 23.115, "power_cpu_cv_mean_watts": 1.715, "power_sys_5v0_mean_watts": 9.156, "gpu_utilization_percent_mean": 75.118, "power_watts_avg": 23.115, "energy_joules_est": 276.84, "duration_seconds": 11.977, "sample_count": 102}, "timestamp": "2026-01-25T19:33:17.629237"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12411.915, "latencies_ms": [12411.915], "images_per_second": 0.081, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a series of three large, copper-colored fire hydrants with ornate designs on top, set against a backdrop of a snowy urban street. The hydrants are positioned on a sidewalk that is partially covered with snow, and the buildings in the background have a mix of architectural styles, with one prominent building displaying a blue glass facade.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24467.9, "ram_available_mb": 38373.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24466.4, "ram_available_mb": 38374.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.981}, "power_stats": {"power_gpu_soc_mean_watts": 22.876, "power_cpu_cv_mean_watts": 1.769, "power_sys_5v0_mean_watts": 9.205, "gpu_utilization_percent_mean": 73.981, "power_watts_avg": 22.876, "energy_joules_est": 283.95, "duration_seconds": 12.413, "sample_count": 105}, "timestamp": "2026-01-25T19:33:32.056626"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12321.964, "latencies_ms": [12321.964], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a jockey is seen riding a brown horse on a dirt track. The jockey, clad in a yellow and green uniform, is wearing a white helmet. The horse, adorned with a black bridle, is in motion, its legs lifted off the ground, indicating a gallop. The track is marked by a white fence on the", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24466.4, "ram_available_mb": 38374.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24452.0, "ram_available_mb": 38388.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.24}, "power_stats": {"power_gpu_soc_mean_watts": 22.821, "power_cpu_cv_mean_watts": 1.778, "power_sys_5v0_mean_watts": 9.195, "gpu_utilization_percent_mean": 74.24, "power_watts_avg": 22.821, "energy_joules_est": 281.21, "duration_seconds": 12.323, "sample_count": 104}, "timestamp": "2026-01-25T19:33:46.426904"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10968.395, "latencies_ms": [10968.395], "images_per_second": 0.091, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "- Horse: 1\n\n- Jockey: 1\n\n- Cart: 1\n\n- Number 8 sign: 1\n\n- Grass: 1\n\n- Fence: 1\n\n- Cart track: 1\n\n- Signs: 2", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24452.0, "ram_available_mb": 38388.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24462.6, "ram_available_mb": 38378.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.785}, "power_stats": {"power_gpu_soc_mean_watts": 23.464, "power_cpu_cv_mean_watts": 1.623, "power_sys_5v0_mean_watts": 9.155, "gpu_utilization_percent_mean": 75.785, "power_watts_avg": 23.464, "energy_joules_est": 257.38, "duration_seconds": 10.969, "sample_count": 93}, "timestamp": "2026-01-25T19:33:59.439351"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12400.549, "latencies_ms": [12400.549], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person wearing a helmet and a yellow and green outfit, riding a brown horse with the number 8 on its back. The horse is in motion, likely racing, and is positioned in the center of the image. In the background, there are banners with the words \"MAGNUM\" and \"WINSTONE\" along", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24462.6, "ram_available_mb": 38378.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24445.8, "ram_available_mb": 38395.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.858}, "power_stats": {"power_gpu_soc_mean_watts": 22.902, "power_cpu_cv_mean_watts": 1.767, "power_sys_5v0_mean_watts": 9.224, "gpu_utilization_percent_mean": 73.858, "power_watts_avg": 22.902, "energy_joules_est": 284.01, "duration_seconds": 12.401, "sample_count": 106}, "timestamp": "2026-01-25T19:34:13.855167"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8154.707, "latencies_ms": [8154.707], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A jockey is riding a brown horse with the number 8 on its saddle, racing on a track with banners for Magnum and Winstone in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24445.8, "ram_available_mb": 38395.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24449.8, "ram_available_mb": 38391.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.348}, "power_stats": {"power_gpu_soc_mean_watts": 24.643, "power_cpu_cv_mean_watts": 1.45, "power_sys_5v0_mean_watts": 9.188, "gpu_utilization_percent_mean": 79.348, "power_watts_avg": 24.643, "energy_joules_est": 200.97, "duration_seconds": 8.155, "sample_count": 69}, "timestamp": "2026-01-25T19:34:24.065058"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9711.429, "latencies_ms": [9711.429], "images_per_second": 0.103, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image captures a moment of a horse race, with the horse and jockey in motion on a dirt track. The lighting is natural, suggesting it is daytime, and the weather appears to be clear with no signs of rain or adverse conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24449.8, "ram_available_mb": 38391.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24447.7, "ram_available_mb": 38393.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.012}, "power_stats": {"power_gpu_soc_mean_watts": 23.687, "power_cpu_cv_mean_watts": 1.587, "power_sys_5v0_mean_watts": 9.239, "gpu_utilization_percent_mean": 76.012, "power_watts_avg": 23.687, "energy_joules_est": 230.05, "duration_seconds": 9.712, "sample_count": 82}, "timestamp": "2026-01-25T19:34:35.818966"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11139.343, "latencies_ms": [11139.343], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a brown dog is standing on a concrete ledge in a backyard. The dog is positioned in the center of the frame, looking directly at the camera with a focused expression. The backyard is enclosed by a wooden fence, and there is a tree nearby.\n\nThere are several oranges scattered around the area, with some close to the dog and others", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24447.7, "ram_available_mb": 38393.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24442.1, "ram_available_mb": 38398.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.389}, "power_stats": {"power_gpu_soc_mean_watts": 20.947, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 69.389, "power_watts_avg": 20.947, "energy_joules_est": 233.35, "duration_seconds": 11.14, "sample_count": 95}, "timestamp": "2026-01-25T19:34:49.008143"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7172.248, "latencies_ms": [7172.248], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "dog: 1, tree: 1, lemon: 3, fence: 1, bush: 1, step: 1, house: 1, shadow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.1, "ram_available_mb": 38398.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24443.3, "ram_available_mb": 38397.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.623}, "power_stats": {"power_gpu_soc_mean_watts": 22.765, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 9.018, "gpu_utilization_percent_mean": 74.623, "power_watts_avg": 22.765, "energy_joules_est": 163.29, "duration_seconds": 7.173, "sample_count": 61}, "timestamp": "2026-01-25T19:34:58.192138"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7796.11, "latencies_ms": [7796.11], "images_per_second": 0.128, "prompt_tokens": 44, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The dog is standing on a ledge in the foreground of the image, with a garden and a fence in the background. The tree with green leaves is behind the dog, and there are yellow fruits hanging from it.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24443.3, "ram_available_mb": 38397.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24446.2, "ram_available_mb": 38394.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.215}, "power_stats": {"power_gpu_soc_mean_watts": 22.287, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 9.064, "gpu_utilization_percent_mean": 72.215, "power_watts_avg": 22.287, "energy_joules_est": 173.77, "duration_seconds": 7.797, "sample_count": 65}, "timestamp": "2026-01-25T19:35:08.002231"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7198.574, "latencies_ms": [7198.574], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A dog is standing on a concrete ledge in a backyard. The backyard has a wooden fence, a small tree with green leaves, and a few yellow fruits hanging from it.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24446.2, "ram_available_mb": 38394.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.7}, "power_stats": {"power_gpu_soc_mean_watts": 22.835, "power_cpu_cv_mean_watts": 1.575, "power_sys_5v0_mean_watts": 9.005, "gpu_utilization_percent_mean": 74.7, "power_watts_avg": 22.835, "energy_joules_est": 164.39, "duration_seconds": 7.199, "sample_count": 60}, "timestamp": "2026-01-25T19:35:17.244599"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6447.584, "latencies_ms": [6447.584], "images_per_second": 0.155, "prompt_tokens": 36, "response_tokens_est": 37, "n_tiles": 16, "output_text": "The dog is a light brown color with a white chest and paws. It is standing on a concrete ledge with a wooden fence and green trees in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24438.7, "ram_available_mb": 38402.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.204}, "power_stats": {"power_gpu_soc_mean_watts": 23.046, "power_cpu_cv_mean_watts": 1.534, "power_sys_5v0_mean_watts": 9.071, "gpu_utilization_percent_mean": 73.204, "power_watts_avg": 23.046, "energy_joules_est": 148.61, "duration_seconds": 6.448, "sample_count": 54}, "timestamp": "2026-01-25T19:35:25.709166"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11173.848, "latencies_ms": [11173.848], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is standing on a wooden bench that is placed on a brick sidewalk. The person is wearing red pants and blue shoes. The bench is positioned against a gray concrete wall. On the bench, there is a blue sign with the text \"WETENER\" written on it. The person's feet are resting on the", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24438.5, "ram_available_mb": 38402.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.2}, "power_stats": {"power_gpu_soc_mean_watts": 20.826, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.2, "power_watts_avg": 20.826, "energy_joules_est": 232.72, "duration_seconds": 11.174, "sample_count": 95}, "timestamp": "2026-01-25T19:35:38.910959"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7738.598, "latencies_ms": [7738.598], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "bench: 1, person: 1, brick: 100, wall: 1, paper: 1, screw: 4, screwdriver: 1, shoes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.5, "ram_available_mb": 38402.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24442.6, "ram_available_mb": 38398.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.591}, "power_stats": {"power_gpu_soc_mean_watts": 22.507, "power_cpu_cv_mean_watts": 1.644, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 73.591, "power_watts_avg": 22.507, "energy_joules_est": 174.19, "duration_seconds": 7.74, "sample_count": 66}, "timestamp": "2026-01-25T19:35:48.693328"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9339.992, "latencies_ms": [9339.992], "images_per_second": 0.107, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "A person is standing on a wooden bench that is positioned against a concrete wall. The bench is in the foreground of the image, while the concrete wall serves as the background. The person's feet are near the edge of the bench, indicating they are close to the bench.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24442.6, "ram_available_mb": 38398.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24443.1, "ram_available_mb": 38397.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.278}, "power_stats": {"power_gpu_soc_mean_watts": 21.536, "power_cpu_cv_mean_watts": 1.824, "power_sys_5v0_mean_watts": 9.019, "gpu_utilization_percent_mean": 70.278, "power_watts_avg": 21.536, "energy_joules_est": 201.16, "duration_seconds": 9.341, "sample_count": 79}, "timestamp": "2026-01-25T19:36:00.092131"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7715.735, "latencies_ms": [7715.735], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A person is standing on a wooden bench with a handwritten note that says \"Wetener you got to be sitting on me.\" The bench is located on a brick pavement with a concrete wall in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24443.1, "ram_available_mb": 38397.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24436.0, "ram_available_mb": 38404.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.692}, "power_stats": {"power_gpu_soc_mean_watts": 22.493, "power_cpu_cv_mean_watts": 1.663, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 73.692, "power_watts_avg": 22.493, "energy_joules_est": 173.56, "duration_seconds": 7.716, "sample_count": 65}, "timestamp": "2026-01-25T19:36:09.839768"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7880.146, "latencies_ms": [7880.146], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A person is standing on a wooden bench with a blue sign that reads \"WETENER you got to be sitting on me!\" The bench is made of wood and the person is wearing red pants and blue shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.0, "ram_available_mb": 38404.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.191}, "power_stats": {"power_gpu_soc_mean_watts": 22.057, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 9.016, "gpu_utilization_percent_mean": 71.191, "power_watts_avg": 22.057, "energy_joules_est": 173.83, "duration_seconds": 7.881, "sample_count": 68}, "timestamp": "2026-01-25T19:36:19.776996"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11148.578, "latencies_ms": [11148.578], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a well-decorated living room with a red couch positioned on the left side of the room. A dining table is set up in the middle of the room, surrounded by chairs. The table is adorned with a white tablecloth, and there are various items on it, including a vase, a bowl, and a cup. \n", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24439.3, "ram_available_mb": 38401.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.175}, "power_stats": {"power_gpu_soc_mean_watts": 20.792, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 69.175, "power_watts_avg": 20.792, "energy_joules_est": 231.81, "duration_seconds": 11.149, "sample_count": 97}, "timestamp": "2026-01-25T19:36:32.989185"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8615.393, "latencies_ms": [8615.393], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Couch: 2\n- Table: 2\n- Chair: 2\n- Lamp: 2\n- Picture frame: 1\n- Sofa: 1\n- Carpet: 1\n- Tablecloth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.3, "ram_available_mb": 38401.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24446.8, "ram_available_mb": 38394.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.603}, "power_stats": {"power_gpu_soc_mean_watts": 22.055, "power_cpu_cv_mean_watts": 1.722, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 72.603, "power_watts_avg": 22.055, "energy_joules_est": 190.03, "duration_seconds": 8.616, "sample_count": 73}, "timestamp": "2026-01-25T19:36:43.618438"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11102.825, "latencies_ms": [11102.825], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The living room features a red sofa in the foreground, positioned near a wooden side table with a lamp and a vase of flowers. In the background, there is a dining area with a white tablecloth, chairs, and a TV mounted on the wall. The curtains in the room create a separation between the living and dining areas, with the living area", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24446.8, "ram_available_mb": 38394.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.758}, "power_stats": {"power_gpu_soc_mean_watts": 20.98, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.758, "power_watts_avg": 20.98, "energy_joules_est": 232.95, "duration_seconds": 11.103, "sample_count": 95}, "timestamp": "2026-01-25T19:36:56.741252"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9289.957, "latencies_ms": [9289.957], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image depicts a well-lit and cozy living room with a red sofa, a small round table with a white tablecloth, and a TV mounted on the wall. The room is decorated with a framed picture on the wall and a lamp on a side table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24431.5, "ram_available_mb": 38409.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.848}, "power_stats": {"power_gpu_soc_mean_watts": 21.615, "power_cpu_cv_mean_watts": 1.784, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 71.848, "power_watts_avg": 21.615, "energy_joules_est": 200.82, "duration_seconds": 9.291, "sample_count": 79}, "timestamp": "2026-01-25T19:37:08.071175"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8641.856, "latencies_ms": [8641.856], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The room is well-lit with natural light coming through the sheer curtains, and the furniture is made of wood with a warm brown finish. The walls are painted in a light beige color, and there is a large painting hanging above the sofa.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.5, "ram_available_mb": 38409.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24436.7, "ram_available_mb": 38404.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.787}, "power_stats": {"power_gpu_soc_mean_watts": 21.744, "power_cpu_cv_mean_watts": 1.793, "power_sys_5v0_mean_watts": 9.014, "gpu_utilization_percent_mean": 70.787, "power_watts_avg": 21.744, "energy_joules_est": 187.92, "duration_seconds": 8.642, "sample_count": 75}, "timestamp": "2026-01-25T19:37:18.730742"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11113.124, "latencies_ms": [11113.124], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a close-up of a cooking pan on a stove containing a stir-fry dish. The dish consists of various pieces of broccoli, carrots, and what appears to be diced ham or sausage. The vegetables are cut into bite-sized pieces and are mixed together, suggesting they are being cooked. The pan is", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24436.7, "ram_available_mb": 38404.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24445.7, "ram_available_mb": 38395.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.365}, "power_stats": {"power_gpu_soc_mean_watts": 20.91, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 69.365, "power_watts_avg": 20.91, "energy_joules_est": 232.39, "duration_seconds": 11.114, "sample_count": 96}, "timestamp": "2026-01-25T19:37:31.881755"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8187.884, "latencies_ms": [8187.884], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "broccoli: 12, carrots: 5, meat: 8, tomatoes: 2, onions: 1, garlic: 1, pepper: 1, olive oil: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24445.7, "ram_available_mb": 38395.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24439.9, "ram_available_mb": 38401.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.29}, "power_stats": {"power_gpu_soc_mean_watts": 22.244, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 72.29, "power_watts_avg": 22.244, "energy_joules_est": 182.15, "duration_seconds": 8.189, "sample_count": 69}, "timestamp": "2026-01-25T19:37:42.095564"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11114.819, "latencies_ms": [11114.819], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a skillet containing a mix of diced vegetables and meat, with the broccoli occupying the most space and being the most prominent object. The carrots and tomatoes are scattered around the broccoli, with the carrots being more towards the left side and the tomatoes more towards the right. The meat is interspersed", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.9, "ram_available_mb": 38401.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24443.6, "ram_available_mb": 38397.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.323}, "power_stats": {"power_gpu_soc_mean_watts": 20.943, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.323, "power_watts_avg": 20.943, "energy_joules_est": 232.79, "duration_seconds": 11.115, "sample_count": 96}, "timestamp": "2026-01-25T19:37:55.241830"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9056.551, "latencies_ms": [9056.551], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a skillet on a stove containing a mix of diced vegetables and chunks of meat, likely being cooked. The vegetables include broccoli and carrots, and the meat appears to be diced ham or a similar type of pork.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24443.6, "ram_available_mb": 38397.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24437.0, "ram_available_mb": 38403.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.705, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 71.0, "power_watts_avg": 21.705, "energy_joules_est": 196.59, "duration_seconds": 9.057, "sample_count": 77}, "timestamp": "2026-01-25T19:38:06.329136"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7974.968, "latencies_ms": [7974.968], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a skillet on a stove containing a mix of cooked vegetables and meat. The vegetables appear to be broccoli and carrots, and the meat looks like it could be diced ham or sausage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.0, "ram_available_mb": 38403.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24433.1, "ram_available_mb": 38407.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.691}, "power_stats": {"power_gpu_soc_mean_watts": 22.077, "power_cpu_cv_mean_watts": 1.719, "power_sys_5v0_mean_watts": 9.014, "gpu_utilization_percent_mean": 71.691, "power_watts_avg": 22.077, "energy_joules_est": 176.08, "duration_seconds": 7.976, "sample_count": 68}, "timestamp": "2026-01-25T19:38:16.322783"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11082.143, "latencies_ms": [11082.143], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a delightful scene of a meal in progress. At the center of the frame, a brown plate holds four hot dogs, each nestled in a soft bun. The hot dogs are generously topped with a vibrant yellow mustard, adding a pop of color to the scene. The plate is placed on a black countertop, providing a stark contrast to", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24433.1, "ram_available_mb": 38407.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 24463.3, "ram_available_mb": 38377.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.574}, "power_stats": {"power_gpu_soc_mean_watts": 20.998, "power_cpu_cv_mean_watts": 1.989, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 69.574, "power_watts_avg": 20.998, "energy_joules_est": 232.72, "duration_seconds": 11.083, "sample_count": 94}, "timestamp": "2026-01-25T19:38:29.475755"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5622.633, "latencies_ms": [5622.633], "images_per_second": 0.178, "prompt_tokens": 39, "response_tokens_est": 28, "n_tiles": 16, "output_text": "hot dog: 4, bun: 4, mustard: 4, plate: 1, magazine: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24463.3, "ram_available_mb": 38377.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24457.2, "ram_available_mb": 38383.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.894}, "power_stats": {"power_gpu_soc_mean_watts": 24.304, "power_cpu_cv_mean_watts": 1.354, "power_sys_5v0_mean_watts": 9.052, "gpu_utilization_percent_mean": 76.894, "power_watts_avg": 24.304, "energy_joules_est": 136.67, "duration_seconds": 5.623, "sample_count": 47}, "timestamp": "2026-01-25T19:38:37.150992"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9048.322, "latencies_ms": [9048.322], "images_per_second": 0.111, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The hot dogs are arranged in a row, with one in the foreground and two slightly behind it, all positioned towards the left side of the plate. The plate is placed on a surface that appears to be a kitchen counter, with a magazine partially visible in the background to the right.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24457.2, "ram_available_mb": 38383.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24460.3, "ram_available_mb": 38380.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.079}, "power_stats": {"power_gpu_soc_mean_watts": 21.756, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 9.025, "gpu_utilization_percent_mean": 72.079, "power_watts_avg": 21.756, "energy_joules_est": 196.87, "duration_seconds": 9.049, "sample_count": 76}, "timestamp": "2026-01-25T19:38:48.222737"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8762.868, "latencies_ms": [8762.868], "images_per_second": 0.114, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a plate with four hot dogs, each with a bun and a generous amount of yellow mustard on top. The plate is placed on a granite countertop, and there is a magazine with the title \"Sports\" visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24460.3, "ram_available_mb": 38380.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24564.5, "ram_available_mb": 38276.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.419}, "power_stats": {"power_gpu_soc_mean_watts": 22.005, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 72.419, "power_watts_avg": 22.005, "energy_joules_est": 192.84, "duration_seconds": 8.764, "sample_count": 74}, "timestamp": "2026-01-25T19:38:58.999656"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8587.804, "latencies_ms": [8587.804], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows three hot dogs with bright yellow mustard on a brown plate, which is placed on a granite countertop. The lighting in the image is bright, highlighting the glossy texture of the mustard and the softness of the buns.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24564.5, "ram_available_mb": 38276.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24565.7, "ram_available_mb": 38275.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.589}, "power_stats": {"power_gpu_soc_mean_watts": 21.864, "power_cpu_cv_mean_watts": 1.749, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 70.589, "power_watts_avg": 21.864, "energy_joules_est": 187.78, "duration_seconds": 8.588, "sample_count": 73}, "timestamp": "2026-01-25T19:39:09.640102"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12272.264, "latencies_ms": [12272.264], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene beach scene. A green umbrella, adorned with a floral pattern, stands tall on the sandy beach, providing a vibrant contrast to the surrounding environment. Two blue and one pink beach chair are positioned under the umbrella, offering a spot of shade for those who prefer to relax there. The beach is not crow", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24565.7, "ram_available_mb": 38275.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.679}, "power_stats": {"power_gpu_soc_mean_watts": 22.901, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 9.225, "gpu_utilization_percent_mean": 73.679, "power_watts_avg": 22.901, "energy_joules_est": 281.06, "duration_seconds": 12.273, "sample_count": 106}, "timestamp": "2026-01-25T19:39:23.946798"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8539.918, "latencies_ms": [8539.918], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "umbrella: 1, chair: 2, person: 4, beach: 1, water: 1, sand: 1, wave: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24577.3, "ram_available_mb": 38263.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.712}, "power_stats": {"power_gpu_soc_mean_watts": 24.354, "power_cpu_cv_mean_watts": 1.404, "power_sys_5v0_mean_watts": 9.173, "gpu_utilization_percent_mean": 78.712, "power_watts_avg": 24.354, "energy_joules_est": 208.0, "duration_seconds": 8.541, "sample_count": 73}, "timestamp": "2026-01-25T19:39:34.519893"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12376.593, "latencies_ms": [12376.593], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two beach chairs, one pink and one blue, positioned close to each other under a green umbrella. The umbrella is on the sand, and the chairs are near the water's edge. In the background, there are four individuals in the water, with one person closer to the shore and the others further out. The water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24577.3, "ram_available_mb": 38263.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24532.4, "ram_available_mb": 38308.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.019}, "power_stats": {"power_gpu_soc_mean_watts": 22.876, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 9.21, "gpu_utilization_percent_mean": 74.019, "power_watts_avg": 22.876, "energy_joules_est": 283.14, "duration_seconds": 12.377, "sample_count": 106}, "timestamp": "2026-01-25T19:39:48.916702"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7139.875, "latencies_ms": [7139.875], "images_per_second": 0.14, "prompt_tokens": 37, "response_tokens_est": 30, "n_tiles": 16, "output_text": "A group of people are swimming in the ocean near a beach with a green umbrella and two folding chairs on the sand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24532.4, "ram_available_mb": 38308.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 81.082}, "power_stats": {"power_gpu_soc_mean_watts": 25.249, "power_cpu_cv_mean_watts": 1.253, "power_sys_5v0_mean_watts": 9.195, "gpu_utilization_percent_mean": 81.082, "power_watts_avg": 25.249, "energy_joules_est": 180.29, "duration_seconds": 7.14, "sample_count": 61}, "timestamp": "2026-01-25T19:39:58.080463"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9699.001, "latencies_ms": [9699.001], "images_per_second": 0.103, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "A vibrant green umbrella is open on the sandy beach, providing shade for two colorful beach chairs, one pink and one blue. The sky is overcast, and the water appears calm with small waves lapping at the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24435.0, "ram_available_mb": 38405.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.659}, "power_stats": {"power_gpu_soc_mean_watts": 23.64, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 9.234, "gpu_utilization_percent_mean": 75.659, "power_watts_avg": 23.64, "energy_joules_est": 229.3, "duration_seconds": 9.7, "sample_count": 82}, "timestamp": "2026-01-25T19:40:09.822410"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11143.252, "latencies_ms": [11143.252], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a vintage kitchen with a green and white color scheme. The kitchen is filled with various appliances and items, including a sink, a refrigerator, a microwave, and a toaster. There are also several bowls placed on the countertops, and a dining table is situated in the room.\n\nA wooden chair is positioned", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 24435.0, "ram_available_mb": 38405.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24436.6, "ram_available_mb": 38404.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.344}, "power_stats": {"power_gpu_soc_mean_watts": 20.835, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.934, "gpu_utilization_percent_mean": 69.344, "power_watts_avg": 20.835, "energy_joules_est": 232.18, "duration_seconds": 11.144, "sample_count": 96}, "timestamp": "2026-01-25T19:40:23.008537"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8642.788, "latencies_ms": [8642.788], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Table: 1\n- Chair: 1\n- Cabinet: 1\n- Stove: 1\n- Oven: 1\n- Microwave: 1\n- Countertop: 1\n- Jar: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24436.6, "ram_available_mb": 38404.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24436.0, "ram_available_mb": 38404.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.26}, "power_stats": {"power_gpu_soc_mean_watts": 21.811, "power_cpu_cv_mean_watts": 1.722, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 71.26, "power_watts_avg": 21.811, "energy_joules_est": 188.52, "duration_seconds": 8.643, "sample_count": 73}, "timestamp": "2026-01-25T19:40:33.689734"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11114.276, "latencies_ms": [11114.276], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden chair positioned near the center of the image, with a table in front of it. The table is surrounded by various kitchen items such as a pot, a bowl, and a jar. In the background, there is a green refrigerator on the right side and a wooden cabinet on the left side. The walls are adorned with", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24436.0, "ram_available_mb": 38404.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24437.5, "ram_available_mb": 38403.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.737}, "power_stats": {"power_gpu_soc_mean_watts": 20.988, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.737, "power_watts_avg": 20.988, "energy_joules_est": 233.28, "duration_seconds": 11.115, "sample_count": 95}, "timestamp": "2026-01-25T19:40:46.844532"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8398.162, "latencies_ms": [8398.162], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image depicts a vintage kitchen with green walls and a patterned wallpaper. The kitchen is furnished with a white refrigerator, a sink, a stove, a table with various items on it, and a chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.5, "ram_available_mb": 38403.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.521}, "power_stats": {"power_gpu_soc_mean_watts": 22.012, "power_cpu_cv_mean_watts": 1.708, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 72.521, "power_watts_avg": 22.012, "energy_joules_est": 184.88, "duration_seconds": 8.399, "sample_count": 71}, "timestamp": "2026-01-25T19:40:57.258000"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8436.275, "latencies_ms": [8436.275], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The kitchen is filled with vibrant green cabinets and a white sink, creating a contrast of colors. The lighting is warm and natural, coming from the windows with white curtains, and the wooden furniture adds a rustic touch to the room.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24439.3, "ram_available_mb": 38401.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.958}, "power_stats": {"power_gpu_soc_mean_watts": 21.802, "power_cpu_cv_mean_watts": 1.757, "power_sys_5v0_mean_watts": 9.019, "gpu_utilization_percent_mean": 70.958, "power_watts_avg": 21.802, "energy_joules_est": 183.94, "duration_seconds": 8.437, "sample_count": 72}, "timestamp": "2026-01-25T19:41:07.738261"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11141.545, "latencies_ms": [11141.545], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a serene woodland, a black and white dog is captured in a moment of pure joy. The dog, adorned with a blue collar, is in the midst of a playful chase, its body poised in mid-air as it leaps towards a red frisbee. The frisbee, a vibrant splash of", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24439.3, "ram_available_mb": 38401.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24445.6, "ram_available_mb": 38395.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.287}, "power_stats": {"power_gpu_soc_mean_watts": 20.913, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 69.287, "power_watts_avg": 20.913, "energy_joules_est": 233.02, "duration_seconds": 11.142, "sample_count": 94}, "timestamp": "2026-01-25T19:41:20.910215"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6864.149, "latencies_ms": [6864.149], "images_per_second": 0.146, "prompt_tokens": 39, "response_tokens_est": 39, "n_tiles": 16, "output_text": "dog: 1, stick: 1, leaves: many, ground: dry, tree: 1, moss: patchy, sunlight: visible, shadows: visible", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24445.6, "ram_available_mb": 38395.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24438.9, "ram_available_mb": 38402.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.78}, "power_stats": {"power_gpu_soc_mean_watts": 23.013, "power_cpu_cv_mean_watts": 1.54, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 74.78, "power_watts_avg": 23.013, "energy_joules_est": 157.98, "duration_seconds": 6.865, "sample_count": 59}, "timestamp": "2026-01-25T19:41:29.817670"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11135.493, "latencies_ms": [11135.493], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a black and white dog is positioned near the center of the image, holding a frisbee in its mouth. The dog is standing on a ground covered with brown leaves, which suggests it is in a park or a wooded area. To the right of the dog, there is a large green tree trunk, indicating that the dog is relatively close to the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.9, "ram_available_mb": 38402.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24442.7, "ram_available_mb": 38398.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.573}, "power_stats": {"power_gpu_soc_mean_watts": 20.936, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 69.573, "power_watts_avg": 20.936, "energy_joules_est": 233.15, "duration_seconds": 11.136, "sample_count": 96}, "timestamp": "2026-01-25T19:41:42.998807"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6957.294, "latencies_ms": [6957.294], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A black and white dog is playing with a frisbee in a wooded area with a large green tree. The ground is covered with fallen leaves, indicating that it is autumn.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.7, "ram_available_mb": 38398.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24432.2, "ram_available_mb": 38408.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.559}, "power_stats": {"power_gpu_soc_mean_watts": 22.874, "power_cpu_cv_mean_watts": 1.561, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 71.559, "power_watts_avg": 22.874, "energy_joules_est": 159.16, "duration_seconds": 6.958, "sample_count": 59}, "timestamp": "2026-01-25T19:41:51.976020"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9997.543, "latencies_ms": [9997.543], "images_per_second": 0.1, "prompt_tokens": 36, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image depicts a scene in a forest with a dog in the foreground. The ground is covered with brown leaves, and there is a large green tree trunk on the right side of the image. The lighting is natural and appears to be coming from the left side of the image, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24432.2, "ram_available_mb": 38408.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24436.3, "ram_available_mb": 38404.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.918}, "power_stats": {"power_gpu_soc_mean_watts": 21.296, "power_cpu_cv_mean_watts": 1.861, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.918, "power_watts_avg": 21.296, "energy_joules_est": 212.92, "duration_seconds": 9.998, "sample_count": 85}, "timestamp": "2026-01-25T19:42:03.984231"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11119.107, "latencies_ms": [11119.107], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is cross-country skiing on a snowy trail. The skier is wearing a white hat, a red backpack, and green boots. They are holding ski poles and appear to be in motion. The trail is surrounded by trees, and the sky is clear and blue. The skier's posture suggests they are in the middle of", "error": null, "sys_before": {"cpu_percent": 10.7, "ram_used_mb": 24436.3, "ram_available_mb": 38404.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24461.8, "ram_available_mb": 38379.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.766}, "power_stats": {"power_gpu_soc_mean_watts": 20.956, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 69.766, "power_watts_avg": 20.956, "energy_joules_est": 233.03, "duration_seconds": 11.12, "sample_count": 94}, "timestamp": "2026-01-25T19:42:17.140374"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7308.365, "latencies_ms": [7308.365], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "person: 1, ski: 2, ski pole: 2, backpack: 1, ski boot: 2, ski trail: 1, tree: many, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24461.8, "ram_available_mb": 38379.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24446.3, "ram_available_mb": 38394.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.262}, "power_stats": {"power_gpu_soc_mean_watts": 22.865, "power_cpu_cv_mean_watts": 1.674, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 74.262, "power_watts_avg": 22.865, "energy_joules_est": 167.12, "duration_seconds": 7.309, "sample_count": 61}, "timestamp": "2026-01-25T19:42:26.478256"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11156.432, "latencies_ms": [11156.432], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a skier is positioned on the left side of the image, moving towards the right along a snow-covered slope. The skier is closer to the viewer than the trees in the background, which line the edge of the slope. The trail in the snow leads from the skier's current position into the distance, suggesting a path that extends far beyond", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24446.3, "ram_available_mb": 38394.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24443.5, "ram_available_mb": 38397.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.263}, "power_stats": {"power_gpu_soc_mean_watts": 20.976, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 70.263, "power_watts_avg": 20.976, "energy_joules_est": 234.03, "duration_seconds": 11.157, "sample_count": 95}, "timestamp": "2026-01-25T19:42:39.691019"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6646.55, "latencies_ms": [6646.55], "images_per_second": 0.15, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A person is cross-country skiing on a snowy trail in a forested area. They are wearing a backpack and using ski poles to navigate the terrain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24443.5, "ram_available_mb": 38397.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24441.6, "ram_available_mb": 38399.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.179}, "power_stats": {"power_gpu_soc_mean_watts": 23.099, "power_cpu_cv_mean_watts": 1.53, "power_sys_5v0_mean_watts": 9.016, "gpu_utilization_percent_mean": 73.179, "power_watts_avg": 23.099, "energy_joules_est": 153.54, "duration_seconds": 6.647, "sample_count": 56}, "timestamp": "2026-01-25T19:42:48.381091"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9174.659, "latencies_ms": [9174.659], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The skier is wearing a white helmet, black pants, and a red backpack, with green boots and red ski poles. The snow is pristine and untouched except for the ski tracks, indicating a fresh snowfall or a well-preserved trail.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24441.6, "ram_available_mb": 38399.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24445.2, "ram_available_mb": 38395.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.385}, "power_stats": {"power_gpu_soc_mean_watts": 21.564, "power_cpu_cv_mean_watts": 1.791, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 71.385, "power_watts_avg": 21.564, "energy_joules_est": 197.86, "duration_seconds": 9.175, "sample_count": 78}, "timestamp": "2026-01-25T19:42:59.577416"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12109.591, "latencies_ms": [12109.591], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a serene landscape, a BNSF Railway train, painted in vibrant hues of orange and black, is captured in motion. The locomotive, bearing the number 6309, is pulling a series of freight cars behind it. The train is traveling on a track that cuts through a tranquil wooded area, with trees standing", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24445.2, "ram_available_mb": 38395.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24444.2, "ram_available_mb": 38396.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.408}, "power_stats": {"power_gpu_soc_mean_watts": 22.61, "power_cpu_cv_mean_watts": 1.788, "power_sys_5v0_mean_watts": 9.145, "gpu_utilization_percent_mean": 73.408, "power_watts_avg": 22.61, "energy_joules_est": 273.81, "duration_seconds": 12.11, "sample_count": 103}, "timestamp": "2026-01-25T19:43:13.725128"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9691.284, "latencies_ms": [9691.284], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "Train: 1\nCar: 1\nTrain car: 1\nTrain tracks: 1\nTrees: 1\nBNSF logo: 1\nNumber: 6309: 1\nFence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24444.2, "ram_available_mb": 38396.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24445.4, "ram_available_mb": 38395.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.256}, "power_stats": {"power_gpu_soc_mean_watts": 23.138, "power_cpu_cv_mean_watts": 1.577, "power_sys_5v0_mean_watts": 9.085, "gpu_utilization_percent_mean": 76.256, "power_watts_avg": 23.138, "energy_joules_est": 224.25, "duration_seconds": 9.692, "sample_count": 82}, "timestamp": "2026-01-25T19:43:25.462563"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7071.151, "latencies_ms": [7071.151], "images_per_second": 0.141, "prompt_tokens": 44, "response_tokens_est": 34, "n_tiles": 16, "output_text": "The train is in the foreground of the image, moving from left to right. The trees in the background appear to be far away and are behind the train.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24445.4, "ram_available_mb": 38395.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24438.9, "ram_available_mb": 38402.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.833}, "power_stats": {"power_gpu_soc_mean_watts": 24.54, "power_cpu_cv_mean_watts": 1.375, "power_sys_5v0_mean_watts": 9.23, "gpu_utilization_percent_mean": 77.833, "power_watts_avg": 24.54, "energy_joules_est": 173.54, "duration_seconds": 7.072, "sample_count": 60}, "timestamp": "2026-01-25T19:43:34.559152"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9233.585, "latencies_ms": [9233.585], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A BNSF train, numbered 6309, is traveling on a railway track with a clear blue sky in the background. The train is surrounded by trees with no leaves, indicating that it might be autumn or winter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.9, "ram_available_mb": 38402.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.974}, "power_stats": {"power_gpu_soc_mean_watts": 23.625, "power_cpu_cv_mean_watts": 1.54, "power_sys_5v0_mean_watts": 9.106, "gpu_utilization_percent_mean": 75.974, "power_watts_avg": 23.625, "energy_joules_est": 218.16, "duration_seconds": 9.234, "sample_count": 78}, "timestamp": "2026-01-25T19:43:45.841042"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6249.949, "latencies_ms": [6249.949], "images_per_second": 0.16, "prompt_tokens": 36, "response_tokens_est": 27, "n_tiles": 16, "output_text": "The train is predominantly orange with black and yellow accents. The sky is clear and blue, indicating good weather conditions.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24447.0, "ram_available_mb": 38393.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.755}, "power_stats": {"power_gpu_soc_mean_watts": 25.19, "power_cpu_cv_mean_watts": 1.224, "power_sys_5v0_mean_watts": 9.275, "gpu_utilization_percent_mean": 79.755, "power_watts_avg": 25.19, "energy_joules_est": 157.45, "duration_seconds": 6.251, "sample_count": 53}, "timestamp": "2026-01-25T19:43:54.111385"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11120.08, "latencies_ms": [11120.08], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with a plate of food on it. The plate contains a variety of food items, including a bowl of broccoli, a piece of bread with guacamole, and a slice of bread with cream cheese. The broccoli is placed in a bowl, while the other food items are spread out on the plate. The arrangement of", "error": null, "sys_before": {"cpu_percent": 9.7, "ram_used_mb": 24447.0, "ram_available_mb": 38393.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24446.3, "ram_available_mb": 38394.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.421}, "power_stats": {"power_gpu_soc_mean_watts": 20.951, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 69.421, "power_watts_avg": 20.951, "energy_joules_est": 233.0, "duration_seconds": 11.121, "sample_count": 95}, "timestamp": "2026-01-25T19:44:07.285995"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7963.838, "latencies_ms": [7963.838], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "plate: 2, piece of bread: 1, avocado spread: 1, broccoli: 1, bowl: 1, wooden table: 1, image: 1, website: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24446.3, "ram_available_mb": 38394.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24450.5, "ram_available_mb": 38390.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.265}, "power_stats": {"power_gpu_soc_mean_watts": 22.433, "power_cpu_cv_mean_watts": 1.69, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 73.265, "power_watts_avg": 22.433, "energy_joules_est": 178.67, "duration_seconds": 7.964, "sample_count": 68}, "timestamp": "2026-01-25T19:44:17.264397"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10948.366, "latencies_ms": [10948.366], "images_per_second": 0.091, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a plate with a piece of bread topped with a green spread, which is positioned to the left of a bowl containing a green vegetable dish. The bowl is placed on the right side of the plate, and both the plate and bowl are situated on a wooden surface that appears to be the background of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24450.5, "ram_available_mb": 38390.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24443.6, "ram_available_mb": 38397.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.532}, "power_stats": {"power_gpu_soc_mean_watts": 20.858, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 69.532, "power_watts_avg": 20.858, "energy_joules_est": 228.37, "duration_seconds": 10.949, "sample_count": 94}, "timestamp": "2026-01-25T19:44:30.262980"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9915.369, "latencies_ms": [9915.369], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image shows a meal consisting of a slice of bread with avocado spread, a bowl of broccoli, and a small bowl of cream cheese, all placed on a wooden table. The setting appears to be a casual dining environment, possibly at home or a cozy restaurant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24443.6, "ram_available_mb": 38397.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24451.1, "ram_available_mb": 38389.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.952}, "power_stats": {"power_gpu_soc_mean_watts": 20.59, "power_cpu_cv_mean_watts": 1.825, "power_sys_5v0_mean_watts": 8.904, "gpu_utilization_percent_mean": 70.952, "power_watts_avg": 20.59, "energy_joules_est": 204.17, "duration_seconds": 9.916, "sample_count": 84}, "timestamp": "2026-01-25T19:44:42.190207"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11080.109, "latencies_ms": [11080.109], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a meal on a wooden table with a blue plate. The plate contains a piece of bread with a green spread, a bowl of broccoli, and a dollop of cream cheese on another piece of bread. The lighting is warm and the colors are vibrant, with the green of the broccoli and avocado spread standing out against the", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24451.1, "ram_available_mb": 38389.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24443.1, "ram_available_mb": 38397.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.681}, "power_stats": {"power_gpu_soc_mean_watts": 20.964, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.681, "power_watts_avg": 20.964, "energy_joules_est": 232.3, "duration_seconds": 11.081, "sample_count": 94}, "timestamp": "2026-01-25T19:44:55.296600"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11136.729, "latencies_ms": [11136.729], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a person wrapped in an orange sleeping bag, lying on a wooden bench in a park. The person appears to be in a state of rest or sleep, with their head resting on a blue backpack. The bench is situated on a grassy area, and there are two parking meters in the background, indicating that the location is likely a", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24443.1, "ram_available_mb": 38397.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24442.9, "ram_available_mb": 38398.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.305}, "power_stats": {"power_gpu_soc_mean_watts": 20.891, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 69.305, "power_watts_avg": 20.891, "energy_joules_est": 232.67, "duration_seconds": 11.137, "sample_count": 95}, "timestamp": "2026-01-25T19:45:08.465548"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7752.002, "latencies_ms": [7752.002], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "bench: 1, sleeping bag: 1, backpack: 1, coat: 1, parking meter: 2, fence: 1, grass: 1, car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.9, "ram_available_mb": 38398.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24445.9, "ram_available_mb": 38395.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.313}, "power_stats": {"power_gpu_soc_mean_watts": 22.43, "power_cpu_cv_mean_watts": 1.673, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 73.313, "power_watts_avg": 22.43, "energy_joules_est": 173.89, "duration_seconds": 7.753, "sample_count": 67}, "timestamp": "2026-01-25T19:45:18.265181"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9939.853, "latencies_ms": [9939.853], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, there is a wooden bench with a sleeping bag and a jacket draped over it. Behind the bench, there are two parking meters standing upright. The parking meters are located further back in the image, behind the fence that runs parallel to the bench.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24445.9, "ram_available_mb": 38395.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24443.8, "ram_available_mb": 38397.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.756}, "power_stats": {"power_gpu_soc_mean_watts": 21.21, "power_cpu_cv_mean_watts": 1.871, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 69.756, "power_watts_avg": 21.21, "energy_joules_est": 210.84, "duration_seconds": 9.94, "sample_count": 86}, "timestamp": "2026-01-25T19:45:30.215905"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9966.866, "latencies_ms": [9966.866], "images_per_second": 0.1, "prompt_tokens": 37, "response_tokens_est": 67, "n_tiles": 16, "output_text": "A person is sleeping on a park bench, covered with an orange sleeping bag, with a blue backpack and a red jacket visible. Behind the bench, there are two parking meters and a wooden fence, indicating that this scene is likely in a public park or a similar outdoor area.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24443.8, "ram_available_mb": 38397.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24441.1, "ram_available_mb": 38399.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.119}, "power_stats": {"power_gpu_soc_mean_watts": 21.287, "power_cpu_cv_mean_watts": 1.806, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 70.119, "power_watts_avg": 21.287, "energy_joules_est": 212.18, "duration_seconds": 9.967, "sample_count": 84}, "timestamp": "2026-01-25T19:45:42.220246"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7860.915, "latencies_ms": [7860.915], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a person wrapped in an orange sleeping bag, lying on a wooden bench. The bench is situated in a grassy area with a wooden fence in the background and two parking meters are visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24441.1, "ram_available_mb": 38399.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24441.8, "ram_available_mb": 38399.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.178, "power_cpu_cv_mean_watts": 1.705, "power_sys_5v0_mean_watts": 9.06, "gpu_utilization_percent_mean": 72.0, "power_watts_avg": 22.178, "energy_joules_est": 174.35, "duration_seconds": 7.861, "sample_count": 66}, "timestamp": "2026-01-25T19:45:52.121677"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11106.629, "latencies_ms": [11106.629], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a room with a white shelf displaying various vases and decorative items. The focal point is a large, colorful vase with a unique design, placed on the shelf. Surrounding the vase are several other vases of different sizes and shapes, as well as a few potted plants. \n\nIn addition to the vases and plants,", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24441.8, "ram_available_mb": 38399.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24467.7, "ram_available_mb": 38373.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.505}, "power_stats": {"power_gpu_soc_mean_watts": 20.898, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.505, "power_watts_avg": 20.898, "energy_joules_est": 232.12, "duration_seconds": 11.107, "sample_count": 95}, "timestamp": "2026-01-25T19:46:05.301312"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8205.981, "latencies_ms": [8205.981], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "Vase: 1\nPlants: 1\nLeaves: 1\nFlowers: 1\nStems: 1\nVases: 2\nCeramic art: 1\nPaper: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24467.7, "ram_available_mb": 38373.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24461.8, "ram_available_mb": 38379.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.29}, "power_stats": {"power_gpu_soc_mean_watts": 22.28, "power_cpu_cv_mean_watts": 1.688, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 72.29, "power_watts_avg": 22.28, "energy_joules_est": 182.84, "duration_seconds": 8.207, "sample_count": 69}, "timestamp": "2026-01-25T19:46:15.523662"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10287.73, "latencies_ms": [10287.73], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The vase with dried flowers is in the foreground, placed on a white pedestal. To the right, there is a wall with various decorative items, including a circular object and a plaque with text. The vase is positioned closer to the viewer than the wall, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24461.8, "ram_available_mb": 38379.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 24463.9, "ram_available_mb": 38377.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.602}, "power_stats": {"power_gpu_soc_mean_watts": 21.193, "power_cpu_cv_mean_watts": 1.961, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 70.602, "power_watts_avg": 21.193, "energy_joules_est": 218.04, "duration_seconds": 10.288, "sample_count": 88}, "timestamp": "2026-01-25T19:46:27.869724"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11289.285, "latencies_ms": [11289.285], "images_per_second": 0.089, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image showcases a collection of vases and decorative items displayed on a shelf. The vases are of different shapes and sizes, with one being a tall, slender glass vase with a unique design, and another being a smaller, round glass vase. The decorative items include a vase with a bird design, a vase with a floral pattern, and a", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24463.9, "ram_available_mb": 38377.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24461.2, "ram_available_mb": 38379.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.792}, "power_stats": {"power_gpu_soc_mean_watts": 21.126, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.905, "gpu_utilization_percent_mean": 70.792, "power_watts_avg": 21.126, "energy_joules_est": 238.51, "duration_seconds": 11.29, "sample_count": 96}, "timestamp": "2026-01-25T19:46:41.212998"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11145.026, "latencies_ms": [11145.026], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a vibrant and colorful display of various vases and decorative items. The vases are made of different materials, including ceramic and glass, and are adorned with various patterns and designs. The lighting in the image is bright and natural, highlighting the intricate details of the objects. The background is a warm orange color, which contrasts nicely", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24461.2, "ram_available_mb": 38379.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24460.2, "ram_available_mb": 38380.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.383}, "power_stats": {"power_gpu_soc_mean_watts": 21.046, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 70.383, "power_watts_avg": 21.046, "energy_joules_est": 234.57, "duration_seconds": 11.146, "sample_count": 94}, "timestamp": "2026-01-25T19:46:54.368941"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11189.38, "latencies_ms": [11189.38], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a skateboarder is performing a trick on a ramp at a skate park. The skateboarder is in the process of grinding the edge of the ramp with his skateboard, showcasing his skills and balance. The ramp is designed for skateboarders to perform various tricks and maneuvers.\n\nThere are several", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24460.2, "ram_available_mb": 38380.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24451.0, "ram_available_mb": 38389.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.156}, "power_stats": {"power_gpu_soc_mean_watts": 20.044, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 70.156, "power_watts_avg": 20.044, "energy_joules_est": 224.29, "duration_seconds": 11.19, "sample_count": 96}, "timestamp": "2026-01-25T19:47:07.596152"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11064.109, "latencies_ms": [11064.109], "images_per_second": 0.09, "prompt_tokens": 39, "response_tokens_est": 77, "n_tiles": 16, "output_text": "- Skateboard: 1\n- Skateboarder: 1\n- Skate park: 1\n- Concrete ramp: 1\n- Skateboard wheels: 4\n- Skateboard trucks: 4\n- Skateboard deck: 1\n- Skateboarder's shoes: 2", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24451.0, "ram_available_mb": 38389.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24449.1, "ram_available_mb": 38391.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.16}, "power_stats": {"power_gpu_soc_mean_watts": 21.146, "power_cpu_cv_mean_watts": 1.874, "power_sys_5v0_mean_watts": 8.935, "gpu_utilization_percent_mean": 70.16, "power_watts_avg": 21.146, "energy_joules_est": 233.97, "duration_seconds": 11.065, "sample_count": 94}, "timestamp": "2026-01-25T19:47:20.699902"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11093.757, "latencies_ms": [11093.757], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a skateboarder is captured in mid-air, performing a trick on a curved ramp. The skateboarder is positioned near the bottom of the ramp, which extends into the background of the image. There are other individuals in the background, likely spectators or other skateboarders, who are situated at various distances from the ramp", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 24449.1, "ram_available_mb": 38391.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24449.7, "ram_available_mb": 38391.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.562}, "power_stats": {"power_gpu_soc_mean_watts": 20.958, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 69.562, "power_watts_avg": 20.958, "energy_joules_est": 232.52, "duration_seconds": 11.094, "sample_count": 96}, "timestamp": "2026-01-25T19:47:33.831704"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8383.607, "latencies_ms": [8383.607], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "A skateboarder is performing a trick on a curved ramp at a skatepark. The skateboarder is wearing a white t-shirt, blue jeans, and a helmet with a colorful design on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24449.7, "ram_available_mb": 38391.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24446.1, "ram_available_mb": 38394.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.708}, "power_stats": {"power_gpu_soc_mean_watts": 22.132, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 72.708, "power_watts_avg": 22.132, "energy_joules_est": 185.56, "duration_seconds": 8.384, "sample_count": 72}, "timestamp": "2026-01-25T19:47:44.238942"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8219.483, "latencies_ms": [8219.483], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The skateboarder is wearing a white t-shirt and blue jeans, and the skateboard is black with blue and white designs. The skate park is made of concrete and the lighting is natural, likely from the sun.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24446.1, "ram_available_mb": 38394.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24447.7, "ram_available_mb": 38393.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.29}, "power_stats": {"power_gpu_soc_mean_watts": 21.978, "power_cpu_cv_mean_watts": 1.741, "power_sys_5v0_mean_watts": 9.017, "gpu_utilization_percent_mean": 71.29, "power_watts_avg": 21.978, "energy_joules_est": 180.66, "duration_seconds": 8.22, "sample_count": 69}, "timestamp": "2026-01-25T19:47:54.495764"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11138.777, "latencies_ms": [11138.777], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a beautifully lit city square at night, featuring a large Christmas tree in the center. The tree is adorned with lights, creating a festive atmosphere. The square is surrounded by buildings, including a clock tower that stands out prominently in the scene.\n\nThere are several people walking around the square, enjoying the holiday ambiance. A few cars", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24447.7, "ram_available_mb": 38393.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24446.9, "ram_available_mb": 38394.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.821}, "power_stats": {"power_gpu_soc_mean_watts": 20.874, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 68.821, "power_watts_avg": 20.874, "energy_joules_est": 232.52, "duration_seconds": 11.139, "sample_count": 95}, "timestamp": "2026-01-25T19:48:07.665240"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9067.1, "latencies_ms": [9067.1], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Clock tower: 1\n- Christmas tree: 1\n- Lights: multiple strings of lights\n- People: multiple individuals\n- Buildings: multiple buildings\n- Cars: 1\n- Trash can: 1\n- Signs: multiple signs", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24446.9, "ram_available_mb": 38394.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24450.2, "ram_available_mb": 38390.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.844}, "power_stats": {"power_gpu_soc_mean_watts": 21.703, "power_cpu_cv_mean_watts": 1.762, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 70.844, "power_watts_avg": 21.703, "energy_joules_est": 196.8, "duration_seconds": 9.068, "sample_count": 77}, "timestamp": "2026-01-25T19:48:18.758290"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10120.214, "latencies_ms": [10120.214], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The large clock tower stands in the background, slightly to the left of the center of the image. In the foreground, there is a large Christmas tree decorated with blue lights on the right side of the image. The street in the middle ground is lined with buildings on both sides, and there are people walking along the sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24450.2, "ram_available_mb": 38390.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24452.3, "ram_available_mb": 38388.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.384}, "power_stats": {"power_gpu_soc_mean_watts": 21.243, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 70.384, "power_watts_avg": 21.243, "energy_joules_est": 215.0, "duration_seconds": 10.121, "sample_count": 86}, "timestamp": "2026-01-25T19:48:30.908674"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8503.291, "latencies_ms": [8503.291], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a festive city square at night, illuminated by string lights and adorned with a large Christmas tree. A prominent clock tower stands in the center, surrounded by buildings with lit windows, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24452.3, "ram_available_mb": 38388.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24445.1, "ram_available_mb": 38395.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.479}, "power_stats": {"power_gpu_soc_mean_watts": 22.064, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 72.479, "power_watts_avg": 22.064, "energy_joules_est": 187.63, "duration_seconds": 8.504, "sample_count": 73}, "timestamp": "2026-01-25T19:48:41.438736"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8570.14, "latencies_ms": [8570.14], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image features a large, ornate clock tower in the center, adorned with twinkling Christmas lights that create a festive atmosphere. The tower is surrounded by a large, snow-covered Christmas tree, adding to the holiday charm of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24445.1, "ram_available_mb": 38395.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24443.6, "ram_available_mb": 38397.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.903}, "power_stats": {"power_gpu_soc_mean_watts": 21.763, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 70.903, "power_watts_avg": 21.763, "energy_joules_est": 186.53, "duration_seconds": 8.571, "sample_count": 72}, "timestamp": "2026-01-25T19:48:52.047373"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11145.219, "latencies_ms": [11145.219], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is captured in the midst of a tennis match on a concrete court. He is dressed in a blue shirt and black shorts, and is holding a yellow tennis racket in his right hand. His left hand is extended, ready to strike the tennis ball that is suspended in the air above him. The court is enclosed by a black fence,", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24443.6, "ram_available_mb": 38397.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24445.8, "ram_available_mb": 38395.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.463}, "power_stats": {"power_gpu_soc_mean_watts": 20.888, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 69.463, "power_watts_avg": 20.888, "energy_joules_est": 232.81, "duration_seconds": 11.146, "sample_count": 95}, "timestamp": "2026-01-25T19:49:05.236009"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7633.637, "latencies_ms": [7633.637], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "boy: 1, tennis racket: 1, tennis ball: 1, tennis court: 1, fence: 1, trees: 1, sky: 1, shadows: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24445.8, "ram_available_mb": 38395.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24441.9, "ram_available_mb": 38399.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.484}, "power_stats": {"power_gpu_soc_mean_watts": 22.463, "power_cpu_cv_mean_watts": 1.639, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 72.484, "power_watts_avg": 22.463, "energy_joules_est": 171.49, "duration_seconds": 7.634, "sample_count": 64}, "timestamp": "2026-01-25T19:49:14.905254"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11110.659, "latencies_ms": [11110.659], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The tennis player is positioned in the foreground on the left side of the image, preparing to hit the tennis ball that is in the air to the right of the player. The background consists of a chain-link fence and trees, indicating that the tennis court is likely located in a park or similar outdoor area. The player is near the baseline of the court, which is", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24441.9, "ram_available_mb": 38399.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24451.8, "ram_available_mb": 38389.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.809}, "power_stats": {"power_gpu_soc_mean_watts": 20.978, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 69.809, "power_watts_avg": 20.978, "energy_joules_est": 233.09, "duration_seconds": 11.111, "sample_count": 94}, "timestamp": "2026-01-25T19:49:28.054961"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6732.835, "latencies_ms": [6732.835], "images_per_second": 0.149, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A person is playing tennis on a court with a fence and trees in the background. The player is in the middle of a forehand swing, preparing to hit the ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24451.8, "ram_available_mb": 38389.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24445.7, "ram_available_mb": 38395.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.298}, "power_stats": {"power_gpu_soc_mean_watts": 23.15, "power_cpu_cv_mean_watts": 1.517, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 75.298, "power_watts_avg": 23.15, "energy_joules_est": 155.88, "duration_seconds": 6.733, "sample_count": 57}, "timestamp": "2026-01-25T19:49:36.819296"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8324.438, "latencies_ms": [8324.438], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a person playing tennis on a court with a clear sky in the background, suggesting it might be a sunny day. The tennis player is wearing a blue shirt and black shorts, and the court has white lines marking the boundaries.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24445.7, "ram_available_mb": 38395.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24442.3, "ram_available_mb": 38398.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.157}, "power_stats": {"power_gpu_soc_mean_watts": 21.93, "power_cpu_cv_mean_watts": 1.745, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 71.157, "power_watts_avg": 21.93, "energy_joules_est": 182.57, "duration_seconds": 8.325, "sample_count": 70}, "timestamp": "2026-01-25T19:49:47.194822"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11128.357, "latencies_ms": [11128.357], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a cozy living room with a fireplace as the focal point. The fireplace is surrounded by a white mantle, and there are two chairs placed in front of it. One chair is positioned on the left side of the fireplace, while the other is on the right side. \n\nIn addition to the chairs, there are two potted plants", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24442.3, "ram_available_mb": 38398.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24442.7, "ram_available_mb": 38398.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.383}, "power_stats": {"power_gpu_soc_mean_watts": 20.888, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 69.383, "power_watts_avg": 20.888, "energy_joules_est": 232.46, "duration_seconds": 11.129, "sample_count": 94}, "timestamp": "2026-01-25T19:50:00.364265"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9197.813, "latencies_ms": [9197.813], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- Bookshelf: 100+ books\n- Picture frame: 1\n- Armchair: 1\n- End table: 2\n- Lamp: 2\n- Plant: 2\n- Fireplace: 1\n- Chair: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24442.7, "ram_available_mb": 38398.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24448.2, "ram_available_mb": 38392.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.628}, "power_stats": {"power_gpu_soc_mean_watts": 21.738, "power_cpu_cv_mean_watts": 1.766, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 71.628, "power_watts_avg": 21.738, "energy_joules_est": 199.95, "duration_seconds": 9.198, "sample_count": 78}, "timestamp": "2026-01-25T19:50:11.603795"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11124.091, "latencies_ms": [11124.091], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a white armchair positioned near the center of the image, with a lamp on a small wooden table to its left. Behind the armchair, a fireplace with a mantelpiece above it serves as a central focal point. To the right of the fireplace, there is a bookshelf filled with books, and further to", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24448.2, "ram_available_mb": 38392.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24434.4, "ram_available_mb": 38406.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.594}, "power_stats": {"power_gpu_soc_mean_watts": 20.936, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.594, "power_watts_avg": 20.936, "energy_joules_est": 232.91, "duration_seconds": 11.125, "sample_count": 96}, "timestamp": "2026-01-25T19:50:24.782851"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8634.121, "latencies_ms": [8634.121], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a fireplace, a chair, and a bookshelf filled with books. The room is furnished with a white sofa, a white armchair, and a small table with a lamp on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.4, "ram_available_mb": 38406.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 24436.5, "ram_available_mb": 38404.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.178}, "power_stats": {"power_gpu_soc_mean_watts": 21.992, "power_cpu_cv_mean_watts": 1.87, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 72.178, "power_watts_avg": 21.992, "energy_joules_est": 189.9, "duration_seconds": 8.635, "sample_count": 73}, "timestamp": "2026-01-25T19:50:35.454753"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8322.531, "latencies_ms": [8322.531], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The room is well-lit with natural light, and the walls are painted in a light color, giving it a bright and airy feel. The furniture is primarily white, with a mix of wood and leather materials, creating a classic and elegant atmosphere.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24436.5, "ram_available_mb": 38404.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24435.1, "ram_available_mb": 38405.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.141}, "power_stats": {"power_gpu_soc_mean_watts": 21.923, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 71.141, "power_watts_avg": 21.923, "energy_joules_est": 182.47, "duration_seconds": 8.323, "sample_count": 71}, "timestamp": "2026-01-25T19:50:45.827658"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11139.998, "latencies_ms": [11139.998], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a group of three zebras is grazing in a grassy field. They are standing close to each other, with one zebra on the left side of the image, another in the middle, and the third one on the right side. The zebras are eating grass from the ground, and their distinctive black and white stripes are clearly visible.", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 24435.1, "ram_available_mb": 38405.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24444.3, "ram_available_mb": 38396.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.611}, "power_stats": {"power_gpu_soc_mean_watts": 20.883, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.932, "gpu_utilization_percent_mean": 69.611, "power_watts_avg": 20.883, "energy_joules_est": 232.65, "duration_seconds": 11.141, "sample_count": 95}, "timestamp": "2026-01-25T19:50:59.040533"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6852.597, "latencies_ms": [6852.597], "images_per_second": 0.146, "prompt_tokens": 39, "response_tokens_est": 39, "n_tiles": 16, "output_text": "zebra: 3, grass: many, trees: sparse, sky: clear, dirt: patchy, sun: bright, shadows: visible, animals: 3", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24444.3, "ram_available_mb": 38396.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24442.1, "ram_available_mb": 38398.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.793}, "power_stats": {"power_gpu_soc_mean_watts": 23.08, "power_cpu_cv_mean_watts": 1.553, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 74.793, "power_watts_avg": 23.08, "energy_joules_est": 158.17, "duration_seconds": 6.853, "sample_count": 58}, "timestamp": "2026-01-25T19:51:07.943894"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10888.132, "latencies_ms": [10888.132], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a group of zebras with one partially obscured by the others. The zebra in the background is further away from the camera, making it appear smaller. The zebras are grazing on the grass, with one standing on the left side, another in the center, and the third on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24442.1, "ram_available_mb": 38398.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24437.9, "ram_available_mb": 38403.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.394}, "power_stats": {"power_gpu_soc_mean_watts": 21.007, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.394, "power_watts_avg": 21.007, "energy_joules_est": 228.74, "duration_seconds": 10.889, "sample_count": 94}, "timestamp": "2026-01-25T19:51:20.855489"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6603.539, "latencies_ms": [6603.539], "images_per_second": 0.151, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "Three zebras are grazing in a grassy field with dry grass and some bushes in the background. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.9, "ram_available_mb": 38403.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24439.3, "ram_available_mb": 38401.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.446}, "power_stats": {"power_gpu_soc_mean_watts": 23.327, "power_cpu_cv_mean_watts": 1.544, "power_sys_5v0_mean_watts": 9.035, "gpu_utilization_percent_mean": 75.446, "power_watts_avg": 23.327, "energy_joules_est": 154.06, "duration_seconds": 6.604, "sample_count": 56}, "timestamp": "2026-01-25T19:51:29.510519"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6658.107, "latencies_ms": [6658.107], "images_per_second": 0.15, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The zebras are grazing in a field with dry grass and sparse trees under a clear blue sky. The lighting is bright and natural, indicating it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24439.3, "ram_available_mb": 38401.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24444.0, "ram_available_mb": 38396.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.036}, "power_stats": {"power_gpu_soc_mean_watts": 22.884, "power_cpu_cv_mean_watts": 1.573, "power_sys_5v0_mean_watts": 9.06, "gpu_utilization_percent_mean": 73.036, "power_watts_avg": 22.884, "energy_joules_est": 152.38, "duration_seconds": 6.659, "sample_count": 56}, "timestamp": "2026-01-25T19:51:38.227436"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11119.179, "latencies_ms": [11119.179], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people sitting around a dining table in a restaurant. There are at least 13 people in the scene, with some sitting at the table and others standing or sitting at nearby tables. The dining area is furnished with multiple chairs, and the tables are adorned with cups, bowls, and bottles.\n\nThe restaurant has a", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24444.0, "ram_available_mb": 38396.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24435.6, "ram_available_mb": 38405.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.432}, "power_stats": {"power_gpu_soc_mean_watts": 20.896, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.432, "power_watts_avg": 20.896, "energy_joules_est": 232.36, "duration_seconds": 11.12, "sample_count": 95}, "timestamp": "2026-01-25T19:51:51.410120"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8409.001, "latencies_ms": [8409.001], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "table: 1\nchairs: 4\npeople: 11\ncigars: 0\ncigarette butts: 0\nglasses: 0\nbottles: 0\ncash: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.6, "ram_available_mb": 38405.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24445.4, "ram_available_mb": 38395.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.639}, "power_stats": {"power_gpu_soc_mean_watts": 22.056, "power_cpu_cv_mean_watts": 1.712, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 72.639, "power_watts_avg": 22.056, "energy_joules_est": 185.48, "duration_seconds": 8.41, "sample_count": 72}, "timestamp": "2026-01-25T19:52:01.855717"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11163.123, "latencies_ms": [11163.123], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden table with several people seated around it, engaged in conversation. The table is surrounded by wooden chairs, and in the background, there are more people seated at other tables, as well as a bar area with a counter and shelves stocked with bottles. The room has a warm, inviting atmosphere with brick walls and hanging", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24445.4, "ram_available_mb": 38395.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 24438.2, "ram_available_mb": 38402.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.611}, "power_stats": {"power_gpu_soc_mean_watts": 20.903, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.935, "gpu_utilization_percent_mean": 69.611, "power_watts_avg": 20.903, "energy_joules_est": 233.36, "duration_seconds": 11.164, "sample_count": 95}, "timestamp": "2026-01-25T19:52:15.041289"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7172.417, "latencies_ms": [7172.417], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image depicts a group of people, including two men in military uniforms, sitting around a wooden table in a restaurant. They appear to be engaged in a conversation or a meal together.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24438.2, "ram_available_mb": 38402.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24446.3, "ram_available_mb": 38394.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.541}, "power_stats": {"power_gpu_soc_mean_watts": 22.624, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 72.541, "power_watts_avg": 22.624, "energy_joules_est": 162.28, "duration_seconds": 7.173, "sample_count": 61}, "timestamp": "2026-01-25T19:52:24.257510"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7317.234, "latencies_ms": [7317.234], "images_per_second": 0.137, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows an indoor setting with warm lighting that highlights the wooden floors and furniture. The walls are adorned with brickwork, giving the space a rustic and cozy ambiance.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24446.3, "ram_available_mb": 38394.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24433.6, "ram_available_mb": 38407.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.661}, "power_stats": {"power_gpu_soc_mean_watts": 22.477, "power_cpu_cv_mean_watts": 1.653, "power_sys_5v0_mean_watts": 9.055, "gpu_utilization_percent_mean": 72.661, "power_watts_avg": 22.477, "energy_joules_est": 164.48, "duration_seconds": 7.318, "sample_count": 62}, "timestamp": "2026-01-25T19:52:33.588143"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11121.188, "latencies_ms": [11121.188], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of white swans swimming in a body of water, possibly a lake or a pond. There are at least 13 swans in the scene, with some swimming close to the shore and others further away. The swans are spread out across the water, creating a beautiful and serene atmosphere.\n\nIn the background, there are several boats floating", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24433.6, "ram_available_mb": 38407.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24442.6, "ram_available_mb": 38398.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.232}, "power_stats": {"power_gpu_soc_mean_watts": 20.869, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.232, "power_watts_avg": 20.869, "energy_joules_est": 232.1, "duration_seconds": 11.122, "sample_count": 95}, "timestamp": "2026-01-25T19:52:46.762602"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6982.704, "latencies_ms": [6982.704], "images_per_second": 0.143, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "swan: 14, boat: 12, water: many, sun: visible, marina: visible, birds: 14, water: many, sky: visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.6, "ram_available_mb": 38398.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24445.1, "ram_available_mb": 38395.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.832, "power_cpu_cv_mean_watts": 1.547, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 75.0, "power_watts_avg": 22.832, "energy_joules_est": 159.44, "duration_seconds": 6.983, "sample_count": 59}, "timestamp": "2026-01-25T19:52:55.781293"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11098.71, "latencies_ms": [11098.71], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a group of white swans are swimming in the water, with some closer to the viewer and others further away. In the background, there are several boats docked at a marina, with some closer to the shore and others further out in the water. The swans are in the middle ground of the image, creating a sense of depth between the boats and", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24445.1, "ram_available_mb": 38395.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.705}, "power_stats": {"power_gpu_soc_mean_watts": 20.921, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 69.705, "power_watts_avg": 20.921, "energy_joules_est": 232.21, "duration_seconds": 11.099, "sample_count": 95}, "timestamp": "2026-01-25T19:53:08.908998"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7940.845, "latencies_ms": [7940.845], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A group of white swans are swimming in a body of water near a marina with several boats docked in the background. The scene is peaceful and serene, with the swans gracefully gliding through the water.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24435.7, "ram_available_mb": 38405.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.239}, "power_stats": {"power_gpu_soc_mean_watts": 22.312, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 73.239, "power_watts_avg": 22.312, "energy_joules_est": 177.19, "duration_seconds": 7.941, "sample_count": 67}, "timestamp": "2026-01-25T19:53:18.880230"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10666.047, "latencies_ms": [10666.047], "images_per_second": 0.094, "prompt_tokens": 36, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The image features a serene scene with a group of white swans swimming in a body of water that reflects a purple hue, likely due to the lighting conditions. The water is calm, and the swans are scattered across the surface, with some closer to the foreground and others further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.7, "ram_available_mb": 38405.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24436.5, "ram_available_mb": 38404.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.778}, "power_stats": {"power_gpu_soc_mean_watts": 21.036, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.778, "power_watts_avg": 21.036, "energy_joules_est": 224.38, "duration_seconds": 10.667, "sample_count": 90}, "timestamp": "2026-01-25T19:53:31.563293"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11166.299, "latencies_ms": [11166.299], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of joy and celebration, set against the backdrop of a beautifully decorated wedding venue. The room is adorned with a white canopy, from which a string of pink and green triangular flags hang, adding a touch of color to the otherwise monochrome setting. The floor, made of wood, adds a rustic charm to", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24436.5, "ram_available_mb": 38404.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24467.4, "ram_available_mb": 38373.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.24}, "power_stats": {"power_gpu_soc_mean_watts": 20.138, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 70.24, "power_watts_avg": 20.138, "energy_joules_est": 224.88, "duration_seconds": 11.167, "sample_count": 96}, "timestamp": "2026-01-25T19:53:44.782918"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8610.56, "latencies_ms": [8610.56], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "wedding cake: 1, guests: 2, triangular flags: 12, microphones: 2, musical instruments: 3, gift boxes: 2, tablecloths: 2, chairs: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24467.4, "ram_available_mb": 38373.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24468.3, "ram_available_mb": 38372.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.181}, "power_stats": {"power_gpu_soc_mean_watts": 21.47, "power_cpu_cv_mean_watts": 1.707, "power_sys_5v0_mean_watts": 8.927, "gpu_utilization_percent_mean": 72.181, "power_watts_avg": 21.47, "energy_joules_est": 184.88, "duration_seconds": 8.611, "sample_count": 72}, "timestamp": "2026-01-25T19:53:55.410108"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11168.05, "latencies_ms": [11168.05], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a couple is standing close together, with the woman in a white dress and the man in a suit, positioned near the center of the image. They are surrounded by a table with a white cloth and various items on it, which is located slightly to their right. In the background, there is a stage with musical equipment and a speaker, which is further away from the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24468.3, "ram_available_mb": 38372.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24470.5, "ram_available_mb": 38370.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.389}, "power_stats": {"power_gpu_soc_mean_watts": 21.021, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 70.389, "power_watts_avg": 21.021, "energy_joules_est": 234.78, "duration_seconds": 11.169, "sample_count": 95}, "timestamp": "2026-01-25T19:54:08.638568"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9069.672, "latencies_ms": [9069.672], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image captures a joyous moment at a wedding reception held in a large, elegantly decorated tent. A bride and groom are sharing a dance, surrounded by festive decorations and a band set up in the background, creating a warm and celebratory atmosphere.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24470.5, "ram_available_mb": 38370.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24467.0, "ram_available_mb": 38373.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.896}, "power_stats": {"power_gpu_soc_mean_watts": 21.799, "power_cpu_cv_mean_watts": 1.757, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 71.896, "power_watts_avg": 21.799, "energy_joules_est": 197.72, "duration_seconds": 9.07, "sample_count": 77}, "timestamp": "2026-01-25T19:54:19.741140"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10350.668, "latencies_ms": [10350.668], "images_per_second": 0.097, "prompt_tokens": 36, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image shows an indoor wedding reception with a large white tent-like ceiling, adorned with pink and green triangular bunting. The lighting is warm and ambient, with a mix of natural light coming from the windows and artificial light from the ceiling fixtures, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24467.0, "ram_available_mb": 38373.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 24465.7, "ram_available_mb": 38375.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.933}, "power_stats": {"power_gpu_soc_mean_watts": 21.201, "power_cpu_cv_mean_watts": 1.979, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 69.933, "power_watts_avg": 21.201, "energy_joules_est": 219.46, "duration_seconds": 10.351, "sample_count": 89}, "timestamp": "2026-01-25T19:54:32.105884"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11131.319, "latencies_ms": [11131.319], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a cozy living room bathed in soft light. Dominating the scene is a blue sofa, its vibrant color contrasting beautifully with the red walls. The sofa is adorned with a white throw pillow, adding a touch of elegance to the room. \n\nTo the left of the sofa, a wooden coffee table stands,", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24465.7, "ram_available_mb": 38375.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24450.7, "ram_available_mb": 38390.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.469}, "power_stats": {"power_gpu_soc_mean_watts": 20.857, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 69.469, "power_watts_avg": 20.857, "energy_joules_est": 232.18, "duration_seconds": 11.132, "sample_count": 96}, "timestamp": "2026-01-25T19:54:45.262997"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8166.124, "latencies_ms": [8166.124], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "- Couch: 1\n\n- Table: 2\n\n- Plant: 3\n\n- Picture: 2\n\n- Window: 2\n\n- Rug: 1\n\n- Lamp: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24450.7, "ram_available_mb": 38390.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24452.6, "ram_available_mb": 38388.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.143}, "power_stats": {"power_gpu_soc_mean_watts": 22.088, "power_cpu_cv_mean_watts": 1.738, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 72.143, "power_watts_avg": 22.088, "energy_joules_est": 180.39, "duration_seconds": 8.167, "sample_count": 70}, "timestamp": "2026-01-25T19:54:55.441801"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11136.42, "latencies_ms": [11136.42], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a blue sofa positioned against the wall, with a small wooden side table to its left. On the right side of the sofa, there is a round glass coffee table. In the background, there are two windows with white curtains; one is on the left side of the image and the other is on the right side. Above the so", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24452.6, "ram_available_mb": 38388.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24447.6, "ram_available_mb": 38393.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.792}, "power_stats": {"power_gpu_soc_mean_watts": 20.899, "power_cpu_cv_mean_watts": 1.943, "power_sys_5v0_mean_watts": 8.986, "gpu_utilization_percent_mean": 69.792, "power_watts_avg": 20.899, "energy_joules_est": 232.75, "duration_seconds": 11.137, "sample_count": 96}, "timestamp": "2026-01-25T19:55:08.638999"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8935.163, "latencies_ms": [8935.163], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a blue sofa, a small table with a lamp and a plant, and two windows with white curtains. The room has a warm and inviting atmosphere with a red wall and a colorful rug on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24447.6, "ram_available_mb": 38393.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24443.2, "ram_available_mb": 38397.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.303}, "power_stats": {"power_gpu_soc_mean_watts": 21.814, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 71.303, "power_watts_avg": 21.814, "energy_joules_est": 194.92, "duration_seconds": 8.936, "sample_count": 76}, "timestamp": "2026-01-25T19:55:19.607975"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7697.691, "latencies_ms": [7697.691], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The room has a warm and cozy atmosphere with red walls and a combination of natural and artificial lighting. The furniture includes a blue sofa, a wooden coffee table, and a round glass table with a plant on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24443.2, "ram_available_mb": 38397.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24443.7, "ram_available_mb": 38397.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.046}, "power_stats": {"power_gpu_soc_mean_watts": 21.555, "power_cpu_cv_mean_watts": 1.712, "power_sys_5v0_mean_watts": 9.047, "gpu_utilization_percent_mean": 73.046, "power_watts_avg": 21.555, "energy_joules_est": 165.94, "duration_seconds": 7.698, "sample_count": 65}, "timestamp": "2026-01-25T19:55:29.337660"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12286.575, "latencies_ms": [12286.575], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a scene featuring a doll with long, flowing red hair and a white headband. The doll is positioned in the center of the image, with its head tilted slightly to the right. It is holding a book in its hands, which is open and resting on a wooden surface. The book appears to be a hardcover with a black cover and white pages.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24443.7, "ram_available_mb": 38397.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24438.5, "ram_available_mb": 38402.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.755}, "power_stats": {"power_gpu_soc_mean_watts": 22.818, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 9.23, "gpu_utilization_percent_mean": 73.755, "power_watts_avg": 22.818, "energy_joules_est": 280.37, "duration_seconds": 12.287, "sample_count": 106}, "timestamp": "2026-01-25T19:55:43.696731"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8375.088, "latencies_ms": [8375.088], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "doll: 1, clock: 2, book: 1, hair: 2, face: 1, eyes: 2, hands: 2, numbers: 12", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.5, "ram_available_mb": 38402.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24460.0, "ram_available_mb": 38380.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.014}, "power_stats": {"power_gpu_soc_mean_watts": 24.509, "power_cpu_cv_mean_watts": 1.393, "power_sys_5v0_mean_watts": 9.177, "gpu_utilization_percent_mean": 79.014, "power_watts_avg": 24.509, "energy_joules_est": 205.28, "duration_seconds": 8.376, "sample_count": 71}, "timestamp": "2026-01-25T19:55:54.124666"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11750.407, "latencies_ms": [11750.407], "images_per_second": 0.085, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The clock is positioned to the left of the image, partially overlapping the figure on the right. The figure appears to be in the foreground, with the clock slightly behind it, creating a sense of depth. The background is a plain, light-colored surface that provides contrast to the darker tones of the figure and the clock.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24460.0, "ram_available_mb": 38380.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24462.5, "ram_available_mb": 38378.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.238}, "power_stats": {"power_gpu_soc_mean_watts": 23.037, "power_cpu_cv_mean_watts": 1.733, "power_sys_5v0_mean_watts": 9.215, "gpu_utilization_percent_mean": 75.238, "power_watts_avg": 23.037, "energy_joules_est": 270.71, "duration_seconds": 11.751, "sample_count": 101}, "timestamp": "2026-01-25T19:56:07.889389"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 12653.712, "latencies_ms": [12653.712], "images_per_second": 0.079, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a whimsical scene with a doll sitting on a wooden surface, surrounded by two large clocks that are shaped like human heads. The doll has long red hair and is wearing a white headband, and the clocks have black numbers and hands. The background is a textured beige color, giving the impression of a dreamlike or surreal setting.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24462.5, "ram_available_mb": 38378.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24466.2, "ram_available_mb": 38374.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.458}, "power_stats": {"power_gpu_soc_mean_watts": 22.947, "power_cpu_cv_mean_watts": 1.736, "power_sys_5v0_mean_watts": 9.158, "gpu_utilization_percent_mean": 74.458, "power_watts_avg": 22.947, "energy_joules_est": 290.38, "duration_seconds": 12.654, "sample_count": 107}, "timestamp": "2026-01-25T19:56:22.564335"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10473.56, "latencies_ms": [10473.56], "images_per_second": 0.095, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image features a doll with a warm, orange-toned hair and a clock with a yellow face and black numbers and hands. The doll is wearing a dark outfit with a white collar, and the clock is placed on a wooden surface with a shadow cast on the wall behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24466.2, "ram_available_mb": 38374.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24472.1, "ram_available_mb": 38368.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_gpu_soc_mean_watts": 23.409, "power_cpu_cv_mean_watts": 1.651, "power_sys_5v0_mean_watts": 9.212, "gpu_utilization_percent_mean": 75.0, "power_watts_avg": 23.409, "energy_joules_est": 245.19, "duration_seconds": 10.474, "sample_count": 89}, "timestamp": "2026-01-25T19:56:35.094045"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12113.188, "latencies_ms": [12113.188], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is sitting on a motorcycle, wearing a helmet and a tan jacket. He is holding a mirror in his hand, likely checking his reflection or looking for something. The motorcycle is parked, and there are other people in the background, possibly waiting or observing the scene.\n\nThere are two other motorcycles visible in the scene,", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24472.1, "ram_available_mb": 38368.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24458.2, "ram_available_mb": 38382.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11579.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.272}, "power_stats": {"power_gpu_soc_mean_watts": 22.73, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 9.128, "gpu_utilization_percent_mean": 73.272, "power_watts_avg": 22.73, "energy_joules_est": 275.35, "duration_seconds": 12.114, "sample_count": 103}, "timestamp": "2026-01-25T19:56:49.270904"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10367.179, "latencies_ms": [10367.179], "images_per_second": 0.096, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Helmet: 1\n\n- Motorcycle: 1\n\n- Mirror: 1\n\n- Man: 1\n\n- Pants: 1\n\n- Jacket: 1\n\n- Boot: 1\n\n- Seat: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24458.2, "ram_available_mb": 38382.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24555.9, "ram_available_mb": 38285.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11613.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.404}, "power_stats": {"power_gpu_soc_mean_watts": 23.228, "power_cpu_cv_mean_watts": 1.633, "power_sys_5v0_mean_watts": 9.108, "gpu_utilization_percent_mean": 75.404, "power_watts_avg": 23.228, "energy_joules_est": 240.83, "duration_seconds": 10.368, "sample_count": 89}, "timestamp": "2026-01-25T19:57:01.656346"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12070.462, "latencies_ms": [12070.462], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The person is seated on a motorcycle, which is positioned in the foreground of the image. In the background, there are other individuals, one of whom is wearing a red helmet, suggesting a public or crowded space. The motorcycle and its rider are the main focus and are located near the bottom of the image, while the background figures are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24555.9, "ram_available_mb": 38285.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24478.6, "ram_available_mb": 38362.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11624.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.225}, "power_stats": {"power_gpu_soc_mean_watts": 22.713, "power_cpu_cv_mean_watts": 1.802, "power_sys_5v0_mean_watts": 9.191, "gpu_utilization_percent_mean": 73.225, "power_watts_avg": 22.713, "energy_joules_est": 274.17, "duration_seconds": 12.071, "sample_count": 102}, "timestamp": "2026-01-25T19:57:15.740343"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7900.584, "latencies_ms": [7900.584], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A man is sitting on a motorcycle, wearing a helmet and holding onto the handlebars. He appears to be in a crowded area with other people and vehicles around him.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24478.6, "ram_available_mb": 38362.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24549.7, "ram_available_mb": 38291.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11608.7, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.761}, "power_stats": {"power_gpu_soc_mean_watts": 24.361, "power_cpu_cv_mean_watts": 1.398, "power_sys_5v0_mean_watts": 9.129, "gpu_utilization_percent_mean": 78.761, "power_watts_avg": 24.361, "energy_joules_est": 192.48, "duration_seconds": 7.901, "sample_count": 67}, "timestamp": "2026-01-25T19:57:25.669686"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11763.999, "latencies_ms": [11763.999], "images_per_second": 0.085, "prompt_tokens": 36, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The image shows a person wearing a light beige shirt and beige pants, sitting on a motorcycle. The motorcycle is predominantly white with red accents. The person is wearing a black helmet with a clear visor. The background is blurred, but it appears to be an outdoor setting with other people and vehicles.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 24487.9, "ram_available_mb": 38353.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24530.5, "ram_available_mb": 38310.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11606.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.8}, "power_stats": {"power_gpu_soc_mean_watts": 22.767, "power_cpu_cv_mean_watts": 1.77, "power_sys_5v0_mean_watts": 9.149, "gpu_utilization_percent_mean": 72.8, "power_watts_avg": 22.767, "energy_joules_est": 267.85, "duration_seconds": 11.765, "sample_count": 100}, "timestamp": "2026-01-25T19:57:39.487450"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11114.616, "latencies_ms": [11114.616], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a kitchen where a large, freshly baked pizza is the star. The pizza, with its golden crust and a generous topping of melted cheese, is resting on a wooden table. The cheese is speckled with green herbs, adding a touch of color and hinting at the flavors within. ", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24442.0, "ram_available_mb": 38398.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24451.9, "ram_available_mb": 38389.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.532}, "power_stats": {"power_gpu_soc_mean_watts": 20.969, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.532, "power_watts_avg": 20.969, "energy_joules_est": 233.08, "duration_seconds": 11.115, "sample_count": 94}, "timestamp": "2026-01-25T19:57:52.635663"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8198.261, "latencies_ms": [8198.261], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "pizza: 1\npiece: 1\ncheese: 1\ntomato sauce: 1\nbasil: 1\ngarlic: 1\nolive oil: 1\ndough: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24451.9, "ram_available_mb": 38389.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24577.0, "ram_available_mb": 38263.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.507}, "power_stats": {"power_gpu_soc_mean_watts": 22.195, "power_cpu_cv_mean_watts": 1.689, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 72.507, "power_watts_avg": 22.195, "energy_joules_est": 181.97, "duration_seconds": 8.199, "sample_count": 69}, "timestamp": "2026-01-25T19:58:02.850740"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11152.982, "latencies_ms": [11152.982], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The pizza is positioned in the foreground of the image, occupying the majority of the space. It is placed on a wooden surface, which is in the middle ground of the image. In the background, there is a person's arm and a kitchen setting with various pots and pans, indicating that the pizza is likely being prepared or served in a restaurant or home kitchen", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24577.0, "ram_available_mb": 38263.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24460.8, "ram_available_mb": 38380.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.389}, "power_stats": {"power_gpu_soc_mean_watts": 20.977, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 69.389, "power_watts_avg": 20.977, "energy_joules_est": 233.97, "duration_seconds": 11.154, "sample_count": 95}, "timestamp": "2026-01-25T19:58:16.016421"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9844.29, "latencies_ms": [9844.29], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 66, "n_tiles": 16, "output_text": "A large pizza with a golden-brown crust and melted cheese is being served by a person in a dark-colored shirt. The pizza is placed on a wooden table, and there are pots and pans in the background, suggesting a restaurant or pizzeria setting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24460.8, "ram_available_mb": 38380.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24456.1, "ram_available_mb": 38384.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.735}, "power_stats": {"power_gpu_soc_mean_watts": 21.497, "power_cpu_cv_mean_watts": 1.819, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 70.735, "power_watts_avg": 21.497, "energy_joules_est": 211.64, "duration_seconds": 9.845, "sample_count": 83}, "timestamp": "2026-01-25T19:58:27.909888"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7592.547, "latencies_ms": [7592.547], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The pizza is topped with green herbs and melted cheese, with a golden-brown crust. The lighting in the image is dim, highlighting the pizza's texture and colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24456.1, "ram_available_mb": 38384.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 24464.3, "ram_available_mb": 38376.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.219}, "power_stats": {"power_gpu_soc_mean_watts": 22.413, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 9.024, "gpu_utilization_percent_mean": 72.219, "power_watts_avg": 22.413, "energy_joules_est": 170.19, "duration_seconds": 7.593, "sample_count": 64}, "timestamp": "2026-01-25T19:58:37.553940"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11135.759, "latencies_ms": [11135.759], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a female tennis player is captured in the midst of a powerful serve on a grass court. She is dressed in a white tennis outfit, complete with a white visor and a white cap, providing protection from the sun. The tennis racket she wields is blue and white, contrasting with the green of the grass court. Her right arm is extended upwards,", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24464.3, "ram_available_mb": 38376.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24463.2, "ram_available_mb": 38377.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.884}, "power_stats": {"power_gpu_soc_mean_watts": 20.851, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 69.884, "power_watts_avg": 20.851, "energy_joules_est": 232.21, "duration_seconds": 11.136, "sample_count": 95}, "timestamp": "2026-01-25T19:58:50.754121"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8211.696, "latencies_ms": [8211.696], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "tennis player: 1\ntennis racket: 1\nnet: 1\ngrass court: 1\nwhite dress: 1\nwhite shoes: 1\nwater bottle: 1\nshadow: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24463.2, "ram_available_mb": 38377.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24461.4, "ram_available_mb": 38379.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.507}, "power_stats": {"power_gpu_soc_mean_watts": 22.261, "power_cpu_cv_mean_watts": 1.671, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 73.507, "power_watts_avg": 22.261, "energy_joules_est": 182.82, "duration_seconds": 8.212, "sample_count": 69}, "timestamp": "2026-01-25T19:59:00.991630"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11147.445, "latencies_ms": [11147.445], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, a tennis player is positioned on the court, with her right arm raised high above her head, holding a tennis racket in her left hand. The net is visible in the background, stretching across the middle of the image. The court itself appears to be a lush green, indicating it is well-maintained and likely a professional tennis court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24461.4, "ram_available_mb": 38379.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24541.3, "ram_available_mb": 38299.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.729}, "power_stats": {"power_gpu_soc_mean_watts": 21.006, "power_cpu_cv_mean_watts": 1.915, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 69.729, "power_watts_avg": 21.006, "energy_joules_est": 234.18, "duration_seconds": 11.148, "sample_count": 96}, "timestamp": "2026-01-25T19:59:14.157260"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6832.661, "latencies_ms": [6832.661], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A tennis player in a white dress is captured in a dynamic pose on a grass court, holding a tennis racket and wearing a visor, likely during a match or practice session.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24442.8, "ram_available_mb": 38398.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24458.7, "ram_available_mb": 38382.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.053}, "power_stats": {"power_gpu_soc_mean_watts": 23.072, "power_cpu_cv_mean_watts": 1.546, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 74.053, "power_watts_avg": 23.072, "energy_joules_est": 157.66, "duration_seconds": 6.833, "sample_count": 57}, "timestamp": "2026-01-25T19:59:23.003597"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6137.392, "latencies_ms": [6137.392], "images_per_second": 0.163, "prompt_tokens": 36, "response_tokens_est": 34, "n_tiles": 16, "output_text": "The tennis player is wearing a white dress and is on a grass court. The lighting appears to be natural sunlight, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24458.7, "ram_available_mb": 38382.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24466.6, "ram_available_mb": 38374.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.039}, "power_stats": {"power_gpu_soc_mean_watts": 23.523, "power_cpu_cv_mean_watts": 1.5, "power_sys_5v0_mean_watts": 9.082, "gpu_utilization_percent_mean": 75.039, "power_watts_avg": 23.523, "energy_joules_est": 144.39, "duration_seconds": 6.138, "sample_count": 51}, "timestamp": "2026-01-25T19:59:31.177332"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11124.849, "latencies_ms": [11124.849], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and well-organized bathroom. Dominating the left side of the frame is a pristine white toilet, its lid closed, standing against a wall painted in a soothing shade of yellow. Above it, a black towel hangs neatly, ready for use. \n\nOn the right, a white shower cur", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24466.6, "ram_available_mb": 38374.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24444.5, "ram_available_mb": 38396.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.266}, "power_stats": {"power_gpu_soc_mean_watts": 20.962, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.266, "power_watts_avg": 20.962, "energy_joules_est": 233.21, "duration_seconds": 11.126, "sample_count": 94}, "timestamp": "2026-01-25T19:59:44.326021"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8511.213, "latencies_ms": [8511.213], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "toilet: 1, towel: 2, shower curtain: 1, toilet paper: 1, shelves: 4, towels: 2, bed: 1, closet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24444.5, "ram_available_mb": 38396.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24437.9, "ram_available_mb": 38403.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.764}, "power_stats": {"power_gpu_soc_mean_watts": 22.123, "power_cpu_cv_mean_watts": 1.708, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 72.764, "power_watts_avg": 22.123, "energy_joules_est": 188.31, "duration_seconds": 8.512, "sample_count": 72}, "timestamp": "2026-01-25T19:59:54.870570"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9457.113, "latencies_ms": [9457.113], "images_per_second": 0.106, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The toilet is located in the foreground on the left side of the image, while the shelves with folded towels are in the background on the right side. The shower curtain is hanging in the middle ground, separating the toilet area from the storage area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.9, "ram_available_mb": 38403.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24435.0, "ram_available_mb": 38405.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.728}, "power_stats": {"power_gpu_soc_mean_watts": 21.445, "power_cpu_cv_mean_watts": 1.829, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 70.728, "power_watts_avg": 21.445, "energy_joules_est": 202.82, "duration_seconds": 9.458, "sample_count": 81}, "timestamp": "2026-01-25T20:00:06.368583"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8183.602, "latencies_ms": [8183.602], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a small, compact bathroom with a white toilet on the left side and a white shelving unit on the right side. The shelves are empty, and there is a towel hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24435.0, "ram_available_mb": 38405.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.057}, "power_stats": {"power_gpu_soc_mean_watts": 22.164, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 73.057, "power_watts_avg": 22.164, "energy_joules_est": 181.4, "duration_seconds": 8.184, "sample_count": 70}, "timestamp": "2026-01-25T20:00:16.605343"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6871.736, "latencies_ms": [6871.736], "images_per_second": 0.146, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image shows a bathroom with a white toilet and a white door. The walls are painted in a light yellow color, and there is a brown towel hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24442.4, "ram_available_mb": 38398.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.017}, "power_stats": {"power_gpu_soc_mean_watts": 22.747, "power_cpu_cv_mean_watts": 1.616, "power_sys_5v0_mean_watts": 9.063, "gpu_utilization_percent_mean": 73.017, "power_watts_avg": 22.747, "energy_joules_est": 156.33, "duration_seconds": 6.872, "sample_count": 58}, "timestamp": "2026-01-25T20:00:25.527078"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11125.859, "latencies_ms": [11125.859], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man and a woman are standing close to each other, holding wine glasses and smiling. They appear to be enjoying a glass of wine together. The woman is wearing glasses, and they both seem to be in a happy and relaxed mood.\n\nThe scene also includes a dining table with a few chairs around it. There are additional", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24442.4, "ram_available_mb": 38398.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24436.3, "ram_available_mb": 38404.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.874}, "power_stats": {"power_gpu_soc_mean_watts": 20.915, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.935, "gpu_utilization_percent_mean": 68.874, "power_watts_avg": 20.915, "energy_joules_est": 232.71, "duration_seconds": 11.126, "sample_count": 95}, "timestamp": "2026-01-25T20:00:38.701996"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8611.57, "latencies_ms": [8611.57], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- People: 3\n- Wine glasses: 2\n- Table: 1\n- Papers: 2\n- Chair: 1\n- Window: 1\n- Outdoor view: 1\n- Grill: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.3, "ram_available_mb": 38404.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24433.5, "ram_available_mb": 38407.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.082}, "power_stats": {"power_gpu_soc_mean_watts": 21.925, "power_cpu_cv_mean_watts": 1.744, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 71.082, "power_watts_avg": 21.925, "energy_joules_est": 188.82, "duration_seconds": 8.612, "sample_count": 73}, "timestamp": "2026-01-25T20:00:49.367838"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9116.476, "latencies_ms": [9116.476], "images_per_second": 0.11, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "In the foreground, a person in a green shirt is holding a glass of red wine up to the camera, with another person in a blue shirt partially visible in the background. The person in the green shirt is standing closer to the camera than the person in the blue shirt.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24433.5, "ram_available_mb": 38407.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24428.2, "ram_available_mb": 38412.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.558}, "power_stats": {"power_gpu_soc_mean_watts": 21.603, "power_cpu_cv_mean_watts": 1.805, "power_sys_5v0_mean_watts": 9.015, "gpu_utilization_percent_mean": 70.558, "power_watts_avg": 21.603, "energy_joules_est": 196.96, "duration_seconds": 9.117, "sample_count": 77}, "timestamp": "2026-01-25T20:01:00.502563"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7981.789, "latencies_ms": [7981.789], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "In the image, a man and a woman are standing in a room with a dining table and chairs. They are holding up wine glasses and smiling, suggesting that they are enjoying a social gathering or celebration.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24428.2, "ram_available_mb": 38412.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24437.0, "ram_available_mb": 38403.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.299}, "power_stats": {"power_gpu_soc_mean_watts": 22.377, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 73.299, "power_watts_avg": 22.377, "energy_joules_est": 178.62, "duration_seconds": 7.982, "sample_count": 67}, "timestamp": "2026-01-25T20:01:10.496949"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9115.014, "latencies_ms": [9115.014], "images_per_second": 0.11, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image shows two individuals holding up wine glasses, with a bright and natural light coming from the window in the background. The room has a casual and relaxed atmosphere, with a green t-shirt and a black t-shirt visible, and a metal container on the table.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24437.0, "ram_available_mb": 38403.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24436.1, "ram_available_mb": 38404.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.078}, "power_stats": {"power_gpu_soc_mean_watts": 21.559, "power_cpu_cv_mean_watts": 1.81, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 71.078, "power_watts_avg": 21.559, "energy_joules_est": 196.52, "duration_seconds": 9.116, "sample_count": 77}, "timestamp": "2026-01-25T20:01:21.639526"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11133.455, "latencies_ms": [11133.455], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a surfer is skillfully riding a wave on a sunny day. The surfer is wearing a black wetsuit and is positioned on a white surfboard. The wave, a beautiful shade of green, is curling over the surfer, creating a tunnel-like effect. The surfer is crouched down on the surfboard", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24436.1, "ram_available_mb": 38404.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24434.9, "ram_available_mb": 38406.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.323}, "power_stats": {"power_gpu_soc_mean_watts": 20.841, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 69.323, "power_watts_avg": 20.841, "energy_joules_est": 232.05, "duration_seconds": 11.134, "sample_count": 96}, "timestamp": "2026-01-25T20:01:34.840041"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7957.556, "latencies_ms": [7957.556], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "wave: 1\nsurfboard: 1\nsurfer: 1\nwater: 1\nsand: 1\nsky: 1\nbathing suits: 1\nbeach: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24434.9, "ram_available_mb": 38406.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24434.5, "ram_available_mb": 38406.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.597}, "power_stats": {"power_gpu_soc_mean_watts": 22.37, "power_cpu_cv_mean_watts": 1.68, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 72.597, "power_watts_avg": 22.37, "energy_joules_est": 178.02, "duration_seconds": 7.958, "sample_count": 67}, "timestamp": "2026-01-25T20:01:44.852865"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11125.256, "latencies_ms": [11125.256], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is curling over to the right side of the image. The wave is the main object, and it appears to be near the surfer, who is in the water. In the background, there is a sandy beach and a clear sky, which are farther away from the surfer compared to the wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.5, "ram_available_mb": 38406.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24434.5, "ram_available_mb": 38406.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.663}, "power_stats": {"power_gpu_soc_mean_watts": 20.955, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.663, "power_watts_avg": 20.955, "energy_joules_est": 233.14, "duration_seconds": 11.126, "sample_count": 95}, "timestamp": "2026-01-25T20:01:58.024686"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7170.948, "latencies_ms": [7170.948], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A surfer is skillfully riding a large wave in the ocean, showcasing their expertise and balance. The wave is curling over the surfer, creating a beautiful and dynamic scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24434.5, "ram_available_mb": 38406.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24433.0, "ram_available_mb": 38407.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.082}, "power_stats": {"power_gpu_soc_mean_watts": 22.87, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 73.082, "power_watts_avg": 22.87, "energy_joules_est": 164.01, "duration_seconds": 7.172, "sample_count": 61}, "timestamp": "2026-01-25T20:02:07.219887"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6528.704, "latencies_ms": [6528.704], "images_per_second": 0.153, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The surfer is riding a wave with a clear blue sky in the background. The water is a beautiful shade of green and the wave is creating a tunnel-like effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24433.0, "ram_available_mb": 38407.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24444.2, "ram_available_mb": 38396.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.218}, "power_stats": {"power_gpu_soc_mean_watts": 23.029, "power_cpu_cv_mean_watts": 1.587, "power_sys_5v0_mean_watts": 9.058, "gpu_utilization_percent_mean": 73.218, "power_watts_avg": 23.029, "energy_joules_est": 150.36, "duration_seconds": 6.529, "sample_count": 55}, "timestamp": "2026-01-25T20:02:15.783912"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11128.546, "latencies_ms": [11128.546], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a wooden table with a backpack and several laptops placed on it. The backpack is positioned on the left side of the table, while the laptops are spread out across the table, with some closer to the backpack and others further away. The arrangement of the laptops and the backpack suggests that the person might be working on a project or prepar", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24444.2, "ram_available_mb": 38396.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24436.1, "ram_available_mb": 38404.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.564}, "power_stats": {"power_gpu_soc_mean_watts": 20.961, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.564, "power_watts_avg": 20.961, "energy_joules_est": 233.28, "duration_seconds": 11.129, "sample_count": 94}, "timestamp": "2026-01-25T20:02:28.945762"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9417.232, "latencies_ms": [9417.232], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "- Laptop: 4\n- Backpack: 1\n- Wires: 1\n- Laptop charger: 1\n- Laptop mouse: 1\n- Laptop keyboard: 1\n- Laptop screen: 1\n- Laptop battery: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24436.1, "ram_available_mb": 38404.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 24448.3, "ram_available_mb": 38392.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.309}, "power_stats": {"power_gpu_soc_mean_watts": 21.617, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 71.309, "power_watts_avg": 21.617, "energy_joules_est": 203.59, "duration_seconds": 9.418, "sample_count": 81}, "timestamp": "2026-01-25T20:02:40.376895"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11118.386, "latencies_ms": [11118.386], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the foreground, there is a red and black backpack placed on a wooden table. Behind the backpack, there are several laptops of different sizes and colors, with the largest one in the background and the smallest one in the foreground. Additionally, there are multiple cords scattered around the table, with some near the laptops and others near the backpack.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24448.3, "ram_available_mb": 38392.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24441.0, "ram_available_mb": 38399.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.621}, "power_stats": {"power_gpu_soc_mean_watts": 20.908, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 69.621, "power_watts_avg": 20.908, "energy_joules_est": 232.48, "duration_seconds": 11.119, "sample_count": 95}, "timestamp": "2026-01-25T20:02:53.507253"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10286.894, "latencies_ms": [10286.894], "images_per_second": 0.097, "prompt_tokens": 37, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image shows a collection of laptops and a backpack placed on a wooden table. The laptops are of different sizes and brands, and they are arranged in a disorganized manner. The backpack is placed in front of the laptops, and there are multiple cords and cables scattered around the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24441.0, "ram_available_mb": 38399.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24447.9, "ram_available_mb": 38393.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.773}, "power_stats": {"power_gpu_soc_mean_watts": 21.323, "power_cpu_cv_mean_watts": 1.857, "power_sys_5v0_mean_watts": 8.917, "gpu_utilization_percent_mean": 70.773, "power_watts_avg": 21.323, "energy_joules_est": 219.36, "duration_seconds": 10.288, "sample_count": 88}, "timestamp": "2026-01-25T20:03:05.824717"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7984.549, "latencies_ms": [7984.549], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a collection of laptops on a wooden table, with one laptop open and displaying a green screen. There is a backpack with a red and black color scheme on the table, and various cables and chargers scattered around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24447.9, "ram_available_mb": 38393.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24450.5, "ram_available_mb": 38390.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.13}, "power_stats": {"power_gpu_soc_mean_watts": 21.989, "power_cpu_cv_mean_watts": 1.729, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 71.13, "power_watts_avg": 21.989, "energy_joules_est": 175.59, "duration_seconds": 7.985, "sample_count": 69}, "timestamp": "2026-01-25T20:03:15.827615"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11130.318, "latencies_ms": [11130.318], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a skier is captured in mid-air, performing a daring jump. The skier, clad in a vibrant red jacket and black pants, is holding ski poles and is in the process of executing a flip. The skis, adorned with yellow and black designs, are clearly visible beneath the skier's feet. The", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24450.5, "ram_available_mb": 38390.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24438.6, "ram_available_mb": 38402.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.505}, "power_stats": {"power_gpu_soc_mean_watts": 20.86, "power_cpu_cv_mean_watts": 1.932, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 69.505, "power_watts_avg": 20.86, "energy_joules_est": 232.19, "duration_seconds": 11.131, "sample_count": 97}, "timestamp": "2026-01-25T20:03:29.006741"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7029.709, "latencies_ms": [7029.709], "images_per_second": 0.142, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "person: 1, ski: 2, snow: 1, mountain: 1, sky: 1, snowboard: 1, snow: 1, person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.6, "ram_available_mb": 38402.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.567}, "power_stats": {"power_gpu_soc_mean_watts": 23.038, "power_cpu_cv_mean_watts": 1.602, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 74.567, "power_watts_avg": 23.038, "energy_joules_est": 161.97, "duration_seconds": 7.031, "sample_count": 60}, "timestamp": "2026-01-25T20:03:38.099126"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10681.973, "latencies_ms": [10681.973], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there is a skier in mid-air, performing a jump with their skis crossed. They are positioned above the snowy slope, indicating they have launched off a jump. In the background, another person is standing on the slope, appearing much smaller due to the distance, and is likely waiting for their turn or watching the performance.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24438.0, "ram_available_mb": 38402.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24438.8, "ram_available_mb": 38402.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.879}, "power_stats": {"power_gpu_soc_mean_watts": 21.029, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.879, "power_watts_avg": 21.029, "energy_joules_est": 224.64, "duration_seconds": 10.683, "sample_count": 91}, "timestamp": "2026-01-25T20:03:50.834124"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6373.702, "latencies_ms": [6373.702], "images_per_second": 0.157, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A skier in a bright orange suit is performing a jump in the air above a snowy slope. In the background, another person is skiing down the slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24438.8, "ram_available_mb": 38402.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24441.7, "ram_available_mb": 38399.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.333}, "power_stats": {"power_gpu_soc_mean_watts": 23.363, "power_cpu_cv_mean_watts": 1.52, "power_sys_5v0_mean_watts": 9.058, "gpu_utilization_percent_mean": 74.333, "power_watts_avg": 23.363, "energy_joules_est": 148.92, "duration_seconds": 6.374, "sample_count": 54}, "timestamp": "2026-01-25T20:03:59.252952"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7301.56, "latencies_ms": [7301.56], "images_per_second": 0.137, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The skier is wearing a bright orange suit and a green helmet, contrasting against the clear blue sky. The snow appears pristine and untouched, indicating fresh powder conditions for skiing.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 24441.7, "ram_available_mb": 38399.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24443.1, "ram_available_mb": 38397.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.435}, "power_stats": {"power_gpu_soc_mean_watts": 22.47, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 9.04, "gpu_utilization_percent_mean": 72.435, "power_watts_avg": 22.47, "energy_joules_est": 164.08, "duration_seconds": 7.302, "sample_count": 62}, "timestamp": "2026-01-25T20:04:08.584708"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11121.854, "latencies_ms": [11121.854], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a small bird is perched on a window sill, looking out at a body of water. The bird appears to be a pigeon, and it is positioned on the left side of the window. The window itself is rectangular and has a white frame. The view outside the window shows a calm body of water, possibly a lake or a river, with a", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 24443.1, "ram_available_mb": 38397.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24457.5, "ram_available_mb": 38383.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.84}, "power_stats": {"power_gpu_soc_mean_watts": 20.875, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.84, "power_watts_avg": 20.875, "energy_joules_est": 232.18, "duration_seconds": 11.123, "sample_count": 94}, "timestamp": "2026-01-25T20:04:21.748553"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9188.006, "latencies_ms": [9188.006], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "1. Window: 1\n2. Seat: 1\n3. Bird: 1\n4. Water: 1\n5. Trees: 1\n6. Snow: 1\n7. Ground: 1\n8. Sky: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24457.5, "ram_available_mb": 38383.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24456.6, "ram_available_mb": 38384.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.474}, "power_stats": {"power_gpu_soc_mean_watts": 21.766, "power_cpu_cv_mean_watts": 1.776, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 71.474, "power_watts_avg": 21.766, "energy_joules_est": 200.0, "duration_seconds": 9.189, "sample_count": 78}, "timestamp": "2026-01-25T20:04:32.974393"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10396.616, "latencies_ms": [10396.616], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image shows a bird in the foreground, perched on what appears to be a window ledge. The bird is facing towards the left side of the image, where a body of water is visible in the background. The window frame is rusted and has a handle on the right side, suggesting it may be an old or neglected structure.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24456.6, "ram_available_mb": 38384.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24465.3, "ram_available_mb": 38375.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.318}, "power_stats": {"power_gpu_soc_mean_watts": 21.169, "power_cpu_cv_mean_watts": 1.852, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 70.318, "power_watts_avg": 21.169, "energy_joules_est": 220.1, "duration_seconds": 10.397, "sample_count": 88}, "timestamp": "2026-01-25T20:04:45.421589"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6871.424, "latencies_ms": [6871.424], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A bird is perched on the edge of a window frame, looking out at a body of water. The window appears to be old and weathered, with peeling paint and rust.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24465.3, "ram_available_mb": 38375.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24536.2, "ram_available_mb": 38304.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.983}, "power_stats": {"power_gpu_soc_mean_watts": 23.091, "power_cpu_cv_mean_watts": 1.54, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 74.983, "power_watts_avg": 23.091, "energy_joules_est": 158.68, "duration_seconds": 6.872, "sample_count": 58}, "timestamp": "2026-01-25T20:04:54.331089"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7914.645, "latencies_ms": [7914.645], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a small bird perched on a window ledge, looking out at a body of water. The window is old and rusted, with a handle on the right side, and the surrounding area is dark and dimly lit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24466.0, "ram_available_mb": 38374.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24554.6, "ram_available_mb": 38286.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.388}, "power_stats": {"power_gpu_soc_mean_watts": 22.271, "power_cpu_cv_mean_watts": 1.703, "power_sys_5v0_mean_watts": 9.026, "gpu_utilization_percent_mean": 72.388, "power_watts_avg": 22.271, "energy_joules_est": 176.28, "duration_seconds": 7.915, "sample_count": 67}, "timestamp": "2026-01-25T20:05:04.264426"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11112.968, "latencies_ms": [11112.968], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seen in a bathroom, standing next to a toilet. He is wearing a red hat and a gray shirt. The bathroom is equipped with a white toilet, a white trash can, and a white bucket. The man appears to be in the process of cleaning the toilet. The bathroom is well-", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24466.0, "ram_available_mb": 38374.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24551.8, "ram_available_mb": 38289.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.589}, "power_stats": {"power_gpu_soc_mean_watts": 20.938, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.589, "power_watts_avg": 20.938, "energy_joules_est": 232.7, "duration_seconds": 11.114, "sample_count": 95}, "timestamp": "2026-01-25T20:05:17.438627"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8542.191, "latencies_ms": [8542.191], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "toilet: 1, trash bag: 1, paint can: 1, paintbrush: 1, paint roller: 1, paint tray: 1, paint bottle: 1, paint container: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.6, "ram_available_mb": 38409.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24532.8, "ram_available_mb": 38308.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.301}, "power_stats": {"power_gpu_soc_mean_watts": 21.95, "power_cpu_cv_mean_watts": 1.722, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 71.301, "power_watts_avg": 21.95, "energy_joules_est": 187.51, "duration_seconds": 8.543, "sample_count": 73}, "timestamp": "2026-01-25T20:05:28.022813"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10620.382, "latencies_ms": [10620.382], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "In the foreground, there is a toilet with a black trash bag in front of it, indicating recent use. In the background, there is a person bending over near the toilet, possibly engaged in cleaning or maintenance. The person is closer to the camera than the toilet, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24454.1, "ram_available_mb": 38386.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.011}, "power_stats": {"power_gpu_soc_mean_watts": 21.118, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 70.011, "power_watts_avg": 21.118, "energy_joules_est": 224.3, "duration_seconds": 10.621, "sample_count": 90}, "timestamp": "2026-01-25T20:05:40.704404"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7652.057, "latencies_ms": [7652.057], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "In a small, cluttered bathroom, a man is seen bending over a toilet, seemingly in the process of fixing or cleaning it. Another man is standing nearby, observing the situation.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24454.1, "ram_available_mb": 38386.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24539.2, "ram_available_mb": 38301.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.554}, "power_stats": {"power_gpu_soc_mean_watts": 22.507, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 73.554, "power_watts_avg": 22.507, "energy_joules_est": 172.24, "duration_seconds": 7.653, "sample_count": 65}, "timestamp": "2026-01-25T20:05:50.398569"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8137.403, "latencies_ms": [8137.403], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image is taken indoors with artificial lighting, and the colors are muted with a predominance of browns and whites. The floor is covered with a blue tarp, and there are various cleaning supplies and containers scattered around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24466.3, "ram_available_mb": 38374.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24536.2, "ram_available_mb": 38304.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.029}, "power_stats": {"power_gpu_soc_mean_watts": 22.117, "power_cpu_cv_mean_watts": 1.724, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 72.029, "power_watts_avg": 22.117, "energy_joules_est": 179.99, "duration_seconds": 8.138, "sample_count": 69}, "timestamp": "2026-01-25T20:06:00.569117"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11092.759, "latencies_ms": [11092.759], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is standing in a hallway, holding an umbrella to protect themselves from the rain. The person is wearing a blue jacket and is positioned in the center of the scene. The hallway is narrow, with red walls on both sides. There are two doors visible in the image, one on the left and one on the right. The person appears", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24536.2, "ram_available_mb": 38304.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24536.9, "ram_available_mb": 38304.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.628}, "power_stats": {"power_gpu_soc_mean_watts": 20.926, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.628, "power_watts_avg": 20.926, "energy_joules_est": 232.14, "duration_seconds": 11.093, "sample_count": 94}, "timestamp": "2026-01-25T20:06:13.688065"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9624.435, "latencies_ms": [9624.435], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "1. Umbrella: 1\n2. Person: 1\n3. Door: 2\n4. Wall: 2\n5. Rain: 1\n6. Light fixture: 1\n7. Mirror: 1\n8. Picture frame: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24536.9, "ram_available_mb": 38304.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24537.6, "ram_available_mb": 38303.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.049}, "power_stats": {"power_gpu_soc_mean_watts": 21.612, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 72.049, "power_watts_avg": 21.612, "energy_joules_est": 208.02, "duration_seconds": 9.625, "sample_count": 82}, "timestamp": "2026-01-25T20:06:25.328965"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11160.303, "latencies_ms": [11160.303], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The person is standing in the foreground, holding an umbrella above their head. The umbrella is positioned in the upper left corner of the image, providing cover from the rain. The background consists of a red wall with a framed picture hanging on it, and a door to the left of the person. The person appears to be standing in a doorway or hallway", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24537.6, "ram_available_mb": 38303.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 24432.9, "ram_available_mb": 38408.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.8}, "power_stats": {"power_gpu_soc_mean_watts": 20.985, "power_cpu_cv_mean_watts": 1.964, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 69.8, "power_watts_avg": 20.985, "energy_joules_est": 234.21, "duration_seconds": 11.161, "sample_count": 95}, "timestamp": "2026-01-25T20:06:38.501472"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8540.99, "latencies_ms": [8540.99], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "A person is standing in a hallway, holding an umbrella with the words \"Capsa Mo\" written on it, suggesting that it is raining. The person is wearing a blue jacket and appears to be walking through the hallway.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24432.9, "ram_available_mb": 38408.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24452.7, "ram_available_mb": 38388.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.222}, "power_stats": {"power_gpu_soc_mean_watts": 22.104, "power_cpu_cv_mean_watts": 1.708, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 72.222, "power_watts_avg": 22.104, "energy_joules_est": 188.8, "duration_seconds": 8.542, "sample_count": 72}, "timestamp": "2026-01-25T20:06:49.089257"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10695.851, "latencies_ms": [10695.851], "images_per_second": 0.093, "prompt_tokens": 36, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The image depicts a person standing under a black umbrella with the words \"Sopas Moni\" written on it, in a corridor with red walls and a reflective floor. The lighting is dim, with a mix of natural light coming from the doorway and artificial light reflecting off the walls, creating a moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24452.7, "ram_available_mb": 38388.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24458.6, "ram_available_mb": 38382.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.505}, "power_stats": {"power_gpu_soc_mean_watts": 21.128, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.505, "power_watts_avg": 21.128, "energy_joules_est": 226.0, "duration_seconds": 10.696, "sample_count": 91}, "timestamp": "2026-01-25T20:07:01.823007"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11132.056, "latencies_ms": [11132.056], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing in front of a wooden gate with a sign that has Chinese characters on it. He is wearing a red shirt and a backpack, and is holding a walking stick. The gate is located in a garden with a stone pathway and a stone wall. There are trees and bushes in the background, and a person in a yellow robe is", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24458.6, "ram_available_mb": 38382.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24435.9, "ram_available_mb": 38405.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.779}, "power_stats": {"power_gpu_soc_mean_watts": 20.887, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.921, "gpu_utilization_percent_mean": 68.779, "power_watts_avg": 20.887, "energy_joules_est": 232.53, "duration_seconds": 11.133, "sample_count": 95}, "timestamp": "2026-01-25T20:07:15.004092"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7505.282, "latencies_ms": [7505.282], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "person: 2, rock: 10, sign: 1, staircase: 1, railing: 1, plant: 5, path: 1, backpack: 1", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24435.9, "ram_available_mb": 38405.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24437.7, "ram_available_mb": 38403.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.143}, "power_stats": {"power_gpu_soc_mean_watts": 22.503, "power_cpu_cv_mean_watts": 1.64, "power_sys_5v0_mean_watts": 9.014, "gpu_utilization_percent_mean": 73.143, "power_watts_avg": 22.503, "energy_joules_est": 168.91, "duration_seconds": 7.506, "sample_count": 63}, "timestamp": "2026-01-25T20:07:24.545729"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10458.115, "latencies_ms": [10458.115], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, there is a person with a backpack standing on a gravel path, looking upwards. Behind this individual, there is a wooden railing with a person in a yellow robe standing on a higher platform to the left. The background is filled with lush greenery and a signpost with multiple directional arrows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.7, "ram_available_mb": 38403.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.753}, "power_stats": {"power_gpu_soc_mean_watts": 21.146, "power_cpu_cv_mean_watts": 1.89, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 69.753, "power_watts_avg": 21.146, "energy_joules_est": 221.16, "duration_seconds": 10.459, "sample_count": 89}, "timestamp": "2026-01-25T20:07:37.060589"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10090.032, "latencies_ms": [10090.032], "images_per_second": 0.099, "prompt_tokens": 37, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image depicts a serene outdoor setting with a man in a red shirt and backpack standing on a gravel path, looking up at a signpost with various directions. In the background, there is a person dressed in a yellow robe, possibly a monk, standing on a wooden bridge or platform.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24439.2, "ram_available_mb": 38401.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24448.1, "ram_available_mb": 38392.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.035}, "power_stats": {"power_gpu_soc_mean_watts": 21.329, "power_cpu_cv_mean_watts": 1.819, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 71.035, "power_watts_avg": 21.329, "energy_joules_est": 215.22, "duration_seconds": 10.091, "sample_count": 85}, "timestamp": "2026-01-25T20:07:49.163913"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10117.227, "latencies_ms": [10117.227], "images_per_second": 0.099, "prompt_tokens": 36, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image depicts a scene with a person in a yellow robe standing on a wooden bridge with a railing, and another person in a red shirt with a backpack and trekking pole standing on the ground below. The environment is lush with greenery, and the lighting suggests it is taken during the day.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24448.1, "ram_available_mb": 38392.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24435.5, "ram_available_mb": 38405.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.302}, "power_stats": {"power_gpu_soc_mean_watts": 21.193, "power_cpu_cv_mean_watts": 1.853, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 70.302, "power_watts_avg": 21.193, "energy_joules_est": 214.43, "duration_seconds": 10.118, "sample_count": 86}, "timestamp": "2026-01-25T20:08:01.324932"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12327.609, "latencies_ms": [12327.609], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of camaraderie among four men in a room that exudes a casual, possibly social atmosphere. The man on the far left, clad in a white shirt and khaki pants, stands with a relaxed posture. Next to him, the man in the center sports a red tie and a white shirt, his smile suggesting a", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24435.5, "ram_available_mb": 38405.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24457.9, "ram_available_mb": 38383.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.8}, "power_stats": {"power_gpu_soc_mean_watts": 22.798, "power_cpu_cv_mean_watts": 1.774, "power_sys_5v0_mean_watts": 9.195, "gpu_utilization_percent_mean": 73.8, "power_watts_avg": 22.798, "energy_joules_est": 281.06, "duration_seconds": 12.328, "sample_count": 105}, "timestamp": "2026-01-25T20:08:15.705667"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10954.723, "latencies_ms": [10954.723], "images_per_second": 0.091, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "- Chair: 2\n\n- Table: 1\n\n- Bottle: 10\n\n- Glass: 5\n\n- Armchair: 1\n\n- Jacket: 1\n\n- Shirt: 4\n\n- Suit: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24457.9, "ram_available_mb": 38383.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24465.9, "ram_available_mb": 38375.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_gpu_soc_mean_watts": 23.43, "power_cpu_cv_mean_watts": 1.641, "power_sys_5v0_mean_watts": 9.153, "gpu_utilization_percent_mean": 76.0, "power_watts_avg": 23.43, "energy_joules_est": 256.69, "duration_seconds": 10.955, "sample_count": 93}, "timestamp": "2026-01-25T20:08:28.721235"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11109.122, "latencies_ms": [11109.122], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "In the foreground, there are four individuals standing close together, with one person slightly in front of the others, creating a sense of depth. In the background, there is a bar area with a counter and bottles, which appears smaller due to the perspective, indicating it is further away from the viewpoint of the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24465.9, "ram_available_mb": 38375.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24516.0, "ram_available_mb": 38324.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.484}, "power_stats": {"power_gpu_soc_mean_watts": 23.173, "power_cpu_cv_mean_watts": 1.695, "power_sys_5v0_mean_watts": 9.201, "gpu_utilization_percent_mean": 74.484, "power_watts_avg": 23.173, "energy_joules_est": 257.45, "duration_seconds": 11.11, "sample_count": 95}, "timestamp": "2026-01-25T20:08:41.886270"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9719.386, "latencies_ms": [9719.386], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "Four men are standing together in a room that appears to be a bar or a similar social setting, with a bar counter and bottles visible in the background. They are all dressed in business casual attire and seem to be posing for a photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24429.3, "ram_available_mb": 38411.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24512.9, "ram_available_mb": 38328.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.646}, "power_stats": {"power_gpu_soc_mean_watts": 23.864, "power_cpu_cv_mean_watts": 1.553, "power_sys_5v0_mean_watts": 9.165, "gpu_utilization_percent_mean": 76.646, "power_watts_avg": 23.864, "energy_joules_est": 231.96, "duration_seconds": 9.72, "sample_count": 82}, "timestamp": "2026-01-25T20:08:53.666061"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8566.633, "latencies_ms": [8566.633], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows four individuals standing indoors with a warm and soft lighting that suggests an indoor setting. The walls are painted in a light color, and there is a red accent wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24512.9, "ram_available_mb": 38328.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24514.1, "ram_available_mb": 38326.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.616}, "power_stats": {"power_gpu_soc_mean_watts": 24.186, "power_cpu_cv_mean_watts": 1.503, "power_sys_5v0_mean_watts": 9.23, "gpu_utilization_percent_mean": 76.616, "power_watts_avg": 24.186, "energy_joules_est": 207.21, "duration_seconds": 8.567, "sample_count": 73}, "timestamp": "2026-01-25T20:09:04.284238"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11107.582, "latencies_ms": [11107.582], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant street scene in what appears to be a European city. The street is bustling with activity, with cars and buses moving along the road. The buildings lining the street are a mix of brick and stone, painted in a variety of colors, adding to the charm of the scene. The sky overhead is overcast, casting a soft light over the", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24434.0, "ram_available_mb": 38406.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24459.9, "ram_available_mb": 38381.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.6}, "power_stats": {"power_gpu_soc_mean_watts": 20.916, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.6, "power_watts_avg": 20.916, "energy_joules_est": 232.34, "duration_seconds": 11.108, "sample_count": 95}, "timestamp": "2026-01-25T20:09:17.424580"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9754.274, "latencies_ms": [9754.274], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "- Buildings: 10\n\n- Cars: 4\n\n- Traffic lights: 1\n\n- Signs: 1\n\n- Pole: 1\n\n- Tree: 0\n\n- Pedestrians: 2\n\n- Bus: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24459.9, "ram_available_mb": 38381.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24458.2, "ram_available_mb": 38382.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.768}, "power_stats": {"power_gpu_soc_mean_watts": 21.606, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 71.768, "power_watts_avg": 21.606, "energy_joules_est": 210.76, "duration_seconds": 9.755, "sample_count": 82}, "timestamp": "2026-01-25T20:09:29.194578"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10711.458, "latencies_ms": [10711.458], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "In the foreground, there is a yellow sign mounted on a black pole, positioned on the right side of the image. The background features a street scene with buildings on the left, a red traffic light in the middle distance, and vehicles on the road. The sign is near the curb, and the pole appears to be damaged at the base.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24458.2, "ram_available_mb": 38382.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24462.8, "ram_available_mb": 38378.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.522}, "power_stats": {"power_gpu_soc_mean_watts": 20.297, "power_cpu_cv_mean_watts": 1.869, "power_sys_5v0_mean_watts": 8.93, "gpu_utilization_percent_mean": 71.522, "power_watts_avg": 20.297, "energy_joules_est": 217.42, "duration_seconds": 10.712, "sample_count": 90}, "timestamp": "2026-01-25T20:09:41.937533"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9074.946, "latencies_ms": [9074.946], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image depicts a city street scene with a yellow sign mounted on a pole, which appears to be damaged and leaning to one side. The street is busy with various vehicles, including cars and buses, and there are buildings on both sides, suggesting an urban environment.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24462.8, "ram_available_mb": 38378.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24537.0, "ram_available_mb": 38303.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.351}, "power_stats": {"power_gpu_soc_mean_watts": 21.814, "power_cpu_cv_mean_watts": 1.753, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 72.351, "power_watts_avg": 21.814, "energy_joules_est": 197.97, "duration_seconds": 9.076, "sample_count": 77}, "timestamp": "2026-01-25T20:09:53.053787"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6812.112, "latencies_ms": [6812.112], "images_per_second": 0.147, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image shows a street scene with a yellow sign mounted on a black pole. The weather appears to be overcast with a cloudy sky, and the street is wet, suggesting recent rain.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24537.0, "ram_available_mb": 38303.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24460.9, "ram_available_mb": 38380.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.937, "power_cpu_cv_mean_watts": 1.602, "power_sys_5v0_mean_watts": 9.052, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 22.937, "energy_joules_est": 156.26, "duration_seconds": 6.813, "sample_count": 57}, "timestamp": "2026-01-25T20:10:01.925596"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11122.318, "latencies_ms": [11122.318], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is standing on a tennis court, holding a tennis racket and preparing to hit a tennis ball. He is wearing a white shirt and black shorts. The court is surrounded by a fence, and there are two red signs on the fence. One sign reads \"VOX SPORTS\" and the other reads \"Believe it,", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24460.9, "ram_available_mb": 38380.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24435.0, "ram_available_mb": 38405.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.872}, "power_stats": {"power_gpu_soc_mean_watts": 20.926, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 69.872, "power_watts_avg": 20.926, "energy_joules_est": 232.76, "duration_seconds": 11.123, "sample_count": 94}, "timestamp": "2026-01-25T20:10:15.086382"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7808.739, "latencies_ms": [7808.739], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "fence: 4\nsign: 2\nbench: 1\ntennis court: 1\nplayer: 1\nracket: 1\nball: 0\nbottle: 0", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24435.0, "ram_available_mb": 38405.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24441.8, "ram_available_mb": 38399.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.727}, "power_stats": {"power_gpu_soc_mean_watts": 21.594, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 72.727, "power_watts_avg": 21.594, "energy_joules_est": 168.64, "duration_seconds": 7.809, "sample_count": 66}, "timestamp": "2026-01-25T20:10:24.941020"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11116.678, "latencies_ms": [11116.678], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is standing on a tennis court holding a racket, positioned near the center of the image. The court is enclosed by a fence in the background, and there are signs attached to the fence, one of which is clearly visible with the text 'VOX SPORTS prince rule the court.' The signs are located at a distance from the person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24441.8, "ram_available_mb": 38399.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 24435.7, "ram_available_mb": 38405.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.811}, "power_stats": {"power_gpu_soc_mean_watts": 20.954, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 69.811, "power_watts_avg": 20.954, "energy_joules_est": 232.95, "duration_seconds": 11.117, "sample_count": 95}, "timestamp": "2026-01-25T20:10:38.099172"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5563.81, "latencies_ms": [5563.81], "images_per_second": 0.18, "prompt_tokens": 37, "response_tokens_est": 27, "n_tiles": 16, "output_text": "A person is playing tennis on a court at night, with a fence and advertisement banners in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24435.7, "ram_available_mb": 38405.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24444.5, "ram_available_mb": 38396.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.065}, "power_stats": {"power_gpu_soc_mean_watts": 23.997, "power_cpu_cv_mean_watts": 1.706, "power_sys_5v0_mean_watts": 9.055, "gpu_utilization_percent_mean": 77.065, "power_watts_avg": 23.997, "energy_joules_est": 133.53, "duration_seconds": 5.564, "sample_count": 46}, "timestamp": "2026-01-25T20:10:45.700860"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6321.387, "latencies_ms": [6321.387], "images_per_second": 0.158, "prompt_tokens": 36, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The tennis court is surrounded by a green fence and there is a red banner with white text on it. The sky is dark, indicating that it is nighttime.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24444.5, "ram_available_mb": 38396.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24443.0, "ram_available_mb": 38397.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.722}, "power_stats": {"power_gpu_soc_mean_watts": 23.149, "power_cpu_cv_mean_watts": 1.55, "power_sys_5v0_mean_watts": 9.078, "gpu_utilization_percent_mean": 73.722, "power_watts_avg": 23.149, "energy_joules_est": 146.35, "duration_seconds": 6.322, "sample_count": 54}, "timestamp": "2026-01-25T20:10:54.057439"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12312.287, "latencies_ms": [12312.287], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment on a snowy ski slope where a group of skiers are gathered around a blue fence. The fence is set up to demarcate a specific area, possibly for a skiing event or a training session. The skiers are equipped with helmets and goggles, indicating that they are prepared for skiing activities. The snow-", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24443.0, "ram_available_mb": 38397.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24459.5, "ram_available_mb": 38381.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.481}, "power_stats": {"power_gpu_soc_mean_watts": 22.861, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 9.209, "gpu_utilization_percent_mean": 74.481, "power_watts_avg": 22.861, "energy_joules_est": 281.49, "duration_seconds": 12.313, "sample_count": 106}, "timestamp": "2026-01-25T20:11:08.434478"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9174.111, "latencies_ms": [9174.111], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "person: 4, ski: 4, helmet: 3, glove: 3, ski pole: 4, fence: 2, ski suit: 2, goggles: 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24459.5, "ram_available_mb": 38381.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24466.0, "ram_available_mb": 38374.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.278}, "power_stats": {"power_gpu_soc_mean_watts": 24.078, "power_cpu_cv_mean_watts": 1.475, "power_sys_5v0_mean_watts": 9.182, "gpu_utilization_percent_mean": 78.278, "power_watts_avg": 24.078, "energy_joules_est": 220.91, "duration_seconds": 9.175, "sample_count": 79}, "timestamp": "2026-01-25T20:11:19.637868"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12195.131, "latencies_ms": [12195.131], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, a person in a black jacket is standing to the left of a blue fence, gesturing with their hand. Behind the fence, two individuals are partially visible; one is wearing a white helmet and the other is wearing a red helmet. The background shows a snowy landscape with a few more people and ski equipment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24466.0, "ram_available_mb": 38374.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24452.3, "ram_available_mb": 38388.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.077}, "power_stats": {"power_gpu_soc_mean_watts": 22.902, "power_cpu_cv_mean_watts": 1.767, "power_sys_5v0_mean_watts": 9.218, "gpu_utilization_percent_mean": 74.077, "power_watts_avg": 22.902, "energy_joules_est": 279.31, "duration_seconds": 12.196, "sample_count": 104}, "timestamp": "2026-01-25T20:11:33.878936"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11055.841, "latencies_ms": [11055.841], "images_per_second": 0.09, "prompt_tokens": 37, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image captures a moment on a snowy slope where a group of skiers are gathered around a blue net, possibly during a break or a casual interaction. The setting appears to be a ski resort, and the skiers are dressed in winter sports gear, suggesting they are engaged in skiing activities.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24452.3, "ram_available_mb": 38388.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24520.9, "ram_available_mb": 38320.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.179}, "power_stats": {"power_gpu_soc_mean_watts": 23.454, "power_cpu_cv_mean_watts": 1.669, "power_sys_5v0_mean_watts": 9.163, "gpu_utilization_percent_mean": 76.179, "power_watts_avg": 23.454, "energy_joules_est": 259.32, "duration_seconds": 11.056, "sample_count": 95}, "timestamp": "2026-01-25T20:11:46.946631"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10275.317, "latencies_ms": [10275.317], "images_per_second": 0.097, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image shows a group of people on a snowy surface, likely engaged in a winter sport, with bright sunlight casting shadows on the ground. The weather appears to be clear and sunny, as indicated by the bright lighting and the absence of any snowfall or wet conditions.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24520.9, "ram_available_mb": 38320.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24462.0, "ram_available_mb": 38378.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.614}, "power_stats": {"power_gpu_soc_mean_watts": 23.413, "power_cpu_cv_mean_watts": 1.638, "power_sys_5v0_mean_watts": 9.244, "gpu_utilization_percent_mean": 75.614, "power_watts_avg": 23.413, "energy_joules_est": 240.59, "duration_seconds": 10.276, "sample_count": 88}, "timestamp": "2026-01-25T20:11:59.277364"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11111.898, "latencies_ms": [11111.898], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment on a wet, cloudy day in a residential neighborhood. A blue truck, adorned with a white logo and the text \"HUISMANGROUP.COM\", is the main subject of the image. The truck is driving on the right side of the road, following the direction of the traffic. The road itself is wet, reflecting", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24462.0, "ram_available_mb": 38378.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.266}, "power_stats": {"power_gpu_soc_mean_watts": 20.883, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 69.266, "power_watts_avg": 20.883, "energy_joules_est": 232.06, "duration_seconds": 11.113, "sample_count": 94}, "timestamp": "2026-01-25T20:12:12.454166"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7299.816, "latencies_ms": [7299.816], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "truck: 1, car: 3, house: 3, tree: 2, street light: 1, sidewalk: 1, grass: 1, cloud: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24445.8, "ram_available_mb": 38395.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.262}, "power_stats": {"power_gpu_soc_mean_watts": 22.664, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 72.262, "power_watts_avg": 22.664, "energy_joules_est": 165.46, "duration_seconds": 7.3, "sample_count": 61}, "timestamp": "2026-01-25T20:12:21.766178"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10017.115, "latencies_ms": [10017.115], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "A blue truck is in the foreground on the left side of the image, moving along a wet street. In the background, there are parked cars on the right side of the street and houses on both sides, indicating a residential area. The truck is closer to the viewer than the parked cars and houses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24445.8, "ram_available_mb": 38395.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24448.3, "ram_available_mb": 38392.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.812}, "power_stats": {"power_gpu_soc_mean_watts": 21.276, "power_cpu_cv_mean_watts": 1.861, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 69.812, "power_watts_avg": 21.276, "energy_joules_est": 213.14, "duration_seconds": 10.018, "sample_count": 85}, "timestamp": "2026-01-25T20:12:33.806961"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6966.857, "latencies_ms": [6966.857], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A blue Huisman Group truck is driving down a wet street in a residential area. The street is lined with houses and trees, and there are other vehicles on the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24448.3, "ram_available_mb": 38392.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.119}, "power_stats": {"power_gpu_soc_mean_watts": 22.975, "power_cpu_cv_mean_watts": 1.547, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 74.119, "power_watts_avg": 22.975, "energy_joules_est": 160.08, "duration_seconds": 6.967, "sample_count": 59}, "timestamp": "2026-01-25T20:12:42.818419"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7145.282, "latencies_ms": [7145.282], "images_per_second": 0.14, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A blue Huisman Group truck is driving on a wet road, indicating recent rain. The sky is overcast, and the street is lined with houses and trees, creating a suburban atmosphere.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24437.6, "ram_available_mb": 38403.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24433.7, "ram_available_mb": 38407.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.367}, "power_stats": {"power_gpu_soc_mean_watts": 22.42, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 9.029, "gpu_utilization_percent_mean": 72.367, "power_watts_avg": 22.42, "energy_joules_est": 160.21, "duration_seconds": 7.146, "sample_count": 60}, "timestamp": "2026-01-25T20:12:51.997280"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11130.239, "latencies_ms": [11130.239], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment at an airport, where a large commercial airplane is in the process of landing. The airplane, painted in a sleek gray color, is positioned in the center of the frame, with its landing gear extended, ready to touch down on the runway. The runway itself is a solid gray, marked with white lines that guide the plane'", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24433.7, "ram_available_mb": 38407.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24437.4, "ram_available_mb": 38403.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.253}, "power_stats": {"power_gpu_soc_mean_watts": 20.91, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.253, "power_watts_avg": 20.91, "energy_joules_est": 232.75, "duration_seconds": 11.131, "sample_count": 95}, "timestamp": "2026-01-25T20:13:05.177944"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7872.775, "latencies_ms": [7872.775], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "runway: 1\nairplane: 1\ntaxiway: 1\nfence: 1\ngrass: 1\nmountains: 1\ntrees: 1\nbuildings: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24437.4, "ram_available_mb": 38403.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24435.9, "ram_available_mb": 38405.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.47}, "power_stats": {"power_gpu_soc_mean_watts": 22.384, "power_cpu_cv_mean_watts": 1.638, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 73.47, "power_watts_avg": 22.384, "energy_joules_est": 176.24, "duration_seconds": 7.873, "sample_count": 66}, "timestamp": "2026-01-25T20:13:15.093217"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11128.049, "latencies_ms": [11128.049], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are several red circular objects mounted on poles, which appear to be part of an airport's ground equipment, possibly related to the runway's lighting or marking system. In the background, there is an airplane on the runway, positioned centrally and slightly to the right of the frame, indicating it is either taxiing", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24435.9, "ram_available_mb": 38405.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24450.1, "ram_available_mb": 38390.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.117}, "power_stats": {"power_gpu_soc_mean_watts": 20.945, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 70.117, "power_watts_avg": 20.945, "energy_joules_est": 233.09, "duration_seconds": 11.129, "sample_count": 94}, "timestamp": "2026-01-25T20:13:28.243427"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7070.276, "latencies_ms": [7070.276], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A large commercial airplane is taxiing on the runway, preparing for takeoff. The airport is surrounded by mountains and the sky is hazy, indicating possible poor air quality.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24450.1, "ram_available_mb": 38390.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24441.7, "ram_available_mb": 38399.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.576}, "power_stats": {"power_gpu_soc_mean_watts": 22.99, "power_cpu_cv_mean_watts": 1.561, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 74.576, "power_watts_avg": 22.99, "energy_joules_est": 162.56, "duration_seconds": 7.071, "sample_count": 59}, "timestamp": "2026-01-25T20:13:37.324070"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10324.012, "latencies_ms": [10324.012], "images_per_second": 0.097, "prompt_tokens": 36, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image features a foggy or hazy day with low visibility, which is evident from the obscured background and the overall grayish tone of the sky. The airplane is a large, commercial jet with a dark body and a white wing, and it is taxiing on the wet runway, which reflects the plane's lights.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24441.7, "ram_available_mb": 38399.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24450.6, "ram_available_mb": 38390.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.897}, "power_stats": {"power_gpu_soc_mean_watts": 21.173, "power_cpu_cv_mean_watts": 1.868, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 69.897, "power_watts_avg": 21.173, "energy_joules_est": 218.6, "duration_seconds": 10.325, "sample_count": 87}, "timestamp": "2026-01-25T20:13:49.667115"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11164.972, "latencies_ms": [11164.972], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people on a beach, with a woman holding a baseball bat in the foreground. There are several other people scattered across the scene, some closer to the water and others further back. A few cars can be seen parked nearby, and a lifeguard tower is visible in the background. The beach appears to be a popular spot for both locals and tourists", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24450.6, "ram_available_mb": 38390.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24435.1, "ram_available_mb": 38405.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.084}, "power_stats": {"power_gpu_soc_mean_watts": 20.85, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.084, "power_watts_avg": 20.85, "energy_joules_est": 232.8, "duration_seconds": 11.166, "sample_count": 95}, "timestamp": "2026-01-25T20:14:02.882948"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7180.117, "latencies_ms": [7180.117], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "people: 9, cars: 2, flags: 1, buildings: 1, palm trees: 1, cones: 1, sand: numerous, beach: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24435.1, "ram_available_mb": 38405.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24431.8, "ram_available_mb": 38409.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.541}, "power_stats": {"power_gpu_soc_mean_watts": 22.774, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 74.541, "power_watts_avg": 22.774, "energy_joules_est": 163.53, "duration_seconds": 7.181, "sample_count": 61}, "timestamp": "2026-01-25T20:14:12.078457"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8800.928, "latencies_ms": [8800.928], "images_per_second": 0.114, "prompt_tokens": 44, "response_tokens_est": 58, "n_tiles": 16, "output_text": "In the foreground, a person is holding a baseball bat up in the air. In the background, there are several other people walking on the beach, with a lifeguard tower and an American flag visible. The person with the bat is closer to the camera than the others.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24431.8, "ram_available_mb": 38409.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24442.2, "ram_available_mb": 38398.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.013}, "power_stats": {"power_gpu_soc_mean_watts": 21.749, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 71.013, "power_watts_avg": 21.749, "energy_joules_est": 191.43, "duration_seconds": 8.802, "sample_count": 75}, "timestamp": "2026-01-25T20:14:22.920266"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7704.845, "latencies_ms": [7704.845], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A group of people are gathered on a sandy beach, with a lifeguard tower and palm trees in the background. One person is holding a baseball bat, and there are traffic cones set up in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.2, "ram_available_mb": 38398.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24440.3, "ram_available_mb": 38400.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.615}, "power_stats": {"power_gpu_soc_mean_watts": 22.604, "power_cpu_cv_mean_watts": 1.638, "power_sys_5v0_mean_watts": 9.016, "gpu_utilization_percent_mean": 73.615, "power_watts_avg": 22.604, "energy_joules_est": 174.17, "duration_seconds": 7.705, "sample_count": 65}, "timestamp": "2026-01-25T20:14:32.640142"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8234.299, "latencies_ms": [8234.299], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a group of people on a sandy beach with a clear sky and bright sunlight casting shadows on the ground. The beach is equipped with a lifeguard tower and a flag, indicating it's a public recreational area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24440.3, "ram_available_mb": 38400.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 24437.3, "ram_available_mb": 38403.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.143}, "power_stats": {"power_gpu_soc_mean_watts": 21.93, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 9.045, "gpu_utilization_percent_mean": 71.143, "power_watts_avg": 21.93, "energy_joules_est": 180.59, "duration_seconds": 8.235, "sample_count": 70}, "timestamp": "2026-01-25T20:14:42.909292"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11082.438, "latencies_ms": [11082.438], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a tennis player is standing on a tennis court, holding a tennis racket in his right hand and a tennis ball in his left hand. He appears to be preparing to serve the ball. The player is wearing a blue shirt and white shorts, and he is positioned near the baseline of the court. The court itself is blue, and the player'", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24437.3, "ram_available_mb": 38403.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24467.9, "ram_available_mb": 38372.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.755}, "power_stats": {"power_gpu_soc_mean_watts": 20.978, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 69.755, "power_watts_avg": 20.978, "energy_joules_est": 232.5, "duration_seconds": 11.083, "sample_count": 94}, "timestamp": "2026-01-25T20:14:56.040137"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7193.579, "latencies_ms": [7193.579], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "ball: 1, racket: 1, shoe: 2, word: 3, letter: 3, player: 1, court: 1, shadow: 1", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24467.9, "ram_available_mb": 38372.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24542.1, "ram_available_mb": 38298.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.689}, "power_stats": {"power_gpu_soc_mean_watts": 22.818, "power_cpu_cv_mean_watts": 1.575, "power_sys_5v0_mean_watts": 9.048, "gpu_utilization_percent_mean": 74.689, "power_watts_avg": 22.818, "energy_joules_est": 164.16, "duration_seconds": 7.194, "sample_count": 61}, "timestamp": "2026-01-25T20:15:05.294007"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11159.49, "latencies_ms": [11159.49], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The tennis player is positioned in the foreground on a tennis court, with the baseline clearly marked in the background. The player is standing to the right side of the image, holding a tennis racket in his right hand and a tennis ball in his left hand, suggesting he is preparing to play a shot. The shadows cast by the player and the racket indicate that the light", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24542.1, "ram_available_mb": 38298.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24463.3, "ram_available_mb": 38377.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.368}, "power_stats": {"power_gpu_soc_mean_watts": 21.002, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 70.368, "power_watts_avg": 21.002, "energy_joules_est": 234.39, "duration_seconds": 11.16, "sample_count": 95}, "timestamp": "2026-01-25T20:15:18.478247"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6847.669, "latencies_ms": [6847.669], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A tennis player is standing on a tennis court, holding a tennis racket and preparing to serve the ball. The court is marked with the word \"BNP\" in large letters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24463.3, "ram_available_mb": 38377.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24464.4, "ram_available_mb": 38376.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.776}, "power_stats": {"power_gpu_soc_mean_watts": 23.04, "power_cpu_cv_mean_watts": 1.56, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 73.776, "power_watts_avg": 23.04, "energy_joules_est": 157.79, "duration_seconds": 6.849, "sample_count": 58}, "timestamp": "2026-01-25T20:15:27.358101"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6229.192, "latencies_ms": [6229.192], "images_per_second": 0.161, "prompt_tokens": 36, "response_tokens_est": 35, "n_tiles": 16, "output_text": "The image features a tennis player on a court with a blue surface and white markings. The lighting is bright, casting a shadow of the player on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24464.4, "ram_available_mb": 38376.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24534.0, "ram_available_mb": 38306.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.377}, "power_stats": {"power_gpu_soc_mean_watts": 23.416, "power_cpu_cv_mean_watts": 1.541, "power_sys_5v0_mean_watts": 9.097, "gpu_utilization_percent_mean": 73.377, "power_watts_avg": 23.416, "energy_joules_est": 145.88, "duration_seconds": 6.23, "sample_count": 53}, "timestamp": "2026-01-25T20:15:35.619907"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11094.764, "latencies_ms": [11094.764], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing in a kitchen, leaning over a black stove top oven. She appears to be cooking or preparing food. The kitchen is equipped with a refrigerator, a sink, and a microwave. There are various kitchen utensils and items scattered around the room, including a knife, a spoon, a bow", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24534.0, "ram_available_mb": 38306.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24578.5, "ram_available_mb": 38262.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.266}, "power_stats": {"power_gpu_soc_mean_watts": 20.917, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 69.266, "power_watts_avg": 20.917, "energy_joules_est": 232.08, "duration_seconds": 11.095, "sample_count": 94}, "timestamp": "2026-01-25T20:15:48.769724"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7852.005, "latencies_ms": [7852.005], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "fan: 1, bowl: 2, pot: 2, ladder: 1, canister: 2, spoon: 1, kettle: 1, spatula: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24578.5, "ram_available_mb": 38262.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24578.7, "ram_available_mb": 38262.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.394}, "power_stats": {"power_gpu_soc_mean_watts": 22.464, "power_cpu_cv_mean_watts": 1.632, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 73.394, "power_watts_avg": 22.464, "energy_joules_est": 176.4, "duration_seconds": 7.853, "sample_count": 66}, "timestamp": "2026-01-25T20:15:58.640319"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11167.988, "latencies_ms": [11167.988], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is standing in front of a large, old-fashioned black stove, which is the main object of focus. To the right of the stove, there is a kitchen counter with various items on it, including a blue pot and a can. In the background, there is a white refrigerator and a shelf with more kitchen items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24578.7, "ram_available_mb": 38262.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24463.1, "ram_available_mb": 38377.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.211}, "power_stats": {"power_gpu_soc_mean_watts": 21.023, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 70.211, "power_watts_avg": 21.023, "energy_joules_est": 234.8, "duration_seconds": 11.169, "sample_count": 95}, "timestamp": "2026-01-25T20:16:11.857038"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9420.855, "latencies_ms": [9420.855], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "A woman is standing in a kitchen, leaning over a black stove, and appears to be cooking or preparing food. The kitchen has a vintage or rustic feel, with a wooden floor, a white refrigerator, and a black hood above the stove.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24463.1, "ram_available_mb": 38377.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24468.6, "ram_available_mb": 38372.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.95}, "power_stats": {"power_gpu_soc_mean_watts": 21.721, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 71.95, "power_watts_avg": 21.721, "energy_joules_est": 204.65, "duration_seconds": 9.422, "sample_count": 80}, "timestamp": "2026-01-25T20:16:23.326091"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11137.685, "latencies_ms": [11137.685], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a kitchen with a vintage feel, featuring a black stove top with a large black hood above it. The lighting is dim, with a ceiling fan visible in the background, and the walls are painted white. There are various kitchen utensils and containers on the stove, including a blue pot and a red and white striped container. The", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24468.6, "ram_available_mb": 38372.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24457.4, "ram_available_mb": 38383.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.389}, "power_stats": {"power_gpu_soc_mean_watts": 20.975, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 70.389, "power_watts_avg": 20.975, "energy_joules_est": 233.63, "duration_seconds": 11.138, "sample_count": 95}, "timestamp": "2026-01-25T20:16:36.487669"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12324.161, "latencies_ms": [12324.161], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene moment in a zoo enclosure, where two majestic giraffes stand tall against the backdrop of a large, beige building. The giraffes, with their distinctive brown and white spotted coats, are positioned on a rocky terrain, their long necks reaching towards the sky. The building behind them is a large,", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24457.4, "ram_available_mb": 38383.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24464.2, "ram_available_mb": 38376.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.577}, "power_stats": {"power_gpu_soc_mean_watts": 22.87, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 9.212, "gpu_utilization_percent_mean": 73.577, "power_watts_avg": 22.87, "energy_joules_est": 281.87, "duration_seconds": 12.325, "sample_count": 104}, "timestamp": "2026-01-25T20:16:50.849459"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8263.367, "latencies_ms": [8263.367], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "giraffe: 2, rock: numerous, building: 1, fence: 1, trees: numerous, grass: visible, ground: rocky, wall: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24464.2, "ram_available_mb": 38376.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24465.1, "ram_available_mb": 38375.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.871}, "power_stats": {"power_gpu_soc_mean_watts": 24.591, "power_cpu_cv_mean_watts": 1.379, "power_sys_5v0_mean_watts": 9.149, "gpu_utilization_percent_mean": 79.871, "power_watts_avg": 24.591, "energy_joules_est": 203.22, "duration_seconds": 8.264, "sample_count": 70}, "timestamp": "2026-01-25T20:17:01.137396"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12416.557, "latencies_ms": [12416.557], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "Two giraffes are standing in the foreground of the image, with one slightly in front of the other, both facing the same direction towards the right side of the image. They are positioned near a rocky ground and a building in the background, which appears to be a part of their enclosure. The trees in the background are at a distance, suggesting they are beyond the immediate", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24465.1, "ram_available_mb": 38375.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24464.5, "ram_available_mb": 38376.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.505}, "power_stats": {"power_gpu_soc_mean_watts": 22.895, "power_cpu_cv_mean_watts": 1.789, "power_sys_5v0_mean_watts": 9.212, "gpu_utilization_percent_mean": 74.505, "power_watts_avg": 22.895, "energy_joules_est": 284.29, "duration_seconds": 12.417, "sample_count": 107}, "timestamp": "2026-01-25T20:17:15.584766"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8372.084, "latencies_ms": [8372.084], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "Two giraffes are standing in a zoo enclosure with a large building in the background. The enclosure is surrounded by a fence and has a rocky ground with some greenery.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24464.5, "ram_available_mb": 38376.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24554.4, "ram_available_mb": 38286.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.915}, "power_stats": {"power_gpu_soc_mean_watts": 24.514, "power_cpu_cv_mean_watts": 1.416, "power_sys_5v0_mean_watts": 9.183, "gpu_utilization_percent_mean": 78.915, "power_watts_avg": 24.514, "energy_joules_est": 205.25, "duration_seconds": 8.373, "sample_count": 71}, "timestamp": "2026-01-25T20:17:26.020112"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11145.056, "latencies_ms": [11145.056], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image features two giraffes with a pattern of dark brown patches separated by lighter lines, standing in front of a beige building with a sloped roof. The lighting is soft and diffused, suggesting an overcast day, and the ground is covered with a mix of rocks and patches of grass.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24499.2, "ram_available_mb": 38341.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24579.0, "ram_available_mb": 38261.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.26}, "power_stats": {"power_gpu_soc_mean_watts": 23.229, "power_cpu_cv_mean_watts": 1.702, "power_sys_5v0_mean_watts": 9.215, "gpu_utilization_percent_mean": 75.26, "power_watts_avg": 23.229, "energy_joules_est": 258.91, "duration_seconds": 11.146, "sample_count": 96}, "timestamp": "2026-01-25T20:17:39.193078"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11089.364, "latencies_ms": [11089.364], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a sunlit field, a young baseball player is captured in the midst of a powerful swing. Dressed in a vibrant green and yellow uniform, the player's helmet gleams under the sunlight, reflecting the intensity of the moment. The player's stance is one of focus and determination, with the bat held high and ready to connect", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 24473.8, "ram_available_mb": 38367.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24471.4, "ram_available_mb": 38369.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.617}, "power_stats": {"power_gpu_soc_mean_watts": 21.09, "power_cpu_cv_mean_watts": 1.934, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 69.617, "power_watts_avg": 21.09, "energy_joules_est": 233.89, "duration_seconds": 11.09, "sample_count": 94}, "timestamp": "2026-01-25T20:17:52.321714"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8505.116, "latencies_ms": [8505.116], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Fence: 1\n- Baseball bat: 1\n- Baseball: 1\n- Pants: 1\n- Socks: 1\n- Helmet: 1\n- Glove: 1\n- Car: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24471.4, "ram_available_mb": 38369.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24486.5, "ram_available_mb": 38354.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.625}, "power_stats": {"power_gpu_soc_mean_watts": 22.082, "power_cpu_cv_mean_watts": 1.729, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 71.625, "power_watts_avg": 22.082, "energy_joules_est": 187.82, "duration_seconds": 8.506, "sample_count": 72}, "timestamp": "2026-01-25T20:18:02.841880"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11102.892, "latencies_ms": [11102.892], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The main object, a young baseball player, is positioned in the foreground on the left side of the image, swinging a bat at a baseball that is in the air to the right of the player. The background features a chain-link fence and some greenery, with a car partially visible behind the fence. The player is closer to the camera than the fence and the", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24486.5, "ram_available_mb": 38354.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24487.3, "ram_available_mb": 38353.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.347}, "power_stats": {"power_gpu_soc_mean_watts": 20.981, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 69.347, "power_watts_avg": 20.981, "energy_joules_est": 232.96, "duration_seconds": 11.103, "sample_count": 95}, "timestamp": "2026-01-25T20:18:15.972365"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8069.281, "latencies_ms": [8069.281], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A young baseball player is in the middle of a swing, wearing a helmet and holding a blue bat. The scene is set on a baseball field with a chain-link fence in the background and a car parked behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24487.3, "ram_available_mb": 38353.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24476.4, "ram_available_mb": 38364.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.544}, "power_stats": {"power_gpu_soc_mean_watts": 22.359, "power_cpu_cv_mean_watts": 1.702, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 72.544, "power_watts_avg": 22.359, "energy_joules_est": 180.44, "duration_seconds": 8.07, "sample_count": 68}, "timestamp": "2026-01-25T20:18:26.068243"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7548.574, "latencies_ms": [7548.574], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a child in a green and yellow baseball uniform swinging a bat at a baseball. The child is wearing a helmet and is standing on a dirt field with a chain link fence in the background.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24476.4, "ram_available_mb": 38364.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24467.8, "ram_available_mb": 38373.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.141}, "power_stats": {"power_gpu_soc_mean_watts": 22.303, "power_cpu_cv_mean_watts": 1.695, "power_sys_5v0_mean_watts": 9.075, "gpu_utilization_percent_mean": 72.141, "power_watts_avg": 22.303, "energy_joules_est": 168.37, "duration_seconds": 7.549, "sample_count": 64}, "timestamp": "2026-01-25T20:18:35.661829"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11125.337, "latencies_ms": [11125.337], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a cobblestone street with a variety of vehicles parked along the side. There are several motorcycles, including a black motorcycle and a vintage car, as well as a bus parked on the street. A truck is also visible in the scene. \n\nThere are multiple people walking around the area, with some closer to the vehicles and others", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24467.8, "ram_available_mb": 38373.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 7.0, "ram_used_mb": 24444.3, "ram_available_mb": 38396.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.789}, "power_stats": {"power_gpu_soc_mean_watts": 20.913, "power_cpu_cv_mean_watts": 2.027, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.789, "power_watts_avg": 20.913, "energy_joules_est": 232.68, "duration_seconds": 11.126, "sample_count": 95}, "timestamp": "2026-01-25T20:18:48.837508"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8636.696, "latencies_ms": [8636.696], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Car: 2\n- Motorcycle: 3\n- Bus: 1\n- Truck: 1\n- Bike: 1\n- Person: 5\n- Triangle barrier: 1\n- Street light: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24444.3, "ram_available_mb": 38396.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24437.5, "ram_available_mb": 38403.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.274}, "power_stats": {"power_gpu_soc_mean_watts": 21.986, "power_cpu_cv_mean_watts": 1.727, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 72.274, "power_watts_avg": 21.986, "energy_joules_est": 189.9, "duration_seconds": 8.637, "sample_count": 73}, "timestamp": "2026-01-25T20:18:59.525585"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11135.825, "latencies_ms": [11135.825], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a black vintage car on the left side of the image, and a black vintage car on the right side of the image. In the background, there are several motorcycles parked in a row, with a red bus on the right side of the image and a red tripod on the left side of the image. There are also several", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24437.5, "ram_available_mb": 38403.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24439.7, "ram_available_mb": 38401.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.167}, "power_stats": {"power_gpu_soc_mean_watts": 20.946, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 69.167, "power_watts_avg": 20.946, "energy_joules_est": 233.26, "duration_seconds": 11.136, "sample_count": 96}, "timestamp": "2026-01-25T20:19:12.675352"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8371.331, "latencies_ms": [8371.331], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image depicts a cobblestone street lined with vintage cars and motorcycles, with a red double-decker bus and a building under construction in the background. People can be seen walking around and admiring the vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.7, "ram_available_mb": 38401.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24437.8, "ram_available_mb": 38403.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.127}, "power_stats": {"power_gpu_soc_mean_watts": 22.071, "power_cpu_cv_mean_watts": 1.714, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 71.127, "power_watts_avg": 22.071, "energy_joules_est": 184.78, "duration_seconds": 8.372, "sample_count": 71}, "timestamp": "2026-01-25T20:19:23.102864"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8916.288, "latencies_ms": [8916.288], "images_per_second": 0.112, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image features a cobblestone street with a mix of vintage and modern vehicles, including a classic black car in the foreground and a red double-decker bus in the background. The weather appears to be clear and sunny, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.8, "ram_available_mb": 38403.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24438.9, "ram_available_mb": 38401.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.307}, "power_stats": {"power_gpu_soc_mean_watts": 21.6, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 71.307, "power_watts_avg": 21.6, "energy_joules_est": 192.61, "duration_seconds": 8.917, "sample_count": 75}, "timestamp": "2026-01-25T20:19:34.039058"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11138.256, "latencies_ms": [11138.256], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a parking lot during sunset. The sun, a radiant orb of light, is positioned in the top left corner, casting a warm, golden glow that bathes the entire scene. The parking lot is filled with cars, their details obscured by the soft focus of the image. The cars are arranged in a grid-like", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24438.9, "ram_available_mb": 38401.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24449.7, "ram_available_mb": 38391.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.33}, "power_stats": {"power_gpu_soc_mean_watts": 20.914, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.33, "power_watts_avg": 20.914, "energy_joules_est": 232.96, "duration_seconds": 11.139, "sample_count": 94}, "timestamp": "2026-01-25T20:19:47.207744"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11284.652, "latencies_ms": [11284.652], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 79, "n_tiles": 16, "output_text": "object: parking meter, count: 2\nobject: sun, count: 1\nobject: sky, count: 1\nobject: buildings, count: 1\nobject: shadows, count: 1\nobject: light, count: 1\nobject: parking lot, count: 1\nobject: parking meter, count: 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24449.7, "ram_available_mb": 38391.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24437.5, "ram_available_mb": 38403.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.26}, "power_stats": {"power_gpu_soc_mean_watts": 21.085, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.928, "gpu_utilization_percent_mean": 70.26, "power_watts_avg": 21.085, "energy_joules_est": 237.95, "duration_seconds": 11.285, "sample_count": 96}, "timestamp": "2026-01-25T20:20:00.548851"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11102.702, "latencies_ms": [11102.702], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The parking meters are positioned in the foreground of the image, appearing closer to the viewer, while the background is dominated by the warm glow of the setting or rising sun, which is further away, creating a sense of depth. The parking meters are on the left side of the image, and there is a clear space between them and the background, emphasizing their prom", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24437.5, "ram_available_mb": 38403.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24442.8, "ram_available_mb": 38398.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.851}, "power_stats": {"power_gpu_soc_mean_watts": 21.0, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 9.016, "gpu_utilization_percent_mean": 69.851, "power_watts_avg": 21.0, "energy_joules_est": 233.17, "duration_seconds": 11.103, "sample_count": 94}, "timestamp": "2026-01-25T20:20:13.673587"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9607.519, "latencies_ms": [9607.519], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image captures a serene sunset with the sun low on the horizon, casting a warm glow over a blurred cityscape. In the foreground, there are two parking meters, one in the center and one on the left, both with a clear view of the sunset.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.8, "ram_available_mb": 38398.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24448.9, "ram_available_mb": 38392.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.349}, "power_stats": {"power_gpu_soc_mean_watts": 21.582, "power_cpu_cv_mean_watts": 1.809, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 71.349, "power_watts_avg": 21.582, "energy_joules_est": 207.36, "duration_seconds": 9.608, "sample_count": 83}, "timestamp": "2026-01-25T20:20:25.311090"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9514.964, "latencies_ms": [9514.964], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image features a warm, golden hue with the sun low on the horizon, casting a soft light that creates a serene atmosphere. The focus is on a pair of round, metallic parking meters with a blurred background of a cityscape, suggesting a peaceful end to the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24448.9, "ram_available_mb": 38392.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24450.9, "ram_available_mb": 38390.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.476}, "power_stats": {"power_gpu_soc_mean_watts": 21.416, "power_cpu_cv_mean_watts": 1.836, "power_sys_5v0_mean_watts": 9.018, "gpu_utilization_percent_mean": 70.476, "power_watts_avg": 21.416, "energy_joules_est": 203.79, "duration_seconds": 9.516, "sample_count": 82}, "timestamp": "2026-01-25T20:20:36.844881"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11126.701, "latencies_ms": [11126.701], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a large suitcase with stickers on it, placed on a sidewalk. Two people are standing next to the suitcase, posing for a picture. The man and woman are smiling and appear to be enjoying the moment. The suitcase is quite large, taking up a significant portion of the sidewalk.\n\nThere are also two handbags", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24450.9, "ram_available_mb": 38390.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24469.7, "ram_available_mb": 38371.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.26}, "power_stats": {"power_gpu_soc_mean_watts": 20.946, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 69.26, "power_watts_avg": 20.946, "energy_joules_est": 233.07, "duration_seconds": 11.127, "sample_count": 96}, "timestamp": "2026-01-25T20:20:50.024849"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10175.375, "latencies_ms": [10175.375], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 69, "n_tiles": 16, "output_text": "- Suitcase: 1\n- Sticker: 4\n- Woman: 1\n- Man: 1\n- Sculpture: 1\n- Sculpture base: 1\n- Sculpture plaque: 1\n- Sculpture plaque text: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24469.7, "ram_available_mb": 38371.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24465.9, "ram_available_mb": 38375.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.698}, "power_stats": {"power_gpu_soc_mean_watts": 21.476, "power_cpu_cv_mean_watts": 1.825, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 71.698, "power_watts_avg": 21.476, "energy_joules_est": 218.54, "duration_seconds": 10.176, "sample_count": 86}, "timestamp": "2026-01-25T20:21:02.227995"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11095.799, "latencies_ms": [11095.799], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large, brown suitcase with various stickers on it, including \"TH\" and \"IND\" on the left side, and \"DHAKA\" and \"TYW\" on the right side. The suitcase is placed on a platform with a plaque that reads \"GOD BLESS AMERICA\" by J. Sew", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24465.9, "ram_available_mb": 38375.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24538.2, "ram_available_mb": 38302.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.766}, "power_stats": {"power_gpu_soc_mean_watts": 20.987, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.766, "power_watts_avg": 20.987, "energy_joules_est": 232.88, "duration_seconds": 11.097, "sample_count": 94}, "timestamp": "2026-01-25T20:21:15.341888"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9376.411, "latencies_ms": [9376.411], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "A large, brown suitcase with various stickers on it is placed on a platform in front of a building with a sign that reads \"Fidelity Investments.\" Two people, a man and a woman, are standing next to the suitcase, smiling and posing for a photo.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24459.6, "ram_available_mb": 38381.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24551.4, "ram_available_mb": 38289.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.911}, "power_stats": {"power_gpu_soc_mean_watts": 21.823, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 71.911, "power_watts_avg": 21.823, "energy_joules_est": 204.64, "duration_seconds": 9.377, "sample_count": 79}, "timestamp": "2026-01-25T20:21:26.765848"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6749.052, "latencies_ms": [6749.052], "images_per_second": 0.148, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A large, brown suitcase with various stickers on it is placed on a platform. The suitcase has a lock and a handle, and the stickers are in different shapes and sizes.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24441.0, "ram_available_mb": 38399.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24458.6, "ram_available_mb": 38382.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.123}, "power_stats": {"power_gpu_soc_mean_watts": 22.896, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 9.072, "gpu_utilization_percent_mean": 73.123, "power_watts_avg": 22.896, "energy_joules_est": 154.54, "duration_seconds": 6.75, "sample_count": 57}, "timestamp": "2026-01-25T20:21:35.574135"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11092.516, "latencies_ms": [11092.516], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a close-up view of a dish consisting of a piece of fish, likely a white fish such as cod or halibut, accompanied by a generous serving of mushrooms and broccoli. The fish is garnished with parsley, adding a touch of green to the dish. The mushrooms are sliced and appear to be cook", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24458.6, "ram_available_mb": 38382.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24436.6, "ram_available_mb": 38404.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.305}, "power_stats": {"power_gpu_soc_mean_watts": 20.989, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 69.305, "power_watts_avg": 20.989, "energy_joules_est": 232.84, "duration_seconds": 11.093, "sample_count": 95}, "timestamp": "2026-01-25T20:21:48.716037"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5617.821, "latencies_ms": [5617.821], "images_per_second": 0.178, "prompt_tokens": 39, "response_tokens_est": 28, "n_tiles": 16, "output_text": "fish: 1, mushrooms: 12, broccoli: 10, parsley: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.6, "ram_available_mb": 38404.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24430.0, "ram_available_mb": 38410.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.234}, "power_stats": {"power_gpu_soc_mean_watts": 23.985, "power_cpu_cv_mean_watts": 1.354, "power_sys_5v0_mean_watts": 9.056, "gpu_utilization_percent_mean": 75.234, "power_watts_avg": 23.985, "energy_joules_est": 134.76, "duration_seconds": 5.618, "sample_count": 47}, "timestamp": "2026-01-25T20:21:56.397016"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10218.005, "latencies_ms": [10218.005], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "In the foreground, there is a plate of food with a piece of fish on the left side, which is nearer to the viewer than the broccoli on the right side. The fish is positioned in the center of the plate, with the mushrooms surrounding it on the left and the broccoli on the right.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24430.0, "ram_available_mb": 38410.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24444.3, "ram_available_mb": 38396.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.805}, "power_stats": {"power_gpu_soc_mean_watts": 21.224, "power_cpu_cv_mean_watts": 1.864, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 69.805, "power_watts_avg": 21.224, "energy_joules_est": 216.88, "duration_seconds": 10.219, "sample_count": 87}, "timestamp": "2026-01-25T20:22:08.628992"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9072.644, "latencies_ms": [9072.644], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a close-up of a dish consisting of a piece of fish, sliced mushrooms, and broccoli, garnished with parsley. The food is cooked and presented on a plate, suggesting a meal ready to be served.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24444.3, "ram_available_mb": 38396.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24436.6, "ram_available_mb": 38404.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.117}, "power_stats": {"power_gpu_soc_mean_watts": 21.719, "power_cpu_cv_mean_watts": 1.773, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 71.117, "power_watts_avg": 21.719, "energy_joules_est": 197.06, "duration_seconds": 9.073, "sample_count": 77}, "timestamp": "2026-01-25T20:22:19.713939"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8866.299, "latencies_ms": [8866.299], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image features a plate of food with a focus on a piece of fish topped with herbs. The lighting in the image highlights the glistening surface of the fish and the fresh green of the parsley, creating a vibrant and appetizing appearance.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24436.6, "ram_available_mb": 38404.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24435.9, "ram_available_mb": 38405.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.32}, "power_stats": {"power_gpu_soc_mean_watts": 21.746, "power_cpu_cv_mean_watts": 1.793, "power_sys_5v0_mean_watts": 9.031, "gpu_utilization_percent_mean": 71.32, "power_watts_avg": 21.746, "energy_joules_est": 192.82, "duration_seconds": 8.867, "sample_count": 75}, "timestamp": "2026-01-25T20:22:30.595683"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11116.078, "latencies_ms": [11116.078], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a display of various vegetables, including a large pile of carrots in a basket, which is the main focus of the scene. The carrots are arranged in a way that they are the most prominent vegetable in the image. In addition to the carrots, there are several other vegetables present, such as broccoli and lettuce, which are", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24435.9, "ram_available_mb": 38405.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24440.9, "ram_available_mb": 38400.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.638}, "power_stats": {"power_gpu_soc_mean_watts": 20.925, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 69.638, "power_watts_avg": 20.925, "energy_joules_est": 232.62, "duration_seconds": 11.117, "sample_count": 94}, "timestamp": "2026-01-25T20:22:43.754987"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8620.237, "latencies_ms": [8620.237], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "carrot: 20, broccoli: 15, lettuce: 10, cabbage: 5, artichoke: 3, zucchini: 2, onion: 1, garlic: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24440.9, "ram_available_mb": 38400.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 24446.4, "ram_available_mb": 38394.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.301}, "power_stats": {"power_gpu_soc_mean_watts": 21.942, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 72.301, "power_watts_avg": 21.942, "energy_joules_est": 189.16, "duration_seconds": 8.621, "sample_count": 73}, "timestamp": "2026-01-25T20:22:54.429740"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11129.175, "latencies_ms": [11129.175], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a basket filled with orange carrots, positioned centrally in the image. Behind the carrots, there are various leafy greens, including broccoli and lettuce, displayed on shelves that recede into the background. To the right, there is a wooden crate with the word 'FOSIL' on", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24446.4, "ram_available_mb": 38394.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24436.7, "ram_available_mb": 38404.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.234}, "power_stats": {"power_gpu_soc_mean_watts": 20.966, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 70.234, "power_watts_avg": 20.966, "energy_joules_est": 233.35, "duration_seconds": 11.13, "sample_count": 94}, "timestamp": "2026-01-25T20:23:07.599125"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8642.66, "latencies_ms": [8642.66], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image showcases a variety of fresh vegetables, including carrots, broccoli, and cabbage, displayed in baskets on a table. The vegetables are arranged in an appealing manner, making them look fresh and ready to be sold.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24436.7, "ram_available_mb": 38404.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.149}, "power_stats": {"power_gpu_soc_mean_watts": 21.902, "power_cpu_cv_mean_watts": 1.742, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 71.149, "power_watts_avg": 21.902, "energy_joules_est": 189.3, "duration_seconds": 8.643, "sample_count": 74}, "timestamp": "2026-01-25T20:23:18.280906"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9856.752, "latencies_ms": [9856.752], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image showcases a variety of vegetables, including vibrant green broccoli and orange carrots, displayed in a market setting. The vegetables are arranged in baskets and bins, with some covered in a blue cloth, and the lighting appears to be natural, possibly from an outdoor market.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24440.1, "ram_available_mb": 38400.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24448.9, "ram_available_mb": 38392.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.488}, "power_stats": {"power_gpu_soc_mean_watts": 21.344, "power_cpu_cv_mean_watts": 1.849, "power_sys_5v0_mean_watts": 9.011, "gpu_utilization_percent_mean": 70.488, "power_watts_avg": 21.344, "energy_joules_est": 210.4, "duration_seconds": 9.857, "sample_count": 84}, "timestamp": "2026-01-25T20:23:30.149533"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11114.021, "latencies_ms": [11114.021], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a donut shop with a conveyor belt system for making and displaying donuts. There are several donuts on the conveyor belt, with some being freshly made and others already glazed. The donuts are placed in various positions along the belt, showcasing the variety of flavors and toppings available.\n\nIn the background, there are people", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24448.9, "ram_available_mb": 38392.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24449.6, "ram_available_mb": 38391.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.574}, "power_stats": {"power_gpu_soc_mean_watts": 20.951, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 69.574, "power_watts_avg": 20.951, "energy_joules_est": 232.86, "duration_seconds": 11.115, "sample_count": 94}, "timestamp": "2026-01-25T20:23:43.290499"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11296.177, "latencies_ms": [11296.177], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "- Donuts: numerous, exact count not visible\n- People: several, exact count not visible\n- Machinery: multiple, exact count not visible\n- Signs: one, 'DANGER HOT' visible\n- Containers: multiple, exact count not visible\n- Faucets: one, visible on the right side\n- Pipes: multiple, exact", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24449.6, "ram_available_mb": 38391.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24448.3, "ram_available_mb": 38392.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.667}, "power_stats": {"power_gpu_soc_mean_watts": 21.125, "power_cpu_cv_mean_watts": 1.881, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 70.667, "power_watts_avg": 21.125, "energy_joules_est": 238.65, "duration_seconds": 11.297, "sample_count": 96}, "timestamp": "2026-01-25T20:23:56.641860"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10892.581, "latencies_ms": [10892.581], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a conveyor belt with donuts moving along it, indicating a production line. In the background, there are people standing at various workstations, suggesting a busy kitchen or bakery environment. The donuts in the foreground appear to be near the 'HOT' sign, indicating they are freshly cooked and hot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24448.3, "ram_available_mb": 38392.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24438.6, "ram_available_mb": 38402.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.904}, "power_stats": {"power_gpu_soc_mean_watts": 20.975, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 69.904, "power_watts_avg": 20.975, "energy_joules_est": 228.49, "duration_seconds": 10.893, "sample_count": 94}, "timestamp": "2026-01-25T20:24:09.550965"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8402.764, "latencies_ms": [8402.764], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a bustling donut shop with a conveyor belt system in the foreground where donuts are being made. People are seen working in the background, and there are various machines and equipment involved in the donut-making process.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24438.6, "ram_available_mb": 38402.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24440.8, "ram_available_mb": 38400.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.718}, "power_stats": {"power_gpu_soc_mean_watts": 22.144, "power_cpu_cv_mean_watts": 1.714, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 72.718, "power_watts_avg": 22.144, "energy_joules_est": 186.08, "duration_seconds": 8.403, "sample_count": 71}, "timestamp": "2026-01-25T20:24:19.998278"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6555.373, "latencies_ms": [6555.373], "images_per_second": 0.153, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image shows a bustling donut shop with a conveyor belt system. The machines and surfaces are predominantly metallic, reflecting the bright overhead lights.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24440.8, "ram_available_mb": 38400.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24437.8, "ram_available_mb": 38403.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.4}, "power_stats": {"power_gpu_soc_mean_watts": 23.023, "power_cpu_cv_mean_watts": 1.558, "power_sys_5v0_mean_watts": 9.078, "gpu_utilization_percent_mean": 73.4, "power_watts_avg": 23.023, "energy_joules_est": 150.94, "duration_seconds": 6.556, "sample_count": 55}, "timestamp": "2026-01-25T20:24:28.604933"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11152.329, "latencies_ms": [11152.329], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is standing in a wooded area, holding a frisbee in their hands. They are wearing a green jacket and appear to be preparing to throw the frisbee. The person is surrounded by trees, creating a natural and serene environment. The frisbee is positioned in the person's hands, ready to be launched", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24437.8, "ram_available_mb": 38403.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24440.0, "ram_available_mb": 38400.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.579}, "power_stats": {"power_gpu_soc_mean_watts": 20.931, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 69.579, "power_watts_avg": 20.931, "energy_joules_est": 233.45, "duration_seconds": 11.153, "sample_count": 95}, "timestamp": "2026-01-25T20:24:41.831748"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8187.944, "latencies_ms": [8187.944], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "tree: numerous\nman: 1\nfrisbee: 1\nleaves: scattered\nground: uneven\ndirt: visible\nbranches: many\ngreen jacket: 1\nwhite socks: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24440.0, "ram_available_mb": 38400.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24437.2, "ram_available_mb": 38403.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.114}, "power_stats": {"power_gpu_soc_mean_watts": 22.249, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 73.114, "power_watts_avg": 22.249, "energy_joules_est": 182.19, "duration_seconds": 8.189, "sample_count": 70}, "timestamp": "2026-01-25T20:24:52.045640"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11134.431, "latencies_ms": [11134.431], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person in a green jacket and white shoes is bending over to pick up an orange frisbee that is near the ground. The person is standing on a dirt path that runs through a wooded area with tall trees in the background. The trees are mostly bare, suggesting that the photo was taken in a season when the trees have shed their leaves", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24437.2, "ram_available_mb": 38403.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24442.5, "ram_available_mb": 38398.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.6}, "power_stats": {"power_gpu_soc_mean_watts": 20.927, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.6, "power_watts_avg": 20.927, "energy_joules_est": 233.02, "duration_seconds": 11.135, "sample_count": 95}, "timestamp": "2026-01-25T20:25:05.196811"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8286.845, "latencies_ms": [8286.845], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A person in a green jacket and white shoes is playing frisbee in a wooded area with a fallen tree trunk in the foreground. The ground is covered with fallen leaves and twigs, indicating it might be autumn.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24442.5, "ram_available_mb": 38398.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.571}, "power_stats": {"power_gpu_soc_mean_watts": 22.262, "power_cpu_cv_mean_watts": 1.716, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 73.571, "power_watts_avg": 22.262, "energy_joules_est": 184.5, "duration_seconds": 8.287, "sample_count": 70}, "timestamp": "2026-01-25T20:25:15.494992"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9008.024, "latencies_ms": [9008.024], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image depicts a person in a green jacket and white shoes playing with an orange frisbee in a wooded area. The ground is covered with dry leaves and the trees have a mix of green and bare branches, suggesting it might be autumn or early spring.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24440.2, "ram_available_mb": 38400.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24443.2, "ram_available_mb": 38397.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.338}, "power_stats": {"power_gpu_soc_mean_watts": 21.611, "power_cpu_cv_mean_watts": 1.805, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 70.338, "power_watts_avg": 21.611, "energy_joules_est": 194.69, "duration_seconds": 9.009, "sample_count": 77}, "timestamp": "2026-01-25T20:25:26.517336"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11106.989, "latencies_ms": [11106.989], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a quaint, small bathroom bathed in soft light. The walls, painted a soothing light beige, contrast with the beige tiled floor. A white toilet stands in the corner, its pristine surface reflecting the room's tranquility. Adjacent to it, a white sink is nestled into a white cabinet, its", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24443.2, "ram_available_mb": 38397.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24457.8, "ram_available_mb": 38383.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.546}, "power_stats": {"power_gpu_soc_mean_watts": 20.866, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 69.546, "power_watts_avg": 20.866, "energy_joules_est": 231.77, "duration_seconds": 11.108, "sample_count": 97}, "timestamp": "2026-01-25T20:25:39.672565"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11172.008, "latencies_ms": [11172.008], "images_per_second": 0.09, "prompt_tokens": 39, "response_tokens_est": 78, "n_tiles": 16, "output_text": "1. Toilet: 1\n2. Sink: 1\n3. Shower curtain: 1\n4. Shower head: 1\n5. Hair dryer: 1\n6. Toilet paper: 1\n7. Toilet brush: 1\n8. Toiletries: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24457.8, "ram_available_mb": 38383.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24473.9, "ram_available_mb": 38367.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.663}, "power_stats": {"power_gpu_soc_mean_watts": 21.151, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 70.663, "power_watts_avg": 21.151, "energy_joules_est": 236.31, "duration_seconds": 11.173, "sample_count": 95}, "timestamp": "2026-01-25T20:25:52.905751"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11262.151, "latencies_ms": [11262.151], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a white sink with a silver faucet on the left side of the image, and a white toilet in the background. On the countertop, there are two cans of AJ's, one in the foreground and one slightly behind it. The shower area is in the background, with striped curtains hanging on the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24473.9, "ram_available_mb": 38367.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24439.8, "ram_available_mb": 38401.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.105}, "power_stats": {"power_gpu_soc_mean_watts": 20.828, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.921, "gpu_utilization_percent_mean": 70.105, "power_watts_avg": 20.828, "energy_joules_est": 234.58, "duration_seconds": 11.263, "sample_count": 95}, "timestamp": "2026-01-25T20:26:06.204856"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9080.258, "latencies_ms": [9080.258], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image depicts a small, well-lit bathroom with a sink, toilet, and bathtub. There are two cans of AJ-1 on the countertop, and a striped shower curtain is hanging in the bathtub.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 24439.8, "ram_available_mb": 38401.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24455.7, "ram_available_mb": 38385.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.13}, "power_stats": {"power_gpu_soc_mean_watts": 21.922, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 72.13, "power_watts_avg": 21.922, "energy_joules_est": 199.07, "duration_seconds": 9.081, "sample_count": 77}, "timestamp": "2026-01-25T20:26:17.306276"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6470.068, "latencies_ms": [6470.068], "images_per_second": 0.155, "prompt_tokens": 36, "response_tokens_est": 37, "n_tiles": 16, "output_text": "The bathroom has a beige color scheme with a tiled floor and walls. There is a white toilet and a bathtub with striped curtains.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24455.7, "ram_available_mb": 38385.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24455.2, "ram_available_mb": 38385.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.204}, "power_stats": {"power_gpu_soc_mean_watts": 23.254, "power_cpu_cv_mean_watts": 1.528, "power_sys_5v0_mean_watts": 9.101, "gpu_utilization_percent_mean": 74.204, "power_watts_avg": 23.254, "energy_joules_est": 150.47, "duration_seconds": 6.471, "sample_count": 54}, "timestamp": "2026-01-25T20:26:25.807488"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11131.213, "latencies_ms": [11131.213], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a kitchen with a large island in the center, which has a sink and a faucet. The island is made of wood and has a black countertop. There are two chairs placed around the island, providing seating for people. The kitchen also has a dining table with chairs on the opposite side.\n\nVarious items are placed on the island and around", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24455.2, "ram_available_mb": 38385.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24451.1, "ram_available_mb": 38389.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.147}, "power_stats": {"power_gpu_soc_mean_watts": 20.951, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 69.147, "power_watts_avg": 20.951, "energy_joules_est": 233.22, "duration_seconds": 11.132, "sample_count": 95}, "timestamp": "2026-01-25T20:26:38.969206"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9521.345, "latencies_ms": [9521.345], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "- Sink: 1\n- Countertop: 1\n- Faucet: 1\n- Coffee maker: 1\n- Coffee pot: 1\n- Bottles: 2\n- Chairs: 2\n- Window: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24451.1, "ram_available_mb": 38389.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 24439.9, "ram_available_mb": 38401.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.42}, "power_stats": {"power_gpu_soc_mean_watts": 21.519, "power_cpu_cv_mean_watts": 1.829, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 70.42, "power_watts_avg": 21.519, "energy_joules_est": 204.9, "duration_seconds": 9.522, "sample_count": 81}, "timestamp": "2026-01-25T20:26:50.503033"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9224.341, "latencies_ms": [9224.341], "images_per_second": 0.108, "prompt_tokens": 44, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The kitchen sink is located in the foreground on the left side of the image, with a wooden countertop extending towards the right. There are two chairs positioned near the countertop in the background, and a window with blinds is visible further back, allowing natural light to enter the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24439.9, "ram_available_mb": 38401.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24446.4, "ram_available_mb": 38394.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.679}, "power_stats": {"power_gpu_soc_mean_watts": 21.563, "power_cpu_cv_mean_watts": 1.828, "power_sys_5v0_mean_watts": 9.009, "gpu_utilization_percent_mean": 70.679, "power_watts_avg": 21.563, "energy_joules_est": 198.92, "duration_seconds": 9.225, "sample_count": 78}, "timestamp": "2026-01-25T20:27:01.738573"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7855.546, "latencies_ms": [7855.546], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image depicts a modern kitchen with a wooden island in the center, featuring a sink and a faucet. There are two chairs placed around the island, and a window with blinds is visible in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24446.4, "ram_available_mb": 38394.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24448.7, "ram_available_mb": 38392.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.119}, "power_stats": {"power_gpu_soc_mean_watts": 22.411, "power_cpu_cv_mean_watts": 1.661, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 73.119, "power_watts_avg": 22.411, "energy_joules_est": 176.07, "duration_seconds": 7.856, "sample_count": 67}, "timestamp": "2026-01-25T20:27:11.611101"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7779.324, "latencies_ms": [7779.324], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The kitchen features a wooden countertop with a black granite countertop, and the cabinets are made of wood with silver handles. The lighting in the room is natural, coming from the windows that show a view of trees outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24448.7, "ram_available_mb": 38392.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24453.8, "ram_available_mb": 38387.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.576}, "power_stats": {"power_gpu_soc_mean_watts": 22.217, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 9.029, "gpu_utilization_percent_mean": 71.576, "power_watts_avg": 22.217, "energy_joules_est": 172.85, "duration_seconds": 7.78, "sample_count": 66}, "timestamp": "2026-01-25T20:27:21.424632"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11100.441, "latencies_ms": [11100.441], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene indoor scene. Dominating the frame is a bed, neatly made with a black comforter adorned with a white daisy pattern. The bed is positioned against a wall, which is partially illuminated by a window on the right side, casting a soft light that enhances the tranquility of the scene. The window, however", "error": null, "sys_before": {"cpu_percent": 11.5, "ram_used_mb": 24453.8, "ram_available_mb": 38387.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24470.6, "ram_available_mb": 38370.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.691}, "power_stats": {"power_gpu_soc_mean_watts": 20.911, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 69.691, "power_watts_avg": 20.911, "energy_joules_est": 232.13, "duration_seconds": 11.101, "sample_count": 94}, "timestamp": "2026-01-25T20:27:34.554809"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8193.405, "latencies_ms": [8193.405], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "bed: 1\ncomforter: 1\nflowers: 10\nbedspread: 1\npillow: 2\nblanket: 1\nsheets: 1\nduvet: 1\n", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24470.6, "ram_available_mb": 38370.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24467.9, "ram_available_mb": 38373.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.27, "power_cpu_cv_mean_watts": 1.671, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 22.27, "energy_joules_est": 182.48, "duration_seconds": 8.194, "sample_count": 69}, "timestamp": "2026-01-25T20:27:44.783171"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10826.658, "latencies_ms": [10826.658], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The bed with a floral patterned blanket is positioned in the foreground of the image, occupying the central space. The pillow is placed on the right side of the bed, and the headboard is visible on the left side, creating a sense of depth. The room's walls and window are in the background, providing context to the setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24467.9, "ram_available_mb": 38373.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24461.0, "ram_available_mb": 38379.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.772}, "power_stats": {"power_gpu_soc_mean_watts": 21.078, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 69.772, "power_watts_avg": 21.078, "energy_joules_est": 228.22, "duration_seconds": 10.827, "sample_count": 92}, "timestamp": "2026-01-25T20:27:57.646405"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7293.605, "latencies_ms": [7293.605], "images_per_second": 0.137, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image depicts a dimly lit bedroom with a bed covered in a dark floral-patterned blanket. A person is partially visible under the blanket, suggesting they are in bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24461.0, "ram_available_mb": 38379.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24467.0, "ram_available_mb": 38373.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.613}, "power_stats": {"power_gpu_soc_mean_watts": 22.809, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 73.613, "power_watts_avg": 22.809, "energy_joules_est": 166.37, "duration_seconds": 7.294, "sample_count": 62}, "timestamp": "2026-01-25T20:28:06.982294"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9594.702, "latencies_ms": [9594.702], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows a dark-colored bedspread with a floral pattern, featuring what appears to be daisies in a monochromatic color scheme. The lighting in the room is dim, with a strong light source coming from the right side, casting shadows and creating a moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24467.0, "ram_available_mb": 38373.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24479.8, "ram_available_mb": 38361.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.358}, "power_stats": {"power_gpu_soc_mean_watts": 21.431, "power_cpu_cv_mean_watts": 1.834, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 70.358, "power_watts_avg": 21.431, "energy_joules_est": 205.64, "duration_seconds": 9.596, "sample_count": 81}, "timestamp": "2026-01-25T20:28:18.624694"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11098.53, "latencies_ms": [11098.53], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photo, a person is skillfully skateboarding on a concrete ledge. The skateboarder is wearing a white t-shirt and black shorts, and their feet are firmly planted on a white skateboard with black wheels. The skateboarder's shadow is cast on the ground, indicating the light source is above", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24479.8, "ram_available_mb": 38361.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24464.0, "ram_available_mb": 38376.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.702}, "power_stats": {"power_gpu_soc_mean_watts": 20.944, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 69.702, "power_watts_avg": 20.944, "energy_joules_est": 232.46, "duration_seconds": 11.099, "sample_count": 94}, "timestamp": "2026-01-25T20:28:31.747819"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7196.465, "latencies_ms": [7196.465], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "person: 1, skateboard: 1, building: 1, sky: 1, shadow: 1, concrete: 1, ground: 1, photo: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24464.0, "ram_available_mb": 38376.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24443.6, "ram_available_mb": 38397.3, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.197}, "power_stats": {"power_gpu_soc_mean_watts": 22.871, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 74.197, "power_watts_avg": 22.871, "energy_joules_est": 164.6, "duration_seconds": 7.197, "sample_count": 61}, "timestamp": "2026-01-25T20:28:40.977301"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9594.854, "latencies_ms": [9594.854], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, positioned on the left side of the image, with their feet on the skateboard. In the background, there is another person standing on the right side, near the edge of the image, and buildings can be seen further back, indicating an urban setting.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24443.6, "ram_available_mb": 38397.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24539.4, "ram_available_mb": 38301.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.79}, "power_stats": {"power_gpu_soc_mean_watts": 21.566, "power_cpu_cv_mean_watts": 1.824, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 71.79, "power_watts_avg": 21.566, "energy_joules_est": 206.94, "duration_seconds": 9.596, "sample_count": 81}, "timestamp": "2026-01-25T20:28:52.590784"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8527.52, "latencies_ms": [8527.52], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "A person is skateboarding on a concrete surface, possibly a skate park, with buildings in the background. The skateboarder is wearing a white t-shirt and dark shorts, and is in the process of performing a trick.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24444.9, "ram_available_mb": 38395.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24462.1, "ram_available_mb": 38378.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.836}, "power_stats": {"power_gpu_soc_mean_watts": 22.053, "power_cpu_cv_mean_watts": 1.734, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 71.836, "power_watts_avg": 22.053, "energy_joules_est": 188.07, "duration_seconds": 8.528, "sample_count": 73}, "timestamp": "2026-01-25T20:29:03.134675"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6982.215, "latencies_ms": [6982.215], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image is in black and white, featuring a person skateboarding on a concrete surface. The lighting is bright and appears to be natural sunlight, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24462.1, "ram_available_mb": 38378.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24535.7, "ram_available_mb": 38305.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.345}, "power_stats": {"power_gpu_soc_mean_watts": 22.344, "power_cpu_cv_mean_watts": 1.622, "power_sys_5v0_mean_watts": 9.031, "gpu_utilization_percent_mean": 73.345, "power_watts_avg": 22.344, "energy_joules_est": 156.03, "duration_seconds": 6.983, "sample_count": 58}, "timestamp": "2026-01-25T20:29:12.135139"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11118.203, "latencies_ms": [11118.203], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a man lying on the floor, surrounded by various items. He is resting his head on his hand, and the scene is filled with personal belongings. There is a laptop on the floor, along with a cell phone, a book, and a tennis racket. A backpack is also present in the scene, placed near the top right corner. \n\nIn addition", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24535.7, "ram_available_mb": 38305.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24566.1, "ram_available_mb": 38274.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.421}, "power_stats": {"power_gpu_soc_mean_watts": 20.983, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 69.421, "power_watts_avg": 20.983, "energy_joules_est": 233.31, "duration_seconds": 11.119, "sample_count": 95}, "timestamp": "2026-01-25T20:29:25.335888"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8860.499, "latencies_ms": [8860.499], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Laptop: 1\n- Cell phone: 1\n- Camera: 1\n- Book: 1\n- Tennis racket: 1\n- Keychain: 1\n- Water bottle: 1\n- Backpack: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24442.6, "ram_available_mb": 38398.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24458.1, "ram_available_mb": 38382.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.149}, "power_stats": {"power_gpu_soc_mean_watts": 21.103, "power_cpu_cv_mean_watts": 1.764, "power_sys_5v0_mean_watts": 8.903, "gpu_utilization_percent_mean": 73.149, "power_watts_avg": 21.103, "energy_joules_est": 187.0, "duration_seconds": 8.861, "sample_count": 74}, "timestamp": "2026-01-25T20:29:36.241309"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11149.473, "latencies_ms": [11149.473], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a tennis racket lying horizontally across the image, with its head resting on a book that has a brown cover. To the right of the racket, there is a black camera lens cap and a smartphone. In the background, there is a laptop with a green chili pepper image on its screen, a black backpack, and", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24458.1, "ram_available_mb": 38382.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24458.7, "ram_available_mb": 38382.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.394}, "power_stats": {"power_gpu_soc_mean_watts": 21.0, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 69.394, "power_watts_avg": 21.0, "energy_joules_est": 234.15, "duration_seconds": 11.15, "sample_count": 94}, "timestamp": "2026-01-25T20:29:49.404805"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11379.789, "latencies_ms": [11379.789], "images_per_second": 0.088, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a cluttered workspace on a carpeted floor with various items scattered around, including a laptop displaying an image of green chili peppers, a camera, a smartphone, a book titled \"The Men Who Built India\" by Philip Mason, a water bottle, a black backpack, a notebook, a pen, and a pair of earphones", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24458.7, "ram_available_mb": 38382.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24476.2, "ram_available_mb": 38364.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.688}, "power_stats": {"power_gpu_soc_mean_watts": 20.879, "power_cpu_cv_mean_watts": 1.89, "power_sys_5v0_mean_watts": 8.898, "gpu_utilization_percent_mean": 69.688, "power_watts_avg": 20.879, "energy_joules_est": 237.61, "duration_seconds": 11.38, "sample_count": 96}, "timestamp": "2026-01-25T20:30:02.807256"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11112.35, "latencies_ms": [11112.35], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a variety of objects on a carpeted floor, including a black laptop with a green chili pepper image on its screen, a red water bottle, a black camera, and a book titled \"The Men Who Built India\" by Philip Mason. The lighting in the image is soft and diffused, and the objects are arranged in a somewhat cluttered manner", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24476.2, "ram_available_mb": 38364.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24451.2, "ram_available_mb": 38389.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.628}, "power_stats": {"power_gpu_soc_mean_watts": 20.944, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 69.628, "power_watts_avg": 20.944, "energy_joules_est": 232.75, "duration_seconds": 11.113, "sample_count": 94}, "timestamp": "2026-01-25T20:30:15.975469"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11131.7, "latencies_ms": [11131.7], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a kitchen countertop, bathed in the soft glow of ambient light. Dominating the scene is a black stove top, its surface gleaming under the light. The stove top is adorned with four burners, each a vibrant shade of blue, red, and green. \n\nTo the left of the stove", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24451.2, "ram_available_mb": 38389.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24557.6, "ram_available_mb": 38283.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.379}, "power_stats": {"power_gpu_soc_mean_watts": 20.933, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 69.379, "power_watts_avg": 20.933, "energy_joules_est": 233.03, "duration_seconds": 11.132, "sample_count": 95}, "timestamp": "2026-01-25T20:30:29.141587"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11320.834, "latencies_ms": [11320.834], "images_per_second": 0.088, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "- Oven: 1\n\n- Burner: 1\n\n- Burner knobs: 6\n\n- Burner knobs (red): 3\n\n- Burner knobs (blue): 3\n\n- Burner knobs (green): 1\n\n- Burner knobs (orange): 1\n\n- Burner knobs", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24449.1, "ram_available_mb": 38391.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24564.1, "ram_available_mb": 38276.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.052}, "power_stats": {"power_gpu_soc_mean_watts": 21.049, "power_cpu_cv_mean_watts": 1.881, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 71.052, "power_watts_avg": 21.049, "energy_joules_est": 238.31, "duration_seconds": 11.321, "sample_count": 96}, "timestamp": "2026-01-25T20:30:42.477656"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11116.797, "latencies_ms": [11116.797], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a black stovetop with four burners, positioned on the left side of the image. Behind the stovetop, on the right side, there is a wall with a marble-patterned backsplash. Above the stovetop, a black vent is mounted on the wall, and to the right of the st", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24564.1, "ram_available_mb": 38276.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 24496.1, "ram_available_mb": 38344.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.021}, "power_stats": {"power_gpu_soc_mean_watts": 20.983, "power_cpu_cv_mean_watts": 2.058, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 70.021, "power_watts_avg": 20.983, "energy_joules_est": 233.28, "duration_seconds": 11.117, "sample_count": 94}, "timestamp": "2026-01-25T20:30:55.608787"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9391.435, "latencies_ms": [9391.435], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image shows a kitchen counter with a black stovetop and a black microwave oven mounted above it. The counter is made of a dark material, possibly granite or marble, and there is a set of colorful spice bottles on the left side of the counter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24496.1, "ram_available_mb": 38344.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24526.0, "ram_available_mb": 38314.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.5}, "power_stats": {"power_gpu_soc_mean_watts": 21.564, "power_cpu_cv_mean_watts": 1.802, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 71.5, "power_watts_avg": 21.564, "energy_joules_est": 202.53, "duration_seconds": 9.392, "sample_count": 80}, "timestamp": "2026-01-25T20:31:07.056663"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10446.989, "latencies_ms": [10446.989], "images_per_second": 0.096, "prompt_tokens": 36, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The kitchen features a black stovetop with a modern design, set against a backdrop of marble tiles in shades of gold, white, and gray. Above the stove, there is a black vent hood, and to the right, a towel hangs on a hook, adding a homey touch to the space.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24526.0, "ram_available_mb": 38314.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24586.4, "ram_available_mb": 38254.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.674}, "power_stats": {"power_gpu_soc_mean_watts": 21.117, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 69.674, "power_watts_avg": 21.117, "energy_joules_est": 220.63, "duration_seconds": 10.448, "sample_count": 89}, "timestamp": "2026-01-25T20:31:19.516352"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11126.493, "latencies_ms": [11126.493], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is sitting at a dining table, enjoying a meal consisting of donuts and coffee. He is holding a donut in his hand, and there are several other donuts on the table, including one with sprinkles. A cup of coffee is also present on the table, along with a tray containing more donuts.\n\nThe d", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24464.4, "ram_available_mb": 38376.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24462.2, "ram_available_mb": 38378.7, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.347}, "power_stats": {"power_gpu_soc_mean_watts": 20.869, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.347, "power_watts_avg": 20.869, "energy_joules_est": 232.21, "duration_seconds": 11.127, "sample_count": 95}, "timestamp": "2026-01-25T20:31:32.696971"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8211.109, "latencies_ms": [8211.109], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "tray: 1\ndonut: 3\ndonuts: 3\nperson: 1\ndonut hole: 1\nchocolate: 1\nsprinkles: 1\ncoffee: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24462.2, "ram_available_mb": 38378.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24486.7, "ram_available_mb": 38354.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.971}, "power_stats": {"power_gpu_soc_mean_watts": 22.32, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 72.971, "power_watts_avg": 22.32, "energy_joules_est": 183.29, "duration_seconds": 8.212, "sample_count": 69}, "timestamp": "2026-01-25T20:31:42.933903"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11158.252, "latencies_ms": [11158.252], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a tray with a chocolate donut topped with colorful sprinkles, a plain donut, and a chocolate-covered pastry. In the background, there is a person sitting at a table with a coffee cup in front of them. The person is holding a donut in their hand and appears to be in a restaurant", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24486.7, "ram_available_mb": 38354.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24482.7, "ram_available_mb": 38358.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.564}, "power_stats": {"power_gpu_soc_mean_watts": 20.988, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 69.564, "power_watts_avg": 20.988, "energy_joules_est": 234.2, "duration_seconds": 11.159, "sample_count": 94}, "timestamp": "2026-01-25T20:31:56.111229"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8297.287, "latencies_ms": [8297.287], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A person is sitting at a table in a cafe, holding a donut and looking at the camera. On the table, there are two donuts, one with chocolate frosting and sprinkles, and another plain donut.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24482.7, "ram_available_mb": 38358.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24451.4, "ram_available_mb": 38389.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.943}, "power_stats": {"power_gpu_soc_mean_watts": 22.256, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 72.943, "power_watts_avg": 22.256, "energy_joules_est": 184.68, "duration_seconds": 8.298, "sample_count": 70}, "timestamp": "2026-01-25T20:32:06.425348"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11152.736, "latencies_ms": [11152.736], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a tray with a variety of donuts, including a chocolate one with sprinkles and a plain glazed one. The tray is placed on a table with a cup of coffee and a glass of soda. The lighting in the image is warm and cozy, creating a comfortable atmosphere. The materials used in the image include wood for the table and t", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24451.4, "ram_available_mb": 38389.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24463.1, "ram_available_mb": 38377.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.76}, "power_stats": {"power_gpu_soc_mean_watts": 20.938, "power_cpu_cv_mean_watts": 1.919, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 69.76, "power_watts_avg": 20.938, "energy_joules_est": 233.53, "duration_seconds": 11.153, "sample_count": 96}, "timestamp": "2026-01-25T20:32:19.616127"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11098.608, "latencies_ms": [11098.608], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a bathroom, bathed in soft light that accentuates the clean lines and modern design. Dominating the scene is a white sink, its pristine surface reflecting the ambient light. The sink is mounted on a sleek, silver faucet that adds a touch of elegance to the overall aesthetic.\n\nAdj", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24463.1, "ram_available_mb": 38377.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24458.7, "ram_available_mb": 38382.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.702}, "power_stats": {"power_gpu_soc_mean_watts": 20.939, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 69.702, "power_watts_avg": 20.939, "energy_joules_est": 232.41, "duration_seconds": 11.099, "sample_count": 94}, "timestamp": "2026-01-25T20:32:32.770425"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11309.043, "latencies_ms": [11309.043], "images_per_second": 0.088, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "1. Toilet: 1\n2. Bathtub: 1\n3. Toilet paper: 1\n4. Toilet brush: 1\n5. Toilet paper holder: 1\n6. Toilet seat cover: 1\n7. Toilet paper roll: 1\n8. Toilet paper disp", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24458.7, "ram_available_mb": 38382.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24450.7, "ram_available_mb": 38390.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.551}, "power_stats": {"power_gpu_soc_mean_watts": 21.065, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.926, "gpu_utilization_percent_mean": 70.551, "power_watts_avg": 21.065, "energy_joules_est": 238.24, "duration_seconds": 11.31, "sample_count": 98}, "timestamp": "2026-01-25T20:32:46.134116"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8268.885, "latencies_ms": [8268.885], "images_per_second": 0.121, "prompt_tokens": 44, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The bathtub is positioned in the background, behind the toilet which is in the foreground. The toilet is closer to the viewer, while the bathtub is further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24450.7, "ram_available_mb": 38390.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24460.4, "ram_available_mb": 38380.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.743}, "power_stats": {"power_gpu_soc_mean_watts": 22.038, "power_cpu_cv_mean_watts": 1.745, "power_sys_5v0_mean_watts": 9.035, "gpu_utilization_percent_mean": 71.743, "power_watts_avg": 22.038, "energy_joules_est": 182.24, "duration_seconds": 8.27, "sample_count": 70}, "timestamp": "2026-01-25T20:32:56.438765"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7529.997, "latencies_ms": [7529.997], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows a modern bathroom with a white sink and a toilet with a cow print seat cover. The walls are tiled in blue and white, and there is a glass shelf above the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24460.4, "ram_available_mb": 38380.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24546.8, "ram_available_mb": 38294.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.524}, "power_stats": {"power_gpu_soc_mean_watts": 22.545, "power_cpu_cv_mean_watts": 1.621, "power_sys_5v0_mean_watts": 9.024, "gpu_utilization_percent_mean": 73.524, "power_watts_avg": 22.545, "energy_joules_est": 169.78, "duration_seconds": 7.531, "sample_count": 63}, "timestamp": "2026-01-25T20:33:06.015060"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8905.962, "latencies_ms": [8905.962], "images_per_second": 0.112, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a bathroom with a checkered black and white floor, a white sink with a blue and white towel, and a toilet with a black and white pattern. The lighting is bright and the materials appear to be a mix of ceramic and glass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24458.2, "ram_available_mb": 38382.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24570.0, "ram_available_mb": 38270.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.221}, "power_stats": {"power_gpu_soc_mean_watts": 21.744, "power_cpu_cv_mean_watts": 1.805, "power_sys_5v0_mean_watts": 9.024, "gpu_utilization_percent_mean": 71.221, "power_watts_avg": 21.744, "energy_joules_est": 193.66, "duration_seconds": 8.907, "sample_count": 77}, "timestamp": "2026-01-25T20:33:16.938982"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11154.392, "latencies_ms": [11154.392], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a statue of a woman holding a teddy bear. The statue is positioned on a brick base and is surrounded by a grassy area. The woman is depicted in a brown dress, and the teddy bear is white. The statue is located in front of a white pillar. The background of the image features a building and a tree. The text on", "error": null, "sys_before": {"cpu_percent": 9.7, "ram_used_mb": 24570.0, "ram_available_mb": 38270.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24530.4, "ram_available_mb": 38310.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.732}, "power_stats": {"power_gpu_soc_mean_watts": 20.746, "power_cpu_cv_mean_watts": 1.932, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.732, "power_watts_avg": 20.746, "energy_joules_est": 231.42, "duration_seconds": 11.155, "sample_count": 97}, "timestamp": "2026-01-25T20:33:30.128316"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9299.878, "latencies_ms": [9299.878], "images_per_second": 0.108, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Stuffed bear: 2\n- Stuffed animal: 2\n- Wooden sign: 1\n- Brick border: 1\n- Grass: 1\n- Dirt: 1\n- Plants: 1\n- Post: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24450.3, "ram_available_mb": 38390.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24581.1, "ram_available_mb": 38259.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.95}, "power_stats": {"power_gpu_soc_mean_watts": 21.73, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 71.95, "power_watts_avg": 21.73, "energy_joules_est": 202.1, "duration_seconds": 9.3, "sample_count": 80}, "timestamp": "2026-01-25T20:33:41.475200"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10284.775, "latencies_ms": [10284.775], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "In the foreground, there is a statue of a person holding a teddy bear, positioned on a pedestal. To the left of the statue, there is a wooden sign with text, and to the right, there is a white pillar. In the background, there are green plants and a pathway leading to a building.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24581.1, "ram_available_mb": 38259.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24576.9, "ram_available_mb": 38264.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.54}, "power_stats": {"power_gpu_soc_mean_watts": 21.223, "power_cpu_cv_mean_watts": 1.869, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 70.54, "power_watts_avg": 21.223, "energy_joules_est": 218.29, "duration_seconds": 10.285, "sample_count": 87}, "timestamp": "2026-01-25T20:33:53.781820"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7633.325, "latencies_ms": [7633.325], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a statue of a woman holding a teddy bear, with a signpost in front of it. The statue is located in a garden or park-like setting with plants and a pathway in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24453.1, "ram_available_mb": 38387.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24577.4, "ram_available_mb": 38263.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.938}, "power_stats": {"power_gpu_soc_mean_watts": 22.623, "power_cpu_cv_mean_watts": 1.652, "power_sys_5v0_mean_watts": 9.02, "gpu_utilization_percent_mean": 73.938, "power_watts_avg": 22.623, "energy_joules_est": 172.7, "duration_seconds": 7.634, "sample_count": 64}, "timestamp": "2026-01-25T20:34:03.438199"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10714.828, "latencies_ms": [10714.828], "images_per_second": 0.093, "prompt_tokens": 36, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image features a weathered, light-colored statue of a woman holding a teddy bear, standing on a pedestal with a sign that reads \"JUSEPH PANIS MISTLE MAIL\". The statue is surrounded by a grassy area with a brick border and a palm tree in the background, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24577.4, "ram_available_mb": 38263.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24606.1, "ram_available_mb": 38234.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.198}, "power_stats": {"power_gpu_soc_mean_watts": 21.122, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 70.198, "power_watts_avg": 21.122, "energy_joules_est": 226.33, "duration_seconds": 10.715, "sample_count": 91}, "timestamp": "2026-01-25T20:34:16.201605"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11120.366, "latencies_ms": [11120.366], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a large group of people gathered in a spacious dining area, enjoying a meal together. They are seated at various dining tables, which are spread throughout the room. The tables are surrounded by chairs, and there are numerous people sitting and standing around them. \n\nIn the background, a large clock is visible, adding a unique touch", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24606.1, "ram_available_mb": 38234.8, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24458.3, "ram_available_mb": 38382.6, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.558}, "power_stats": {"power_gpu_soc_mean_watts": 20.84, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 69.558, "power_watts_avg": 20.84, "energy_joules_est": 231.76, "duration_seconds": 11.121, "sample_count": 95}, "timestamp": "2026-01-25T20:34:29.359306"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8300.724, "latencies_ms": [8300.724], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "people: 20+\nchairs: numerous\ntables: multiple\nglasses: 1\nclock: 1\nbar: 1\nlight fixtures: 2\nwindows: 1\nceiling beams: multiple", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24458.3, "ram_available_mb": 38382.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24490.6, "ram_available_mb": 38350.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.5}, "power_stats": {"power_gpu_soc_mean_watts": 22.201, "power_cpu_cv_mean_watts": 1.699, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 72.5, "power_watts_avg": 22.201, "energy_joules_est": 184.3, "duration_seconds": 8.301, "sample_count": 70}, "timestamp": "2026-01-25T20:34:39.709735"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11108.921, "latencies_ms": [11108.921], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a large group of people seated at tables, facing towards the center of the room where a large clock is prominently displayed. The tables are arranged in a semi-circular fashion, creating a sense of a communal dining area. The clock, which is the main object in the background, is large and occupies a significant portion", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24490.6, "ram_available_mb": 38350.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24483.1, "ram_available_mb": 38357.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.589}, "power_stats": {"power_gpu_soc_mean_watts": 20.979, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 69.589, "power_watts_avg": 20.979, "energy_joules_est": 233.07, "duration_seconds": 11.11, "sample_count": 95}, "timestamp": "2026-01-25T20:34:52.847687"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8614.805, "latencies_ms": [8614.805], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image depicts a bustling restaurant with a large, ornate clock as a focal point on the wall. People are seated at tables, enjoying their meals and drinks, while others are standing and socializing in the open space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24483.1, "ram_available_mb": 38357.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 24458.1, "ram_available_mb": 38382.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.877}, "power_stats": {"power_gpu_soc_mean_watts": 22.043, "power_cpu_cv_mean_watts": 2.041, "power_sys_5v0_mean_watts": 9.033, "gpu_utilization_percent_mean": 72.877, "power_watts_avg": 22.043, "energy_joules_est": 189.91, "duration_seconds": 8.615, "sample_count": 73}, "timestamp": "2026-01-25T20:35:03.497416"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8322.135, "latencies_ms": [8322.135], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts an indoor setting with warm lighting that casts a cozy ambiance. The ceiling is made of dark wood beams, and the walls are adorned with a large, ornate clock that dominates the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24458.1, "ram_available_mb": 38382.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24494.3, "ram_available_mb": 38346.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.171}, "power_stats": {"power_gpu_soc_mean_watts": 21.965, "power_cpu_cv_mean_watts": 1.751, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 71.171, "power_watts_avg": 21.965, "energy_joules_est": 182.81, "duration_seconds": 8.323, "sample_count": 70}, "timestamp": "2026-01-25T20:35:13.842409"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10987.419, "latencies_ms": [10987.419], "images_per_second": 0.091, "prompt_tokens": 24, "response_tokens_est": 78, "n_tiles": 16, "output_text": "In the image, a man and a child are skiing down a snowy mountain. The man is standing in the foreground, while the child is skiing behind him. Both of them are wearing skis and appear to be enjoying their time on the slopes. The scene is set in a beautiful, snowy environment, with trees visible in the background.", "error": null, "sys_before": {"cpu_percent": 13.0, "ram_used_mb": 24494.3, "ram_available_mb": 38346.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24505.0, "ram_available_mb": 38335.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.011}, "power_stats": {"power_gpu_soc_mean_watts": 20.919, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 70.011, "power_watts_avg": 20.919, "energy_joules_est": 229.86, "duration_seconds": 10.988, "sample_count": 94}, "timestamp": "2026-01-25T20:35:26.884049"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6847.268, "latencies_ms": [6847.268], "images_per_second": 0.146, "prompt_tokens": 39, "response_tokens_est": 39, "n_tiles": 16, "output_text": "person: 2, ski: 2, tree: 4, rock: 3, snow: numerous, shadow: 1, sun: 1, mountain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24505.0, "ram_available_mb": 38335.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24548.5, "ram_available_mb": 38292.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.741}, "power_stats": {"power_gpu_soc_mean_watts": 23.112, "power_cpu_cv_mean_watts": 1.546, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 74.741, "power_watts_avg": 23.112, "energy_joules_est": 158.27, "duration_seconds": 6.848, "sample_count": 58}, "timestamp": "2026-01-25T20:35:35.754291"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10703.387, "latencies_ms": [10703.387], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there is a person standing on skis, positioned near the center of the image. To the left and slightly in the background, there is a child also on skis, who appears to be further down the slope. The skis of both individuals are parallel to each other, indicating they are on the same slope but at different points.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24548.5, "ram_available_mb": 38292.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24495.4, "ram_available_mb": 38345.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.286}, "power_stats": {"power_gpu_soc_mean_watts": 21.117, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 70.286, "power_watts_avg": 21.117, "energy_joules_est": 226.04, "duration_seconds": 10.704, "sample_count": 91}, "timestamp": "2026-01-25T20:35:48.505752"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6878.457, "latencies_ms": [6878.457], "images_per_second": 0.145, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "In the image, a man and a child are skiing on a snowy mountain. The man is standing upright on skis, while the child is skiing behind him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24495.4, "ram_available_mb": 38345.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24519.4, "ram_available_mb": 38321.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.793}, "power_stats": {"power_gpu_soc_mean_watts": 23.099, "power_cpu_cv_mean_watts": 1.54, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 74.793, "power_watts_avg": 23.099, "energy_joules_est": 158.9, "duration_seconds": 6.879, "sample_count": 58}, "timestamp": "2026-01-25T20:35:57.411338"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9158.794, "latencies_ms": [9158.794], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image depicts a snowy landscape with a clear blue sky and sunlight casting shadows on the snow. There are two individuals, one wearing a black jacket and the other in a pink jacket, both standing on skis with ski poles visible on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24519.4, "ram_available_mb": 38321.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24611.6, "ram_available_mb": 38229.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.494}, "power_stats": {"power_gpu_soc_mean_watts": 21.629, "power_cpu_cv_mean_watts": 1.789, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 70.494, "power_watts_avg": 21.629, "energy_joules_est": 198.11, "duration_seconds": 9.159, "sample_count": 77}, "timestamp": "2026-01-25T20:36:08.610239"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11104.473, "latencies_ms": [11104.473], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a pair of feet wearing black sandals, standing on a wooden floor. The feet are positioned near a collection of cell phones, which are scattered around the area. There are three cell phones in total, with one located to the left of the feet and two others on the right side. The arrangement of the phones and the feet creates a scene that suggests a", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24611.6, "ram_available_mb": 38229.3, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24490.4, "ram_available_mb": 38350.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.34}, "power_stats": {"power_gpu_soc_mean_watts": 21.026, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.34, "power_watts_avg": 21.026, "energy_joules_est": 233.5, "duration_seconds": 11.105, "sample_count": 94}, "timestamp": "2026-01-25T20:36:21.768921"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6148.28, "latencies_ms": [6148.28], "images_per_second": 0.163, "prompt_tokens": 39, "response_tokens_est": 33, "n_tiles": 16, "output_text": "- Cell phone: 3\n- Battery: 1\n- Remote control: 1\n- Flip-flop: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24490.4, "ram_available_mb": 38350.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24507.9, "ram_available_mb": 38333.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.019}, "power_stats": {"power_gpu_soc_mean_watts": 23.568, "power_cpu_cv_mean_watts": 1.478, "power_sys_5v0_mean_watts": 9.051, "gpu_utilization_percent_mean": 76.019, "power_watts_avg": 23.568, "energy_joules_est": 144.92, "duration_seconds": 6.149, "sample_count": 52}, "timestamp": "2026-01-25T20:36:29.964871"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11108.106, "latencies_ms": [11108.106], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two feet wearing black sandals with white nail polish. Behind the feet, on the wooden surface, there are three cell phones placed in a line from left to right. The first phone is the smallest and placed farthest to the left, the second phone is in the middle and slightly larger, and the third phone is the largest and", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24507.9, "ram_available_mb": 38333.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24551.1, "ram_available_mb": 38289.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.219}, "power_stats": {"power_gpu_soc_mean_watts": 20.96, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.219, "power_watts_avg": 20.96, "energy_joules_est": 232.84, "duration_seconds": 11.109, "sample_count": 96}, "timestamp": "2026-01-25T20:36:43.093442"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8152.883, "latencies_ms": [8152.883], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a wooden table with two feet wearing black sandals, and three cell phones scattered across the surface. The phones appear to be old or damaged, with one having its back cover removed and another showing its internal components.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24551.1, "ram_available_mb": 38289.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24525.5, "ram_available_mb": 38315.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.913}, "power_stats": {"power_gpu_soc_mean_watts": 22.275, "power_cpu_cv_mean_watts": 1.7, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 72.913, "power_watts_avg": 22.275, "energy_joules_est": 181.62, "duration_seconds": 8.153, "sample_count": 69}, "timestamp": "2026-01-25T20:36:53.274748"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8438.989, "latencies_ms": [8438.989], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image features a wooden surface with a pair of black sandals on either side, a broken cell phone in the center, and a small black case and a white cell phone on the right side. The lighting is natural and the wooden surface has a warm tone.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24463.7, "ram_available_mb": 38377.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24487.6, "ram_available_mb": 38353.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.685}, "power_stats": {"power_gpu_soc_mean_watts": 21.836, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 9.039, "gpu_utilization_percent_mean": 70.685, "power_watts_avg": 21.836, "energy_joules_est": 184.29, "duration_seconds": 8.44, "sample_count": 73}, "timestamp": "2026-01-25T20:37:03.755613"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12079.051, "latencies_ms": [12079.051], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a breathtaking view of the Houses of Parliament and the Elizabeth Tower, also known as Big Ben, in London, England. The iconic clock tower stands tall in the background, its golden facade gleaming under the soft light of the setting sun. The Elizabeth Tower, a symbol of British history and culture, is a prominent feature in the image.\n\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24487.6, "ram_available_mb": 38353.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24461.1, "ram_available_mb": 38379.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.233}, "power_stats": {"power_gpu_soc_mean_watts": 22.603, "power_cpu_cv_mean_watts": 1.788, "power_sys_5v0_mean_watts": 9.146, "gpu_utilization_percent_mean": 73.233, "power_watts_avg": 22.603, "energy_joules_est": 273.04, "duration_seconds": 12.08, "sample_count": 103}, "timestamp": "2026-01-25T20:37:17.896294"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9444.353, "latencies_ms": [9444.353], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "- Boats: 3\n- Buildings: 2\n- Windows: Over 100\n- Trees: Over 10\n- Clouds: Over 10\n- Sky: 1\n- Water: 1", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24461.1, "ram_available_mb": 38379.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24517.3, "ram_available_mb": 38323.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.575}, "power_stats": {"power_gpu_soc_mean_watts": 23.564, "power_cpu_cv_mean_watts": 1.561, "power_sys_5v0_mean_watts": 9.12, "gpu_utilization_percent_mean": 76.575, "power_watts_avg": 23.564, "energy_joules_est": 222.56, "duration_seconds": 9.445, "sample_count": 80}, "timestamp": "2026-01-25T20:37:29.397576"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12049.762, "latencies_ms": [12049.762], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a boat named 'PACE' on the left side of the image, closer to the viewer than the main buildings. The large, ornate building known as the Houses of Parliament is in the background, with the clock tower Big Ben visible on the right side. The river Thames separates the boats in the foreground from the grand architecture of the", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24517.3, "ram_available_mb": 38323.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24642.4, "ram_available_mb": 38198.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.137}, "power_stats": {"power_gpu_soc_mean_watts": 22.735, "power_cpu_cv_mean_watts": 1.798, "power_sys_5v0_mean_watts": 9.19, "gpu_utilization_percent_mean": 73.137, "power_watts_avg": 22.735, "energy_joules_est": 273.97, "duration_seconds": 12.05, "sample_count": 102}, "timestamp": "2026-01-25T20:37:43.468826"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 12344.174, "latencies_ms": [12344.174], "images_per_second": 0.081, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene evening view of the Palace of Westminster, the official residence of the British Prime Minister, located on the banks of the River Thames in London. The iconic clock tower, Big Ben, stands tall in the background, illuminated against the dusky sky, while the Houses of Parliament are bathed in a warm, orange glow from the", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24498.6, "ram_available_mb": 38342.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24499.0, "ram_available_mb": 38341.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.514}, "power_stats": {"power_gpu_soc_mean_watts": 22.752, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 9.097, "gpu_utilization_percent_mean": 73.514, "power_watts_avg": 22.752, "energy_joules_est": 280.87, "duration_seconds": 12.345, "sample_count": 105}, "timestamp": "2026-01-25T20:37:57.874187"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8580.587, "latencies_ms": [8580.587], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The sky is overcast with a mix of blue and gray hues, suggesting a cloudy day. The buildings are illuminated with warm yellow and orange lights, creating a contrast against the cooler tones of the sky.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24499.0, "ram_available_mb": 38341.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24453.9, "ram_available_mb": 38387.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.972}, "power_stats": {"power_gpu_soc_mean_watts": 23.784, "power_cpu_cv_mean_watts": 1.535, "power_sys_5v0_mean_watts": 9.207, "gpu_utilization_percent_mean": 75.972, "power_watts_avg": 23.784, "energy_joules_est": 204.09, "duration_seconds": 8.581, "sample_count": 72}, "timestamp": "2026-01-25T20:38:08.466864"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11133.271, "latencies_ms": [11133.271], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a spacious living room with a variety of furniture and decorations. There are two couches, one on the left side of the room and another on the right side. A TV is placed in the center of the room, and a fan is located above it. The room features a large window, providing ample natural light.\n\nSeveral", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24453.9, "ram_available_mb": 38387.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24572.2, "ram_available_mb": 38268.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.819}, "power_stats": {"power_gpu_soc_mean_watts": 20.953, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 69.819, "power_watts_avg": 20.953, "energy_joules_est": 233.29, "duration_seconds": 11.134, "sample_count": 94}, "timestamp": "2026-01-25T20:38:21.653169"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8293.043, "latencies_ms": [8293.043], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "- Chair: 1\n- Couch: 2\n- Sofa: 1\n- Table: 3\n- Television: 1\n- Plant: 3\n- Fan: 1\n- Mirror: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24572.2, "ram_available_mb": 38268.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24605.4, "ram_available_mb": 38235.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.714}, "power_stats": {"power_gpu_soc_mean_watts": 22.025, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 72.714, "power_watts_avg": 22.025, "energy_joules_est": 182.67, "duration_seconds": 8.294, "sample_count": 70}, "timestamp": "2026-01-25T20:38:31.998134"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11124.121, "latencies_ms": [11124.121], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a living room with a variety of furniture, including a sofa, a chair, and a coffee table. The sofa is positioned near the center of the room, with the chair to its left and the coffee table in front of it. In the background, there are windows that let in natural light and a ceiling fan that is", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24458.4, "ram_available_mb": 38382.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24479.7, "ram_available_mb": 38361.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.811}, "power_stats": {"power_gpu_soc_mean_watts": 20.887, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 69.811, "power_watts_avg": 20.887, "energy_joules_est": 232.36, "duration_seconds": 11.125, "sample_count": 95}, "timestamp": "2026-01-25T20:38:45.168701"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8275.138, "latencies_ms": [8275.138], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a variety of furniture, including a couch, chairs, and a coffee table. There are multiple potted plants placed around the room, adding a touch of greenery to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24479.7, "ram_available_mb": 38361.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24576.1, "ram_available_mb": 38264.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.761}, "power_stats": {"power_gpu_soc_mean_watts": 22.169, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 71.761, "power_watts_avg": 22.169, "energy_joules_est": 183.47, "duration_seconds": 8.276, "sample_count": 71}, "timestamp": "2026-01-25T20:38:55.473927"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9892.35, "latencies_ms": [9892.35], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The room is well-lit with natural light coming through the large windows, and the walls are painted in a light color, giving the space a bright and airy feel. The furniture is a mix of dark and light colors, with a prominent green sofa and a red area rug adding pops of color to the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24576.1, "ram_available_mb": 38264.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 7.4, "ram_used_mb": 24459.5, "ram_available_mb": 38381.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.94}, "power_stats": {"power_gpu_soc_mean_watts": 21.25, "power_cpu_cv_mean_watts": 2.136, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 69.94, "power_watts_avg": 21.25, "energy_joules_est": 210.23, "duration_seconds": 9.893, "sample_count": 84}, "timestamp": "2026-01-25T20:39:07.390879"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11099.637, "latencies_ms": [11099.637], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene on a city street. Dominating the foreground is a red pole, which supports two black parking meters. The pole is positioned on the right side of the image, standing upright and casting a shadow on the ground. \n\nBehind the pole, a brick building with large windows can be seen. The building's facade is ad", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24459.5, "ram_available_mb": 38381.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24458.0, "ram_available_mb": 38382.9, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.553}, "power_stats": {"power_gpu_soc_mean_watts": 21.0, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.553, "power_watts_avg": 21.0, "energy_joules_est": 233.11, "duration_seconds": 11.1, "sample_count": 94}, "timestamp": "2026-01-25T20:39:20.514801"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7504.401, "latencies_ms": [7504.401], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "parking meter: 2, red pole: 1, building: 1, window: 4, sign: 1, leaf: 1, brick: 1, parking meter: 2", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24458.0, "ram_available_mb": 38382.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24462.9, "ram_available_mb": 38378.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.079}, "power_stats": {"power_gpu_soc_mean_watts": 22.67, "power_cpu_cv_mean_watts": 1.64, "power_sys_5v0_mean_watts": 9.005, "gpu_utilization_percent_mean": 72.079, "power_watts_avg": 22.67, "energy_joules_est": 170.14, "duration_seconds": 7.505, "sample_count": 63}, "timestamp": "2026-01-25T20:39:30.041286"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11169.526, "latencies_ms": [11169.526], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a red pole with a circular object attached to it, which is positioned in front of a parking meter. The parking meter is located on the sidewalk to the left of the pole. In the background, there is a building with a large window displaying the text '40 YEARS OF SAVING LIVES'. The building is situated", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24462.9, "ram_available_mb": 38378.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24460.4, "ram_available_mb": 38380.4, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.989}, "power_stats": {"power_gpu_soc_mean_watts": 20.969, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.989, "power_watts_avg": 20.969, "energy_joules_est": 234.23, "duration_seconds": 11.17, "sample_count": 95}, "timestamp": "2026-01-25T20:39:43.263589"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7627.597, "latencies_ms": [7627.597], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a red parking meter on a sidewalk in front of a building with a sign that reads \"40 years of saving lives.\" The parking meter is empty and there are no cars parked nearby.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24460.4, "ram_available_mb": 38380.4, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24583.5, "ram_available_mb": 38257.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.156}, "power_stats": {"power_gpu_soc_mean_watts": 22.503, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 9.028, "gpu_utilization_percent_mean": 72.156, "power_watts_avg": 22.503, "energy_joules_est": 171.66, "duration_seconds": 7.628, "sample_count": 64}, "timestamp": "2026-01-25T20:39:52.945457"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7910.517, "latencies_ms": [7910.517], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image features a red metal pole with a circular design at the top, supporting two black parking meters. The background shows a building with large windows and a sign that reads \"40 YEARS OF SAVING LIVES.\"", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24583.5, "ram_available_mb": 38257.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24479.6, "ram_available_mb": 38361.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.971}, "power_stats": {"power_gpu_soc_mean_watts": 22.178, "power_cpu_cv_mean_watts": 1.743, "power_sys_5v0_mean_watts": 9.046, "gpu_utilization_percent_mean": 71.971, "power_watts_avg": 22.178, "energy_joules_est": 175.45, "duration_seconds": 7.911, "sample_count": 68}, "timestamp": "2026-01-25T20:40:02.895831"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11131.551, "latencies_ms": [11131.551], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a man and a woman sitting on a couch in a living room, watching television. They are surrounded by a cozy atmosphere, with a dining table nearby. On the table, there are several cups, a cell phone, and a bowl. A vase is also present on the table, adding a decorative touch to the scene.\n\nIn the room", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24479.6, "ram_available_mb": 38361.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24498.7, "ram_available_mb": 38342.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.379}, "power_stats": {"power_gpu_soc_mean_watts": 20.948, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.379, "power_watts_avg": 20.948, "energy_joules_est": 233.2, "duration_seconds": 11.132, "sample_count": 95}, "timestamp": "2026-01-25T20:40:16.074311"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8824.143, "latencies_ms": [8824.143], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 57, "n_tiles": 16, "output_text": "- Television: 1\n- Couch: 1\n- Cup: 2\n- Snacks: 1\n- Candle: 1\n- Cell phone: 1\n- Glass: 2\n- Poinsettia plant: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24498.7, "ram_available_mb": 38342.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24488.5, "ram_available_mb": 38352.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.64}, "power_stats": {"power_gpu_soc_mean_watts": 21.93, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 70.64, "power_watts_avg": 21.93, "energy_joules_est": 193.53, "duration_seconds": 8.825, "sample_count": 75}, "timestamp": "2026-01-25T20:40:26.939623"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11112.68, "latencies_ms": [11112.68], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a coffee table with various items on it, including a red poinsettia plant, a jar of candy, and a plate of snacks. Behind the coffee table, there is a couple sitting on a couch, with the man on the right side and the woman on the left. In the background, there is a wooden entertainment center with", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24488.5, "ram_available_mb": 38352.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24496.2, "ram_available_mb": 38344.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.521}, "power_stats": {"power_gpu_soc_mean_watts": 20.953, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 69.521, "power_watts_avg": 20.953, "energy_joules_est": 232.86, "duration_seconds": 11.113, "sample_count": 96}, "timestamp": "2026-01-25T20:40:40.090287"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8281.971, "latencies_ms": [8281.971], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A couple is sitting on a couch in a cozy living room, watching television and enjoying a snack of chips. The room is decorated with a Christmas tree and other festive decorations, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24496.2, "ram_available_mb": 38344.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24583.4, "ram_available_mb": 38257.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.814}, "power_stats": {"power_gpu_soc_mean_watts": 22.037, "power_cpu_cv_mean_watts": 1.727, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 71.814, "power_watts_avg": 22.037, "energy_joules_est": 182.52, "duration_seconds": 8.283, "sample_count": 70}, "timestamp": "2026-01-25T20:40:50.401902"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7860.52, "latencies_ms": [7860.52], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a cozy indoor setting with warm lighting, primarily from the television screen and the ambient lighting. The colors are muted with earth tones, and the materials visible include wood furniture and fabric couches.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24459.8, "ram_available_mb": 38381.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24455.9, "ram_available_mb": 38385.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.894}, "power_stats": {"power_gpu_soc_mean_watts": 22.208, "power_cpu_cv_mean_watts": 1.699, "power_sys_5v0_mean_watts": 9.051, "gpu_utilization_percent_mean": 71.894, "power_watts_avg": 22.208, "energy_joules_est": 174.58, "duration_seconds": 7.861, "sample_count": 66}, "timestamp": "2026-01-25T20:41:00.296506"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11133.706, "latencies_ms": [11133.706], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person's hand is holding a white remote control, which is positioned in front of a toilet. The remote control is being used to operate the toilet, as it is placed on top of the toilet tank. The toilet itself is white and appears to be in a bathroom setting. The person's hand is visible on the", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24455.9, "ram_available_mb": 38385.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24496.9, "ram_available_mb": 38344.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.723}, "power_stats": {"power_gpu_soc_mean_watts": 20.91, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.723, "power_watts_avg": 20.91, "energy_joules_est": 232.82, "duration_seconds": 11.134, "sample_count": 94}, "timestamp": "2026-01-25T20:41:13.456714"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7517.907, "latencies_ms": [7517.907], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "toilet: 1, remote control: 1, floor: 1, wall: 1, person: 1, hand: 1, bathroom: 1, control panel: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24496.9, "ram_available_mb": 38344.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24573.8, "ram_available_mb": 38267.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.095}, "power_stats": {"power_gpu_soc_mean_watts": 22.685, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 73.095, "power_watts_avg": 22.685, "energy_joules_est": 170.56, "duration_seconds": 7.519, "sample_count": 63}, "timestamp": "2026-01-25T20:41:23.019734"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10416.95, "latencies_ms": [10416.95], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, a person's hand is holding a white device with multiple buttons, positioned near the center of the image. The device is being used to operate a toilet, which is located in the background. The toilet is white and has a control panel on its side, with a barrier arm in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24450.2, "ram_available_mb": 38390.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24573.5, "ram_available_mb": 38267.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.011}, "power_stats": {"power_gpu_soc_mean_watts": 21.266, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 70.011, "power_watts_avg": 21.266, "energy_joules_est": 221.54, "duration_seconds": 10.418, "sample_count": 88}, "timestamp": "2026-01-25T20:41:35.475051"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8184.964, "latencies_ms": [8184.964], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A person is holding a remote control in front of a toilet, which is located in a small, enclosed space with white walls. The remote control has multiple buttons, and the toilet has a digital display and a control panel.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24478.2, "ram_available_mb": 38362.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24516.9, "ram_available_mb": 38324.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.304}, "power_stats": {"power_gpu_soc_mean_watts": 22.195, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 71.304, "power_watts_avg": 22.195, "energy_joules_est": 181.68, "duration_seconds": 8.186, "sample_count": 69}, "timestamp": "2026-01-25T20:41:45.720126"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8104.532, "latencies_ms": [8104.532], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a person's hand holding a white electronic device with multiple buttons, which is being inserted into a toilet. The toilet is white and has a modern design with a closed lid and a visible control panel on the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24455.2, "ram_available_mb": 38385.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24507.7, "ram_available_mb": 38333.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.072}, "power_stats": {"power_gpu_soc_mean_watts": 22.028, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 9.042, "gpu_utilization_percent_mean": 71.072, "power_watts_avg": 22.028, "energy_joules_est": 178.54, "duration_seconds": 8.105, "sample_count": 69}, "timestamp": "2026-01-25T20:41:55.875549"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11119.913, "latencies_ms": [11119.913], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a thrilling moment of a snowboarder performing a jump on a snowy hill. The snowboarder is in mid-air, skillfully maneuvering their snowboard as they soar above the snow. \n\nThere are several people watching the snowboarder's performance from various positions on the hill. Some of them are closer to the snowboard", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24507.7, "ram_available_mb": 38333.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24516.4, "ram_available_mb": 38324.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.617}, "power_stats": {"power_gpu_soc_mean_watts": 20.95, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.617, "power_watts_avg": 20.95, "energy_joules_est": 232.97, "duration_seconds": 11.121, "sample_count": 94}, "timestamp": "2026-01-25T20:42:09.025379"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7503.403, "latencies_ms": [7503.403], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "people: 10, snowboarder: 1, snowboard: 1, snow: 1, hill: 1, flag: 2, fence: 1, crowd: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24516.4, "ram_available_mb": 38324.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24496.1, "ram_available_mb": 38344.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.484}, "power_stats": {"power_gpu_soc_mean_watts": 22.639, "power_cpu_cv_mean_watts": 1.633, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 73.484, "power_watts_avg": 22.639, "energy_joules_est": 169.88, "duration_seconds": 7.504, "sample_count": 64}, "timestamp": "2026-01-25T20:42:18.577487"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9283.724, "latencies_ms": [9283.724], "images_per_second": 0.108, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "In the foreground, there is a snow-covered slope with a ramp and a group of people standing on the left side. In the background, there is a clear blue sky. In the center of the image, a snowboarder is performing a trick in the air above the ramp.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24496.1, "ram_available_mb": 38344.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24487.3, "ram_available_mb": 38353.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.203}, "power_stats": {"power_gpu_soc_mean_watts": 21.59, "power_cpu_cv_mean_watts": 1.835, "power_sys_5v0_mean_watts": 9.056, "gpu_utilization_percent_mean": 70.203, "power_watts_avg": 21.59, "energy_joules_est": 200.45, "duration_seconds": 9.284, "sample_count": 79}, "timestamp": "2026-01-25T20:42:29.878881"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9437.41, "latencies_ms": [9437.41], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image captures a thrilling moment at a snowboarding event where a snowboarder is performing a daring trick in mid-air above a snow-covered ramp. Spectators are gathered on the side of the ramp, watching the snowboarder's impressive jump.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24487.3, "ram_available_mb": 38353.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24459.7, "ram_available_mb": 38381.2, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.05}, "power_stats": {"power_gpu_soc_mean_watts": 21.633, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 71.05, "power_watts_avg": 21.633, "energy_joules_est": 204.17, "duration_seconds": 9.438, "sample_count": 80}, "timestamp": "2026-01-25T20:42:41.362402"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10231.178, "latencies_ms": [10231.178], "images_per_second": 0.098, "prompt_tokens": 36, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The image captures a snowboarder in mid-air performing a trick against a clear blue sky. The snowboarder is wearing a red and white outfit, and the snowboard is white with red and black stripes. The snow-covered ground and the bright sunlight create a vibrant contrast with the blue sky.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24459.7, "ram_available_mb": 38381.2, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24446.4, "ram_available_mb": 38394.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.77}, "power_stats": {"power_gpu_soc_mean_watts": 21.17, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 69.77, "power_watts_avg": 21.17, "energy_joules_est": 216.61, "duration_seconds": 10.232, "sample_count": 87}, "timestamp": "2026-01-25T20:42:53.616980"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11126.709, "latencies_ms": [11126.709], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a small, well-lit room with a desk and a computer chair. The desk is equipped with a laptop, a keyboard, and a mouse, creating a functional workspace. A potted plant is placed near the desk, adding a touch of greenery to the room. A couch can be seen in the background, providing a comfortable seating area.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24446.4, "ram_available_mb": 38394.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 7.6, "ram_used_mb": 24492.5, "ram_available_mb": 38348.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.447}, "power_stats": {"power_gpu_soc_mean_watts": 20.907, "power_cpu_cv_mean_watts": 2.084, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 69.447, "power_watts_avg": 20.907, "energy_joules_est": 232.64, "duration_seconds": 11.127, "sample_count": 94}, "timestamp": "2026-01-25T20:43:06.793483"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8646.22, "latencies_ms": [8646.22], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Desk: 1\n- Chair: 1\n- Laptop: 1\n- Keyboard: 1\n- Computer mouse: 1\n- Plant: 1\n- Cable: 1\n- Floor lamp: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24492.5, "ram_available_mb": 38348.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24564.2, "ram_available_mb": 38276.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.384}, "power_stats": {"power_gpu_soc_mean_watts": 22.071, "power_cpu_cv_mean_watts": 1.717, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 72.384, "power_watts_avg": 22.071, "energy_joules_est": 190.85, "duration_seconds": 8.647, "sample_count": 73}, "timestamp": "2026-01-25T20:43:17.465964"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10115.806, "latencies_ms": [10115.806], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there is a black office chair positioned in front of a desk with a laptop, a monitor, a keyboard, and a mouse. The desk is situated against a wall with a bookshelf in the background. To the right of the desk, there is a potted plant on a small table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24458.9, "ram_available_mb": 38382.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24564.3, "ram_available_mb": 38276.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.329}, "power_stats": {"power_gpu_soc_mean_watts": 21.279, "power_cpu_cv_mean_watts": 1.87, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 70.329, "power_watts_avg": 21.279, "energy_joules_est": 215.27, "duration_seconds": 10.116, "sample_count": 85}, "timestamp": "2026-01-25T20:43:29.614578"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8181.755, "latencies_ms": [8181.755], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image depicts a well-lit room with a desk and chair set up for work or study. On the desk, there is a laptop, a monitor, a keyboard, a mouse, and a lamp providing additional lighting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24564.3, "ram_available_mb": 38276.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24493.2, "ram_available_mb": 38347.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.971}, "power_stats": {"power_gpu_soc_mean_watts": 22.275, "power_cpu_cv_mean_watts": 1.688, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 72.971, "power_watts_avg": 22.275, "energy_joules_est": 182.26, "duration_seconds": 8.182, "sample_count": 69}, "timestamp": "2026-01-25T20:43:39.842146"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7200.302, "latencies_ms": [7200.302], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The room is well-lit with a combination of natural light from the window and artificial light from a desk lamp. The desk is made of wood and has a white keyboard and a silver laptop on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24493.2, "ram_available_mb": 38347.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24498.0, "ram_available_mb": 38342.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.344}, "power_stats": {"power_gpu_soc_mean_watts": 22.538, "power_cpu_cv_mean_watts": 1.641, "power_sys_5v0_mean_watts": 9.041, "gpu_utilization_percent_mean": 72.344, "power_watts_avg": 22.538, "energy_joules_est": 162.29, "duration_seconds": 7.201, "sample_count": 61}, "timestamp": "2026-01-25T20:43:49.056001"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11110.477, "latencies_ms": [11110.477], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is riding a motorcycle on a gravel road that winds through a mountainous landscape. The rider is wearing a black helmet and is positioned in the center of the image, with the motorcycle slightly to the left. The road is surrounded by green hills and trees, creating a scenic backdrop. The sky above is blue with a", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24498.0, "ram_available_mb": 38342.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24508.7, "ram_available_mb": 38332.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.521}, "power_stats": {"power_gpu_soc_mean_watts": 20.945, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 69.521, "power_watts_avg": 20.945, "energy_joules_est": 232.72, "duration_seconds": 11.111, "sample_count": 96}, "timestamp": "2026-01-25T20:44:02.218416"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8475.751, "latencies_ms": [8475.751], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "motorcycle: 2\n\nmountain: 1\n\ncloud: 10\n\npath: 1\n\nrider: 1\n\nbackpack: 1\n\ntire: 1\n\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24508.7, "ram_available_mb": 38332.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24481.4, "ram_available_mb": 38359.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.583}, "power_stats": {"power_gpu_soc_mean_watts": 22.043, "power_cpu_cv_mean_watts": 1.741, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 70.583, "power_watts_avg": 22.043, "energy_joules_est": 186.84, "duration_seconds": 8.476, "sample_count": 72}, "timestamp": "2026-01-25T20:44:12.745391"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10882.206, "latencies_ms": [10882.206], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a motorcycle with a rider on the left side of the image, positioned on a gravel path that leads into the distance. The background features a vast landscape of rolling hills and a clear blue sky with scattered clouds. The motorcycle and rider are near the viewer, while the hills and sky are far in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24481.4, "ram_available_mb": 38359.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24497.9, "ram_available_mb": 38343.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.543}, "power_stats": {"power_gpu_soc_mean_watts": 20.985, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 69.543, "power_watts_avg": 20.985, "energy_joules_est": 228.38, "duration_seconds": 10.883, "sample_count": 94}, "timestamp": "2026-01-25T20:44:25.666410"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8034.685, "latencies_ms": [8034.685], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A person is riding a motorcycle on a gravel road that winds through a hilly landscape under a blue sky with scattered clouds. The terrain is covered with greenery and the road appears to be in a remote or rural area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24497.9, "ram_available_mb": 38343.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24504.7, "ram_available_mb": 38336.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.132}, "power_stats": {"power_gpu_soc_mean_watts": 22.252, "power_cpu_cv_mean_watts": 1.701, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 72.132, "power_watts_avg": 22.252, "energy_joules_est": 178.8, "duration_seconds": 8.035, "sample_count": 68}, "timestamp": "2026-01-25T20:44:35.723268"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7427.012, "latencies_ms": [7427.012], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image features a clear blue sky with scattered white clouds, and the lighting suggests it is a sunny day. The landscape is predominantly green with hilly terrain, and the ground is covered with gravel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24504.7, "ram_available_mb": 38336.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24474.1, "ram_available_mb": 38366.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.172}, "power_stats": {"power_gpu_soc_mean_watts": 22.351, "power_cpu_cv_mean_watts": 1.689, "power_sys_5v0_mean_watts": 9.037, "gpu_utilization_percent_mean": 72.172, "power_watts_avg": 22.351, "energy_joules_est": 166.02, "duration_seconds": 7.428, "sample_count": 64}, "timestamp": "2026-01-25T20:44:45.164084"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 8101.268, "latencies_ms": [8101.268], "images_per_second": 0.123, "prompt_tokens": 24, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a well-lit kitchen with wooden cabinets and white appliances. There's a dining table with a bowl of oranges and bananas on it. The kitchen has a modern design with a clean and organized appearance.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24474.1, "ram_available_mb": 38366.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24569.7, "ram_available_mb": 38271.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.58}, "power_stats": {"power_gpu_soc_mean_watts": 22.02, "power_cpu_cv_mean_watts": 1.741, "power_sys_5v0_mean_watts": 9.021, "gpu_utilization_percent_mean": 71.58, "power_watts_avg": 22.02, "energy_joules_est": 178.4, "duration_seconds": 8.102, "sample_count": 69}, "timestamp": "2026-01-25T20:44:55.294309"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7808.173, "latencies_ms": [7808.173], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "table: 1\noranges: 3\nbananas: 2\nfridge: 1\ndoor: 2\nwindow: 1\nlight fixture: 1\ncabinet: 1\n", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24508.1, "ram_available_mb": 38332.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24496.5, "ram_available_mb": 38344.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.97}, "power_stats": {"power_gpu_soc_mean_watts": 22.628, "power_cpu_cv_mean_watts": 1.692, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 71.97, "power_watts_avg": 22.628, "energy_joules_est": 176.7, "duration_seconds": 7.809, "sample_count": 66}, "timestamp": "2026-01-25T20:45:05.129443"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11077.154, "latencies_ms": [11077.154], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden dining table with a bowl of oranges and bananas placed on it, indicating it is the area where food is served. The kitchen is in the background, with wooden cabinets and a white refrigerator, suggesting it is the area where food is prepared. The living area is to the left, with a white door leading to another", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24496.5, "ram_available_mb": 38344.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24503.3, "ram_available_mb": 38337.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.213}, "power_stats": {"power_gpu_soc_mean_watts": 21.054, "power_cpu_cv_mean_watts": 1.934, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 69.213, "power_watts_avg": 21.054, "energy_joules_est": 233.23, "duration_seconds": 11.078, "sample_count": 94}, "timestamp": "2026-01-25T20:45:18.220364"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7827.076, "latencies_ms": [7827.076], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image depicts a well-lit, modern kitchen with wooden cabinets and white appliances. A bowl of oranges and bananas is placed on the kitchen island, suggesting a focus on healthy eating.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24503.3, "ram_available_mb": 38337.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24459.9, "ram_available_mb": 38381.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.773}, "power_stats": {"power_gpu_soc_mean_watts": 22.378, "power_cpu_cv_mean_watts": 1.686, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 72.773, "power_watts_avg": 22.378, "energy_joules_est": 175.17, "duration_seconds": 7.828, "sample_count": 66}, "timestamp": "2026-01-25T20:45:28.080103"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8145.488, "latencies_ms": [8145.488], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The kitchen features wooden cabinets and white appliances, creating a warm and modern aesthetic. The lighting is bright and natural, coming from the ceiling lights and the window, which suggests a well-lit and inviting space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24459.9, "ram_available_mb": 38381.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24583.7, "ram_available_mb": 38257.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.681}, "power_stats": {"power_gpu_soc_mean_watts": 21.842, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 9.004, "gpu_utilization_percent_mean": 71.681, "power_watts_avg": 21.842, "energy_joules_est": 177.93, "duration_seconds": 8.146, "sample_count": 69}, "timestamp": "2026-01-25T20:45:38.247749"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11142.327, "latencies_ms": [11142.327], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a female tennis player is in the midst of a powerful serve on a vibrant green tennis court. She is dressed in a white tank top and a black skirt, her white shoes contrasting with the court. The tennis racket in her hand is poised to strike the yellow tennis ball that hovers in the air above her head. The court is marked with", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24463.4, "ram_available_mb": 38377.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24480.8, "ram_available_mb": 38360.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.719}, "power_stats": {"power_gpu_soc_mean_watts": 20.862, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.719, "power_watts_avg": 20.862, "energy_joules_est": 232.47, "duration_seconds": 11.143, "sample_count": 96}, "timestamp": "2026-01-25T20:45:51.450562"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8170.458, "latencies_ms": [8170.458], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "ball: 1, tennis racket: 1, tennis player: 1, tennis outfit: 1, tennis court: 1, tennis ball: 1, tennis racket: 1, tennis outfit: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24480.8, "ram_available_mb": 38360.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24550.2, "ram_available_mb": 38290.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.232}, "power_stats": {"power_gpu_soc_mean_watts": 22.153, "power_cpu_cv_mean_watts": 1.706, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 72.232, "power_watts_avg": 22.153, "energy_joules_est": 181.02, "duration_seconds": 8.171, "sample_count": 69}, "timestamp": "2026-01-25T20:46:01.655409"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11123.559, "latencies_ms": [11123.559], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a female tennis player is positioned near the baseline, preparing to hit a tennis ball that is in the air to her right. The ball is closer to the camera than the player, creating a sense of depth. In the background, there is a blue advertisement board with the word 'POLO' on it, and a person standing behind it, slightly", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24496.6, "ram_available_mb": 38344.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24560.2, "ram_available_mb": 38280.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.547}, "power_stats": {"power_gpu_soc_mean_watts": 20.968, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 69.547, "power_watts_avg": 20.968, "energy_joules_est": 233.25, "duration_seconds": 11.124, "sample_count": 95}, "timestamp": "2026-01-25T20:46:14.808005"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9086.137, "latencies_ms": [9086.137], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "A female tennis player is in the middle of a serve on a tennis court with a \"POLO\" advertisement visible in the background. She is wearing a white top, black skirt, and white shoes, and is holding a tennis racket in her right hand.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24455.0, "ram_available_mb": 38385.9, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24493.5, "ram_available_mb": 38347.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.156}, "power_stats": {"power_gpu_soc_mean_watts": 21.771, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 72.156, "power_watts_avg": 21.771, "energy_joules_est": 197.83, "duration_seconds": 9.087, "sample_count": 77}, "timestamp": "2026-01-25T20:46:25.944155"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7748.393, "latencies_ms": [7748.393], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The tennis player is wearing a white top and black skirt, and is in the process of serving the ball. The court is green with a blue boundary line, and there is a blue \"POLO\" sign on the ground.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24493.5, "ram_available_mb": 38347.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24588.4, "ram_available_mb": 38252.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.561}, "power_stats": {"power_gpu_soc_mean_watts": 22.195, "power_cpu_cv_mean_watts": 1.729, "power_sys_5v0_mean_watts": 9.044, "gpu_utilization_percent_mean": 71.561, "power_watts_avg": 22.195, "energy_joules_est": 171.99, "duration_seconds": 7.749, "sample_count": 66}, "timestamp": "2026-01-25T20:46:35.748080"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12227.729, "latencies_ms": [12227.729], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene on a sunny day. Dominating the foreground is a red fire hydrant, its white cap gleaming under the sunlight. The hydrant is situated on a sidewalk, which extends into the distance, inviting pedestrians to take a leisurely stroll. \n\nAbove the hydrant, a yellow", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 24481.6, "ram_available_mb": 38359.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24480.7, "ram_available_mb": 38360.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11579.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.667}, "power_stats": {"power_gpu_soc_mean_watts": 21.944, "power_cpu_cv_mean_watts": 1.8, "power_sys_5v0_mean_watts": 9.109, "gpu_utilization_percent_mean": 73.667, "power_watts_avg": 21.944, "energy_joules_est": 268.34, "duration_seconds": 12.228, "sample_count": 105}, "timestamp": "2026-01-25T20:46:49.996059"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8822.517, "latencies_ms": [8822.517], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "sign: 1, pedestrian: 1, fire hydrant: 1, tree: 1, building: 1, person: 1, wheelchair: 1, sidewalk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24480.7, "ram_available_mb": 38360.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24491.2, "ram_available_mb": 38349.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11613.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.882}, "power_stats": {"power_gpu_soc_mean_watts": 23.624, "power_cpu_cv_mean_watts": 1.507, "power_sys_5v0_mean_watts": 9.106, "gpu_utilization_percent_mean": 76.882, "power_watts_avg": 23.624, "energy_joules_est": 208.44, "duration_seconds": 8.823, "sample_count": 76}, "timestamp": "2026-01-25T20:47:00.836717"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12085.646, "latencies_ms": [12085.646], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a red fire hydrant on the left side of the image, positioned on a sidewalk. Behind the hydrant, a pedestrian crossing sign is mounted on a pole to the right, indicating a crossing area for pedestrians. The background features a residential area with houses and trees, suggesting that the hydrant and sign are located in", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24491.2, "ram_available_mb": 38349.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 24512.8, "ram_available_mb": 38328.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11624.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.369}, "power_stats": {"power_gpu_soc_mean_watts": 22.722, "power_cpu_cv_mean_watts": 2.092, "power_sys_5v0_mean_watts": 9.184, "gpu_utilization_percent_mean": 73.369, "power_watts_avg": 22.722, "energy_joules_est": 274.62, "duration_seconds": 12.086, "sample_count": 103}, "timestamp": "2026-01-25T20:47:14.973724"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8464.673, "latencies_ms": [8464.673], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A bright yellow pedestrian crossing sign is mounted on a pole next to a red fire hydrant on a sidewalk. A person is sitting on a stroller on the grassy area beside the sidewalk.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24512.8, "ram_available_mb": 38328.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24496.6, "ram_available_mb": 38344.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11608.7, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.583}, "power_stats": {"power_gpu_soc_mean_watts": 24.096, "power_cpu_cv_mean_watts": 1.468, "power_sys_5v0_mean_watts": 9.143, "gpu_utilization_percent_mean": 77.583, "power_watts_avg": 24.096, "energy_joules_est": 203.98, "duration_seconds": 8.465, "sample_count": 72}, "timestamp": "2026-01-25T20:47:25.465368"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7614.225, "latencies_ms": [7614.225], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A bright yellow pedestrian crossing sign is mounted on a pole, with a red fire hydrant in the foreground. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24496.6, "ram_available_mb": 38344.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24477.9, "ram_available_mb": 38363.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11606.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.415}, "power_stats": {"power_gpu_soc_mean_watts": 24.356, "power_cpu_cv_mean_watts": 1.454, "power_sys_5v0_mean_watts": 9.223, "gpu_utilization_percent_mean": 77.415, "power_watts_avg": 24.356, "energy_joules_est": 185.47, "duration_seconds": 7.615, "sample_count": 65}, "timestamp": "2026-01-25T20:47:35.134028"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11147.476, "latencies_ms": [11147.476], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a small, compact bathroom bathed in a soft, natural light. Dominating the scene is a pristine white toilet, its lid closed, standing against a stark white wall. The toilet is positioned centrally in the image, drawing the viewer's attention. \n\nTo the left of the toilet, a white", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24477.9, "ram_available_mb": 38363.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24495.3, "ram_available_mb": 38345.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.158}, "power_stats": {"power_gpu_soc_mean_watts": 20.864, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.158, "power_watts_avg": 20.864, "energy_joules_est": 232.59, "duration_seconds": 11.148, "sample_count": 95}, "timestamp": "2026-01-25T20:47:48.315530"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7648.338, "latencies_ms": [7648.338], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "toilet: 1, tank: 1, flush handle: 1, pipe: 1, wall: 1, floor: 1, trash can: 1, pedal: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24495.3, "ram_available_mb": 38345.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24492.1, "ram_available_mb": 38348.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.138}, "power_stats": {"power_gpu_soc_mean_watts": 22.431, "power_cpu_cv_mean_watts": 1.633, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 72.138, "power_watts_avg": 22.431, "energy_joules_est": 171.57, "duration_seconds": 7.649, "sample_count": 65}, "timestamp": "2026-01-25T20:47:57.981889"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9890.305, "latencies_ms": [9890.305], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The toilet is located in the foreground of the image, positioned towards the right side. It is adjacent to a white tank on top and a green container to its left. In the background, there is a white wall that extends to the top of the image, and a white floor that is visible at the bottom.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24492.1, "ram_available_mb": 38348.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24481.5, "ram_available_mb": 38359.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.988}, "power_stats": {"power_gpu_soc_mean_watts": 21.323, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 69.988, "power_watts_avg": 21.323, "energy_joules_est": 210.9, "duration_seconds": 9.891, "sample_count": 84}, "timestamp": "2026-01-25T20:48:09.928325"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9731.406, "latencies_ms": [9731.406], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image depicts a small, cramped bathroom with a toilet and a trash can. The toilet is white and appears to be in a poor condition, with a broken seat and a missing lid. The trash can is green and is placed next to the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24481.5, "ram_available_mb": 38359.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24478.4, "ram_available_mb": 38362.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.542}, "power_stats": {"power_gpu_soc_mean_watts": 21.56, "power_cpu_cv_mean_watts": 1.809, "power_sys_5v0_mean_watts": 8.932, "gpu_utilization_percent_mean": 71.542, "power_watts_avg": 21.56, "energy_joules_est": 209.82, "duration_seconds": 9.732, "sample_count": 83}, "timestamp": "2026-01-25T20:48:21.699392"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6869.265, "latencies_ms": [6869.265], "images_per_second": 0.146, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image shows a small, compact bathroom with a white toilet and a white tank. The walls are painted in a light color, and there is a brown carpet on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24478.4, "ram_available_mb": 38362.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24575.5, "ram_available_mb": 38265.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.636, "power_cpu_cv_mean_watts": 1.622, "power_sys_5v0_mean_watts": 9.055, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 22.636, "energy_joules_est": 155.51, "duration_seconds": 6.87, "sample_count": 59}, "timestamp": "2026-01-25T20:48:30.594648"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11136.494, "latencies_ms": [11136.494], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a lone skier is making their way down a snowy mountain slope. The skier is wearing a red jacket and black pants, and is holding ski poles in their hands. They are skiing on a trail that is covered in fresh snow, and the trail is marked with ski tracks. The skier is moving towards the right side of the image", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24453.2, "ram_available_mb": 38387.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24513.7, "ram_available_mb": 38327.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.379}, "power_stats": {"power_gpu_soc_mean_watts": 20.895, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.929, "gpu_utilization_percent_mean": 69.379, "power_watts_avg": 20.895, "energy_joules_est": 232.71, "duration_seconds": 11.137, "sample_count": 95}, "timestamp": "2026-01-25T20:48:43.796935"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8279.263, "latencies_ms": [8279.263], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "mountain: 1, ski tracks: multiple, skier: 1, red jacket: 1, black pants: 1, ski poles: 2, snow: multiple, blue sky: 1, trees: 0", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24513.7, "ram_available_mb": 38327.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24497.2, "ram_available_mb": 38343.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.643}, "power_stats": {"power_gpu_soc_mean_watts": 22.225, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 72.643, "power_watts_avg": 22.225, "energy_joules_est": 184.02, "duration_seconds": 8.28, "sample_count": 70}, "timestamp": "2026-01-25T20:48:54.125600"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8672.747, "latencies_ms": [8672.747], "images_per_second": 0.115, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "In the foreground, there is a person skiing on the left side of the image, moving towards the right. The background features a snowy mountain with a clear blue sky above it. The mountain appears to be quite large and is located further away from the skier.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24497.2, "ram_available_mb": 38343.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24550.4, "ram_available_mb": 38290.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.554}, "power_stats": {"power_gpu_soc_mean_watts": 21.722, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 70.554, "power_watts_avg": 21.722, "energy_joules_est": 188.4, "duration_seconds": 8.673, "sample_count": 74}, "timestamp": "2026-01-25T20:49:04.832859"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6810.418, "latencies_ms": [6810.418], "images_per_second": 0.147, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A person in a red jacket and black pants is skiing down a snowy mountain slope. The sky is clear and blue, and the mountain appears to be quite steep.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24550.4, "ram_available_mb": 38290.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24564.1, "ram_available_mb": 38276.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.397}, "power_stats": {"power_gpu_soc_mean_watts": 22.935, "power_cpu_cv_mean_watts": 1.56, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 73.397, "power_watts_avg": 22.935, "energy_joules_est": 156.21, "duration_seconds": 6.811, "sample_count": 58}, "timestamp": "2026-01-25T20:49:13.655833"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7533.955, "latencies_ms": [7533.955], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image features a person skiing on a snowy mountain under a clear blue sky. The skier is wearing a bright red jacket and black pants, and the snow appears to be fresh and powdery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24564.1, "ram_available_mb": 38276.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24612.8, "ram_available_mb": 38228.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.578}, "power_stats": {"power_gpu_soc_mean_watts": 22.284, "power_cpu_cv_mean_watts": 1.689, "power_sys_5v0_mean_watts": 9.06, "gpu_utilization_percent_mean": 72.578, "power_watts_avg": 22.284, "energy_joules_est": 167.9, "duration_seconds": 7.535, "sample_count": 64}, "timestamp": "2026-01-25T20:49:23.238864"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11104.795, "latencies_ms": [11104.795], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is skiing down a snowy hill, wearing a red and white jacket and black pants. He is holding two ski poles and appears to be enjoying the winter activity. There are other people in the background, also skiing and snowboarding, making the scene lively and active. The snow-covered hill and the presence of multiple", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24612.8, "ram_available_mb": 38228.1, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24481.8, "ram_available_mb": 38359.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.968}, "power_stats": {"power_gpu_soc_mean_watts": 20.936, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.968, "power_watts_avg": 20.936, "energy_joules_est": 232.5, "duration_seconds": 11.105, "sample_count": 94}, "timestamp": "2026-01-25T20:49:36.375717"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9546.905, "latencies_ms": [9546.905], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Skier: 1\n2. Trees: numerous\n3. Mountain: 1\n4. Snow: covering ground and trees\n5. Skis: 2\n6. Pole: 1\n7. Jacket: 1\n8. Hat: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24481.8, "ram_available_mb": 38359.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24585.1, "ram_available_mb": 38255.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.222}, "power_stats": {"power_gpu_soc_mean_watts": 21.63, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 72.222, "power_watts_avg": 21.63, "energy_joules_est": 206.51, "duration_seconds": 9.547, "sample_count": 81}, "timestamp": "2026-01-25T20:49:47.945722"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10145.329, "latencies_ms": [10145.329], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, a skier wearing a red jacket with the number 30 is skiing on a snowy slope. In the background, there are more trees covered in snow and another person can be seen skiing further down the slope. The skier is closer to the camera than the other person and the trees.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24585.1, "ram_available_mb": 38255.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24513.6, "ram_available_mb": 38327.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.081}, "power_stats": {"power_gpu_soc_mean_watts": 21.328, "power_cpu_cv_mean_watts": 1.872, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 70.081, "power_watts_avg": 21.328, "energy_joules_est": 216.39, "duration_seconds": 10.146, "sample_count": 86}, "timestamp": "2026-01-25T20:50:00.104232"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6813.397, "latencies_ms": [6813.397], "images_per_second": 0.147, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A person is cross-country skiing in a snowy forest. The skier is wearing a red jacket with the number 30 on it and a grey beanie.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24513.6, "ram_available_mb": 38327.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24575.8, "ram_available_mb": 38265.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.879}, "power_stats": {"power_gpu_soc_mean_watts": 23.115, "power_cpu_cv_mean_watts": 1.574, "power_sys_5v0_mean_watts": 9.021, "gpu_utilization_percent_mean": 73.879, "power_watts_avg": 23.115, "energy_joules_est": 157.51, "duration_seconds": 6.814, "sample_count": 58}, "timestamp": "2026-01-25T20:50:08.974169"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8356.674, "latencies_ms": [8356.674], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a snowy landscape with a person skiing. The skier is wearing a red jacket, black pants, and a gray beanie. The trees in the background are covered in snow, and the sky is overcast.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24575.8, "ram_available_mb": 38265.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24506.6, "ram_available_mb": 38334.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.197}, "power_stats": {"power_gpu_soc_mean_watts": 21.968, "power_cpu_cv_mean_watts": 1.743, "power_sys_5v0_mean_watts": 9.01, "gpu_utilization_percent_mean": 71.197, "power_watts_avg": 21.968, "energy_joules_est": 183.59, "duration_seconds": 8.357, "sample_count": 71}, "timestamp": "2026-01-25T20:50:19.368539"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11162.724, "latencies_ms": [11162.724], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility in a workspace. Dominating the scene is a sleek, silver monitor, its screen alive with the vibrant colors of a PowerPoint presentation. The presentation itself is a study in simplicity, featuring a black and white image of a desk, complete with a keyboard and mouse, against a stark white background. The monitor is perched", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24506.6, "ram_available_mb": 38334.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24584.0, "ram_available_mb": 38256.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.358}, "power_stats": {"power_gpu_soc_mean_watts": 20.814, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 69.358, "power_watts_avg": 20.814, "energy_joules_est": 232.35, "duration_seconds": 11.163, "sample_count": 95}, "timestamp": "2026-01-25T20:50:32.563209"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7059.388, "latencies_ms": [7059.388], "images_per_second": 0.142, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "monitor: 1, keyboard: 1, mouse: 1, desk: 1, wall: 1, screen: 1, image: 1, text: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24461.8, "ram_available_mb": 38379.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24583.1, "ram_available_mb": 38257.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.9}, "power_stats": {"power_gpu_soc_mean_watts": 22.93, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 9.037, "gpu_utilization_percent_mean": 73.9, "power_watts_avg": 22.93, "energy_joules_est": 161.88, "duration_seconds": 7.06, "sample_count": 60}, "timestamp": "2026-01-25T20:50:41.656690"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9022.767, "latencies_ms": [9022.767], "images_per_second": 0.111, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The computer monitor is positioned on the left side of the image, displaying a workspace on its screen. In the foreground, there is a black keyboard and mouse placed on a desk. The background is a plain wall with the word 'WORKPLACE' written vertically.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24504.6, "ram_available_mb": 38336.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24505.4, "ram_available_mb": 38335.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.779}, "power_stats": {"power_gpu_soc_mean_watts": 21.727, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 71.779, "power_watts_avg": 21.727, "energy_joules_est": 196.05, "duration_seconds": 9.023, "sample_count": 77}, "timestamp": "2026-01-25T20:50:52.734798"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8406.708, "latencies_ms": [8406.708], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a workspace with a desktop computer, a keyboard, and a mouse placed on a desk. The computer screen displays a black and white image of a workspace with a keyboard and mouse, creating a visual representation of the actual workspace.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24505.4, "ram_available_mb": 38335.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24503.5, "ram_available_mb": 38337.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.486}, "power_stats": {"power_gpu_soc_mean_watts": 22.103, "power_cpu_cv_mean_watts": 1.724, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 71.486, "power_watts_avg": 22.103, "energy_joules_est": 185.83, "duration_seconds": 8.407, "sample_count": 72}, "timestamp": "2026-01-25T20:51:03.174457"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7270.952, "latencies_ms": [7270.952], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image is in black and white, featuring a workplace setting with a computer monitor displaying a desktop wallpaper. The monitor is placed on a white desk with a black keyboard and mouse in front of it.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24503.5, "ram_available_mb": 38337.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 24583.6, "ram_available_mb": 38257.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.016}, "power_stats": {"power_gpu_soc_mean_watts": 22.586, "power_cpu_cv_mean_watts": 1.944, "power_sys_5v0_mean_watts": 9.068, "gpu_utilization_percent_mean": 73.016, "power_watts_avg": 22.586, "energy_joules_est": 164.24, "duration_seconds": 7.272, "sample_count": 62}, "timestamp": "2026-01-25T20:51:12.475845"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11108.542, "latencies_ms": [11108.542], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is sitting at a table in a restaurant, holding a plate with a bagel on it. She is smiling and appears to be enjoying her meal. The table is set with a cup, a sandwich, and a bag. There are other people in the background, possibly waiting for their orders or enjoying their meals as well. The restaurant has", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24583.6, "ram_available_mb": 38257.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24498.9, "ram_available_mb": 38342.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.625}, "power_stats": {"power_gpu_soc_mean_watts": 20.857, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 69.625, "power_watts_avg": 20.857, "energy_joules_est": 231.7, "duration_seconds": 11.109, "sample_count": 96}, "timestamp": "2026-01-25T20:51:25.627310"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9297.89, "latencies_ms": [9297.89], "images_per_second": 0.108, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Woman: 1\n\n- Bagel: 1\n\n- Coffee cup: 1\n\n- Table: 1\n\n- Window: 1\n\n- Train car: 1\n\n- Passengers: 1\n\n- Sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24498.9, "ram_available_mb": 38342.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24588.0, "ram_available_mb": 38252.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.449}, "power_stats": {"power_gpu_soc_mean_watts": 21.689, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 71.449, "power_watts_avg": 21.689, "energy_joules_est": 201.68, "duration_seconds": 9.299, "sample_count": 78}, "timestamp": "2026-01-25T20:51:36.951494"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10057.078, "latencies_ms": [10057.078], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, a person is holding a bagel in their left hand, with a cup on the table in front of them. In the background, there are other people seated, and a window shows a view of greenery outside. The person with the bagel is seated closer to the camera than the other individuals.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24465.8, "ram_available_mb": 38375.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24589.2, "ram_available_mb": 38251.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.106}, "power_stats": {"power_gpu_soc_mean_watts": 21.295, "power_cpu_cv_mean_watts": 1.847, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 70.106, "power_watts_avg": 21.295, "energy_joules_est": 214.18, "duration_seconds": 10.058, "sample_count": 85}, "timestamp": "2026-01-25T20:51:49.020893"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6636.2, "latencies_ms": [6636.2], "images_per_second": 0.151, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A woman is sitting at a table in a train, holding a bagel in her hand. There are other passengers in the background, and the train appears to be in motion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24589.2, "ram_available_mb": 38251.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24510.4, "ram_available_mb": 38330.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.232}, "power_stats": {"power_gpu_soc_mean_watts": 23.179, "power_cpu_cv_mean_watts": 1.509, "power_sys_5v0_mean_watts": 9.007, "gpu_utilization_percent_mean": 73.232, "power_watts_avg": 23.179, "energy_joules_est": 153.84, "duration_seconds": 6.637, "sample_count": 56}, "timestamp": "2026-01-25T20:51:57.693943"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11108.009, "latencies_ms": [11108.009], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The image shows a person holding a bagel with a light-colored interior, possibly cream cheese, in a setting with warm indoor lighting. The person is wearing a purple striped shirt, and there is a window in the background showing a blurred view of greenery and a building, suggesting it might be a sunny day outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24510.4, "ram_available_mb": 38330.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24499.7, "ram_available_mb": 38341.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.645}, "power_stats": {"power_gpu_soc_mean_watts": 20.881, "power_cpu_cv_mean_watts": 1.907, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 69.645, "power_watts_avg": 20.881, "energy_joules_est": 231.96, "duration_seconds": 11.109, "sample_count": 93}, "timestamp": "2026-01-25T20:52:10.817565"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11137.037, "latencies_ms": [11137.037], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two zebras standing in a grassy field. They are grazing on the grass, with one zebra on the left and the other on the right. The zebras are facing each other, and their heads are close together as they eat. The field is lush and green, providing a natural environment for the zebras to graze.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24499.7, "ram_available_mb": 38341.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24505.2, "ram_available_mb": 38335.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.905, "power_cpu_cv_mean_watts": 1.939, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 70.0, "power_watts_avg": 20.905, "energy_joules_est": 232.83, "duration_seconds": 11.138, "sample_count": 95}, "timestamp": "2026-01-25T20:52:24.000289"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7158.725, "latencies_ms": [7158.725], "images_per_second": 0.14, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "zebra: 3, grass: many, dirt: patches, flowers: few, trees: none visible, fence: none visible, sky: not visible, water: not visible", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24505.2, "ram_available_mb": 38335.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24575.6, "ram_available_mb": 38265.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.934}, "power_stats": {"power_gpu_soc_mean_watts": 22.763, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.018, "gpu_utilization_percent_mean": 73.934, "power_watts_avg": 22.763, "energy_joules_est": 162.97, "duration_seconds": 7.159, "sample_count": 61}, "timestamp": "2026-01-25T20:52:33.211110"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9105.441, "latencies_ms": [9105.441], "images_per_second": 0.11, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "In the foreground, there are two zebras standing close to each other, with one slightly in front of the other. They are both facing the same direction, grazing on the grass. In the background, there is another zebra partially visible, further away from the main subjects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24497.2, "ram_available_mb": 38343.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24506.2, "ram_available_mb": 38334.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.688}, "power_stats": {"power_gpu_soc_mean_watts": 21.627, "power_cpu_cv_mean_watts": 1.805, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 70.688, "power_watts_avg": 21.627, "energy_joules_est": 196.94, "duration_seconds": 9.106, "sample_count": 77}, "timestamp": "2026-01-25T20:52:44.347967"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6946.124, "latencies_ms": [6946.124], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "Two zebras are grazing in a grassy field with other zebras in the background. The zebras are standing close together and appear to be enjoying their meal.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24506.2, "ram_available_mb": 38334.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24516.9, "ram_available_mb": 38324.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.763}, "power_stats": {"power_gpu_soc_mean_watts": 23.006, "power_cpu_cv_mean_watts": 1.575, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 74.763, "power_watts_avg": 23.006, "energy_joules_est": 159.82, "duration_seconds": 6.947, "sample_count": 59}, "timestamp": "2026-01-25T20:52:53.355982"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6987.432, "latencies_ms": [6987.432], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The zebras are grazing in a lush green field with their distinctive black and white stripes clearly visible. The lighting is natural and bright, suggesting it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24455.3, "ram_available_mb": 38385.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24482.1, "ram_available_mb": 38358.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.797}, "power_stats": {"power_gpu_soc_mean_watts": 22.652, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.08, "gpu_utilization_percent_mean": 72.797, "power_watts_avg": 22.652, "energy_joules_est": 158.29, "duration_seconds": 6.988, "sample_count": 59}, "timestamp": "2026-01-25T20:53:02.361183"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11151.316, "latencies_ms": [11151.316], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a busy street scene with several people riding bicycles and motorcycles. There are two men riding bicycles, one in the foreground and another further back. A man is riding a motorcycle, and another person is riding a scooter. \n\nIn addition to the riders, there are two more people on the street", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24482.1, "ram_available_mb": 38358.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24504.1, "ram_available_mb": 38336.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.553}, "power_stats": {"power_gpu_soc_mean_watts": 20.855, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 69.553, "power_watts_avg": 20.855, "energy_joules_est": 232.57, "duration_seconds": 11.152, "sample_count": 94}, "timestamp": "2026-01-25T20:53:15.584847"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9174.364, "latencies_ms": [9174.364], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- Bicycle: 1\n\n- Motorcycle: 2\n\n- Plant: 1\n\n- Storefront: 1\n\n- Sign: 1\n\n- Helmet: 1\n\n- Chair: 1\n\n- Cart: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24504.1, "ram_available_mb": 38336.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24496.4, "ram_available_mb": 38344.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.291}, "power_stats": {"power_gpu_soc_mean_watts": 21.793, "power_cpu_cv_mean_watts": 1.774, "power_sys_5v0_mean_watts": 8.935, "gpu_utilization_percent_mean": 72.291, "power_watts_avg": 21.793, "energy_joules_est": 199.95, "duration_seconds": 9.175, "sample_count": 79}, "timestamp": "2026-01-25T20:53:26.815439"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11110.039, "latencies_ms": [11110.039], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a green bicycle with a person riding it, positioned to the left of the frame. Behind the bicycle, there is a red motorcycle lying on the ground, and further back, there is a blue scooter with a person riding it. To the right of the scooter, there is another person standing and holding", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24496.4, "ram_available_mb": 38344.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24561.1, "ram_available_mb": 38279.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.427}, "power_stats": {"power_gpu_soc_mean_watts": 20.973, "power_cpu_cv_mean_watts": 1.919, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 69.427, "power_watts_avg": 20.973, "energy_joules_est": 233.02, "duration_seconds": 11.111, "sample_count": 96}, "timestamp": "2026-01-25T20:53:39.947595"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8298.794, "latencies_ms": [8298.794], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image depicts a street scene with two individuals riding bicycles and a motorcycle. In the background, there is a store with a Coca-Cola sign and a person wearing a helmet riding a motorcycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24457.4, "ram_available_mb": 38383.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24552.4, "ram_available_mb": 38288.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.071}, "power_stats": {"power_gpu_soc_mean_watts": 22.196, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 73.071, "power_watts_avg": 22.196, "energy_joules_est": 184.21, "duration_seconds": 8.299, "sample_count": 70}, "timestamp": "2026-01-25T20:53:50.278669"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9872.02, "latencies_ms": [9872.02], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image features a vibrant green bicycle with a basket in the foreground, set against a backdrop of a Coca-Cola store with red and white signage. The weather appears to be overcast, as the lighting is diffused and there are no harsh shadows visible on the ground.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24448.8, "ram_available_mb": 38392.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24550.6, "ram_available_mb": 38290.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.81}, "power_stats": {"power_gpu_soc_mean_watts": 21.276, "power_cpu_cv_mean_watts": 1.85, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 69.81, "power_watts_avg": 21.276, "energy_joules_est": 210.05, "duration_seconds": 9.873, "sample_count": 84}, "timestamp": "2026-01-25T20:54:02.206743"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11121.873, "latencies_ms": [11121.873], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a tennis match in progress on a grass court. Two players are actively engaged in the game, with one player holding a tennis racket and the other preparing to hit the ball. The tennis ball is visible in the air, and the players are focused on their game.\n\nThere are several chairs placed around the court, likely for the players to rest or for", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 24550.6, "ram_available_mb": 38290.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24480.0, "ram_available_mb": 38360.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.436}, "power_stats": {"power_gpu_soc_mean_watts": 20.914, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.436, "power_watts_avg": 20.914, "energy_joules_est": 232.62, "duration_seconds": 11.122, "sample_count": 94}, "timestamp": "2026-01-25T20:54:15.374077"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7939.991, "latencies_ms": [7939.991], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "audience: numerous\nplayer: 2\nball: 1\nnet: 1\ncourt: 1\nchairs: 2\nbags: 2\ncamera operators: 2\nequipment: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24480.0, "ram_available_mb": 38360.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24571.9, "ram_available_mb": 38269.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.493}, "power_stats": {"power_gpu_soc_mean_watts": 22.387, "power_cpu_cv_mean_watts": 1.667, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 73.493, "power_watts_avg": 22.387, "energy_joules_est": 177.77, "duration_seconds": 7.941, "sample_count": 67}, "timestamp": "2026-01-25T20:54:25.330264"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11149.718, "latencies_ms": [11149.718], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a tennis player dressed in white is positioned near the baseline, holding a tennis racket and preparing to hit the ball. The ball is in the air, slightly above and in front of the player, indicating a forehand shot is being played. In the background, another player is seen in a crouched position, ready to return the shot. The audience", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24571.9, "ram_available_mb": 38269.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24461.1, "ram_available_mb": 38379.8, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.298}, "power_stats": {"power_gpu_soc_mean_watts": 20.952, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.298, "power_watts_avg": 20.952, "energy_joules_est": 233.62, "duration_seconds": 11.15, "sample_count": 94}, "timestamp": "2026-01-25T20:54:38.501385"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8900.101, "latencies_ms": [8900.101], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image captures a tennis match in progress on a grass court, with a player in white attire actively engaged in the game, holding a tennis racket and preparing to hit the ball. Spectators are seated in the stands, watching the match intently.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24461.1, "ram_available_mb": 38379.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24463.9, "ram_available_mb": 38377.0, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.893}, "power_stats": {"power_gpu_soc_mean_watts": 21.722, "power_cpu_cv_mean_watts": 1.762, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 70.893, "power_watts_avg": 21.722, "energy_joules_est": 193.34, "duration_seconds": 8.901, "sample_count": 75}, "timestamp": "2026-01-25T20:54:49.439932"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7541.181, "latencies_ms": [7541.181], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The tennis court is surrounded by green seats, and the players are wearing white and red attire. The lighting appears to be natural, likely from the sun, as there are no visible artificial light sources on the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24463.9, "ram_available_mb": 38377.0, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24483.8, "ram_available_mb": 38357.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.047}, "power_stats": {"power_gpu_soc_mean_watts": 22.233, "power_cpu_cv_mean_watts": 1.689, "power_sys_5v0_mean_watts": 9.029, "gpu_utilization_percent_mean": 72.047, "power_watts_avg": 22.233, "energy_joules_est": 167.68, "duration_seconds": 7.542, "sample_count": 64}, "timestamp": "2026-01-25T20:54:59.022361"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11127.816, "latencies_ms": [11127.816], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a cozy living room with a couch positioned near a large window. The room is adorned with several potted plants, adding a touch of greenery to the space. A TV is placed on the left side of the room, and a remote control can be seen nearby. A small orange rug is placed in front of the couch, providing a warm and inv", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24483.8, "ram_available_mb": 38357.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 7.4, "ram_used_mb": 24571.8, "ram_available_mb": 38269.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.421}, "power_stats": {"power_gpu_soc_mean_watts": 20.92, "power_cpu_cv_mean_watts": 2.087, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.421, "power_watts_avg": 20.92, "energy_joules_est": 232.81, "duration_seconds": 11.128, "sample_count": 95}, "timestamp": "2026-01-25T20:55:12.201853"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8752.796, "latencies_ms": [8752.796], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Couch: 1\n- Plant: 3\n- Television: 1\n- Table: 1\n- Remote control: 1\n- Curtains: 2\n- Rug: 1\n- Pillow: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24449.6, "ram_available_mb": 38391.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24543.0, "ram_available_mb": 38297.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.919}, "power_stats": {"power_gpu_soc_mean_watts": 21.9, "power_cpu_cv_mean_watts": 1.753, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 70.919, "power_watts_avg": 21.9, "energy_joules_est": 191.7, "duration_seconds": 8.753, "sample_count": 74}, "timestamp": "2026-01-25T20:55:23.013140"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10771.667, "latencies_ms": [10771.667], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground, there is a brown sofa positioned against the wall on the right side of the image. In the background, there is a large window with white curtains, and a potted plant is placed on the windowsill. The sofa is located near the window, providing a comfortable seating area with a view of the outdoors.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24481.2, "ram_available_mb": 38359.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24509.9, "ram_available_mb": 38331.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.837}, "power_stats": {"power_gpu_soc_mean_watts": 21.03, "power_cpu_cv_mean_watts": 1.919, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 69.837, "power_watts_avg": 21.03, "energy_joules_est": 226.54, "duration_seconds": 10.772, "sample_count": 92}, "timestamp": "2026-01-25T20:55:35.838859"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8526.974, "latencies_ms": [8526.974], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a brown couch, a wooden coffee table, and a flat-screen TV mounted on the wall. There are several potted plants on the windowsill, adding a touch of greenery to the space.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24509.9, "ram_available_mb": 38331.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24508.7, "ram_available_mb": 38332.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.611}, "power_stats": {"power_gpu_soc_mean_watts": 22.083, "power_cpu_cv_mean_watts": 1.702, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 72.611, "power_watts_avg": 22.083, "energy_joules_est": 188.31, "duration_seconds": 8.528, "sample_count": 72}, "timestamp": "2026-01-25T20:55:46.380800"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8908.266, "latencies_ms": [8908.266], "images_per_second": 0.112, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The room is well-lit with natural light coming through the large window with sheer curtains. The furniture includes a brown sofa with a white pillow, a wooden entertainment center with a flat-screen TV, and a wooden coffee table with various items on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24508.7, "ram_available_mb": 38332.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24455.4, "ram_available_mb": 38385.5, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.197}, "power_stats": {"power_gpu_soc_mean_watts": 21.566, "power_cpu_cv_mean_watts": 1.818, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 70.197, "power_watts_avg": 21.566, "energy_joules_est": 192.13, "duration_seconds": 8.909, "sample_count": 76}, "timestamp": "2026-01-25T20:55:57.339965"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11147.844, "latencies_ms": [11147.844], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing on a tennis court, holding a tennis racket in her right hand. She is wearing a red dress and a white visor, which suggests that she is a tennis player. The court appears to be a clay court, as it has a reddish-brown color.\n\nThere are a few other people in the background, but", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24455.4, "ram_available_mb": 38385.5, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24568.5, "ram_available_mb": 38272.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.253}, "power_stats": {"power_gpu_soc_mean_watts": 20.889, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.931, "gpu_utilization_percent_mean": 69.253, "power_watts_avg": 20.889, "energy_joules_est": 232.88, "duration_seconds": 11.149, "sample_count": 95}, "timestamp": "2026-01-25T20:56:10.538135"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9315.211, "latencies_ms": [9315.211], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Player: 1\n\n- Racket: 1\n\n- Dress: 1\n\n- Ground: 1\n\n- Sand: 1\n\n- Net: 1\n\n- Shoe: 2\n\n- Towel: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24568.2, "ram_available_mb": 38272.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24569.8, "ram_available_mb": 38271.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.608}, "power_stats": {"power_gpu_soc_mean_watts": 21.503, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.929, "gpu_utilization_percent_mean": 70.608, "power_watts_avg": 21.503, "energy_joules_est": 200.32, "duration_seconds": 9.316, "sample_count": 79}, "timestamp": "2026-01-25T20:56:21.893341"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9775.878, "latencies_ms": [9775.878], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "In the foreground, a woman dressed in a red tennis dress and white visor is holding a tennis racket and appears to be walking on a clay tennis court. In the background, there are two green tennis courts with a crocodile logo on them. The woman is closer to the camera than the green courts.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24447.6, "ram_available_mb": 38393.3, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24579.9, "ram_available_mb": 38261.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.627}, "power_stats": {"power_gpu_soc_mean_watts": 21.369, "power_cpu_cv_mean_watts": 1.853, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 70.627, "power_watts_avg": 21.369, "energy_joules_est": 208.91, "duration_seconds": 9.776, "sample_count": 83}, "timestamp": "2026-01-25T20:56:33.715223"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9600.32, "latencies_ms": [9600.32], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "A woman in a red dress and white visor is standing on a clay tennis court, holding a tennis racket and looking towards the right side of the image. There are a few people in the background, and a green mat with a crocodile logo is visible on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24579.9, "ram_available_mb": 38261.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24580.9, "ram_available_mb": 38260.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.63}, "power_stats": {"power_gpu_soc_mean_watts": 21.573, "power_cpu_cv_mean_watts": 1.795, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 70.63, "power_watts_avg": 21.573, "energy_joules_est": 207.12, "duration_seconds": 9.601, "sample_count": 81}, "timestamp": "2026-01-25T20:56:45.334518"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7643.442, "latencies_ms": [7643.442], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The tennis player is wearing a bright orange dress and a white visor, which stands out against the red clay court. The court appears to be dry and dusty, indicating that the weather might be sunny and hot.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24580.9, "ram_available_mb": 38260.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24483.6, "ram_available_mb": 38357.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.8}, "power_stats": {"power_gpu_soc_mean_watts": 22.231, "power_cpu_cv_mean_watts": 1.688, "power_sys_5v0_mean_watts": 9.034, "gpu_utilization_percent_mean": 71.8, "power_watts_avg": 22.231, "energy_joules_est": 169.94, "duration_seconds": 7.644, "sample_count": 65}, "timestamp": "2026-01-25T20:56:55.037428"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11115.808, "latencies_ms": [11115.808], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a busy city street filled with numerous cars and people. There are at least 13 cars visible, with some parked along the side of the road and others driving down the street. The street is bustling with activity, as people can be seen walking and engaging in various activities.\n\nThere are at least 11 people in the scene, some", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24483.6, "ram_available_mb": 38357.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24517.6, "ram_available_mb": 38323.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.617}, "power_stats": {"power_gpu_soc_mean_watts": 20.94, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 69.617, "power_watts_avg": 20.94, "energy_joules_est": 232.78, "duration_seconds": 11.116, "sample_count": 94}, "timestamp": "2026-01-25T20:57:08.207207"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7299.451, "latencies_ms": [7299.451], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "car: 10\nbuilding: 5\nsign: 2\nperson: 5\nwindow: 15\nstreet: 1\nairplane: 1\ntree: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24517.6, "ram_available_mb": 38323.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24569.7, "ram_available_mb": 38271.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.371}, "power_stats": {"power_gpu_soc_mean_watts": 22.709, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 74.371, "power_watts_avg": 22.709, "energy_joules_est": 165.78, "duration_seconds": 7.3, "sample_count": 62}, "timestamp": "2026-01-25T20:57:17.565136"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11095.914, "latencies_ms": [11095.914], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are several cars parked along the street, with one white car in the left foreground and others in the background. The buildings in the background are taller and appear to be further away, with the tallest building on the right side of the image. The street sign is located in the upper left corner of the image, indicating its position relative to the other objects", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24569.7, "ram_available_mb": 38271.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24503.7, "ram_available_mb": 38337.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.266}, "power_stats": {"power_gpu_soc_mean_watts": 21.043, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 70.266, "power_watts_avg": 21.043, "energy_joules_est": 233.5, "duration_seconds": 11.097, "sample_count": 94}, "timestamp": "2026-01-25T20:57:30.708243"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7949.973, "latencies_ms": [7949.973], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image depicts a busy street scene in a city with multiple cars parked and driving along the road. There are several buildings with storefronts, including a restaurant and a bar, and people are sitting at outdoor tables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24503.7, "ram_available_mb": 38337.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24558.3, "ram_available_mb": 38282.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.91}, "power_stats": {"power_gpu_soc_mean_watts": 22.196, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 71.91, "power_watts_avg": 22.196, "energy_joules_est": 176.47, "duration_seconds": 7.951, "sample_count": 67}, "timestamp": "2026-01-25T20:57:40.694175"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7653.7, "latencies_ms": [7653.7], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image shows a cloudy day with overcast skies, casting a soft light over the scene. The buildings are primarily in shades of red, brown, and green, with a mix of modern and older architectural styles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24453.1, "ram_available_mb": 38387.8, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24573.9, "ram_available_mb": 38267.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.969}, "power_stats": {"power_gpu_soc_mean_watts": 22.184, "power_cpu_cv_mean_watts": 1.688, "power_sys_5v0_mean_watts": 9.024, "gpu_utilization_percent_mean": 71.969, "power_watts_avg": 22.184, "energy_joules_est": 169.8, "duration_seconds": 7.654, "sample_count": 65}, "timestamp": "2026-01-25T20:57:50.365789"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11098.478, "latencies_ms": [11098.478], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the image, a woman is playing tennis on a court. She is wearing a pink outfit and is in the process of hitting a tennis ball with her racket. The woman is positioned near the center of the court, and the tennis ball is located to her right. The scene captures the intensity and focus of the player as she prepares to make her shot.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24450.3, "ram_available_mb": 38390.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24551.9, "ram_available_mb": 38289.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.298}, "power_stats": {"power_gpu_soc_mean_watts": 20.957, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 69.298, "power_watts_avg": 20.957, "energy_joules_est": 232.6, "duration_seconds": 11.099, "sample_count": 94}, "timestamp": "2026-01-25T20:58:03.509160"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8613.209, "latencies_ms": [8613.209], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "tennis player: 1\ntennis racket: 1\ntennis ball: 1\ngreen court: 1\nwhite line: 1\nred outfit: 1\nwhite visor: 1\nwhite sneakers: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24448.2, "ram_available_mb": 38392.7, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24504.3, "ram_available_mb": 38336.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.055}, "power_stats": {"power_gpu_soc_mean_watts": 21.982, "power_cpu_cv_mean_watts": 1.728, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 73.055, "power_watts_avg": 21.982, "energy_joules_est": 189.35, "duration_seconds": 8.614, "sample_count": 73}, "timestamp": "2026-01-25T20:58:14.140516"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9101.269, "latencies_ms": [9101.269], "images_per_second": 0.11, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The tennis player is positioned in the foreground on the left side of the image, preparing to hit the tennis ball that is near the far right side of the image. The tennis court has a blue surface with white boundary lines, and the player is wearing a pink outfit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24504.3, "ram_available_mb": 38336.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24574.0, "ram_available_mb": 38266.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.351}, "power_stats": {"power_gpu_soc_mean_watts": 21.6, "power_cpu_cv_mean_watts": 1.8, "power_sys_5v0_mean_watts": 9.007, "gpu_utilization_percent_mean": 71.351, "power_watts_avg": 21.6, "energy_joules_est": 196.6, "duration_seconds": 9.102, "sample_count": 77}, "timestamp": "2026-01-25T20:58:25.278860"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9092.348, "latencies_ms": [9092.348], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "A female tennis player is in the middle of a forehand swing, wearing a pink outfit and white visor, on a tennis court with a green surface and a blue boundary line. A tennis ball is visible in the air, indicating that she is about to hit it.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24574.0, "ram_available_mb": 38266.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24489.4, "ram_available_mb": 38351.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.494}, "power_stats": {"power_gpu_soc_mean_watts": 21.153, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 71.494, "power_watts_avg": 21.153, "energy_joules_est": 192.34, "duration_seconds": 9.093, "sample_count": 77}, "timestamp": "2026-01-25T20:58:36.401653"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7234.289, "latencies_ms": [7234.289], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The tennis player is wearing a pink outfit and white shoes, and is holding a yellow tennis racket. The court is blue and green, and there is a yellow tennis ball in the air.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24489.4, "ram_available_mb": 38351.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24591.0, "ram_available_mb": 38249.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.903}, "power_stats": {"power_gpu_soc_mean_watts": 22.328, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 9.019, "gpu_utilization_percent_mean": 71.903, "power_watts_avg": 22.328, "energy_joules_est": 161.54, "duration_seconds": 7.235, "sample_count": 62}, "timestamp": "2026-01-25T20:58:45.670905"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12332.89, "latencies_ms": [12332.89], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility in a lush, green landscape. Dominating the scene is a train, painted in hues of blue and orange, with a white roof. The train is stationary, resting on a track that cuts through the verdant greenery. The track itself is nestled amidst a field of tall grass and bushes, adding a", "error": null, "sys_before": {"cpu_percent": 8.1, "ram_used_mb": 24452.3, "ram_available_mb": 38388.6, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24472.8, "ram_available_mb": 38368.1, "ram_percent": 38.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.4}, "power_stats": {"power_gpu_soc_mean_watts": 22.833, "power_cpu_cv_mean_watts": 1.766, "power_sys_5v0_mean_watts": 9.196, "gpu_utilization_percent_mean": 73.4, "power_watts_avg": 22.833, "energy_joules_est": 281.61, "duration_seconds": 12.334, "sample_count": 105}, "timestamp": "2026-01-25T20:59:00.042944"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9393.247, "latencies_ms": [9393.247], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "Train: 1\nTrack: 1\nRailroad ties: 1\nGrass: 1\nWeeds: 1\nLeaves: 1\nTrees: 1\nBushes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24472.8, "ram_available_mb": 38368.1, "ram_percent": 38.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24586.6, "ram_available_mb": 38254.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.215}, "power_stats": {"power_gpu_soc_mean_watts": 24.019, "power_cpu_cv_mean_watts": 1.495, "power_sys_5v0_mean_watts": 9.141, "gpu_utilization_percent_mean": 78.215, "power_watts_avg": 24.019, "energy_joules_est": 225.63, "duration_seconds": 9.394, "sample_count": 79}, "timestamp": "2026-01-25T20:59:11.452029"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10841.406, "latencies_ms": [10841.406], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The train is positioned in the background, moving along the tracks that are in the foreground. The tracks curve to the right in the image, and the train is traveling in that direction. The surrounding area is filled with greenery, indicating that the train is likely traveling through a rural or natural environment.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 24586.6, "ram_available_mb": 38254.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24542.9, "ram_available_mb": 38298.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.228}, "power_stats": {"power_gpu_soc_mean_watts": 23.301, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 9.225, "gpu_utilization_percent_mean": 75.228, "power_watts_avg": 23.301, "energy_joules_est": 252.63, "duration_seconds": 10.842, "sample_count": 92}, "timestamp": "2026-01-25T20:59:24.315337"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9322.719, "latencies_ms": [9322.719], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A train is traveling through a rural area with lush greenery on both sides of the tracks. The train appears to be a passenger train with multiple cars, and it is moving along the tracks that are surrounded by grass and trees.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24542.9, "ram_available_mb": 38298.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24607.6, "ram_available_mb": 38233.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.5}, "power_stats": {"power_gpu_soc_mean_watts": 24.025, "power_cpu_cv_mean_watts": 1.522, "power_sys_5v0_mean_watts": 9.126, "gpu_utilization_percent_mean": 78.5, "power_watts_avg": 24.025, "energy_joules_est": 223.99, "duration_seconds": 9.323, "sample_count": 80}, "timestamp": "2026-01-25T20:59:35.665362"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8211.181, "latencies_ms": [8211.181], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The train in the image is predominantly blue and orange with a white front. It is traveling on a track that is surrounded by greenery and appears to be in a rural or natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24520.5, "ram_available_mb": 38320.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24608.6, "ram_available_mb": 38232.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.486}, "power_stats": {"power_gpu_soc_mean_watts": 24.361, "power_cpu_cv_mean_watts": 1.459, "power_sys_5v0_mean_watts": 9.267, "gpu_utilization_percent_mean": 77.486, "power_watts_avg": 24.361, "energy_joules_est": 200.05, "duration_seconds": 8.212, "sample_count": 70}, "timestamp": "2026-01-25T20:59:45.939146"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11123.497, "latencies_ms": [11123.497], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two cats lying on a pink couch. One cat is sleeping on the left side of the couch, while the other cat is sleeping on the right side. Both cats are curled up and appear to be in a relaxed state. \n\nThere are two remotes placed on the couch, one on the left side and", "error": null, "sys_before": {"cpu_percent": 6.5, "ram_used_mb": 24522.6, "ram_available_mb": 38318.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24585.9, "ram_available_mb": 38255.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.189}, "power_stats": {"power_gpu_soc_mean_watts": 20.945, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.189, "power_watts_avg": 20.945, "energy_joules_est": 233.0, "duration_seconds": 11.124, "sample_count": 95}, "timestamp": "2026-01-25T20:59:59.138167"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4392.284, "latencies_ms": [4392.284], "images_per_second": 0.228, "prompt_tokens": 39, "response_tokens_est": 17, "n_tiles": 16, "output_text": "couch: 1\ncat: 2\nremote control: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24515.7, "ram_available_mb": 38325.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24524.3, "ram_available_mb": 38316.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.459}, "power_stats": {"power_gpu_soc_mean_watts": 25.85, "power_cpu_cv_mean_watts": 1.115, "power_sys_5v0_mean_watts": 9.143, "gpu_utilization_percent_mean": 78.459, "power_watts_avg": 25.85, "energy_joules_est": 113.56, "duration_seconds": 4.393, "sample_count": 37}, "timestamp": "2026-01-25T21:00:05.542302"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11098.09, "latencies_ms": [11098.09], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The two cats are lying on a pink couch with their bodies parallel to each other, occupying the central space of the image. The remote controls are placed on the couch, with one remote to the left and slightly behind the other cat, and the second remote is to the right and slightly in front of the other cat. The couch itself is the main foreground object,", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24524.5, "ram_available_mb": 38316.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24585.4, "ram_available_mb": 38255.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.674}, "power_stats": {"power_gpu_soc_mean_watts": 20.914, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 69.674, "power_watts_avg": 20.914, "energy_joules_est": 232.12, "duration_seconds": 11.099, "sample_count": 95}, "timestamp": "2026-01-25T21:00:18.677068"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6469.426, "latencies_ms": [6469.426], "images_per_second": 0.155, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "Two cats are sleeping on a pink couch with two remote controls nearby. The couch appears to be in a living room or a similar indoor setting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24585.4, "ram_available_mb": 38255.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24595.4, "ram_available_mb": 38245.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.691}, "power_stats": {"power_gpu_soc_mean_watts": 23.255, "power_cpu_cv_mean_watts": 1.536, "power_sys_5v0_mean_watts": 9.02, "gpu_utilization_percent_mean": 74.691, "power_watts_avg": 23.255, "energy_joules_est": 150.46, "duration_seconds": 6.47, "sample_count": 55}, "timestamp": "2026-01-25T21:00:27.183917"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6746.403, "latencies_ms": [6746.403], "images_per_second": 0.148, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "Two cats are lying on a pink couch with two remote controls nearby. The couch has a soft, plush texture and the cats appear to be sleeping comfortably.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24595.4, "ram_available_mb": 38245.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24546.2, "ram_available_mb": 38294.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.895}, "power_stats": {"power_gpu_soc_mean_watts": 22.855, "power_cpu_cv_mean_watts": 1.63, "power_sys_5v0_mean_watts": 9.071, "gpu_utilization_percent_mean": 73.895, "power_watts_avg": 22.855, "energy_joules_est": 154.2, "duration_seconds": 6.747, "sample_count": 57}, "timestamp": "2026-01-25T21:00:35.986249"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11144.544, "latencies_ms": [11144.544], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seen surfing on a wave in a river. He is wearing a black wetsuit and is skillfully riding the wave with a blue surfboard. The river is surrounded by trees, creating a serene and natural setting for the surfing activity.\n\nThere are a few benches visible in the scene, likely placed for people", "error": null, "sys_before": {"cpu_percent": 13.6, "ram_used_mb": 24546.2, "ram_available_mb": 38294.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24525.9, "ram_available_mb": 38315.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.989}, "power_stats": {"power_gpu_soc_mean_watts": 20.907, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 68.989, "power_watts_avg": 20.907, "energy_joules_est": 233.01, "duration_seconds": 11.145, "sample_count": 95}, "timestamp": "2026-01-25T21:00:49.171711"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9511.299, "latencies_ms": [9511.299], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Surfboard: 1\n2. Man: 1\n3. Water: 1\n4. Bridge: 1\n5. Trees: 1\n6. Bench: 1\n7. Leaves: 1\n8. Statue: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24525.9, "ram_available_mb": 38315.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24523.7, "ram_available_mb": 38317.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.263}, "power_stats": {"power_gpu_soc_mean_watts": 21.699, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 71.263, "power_watts_avg": 21.699, "energy_joules_est": 206.4, "duration_seconds": 9.512, "sample_count": 80}, "timestamp": "2026-01-25T21:01:00.694455"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10548.758, "latencies_ms": [10548.758], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "In the foreground, a person in a black wetsuit is surfing on a wave in the water. To the left, there is a concrete structure, possibly a bridge or a dam, with trees and foliage in the background. Further back, there is another person holding a blue surfboard, standing on the edge of the water.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24523.7, "ram_available_mb": 38317.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24537.5, "ram_available_mb": 38303.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.09}, "power_stats": {"power_gpu_soc_mean_watts": 21.126, "power_cpu_cv_mean_watts": 1.894, "power_sys_5v0_mean_watts": 9.014, "gpu_utilization_percent_mean": 70.09, "power_watts_avg": 21.126, "energy_joules_est": 222.87, "duration_seconds": 10.549, "sample_count": 89}, "timestamp": "2026-01-25T21:01:13.282546"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6938.488, "latencies_ms": [6938.488], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A person in a black wetsuit is surfing a wave in a river with a concrete bridge in the background. There is another person with a blue surfboard in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24537.5, "ram_available_mb": 38303.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24594.2, "ram_available_mb": 38246.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.466}, "power_stats": {"power_gpu_soc_mean_watts": 23.034, "power_cpu_cv_mean_watts": 1.554, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 74.466, "power_watts_avg": 23.034, "energy_joules_est": 159.84, "duration_seconds": 6.939, "sample_count": 58}, "timestamp": "2026-01-25T21:01:22.242418"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8769.658, "latencies_ms": [8769.658], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image depicts a person surfing on a wave in a river with a bridge in the background. The surfer is wearing a black wetsuit and the water is a murky brown color. The sky is overcast and the overall lighting is dim.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24517.5, "ram_available_mb": 38323.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24597.4, "ram_available_mb": 38243.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.162}, "power_stats": {"power_gpu_soc_mean_watts": 21.684, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 71.162, "power_watts_avg": 21.684, "energy_joules_est": 190.17, "duration_seconds": 8.77, "sample_count": 74}, "timestamp": "2026-01-25T21:01:33.054039"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12310.755, "latencies_ms": [12310.755], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman and a young child are standing in a grassy field, flying a kite together. The woman is wearing a black jacket and jeans, while the child is dressed in a pink shirt and jeans. They are both holding onto the kite string, which is attached to a colorful kite that is soaring in the sky.\n", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24597.4, "ram_available_mb": 38243.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24597.7, "ram_available_mb": 38243.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.5}, "power_stats": {"power_gpu_soc_mean_watts": 22.811, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 9.205, "gpu_utilization_percent_mean": 74.5, "power_watts_avg": 22.811, "energy_joules_est": 280.84, "duration_seconds": 12.311, "sample_count": 104}, "timestamp": "2026-01-25T21:01:47.389585"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9398.613, "latencies_ms": [9398.613], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "person: 2, kite: 1, kite string: 2, kite: 1, kite string: 2, kite: 1, kite string: 2, kite: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24597.7, "ram_available_mb": 38243.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24598.7, "ram_available_mb": 38242.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.862}, "power_stats": {"power_gpu_soc_mean_watts": 24.02, "power_cpu_cv_mean_watts": 1.512, "power_sys_5v0_mean_watts": 9.165, "gpu_utilization_percent_mean": 77.862, "power_watts_avg": 24.02, "energy_joules_est": 225.77, "duration_seconds": 9.399, "sample_count": 80}, "timestamp": "2026-01-25T21:01:58.818339"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10749.985, "latencies_ms": [10749.985], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "In the foreground, there is a person standing with their back to the camera, holding a kite that is flying in the air to their right. The person is standing on a grassy field with trees in the background. There are other people in the distance, some of whom are also flying kites.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24598.7, "ram_available_mb": 38242.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24561.5, "ram_available_mb": 38279.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.989}, "power_stats": {"power_gpu_soc_mean_watts": 23.316, "power_cpu_cv_mean_watts": 1.684, "power_sys_5v0_mean_watts": 9.221, "gpu_utilization_percent_mean": 75.989, "power_watts_avg": 23.316, "energy_joules_est": 250.66, "duration_seconds": 10.751, "sample_count": 93}, "timestamp": "2026-01-25T21:02:11.599873"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8008.632, "latencies_ms": [8008.632], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "In a park, a person is flying a colorful kite while standing on a grassy field. There are other people in the background, also enjoying the outdoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24561.5, "ram_available_mb": 38279.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24561.8, "ram_available_mb": 38279.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.809}, "power_stats": {"power_gpu_soc_mean_watts": 24.754, "power_cpu_cv_mean_watts": 1.354, "power_sys_5v0_mean_watts": 9.202, "gpu_utilization_percent_mean": 78.809, "power_watts_avg": 24.754, "energy_joules_est": 198.26, "duration_seconds": 8.009, "sample_count": 68}, "timestamp": "2026-01-25T21:02:21.644982"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12413.406, "latencies_ms": [12413.406], "images_per_second": 0.081, "prompt_tokens": 36, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The image shows a person in a black leather jacket with purple and white accents, standing in a grassy field with arms outstretched, holding onto a colorful kite with a design that includes orange, pink, and blue. The weather appears to be clear and sunny, as indicated by the bright lighting and shadows cast on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24561.8, "ram_available_mb": 38279.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24563.2, "ram_available_mb": 38277.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.415}, "power_stats": {"power_gpu_soc_mean_watts": 22.853, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 9.174, "gpu_utilization_percent_mean": 73.415, "power_watts_avg": 22.853, "energy_joules_est": 283.7, "duration_seconds": 12.414, "sample_count": 106}, "timestamp": "2026-01-25T21:02:36.109731"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11123.163, "latencies_ms": [11123.163], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a young man is captured in the midst of a powerful swing. He's dressed in a crisp white shirt and black shorts, his red cap adding a pop of color to the scene. His right hand grips a yellow tennis racket, poised to strike the yellow tennis ball that hovers near the net. The green tennis court beneath", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24563.2, "ram_available_mb": 38277.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24580.4, "ram_available_mb": 38260.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.705}, "power_stats": {"power_gpu_soc_mean_watts": 20.902, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.705, "power_watts_avg": 20.902, "energy_joules_est": 232.51, "duration_seconds": 11.124, "sample_count": 95}, "timestamp": "2026-01-25T21:02:49.256297"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8396.754, "latencies_ms": [8396.754], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "- Boy: 1\n- Racket: 1\n- Tennis ball: 1\n- Net: 1\n- Tennis court: 1\n- Sign: 1\n- Jacket: 1\n- Hat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24527.0, "ram_available_mb": 38313.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24593.4, "ram_available_mb": 38247.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.634}, "power_stats": {"power_gpu_soc_mean_watts": 22.095, "power_cpu_cv_mean_watts": 1.726, "power_sys_5v0_mean_watts": 9.008, "gpu_utilization_percent_mean": 71.634, "power_watts_avg": 22.095, "energy_joules_est": 185.54, "duration_seconds": 8.397, "sample_count": 71}, "timestamp": "2026-01-25T21:02:59.716184"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10888.305, "latencies_ms": [10888.305], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, a tennis player is positioned near the baseline, preparing to hit a tennis ball that is in the air to his right. The background features a green tarp with a banner that reads \"Are you next? National Masters.\" The player is closer to the viewer than the tarp, indicating he is the main subject of the image.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24516.7, "ram_available_mb": 38324.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24589.7, "ram_available_mb": 38251.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.967}, "power_stats": {"power_gpu_soc_mean_watts": 21.082, "power_cpu_cv_mean_watts": 1.911, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 69.967, "power_watts_avg": 21.082, "energy_joules_est": 229.56, "duration_seconds": 10.889, "sample_count": 92}, "timestamp": "2026-01-25T21:03:12.617765"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8895.633, "latencies_ms": [8895.633], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "A tennis player is captured in the middle of a backhand swing, wearing a white shirt, black shorts, and a red cap. The setting appears to be a tennis court with a green backdrop and a banner that reads \"Are you next? National Masters.\"", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24521.4, "ram_available_mb": 38319.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 7.4, "ram_used_mb": 24597.0, "ram_available_mb": 38243.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.513}, "power_stats": {"power_gpu_soc_mean_watts": 21.981, "power_cpu_cv_mean_watts": 2.129, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 72.513, "power_watts_avg": 21.981, "energy_joules_est": 195.55, "duration_seconds": 8.896, "sample_count": 76}, "timestamp": "2026-01-25T21:03:23.570220"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8763.403, "latencies_ms": [8763.403], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image shows a tennis player in action on a court with a greenish hue, likely due to the lighting conditions. The player is wearing a white shirt and black shorts, and is in the process of hitting a yellow tennis ball with a yellow racket.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24597.0, "ram_available_mb": 38243.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24598.0, "ram_available_mb": 38242.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.608}, "power_stats": {"power_gpu_soc_mean_watts": 21.727, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 9.008, "gpu_utilization_percent_mean": 70.608, "power_watts_avg": 21.727, "energy_joules_est": 190.41, "duration_seconds": 8.764, "sample_count": 74}, "timestamp": "2026-01-25T21:03:34.371758"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11146.9, "latencies_ms": [11146.9], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a room that appears to be in the process of being packed or unpacked. There is a bed with a brown comforter, and a chair is placed near the bed. The room is filled with various items, including a backpack, a handbag, and several boxes. Some of the boxes are stacked on top of each other, while others are", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24598.0, "ram_available_mb": 38242.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24525.2, "ram_available_mb": 38315.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.691}, "power_stats": {"power_gpu_soc_mean_watts": 20.91, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.691, "power_watts_avg": 20.91, "energy_joules_est": 233.09, "duration_seconds": 11.148, "sample_count": 94}, "timestamp": "2026-01-25T21:03:47.573152"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8726.935, "latencies_ms": [8726.935], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Bed: 1\n- Chair: 1\n- Backpack: 2\n- Luggage: 1\n- Box: 5\n- Pillow: 1\n- Blanket: 1\n- Lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24525.2, "ram_available_mb": 38315.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24519.1, "ram_available_mb": 38321.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.689}, "power_stats": {"power_gpu_soc_mean_watts": 21.864, "power_cpu_cv_mean_watts": 1.743, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 71.689, "power_watts_avg": 21.864, "energy_joules_est": 190.82, "duration_seconds": 8.728, "sample_count": 74}, "timestamp": "2026-01-25T21:03:58.317144"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11115.696, "latencies_ms": [11115.696], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a bed with a brown blanket and pillows, and a pile of clothes and bags on the floor. Behind the bed, there are several cardboard boxes stacked against the wall, and a chair is placed near the corner of the room. The room appears to be in the process of being moved into or out of, as there are boxes", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24519.1, "ram_available_mb": 38321.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24518.1, "ram_available_mb": 38322.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.191}, "power_stats": {"power_gpu_soc_mean_watts": 20.976, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 69.191, "power_watts_avg": 20.976, "energy_joules_est": 233.18, "duration_seconds": 11.116, "sample_count": 94}, "timestamp": "2026-01-25T21:04:11.460927"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9912.704, "latencies_ms": [9912.704], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image depicts a room that is in the process of being moved into or out of, with a bed covered in a brown sleeping bag, a chair, and various pieces of furniture and boxes scattered around. The room has a brick wall and a wooden door, and the lighting suggests it is nighttime.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24518.1, "ram_available_mb": 38322.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24580.3, "ram_available_mb": 38260.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.247}, "power_stats": {"power_gpu_soc_mean_watts": 21.388, "power_cpu_cv_mean_watts": 1.852, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 70.247, "power_watts_avg": 21.388, "energy_joules_est": 212.03, "duration_seconds": 9.913, "sample_count": 85}, "timestamp": "2026-01-25T21:04:23.436332"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8091.558, "latencies_ms": [8091.558], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The room is dimly lit with a mix of natural and artificial light, creating a cozy atmosphere. The walls are made of brick, and there is a bed covered with a brown blanket, a chair, and various items scattered around the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24519.3, "ram_available_mb": 38321.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24574.2, "ram_available_mb": 38266.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.618}, "power_stats": {"power_gpu_soc_mean_watts": 22.101, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 9.032, "gpu_utilization_percent_mean": 71.618, "power_watts_avg": 22.101, "energy_joules_est": 178.85, "duration_seconds": 8.092, "sample_count": 68}, "timestamp": "2026-01-25T21:04:33.547725"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11130.13, "latencies_ms": [11130.13], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a rider is seen on a brown horse, both of them in mid-air as they jump over a wooden obstacle. The rider is wearing a red and green striped shirt, white pants, and a green helmet. The horse, adorned with a white saddle pad and a black bridle, is equipped with a pink", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24510.7, "ram_available_mb": 38330.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24580.9, "ram_available_mb": 38260.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.062}, "power_stats": {"power_gpu_soc_mean_watts": 20.767, "power_cpu_cv_mean_watts": 1.924, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.062, "power_watts_avg": 20.767, "energy_joules_est": 231.15, "duration_seconds": 11.131, "sample_count": 97}, "timestamp": "2026-01-25T21:04:46.738222"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7730.131, "latencies_ms": [7730.131], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "horse: 1, rider: 1, helmet: 1, number 76: 1, flower: 1, wooden fence: 1, grass: 1, tree: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24519.1, "ram_available_mb": 38321.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24526.9, "ram_available_mb": 38314.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.742}, "power_stats": {"power_gpu_soc_mean_watts": 22.315, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 72.742, "power_watts_avg": 22.315, "energy_joules_est": 172.51, "duration_seconds": 7.731, "sample_count": 66}, "timestamp": "2026-01-25T21:04:56.511865"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11181.37, "latencies_ms": [11181.37], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The horse and rider are in the foreground, jumping over a wooden obstacle. In the background, there are trees and more open grassy areas, indicating the setting is likely an outdoor equestrian course. The rider is positioned on the horse's left side, and they are both centered over the jump, suggesting they are in the middle of the course.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24526.9, "ram_available_mb": 38314.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24595.5, "ram_available_mb": 38245.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.695}, "power_stats": {"power_gpu_soc_mean_watts": 20.593, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 69.695, "power_watts_avg": 20.593, "energy_joules_est": 230.27, "duration_seconds": 11.182, "sample_count": 95}, "timestamp": "2026-01-25T21:05:09.714144"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8395.339, "latencies_ms": [8395.339], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "A rider in a red and green outfit is seen jumping a horse over a wooden obstacle in an outdoor setting with trees in the background. The horse is wearing a white saddle pad with the word \"LISTON\" on it.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24595.5, "ram_available_mb": 38245.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24584.1, "ram_available_mb": 38256.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.859}, "power_stats": {"power_gpu_soc_mean_watts": 21.997, "power_cpu_cv_mean_watts": 1.737, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 71.859, "power_watts_avg": 21.997, "energy_joules_est": 184.69, "duration_seconds": 8.396, "sample_count": 71}, "timestamp": "2026-01-25T21:05:20.152138"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11085.836, "latencies_ms": [11085.836], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene of a horse and rider in mid-air, with the horse's coat a rich chestnut color and the rider wearing a striking red and green outfit. The lighting is bright and natural, suggesting the photo was taken on a sunny day, and the background is filled with lush green trees, indicating an outdoor", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24584.1, "ram_available_mb": 38256.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24531.2, "ram_available_mb": 38309.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.617}, "power_stats": {"power_gpu_soc_mean_watts": 20.98, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.617, "power_watts_avg": 20.98, "energy_joules_est": 232.59, "duration_seconds": 11.086, "sample_count": 94}, "timestamp": "2026-01-25T21:05:33.263901"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11150.776, "latencies_ms": [11150.776], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features two men sitting under a large umbrella on a sidewalk. They are both wearing ties, and one of them is wearing a white shirt. The men appear to be enjoying their time together, possibly having a conversation or taking a break. \n\nIn the background, there are several cars parked along the street, with one car closer to the", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24531.2, "ram_available_mb": 38309.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24516.8, "ram_available_mb": 38324.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.344}, "power_stats": {"power_gpu_soc_mean_watts": 20.896, "power_cpu_cv_mean_watts": 1.919, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 69.344, "power_watts_avg": 20.896, "energy_joules_est": 233.02, "duration_seconds": 11.151, "sample_count": 96}, "timestamp": "2026-01-25T21:05:46.438130"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9507.035, "latencies_ms": [9507.035], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Chair: 1\n2. Table: 1\n3. Umbrella: 1\n4. Cars: 2\n5. Bicycle: 1\n6. Men: 2\n7. Sign: 1\n8. Chair leg: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24516.8, "ram_available_mb": 38324.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24590.4, "ram_available_mb": 38250.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.543}, "power_stats": {"power_gpu_soc_mean_watts": 21.548, "power_cpu_cv_mean_watts": 1.8, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 70.543, "power_watts_avg": 21.548, "energy_joules_est": 204.87, "duration_seconds": 9.508, "sample_count": 81}, "timestamp": "2026-01-25T21:05:57.963528"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11115.526, "latencies_ms": [11115.526], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a table with various objects on it, and two men are seated on chairs on either side of the table. The man on the left is seated closer to the camera, while the man on the right is seated further back. In the background, there is a car parked on the street and a bicycle leaning against a pole.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24528.6, "ram_available_mb": 38312.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24591.7, "ram_available_mb": 38249.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.505}, "power_stats": {"power_gpu_soc_mean_watts": 20.972, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.505, "power_watts_avg": 20.972, "energy_joules_est": 233.13, "duration_seconds": 11.116, "sample_count": 95}, "timestamp": "2026-01-25T21:06:11.105825"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7610.567, "latencies_ms": [7610.567], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "Two men are sitting under an umbrella on a cobblestone street, with a car and a bicycle in the background. The scene appears to be in an urban area with shops and buildings nearby.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24591.7, "ram_available_mb": 38249.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24577.1, "ram_available_mb": 38263.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.812}, "power_stats": {"power_gpu_soc_mean_watts": 22.541, "power_cpu_cv_mean_watts": 1.652, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 73.812, "power_watts_avg": 22.541, "energy_joules_est": 171.57, "duration_seconds": 7.611, "sample_count": 64}, "timestamp": "2026-01-25T21:06:20.750428"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7202.351, "latencies_ms": [7202.351], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image is in black and white, with a high contrast between the shades of gray. The weather appears to be overcast, as the lighting is diffused and there are no harsh shadows.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24577.1, "ram_available_mb": 38263.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24587.1, "ram_available_mb": 38253.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.393}, "power_stats": {"power_gpu_soc_mean_watts": 22.514, "power_cpu_cv_mean_watts": 1.648, "power_sys_5v0_mean_watts": 9.041, "gpu_utilization_percent_mean": 72.393, "power_watts_avg": 22.514, "energy_joules_est": 162.17, "duration_seconds": 7.203, "sample_count": 61}, "timestamp": "2026-01-25T21:06:29.980829"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11134.192, "latencies_ms": [11134.192], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a small, clean, and well-organized kitchen with white cabinets and appliances. The kitchen is equipped with a refrigerator, a stove, and a dishwasher. The refrigerator is positioned on the right side of the kitchen, while the stove and dishwasher are located on the left side.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24517.0, "ram_available_mb": 38323.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24592.9, "ram_available_mb": 38248.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.415}, "power_stats": {"power_gpu_soc_mean_watts": 20.904, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.415, "power_watts_avg": 20.904, "energy_joules_est": 232.76, "duration_seconds": 11.135, "sample_count": 94}, "timestamp": "2026-01-25T21:06:43.142989"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11302.678, "latencies_ms": [11302.678], "images_per_second": 0.088, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "fan: 1\nceiling: 1\nlight fixture: 1\nblinds: 2\nwindows: 1\nkitchen cabinets: 10\nfridge: 1\nmicrowave: 1\ndishwasher: 1\nstove: 1\noven: 1\nkitchen utensils: 5\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24520.9, "ram_available_mb": 38320.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24609.0, "ram_available_mb": 38231.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.155}, "power_stats": {"power_gpu_soc_mean_watts": 20.993, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.926, "gpu_utilization_percent_mean": 70.155, "power_watts_avg": 20.993, "energy_joules_est": 237.29, "duration_seconds": 11.303, "sample_count": 97}, "timestamp": "2026-01-25T21:06:56.471260"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11117.776, "latencies_ms": [11117.776], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The kitchen is well-lit with natural light coming from the window, which is located on the right side of the image. The refrigerator, which is a large appliance, is positioned on the right side of the image, near the back wall. The stove and oven are on the left side of the image, with the stove being closer to the foreground and", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24609.0, "ram_available_mb": 38231.9, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24523.0, "ram_available_mb": 38317.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.99}, "power_stats": {"power_gpu_soc_mean_watts": 21.045, "power_cpu_cv_mean_watts": 1.919, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.99, "power_watts_avg": 21.045, "energy_joules_est": 233.99, "duration_seconds": 11.118, "sample_count": 96}, "timestamp": "2026-01-25T21:07:09.627347"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8307.31, "latencies_ms": [8307.31], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image depicts a well-lit kitchen with white cabinets and a black countertop. A ceiling fan with a light fixture hangs above the kitchen, and there are various kitchen utensils and items scattered on the countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24523.0, "ram_available_mb": 38317.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 24541.4, "ram_available_mb": 38299.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.352}, "power_stats": {"power_gpu_soc_mean_watts": 22.151, "power_cpu_cv_mean_watts": 1.907, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 72.352, "power_watts_avg": 22.151, "energy_joules_est": 184.03, "duration_seconds": 8.308, "sample_count": 71}, "timestamp": "2026-01-25T21:07:19.954920"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6021.309, "latencies_ms": [6021.309], "images_per_second": 0.166, "prompt_tokens": 36, "response_tokens_est": 33, "n_tiles": 16, "output_text": "The kitchen has a dark ceiling with a ceiling fan that has multiple lights. The cabinets are white and there is a window with white blinds.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24541.4, "ram_available_mb": 38299.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24527.6, "ram_available_mb": 38313.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.608}, "power_stats": {"power_gpu_soc_mean_watts": 23.608, "power_cpu_cv_mean_watts": 1.484, "power_sys_5v0_mean_watts": 9.119, "gpu_utilization_percent_mean": 75.608, "power_watts_avg": 23.608, "energy_joules_est": 142.17, "duration_seconds": 6.022, "sample_count": 51}, "timestamp": "2026-01-25T21:07:28.019444"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11121.764, "latencies_ms": [11121.764], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a baby is peacefully sleeping on a bed. The baby is lying on its side, with its head resting on a pillow. The bed is covered with a blanket that has a pattern of trains on it. The room is dimly lit, with a lamp providing the main source of light. The walls of the room are painted in a dark color, creating", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24527.6, "ram_available_mb": 38313.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24584.1, "ram_available_mb": 38256.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.463}, "power_stats": {"power_gpu_soc_mean_watts": 20.868, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.463, "power_watts_avg": 20.868, "energy_joules_est": 232.1, "duration_seconds": 11.122, "sample_count": 95}, "timestamp": "2026-01-25T21:07:41.200721"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9042.299, "latencies_ms": [9042.299], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Trains: 10\n- Flowers: 4\n- Stuffed animal: 1\n- Pillow: 1\n- Bed: 1\n- Sheet: 1\n- Nightstand: 1\n- Lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24522.2, "ram_available_mb": 38318.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24591.3, "ram_available_mb": 38249.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.39}, "power_stats": {"power_gpu_soc_mean_watts": 21.933, "power_cpu_cv_mean_watts": 1.774, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 72.39, "power_watts_avg": 21.933, "energy_joules_est": 198.34, "duration_seconds": 9.043, "sample_count": 77}, "timestamp": "2026-01-25T21:07:52.271500"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8994.764, "latencies_ms": [8994.764], "images_per_second": 0.111, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The baby is lying on the left side of the bed, closer to the foreground, while the lamp is on the right side, further in the background. The baby appears to be in a comfortable position, with a pillow and a blanket providing a sense of security and warmth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24591.3, "ram_available_mb": 38249.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24544.9, "ram_available_mb": 38296.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.636}, "power_stats": {"power_gpu_soc_mean_watts": 21.822, "power_cpu_cv_mean_watts": 1.81, "power_sys_5v0_mean_watts": 9.024, "gpu_utilization_percent_mean": 70.636, "power_watts_avg": 21.822, "energy_joules_est": 196.3, "duration_seconds": 8.995, "sample_count": 77}, "timestamp": "2026-01-25T21:08:03.311026"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7723.283, "latencies_ms": [7723.283], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A baby is sleeping peacefully on a bed with a blue blanket that has white daisies and a pattern of trains. The room is dimly lit, with a lamp providing a soft glow in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24544.9, "ram_available_mb": 38296.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24611.8, "ram_available_mb": 38229.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.712}, "power_stats": {"power_gpu_soc_mean_watts": 22.481, "power_cpu_cv_mean_watts": 1.669, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 72.712, "power_watts_avg": 22.481, "energy_joules_est": 173.64, "duration_seconds": 7.724, "sample_count": 66}, "timestamp": "2026-01-25T21:08:13.080364"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11163.637, "latencies_ms": [11163.637], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a child sleeping on a bed with a dark blue blanket that has a floral pattern with white and yellow flowers. The child is wearing a white tank top and is lying on a pillow with a blue and white checkered pattern. The room is dimly lit, with a lamp providing the main source of light, casting a warm glow on the child and the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24611.8, "ram_available_mb": 38229.1, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24529.4, "ram_available_mb": 38311.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.789}, "power_stats": {"power_gpu_soc_mean_watts": 20.936, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 69.789, "power_watts_avg": 20.936, "energy_joules_est": 233.74, "duration_seconds": 11.164, "sample_count": 95}, "timestamp": "2026-01-25T21:08:26.284725"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12094.875, "latencies_ms": [12094.875], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene on a highway, where a green highway sign stands prominently. The sign, which is rectangular in shape, is affixed to a metal pole. It bears the text \"NO TRUCKS\" in bold, white letters at the top, indicating a restriction for trucks on this route. \n\nJust below this, the sign displays", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24529.4, "ram_available_mb": 38311.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24526.0, "ram_available_mb": 38314.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.825}, "power_stats": {"power_gpu_soc_mean_watts": 22.587, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 9.147, "gpu_utilization_percent_mean": 72.825, "power_watts_avg": 22.587, "energy_joules_est": 273.2, "duration_seconds": 12.095, "sample_count": 103}, "timestamp": "2026-01-25T21:08:40.440719"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 12169.62, "latencies_ms": [12169.62], "images_per_second": 0.082, "prompt_tokens": 39, "response_tokens_est": 77, "n_tiles": 16, "output_text": "object: sign, count: 2\nobject: highway, count: 1\nobject: city, count: 2\nobject: route number, count: 1\nobject: direction, count: 1\nobject: graffiti, count: 1\nobject: trucks, count: 1\nobject: bridge, count: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24526.0, "ram_available_mb": 38314.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24521.6, "ram_available_mb": 38319.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.717}, "power_stats": {"power_gpu_soc_mean_watts": 22.116, "power_cpu_cv_mean_watts": 1.76, "power_sys_5v0_mean_watts": 9.083, "gpu_utilization_percent_mean": 74.717, "power_watts_avg": 22.116, "energy_joules_est": 269.16, "duration_seconds": 12.17, "sample_count": 106}, "timestamp": "2026-01-25T21:08:54.632507"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12151.726, "latencies_ms": [12151.726], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The 'NO TRUCKS' sign is positioned in the upper left corner, above the main highway sign. The highway sign is in the foreground, indicating directions to Queens and the Bronx, and is located in the left foreground of the image. The 'Interstate 278' logo is centrally placed on the highway sign, and the 'Dept.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24521.6, "ram_available_mb": 38319.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24604.6, "ram_available_mb": 38236.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.272}, "power_stats": {"power_gpu_soc_mean_watts": 22.134, "power_cpu_cv_mean_watts": 1.804, "power_sys_5v0_mean_watts": 9.108, "gpu_utilization_percent_mean": 73.272, "power_watts_avg": 22.134, "energy_joules_est": 268.98, "duration_seconds": 12.152, "sample_count": 103}, "timestamp": "2026-01-25T21:09:08.798230"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9784.163, "latencies_ms": [9784.163], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a green highway sign with white text that reads \"NO TRUCKS\" at the top and \"EAST Interstate 278 Queens Bronx\" at the bottom. The sign is mounted on a metal structure with a bridge in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24604.6, "ram_available_mb": 38236.2, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24546.8, "ram_available_mb": 38294.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.711}, "power_stats": {"power_gpu_soc_mean_watts": 23.473, "power_cpu_cv_mean_watts": 1.578, "power_sys_5v0_mean_watts": 9.097, "gpu_utilization_percent_mean": 75.711, "power_watts_avg": 23.473, "energy_joules_est": 229.68, "duration_seconds": 9.785, "sample_count": 83}, "timestamp": "2026-01-25T21:09:20.613376"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7477.661, "latencies_ms": [7477.661], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The sign is predominantly green with white lettering, indicating it is likely a directional highway sign. It is mounted on a metal structure with a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24546.8, "ram_available_mb": 38294.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24628.2, "ram_available_mb": 38212.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.079}, "power_stats": {"power_gpu_soc_mean_watts": 24.301, "power_cpu_cv_mean_watts": 1.411, "power_sys_5v0_mean_watts": 9.203, "gpu_utilization_percent_mean": 78.079, "power_watts_avg": 24.301, "energy_joules_est": 181.73, "duration_seconds": 7.478, "sample_count": 63}, "timestamp": "2026-01-25T21:09:30.140024"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11122.578, "latencies_ms": [11122.578], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a vintage red Chevrolet truck parked in a parking lot. The truck is positioned in the center of the image, with its rear facing the viewer. The parking lot appears to be quite empty, with only a few other cars visible in the background. The truck is parked under a street light, which is located on the", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24522.8, "ram_available_mb": 38318.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24596.4, "ram_available_mb": 38244.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.281}, "power_stats": {"power_gpu_soc_mean_watts": 20.173, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 70.281, "power_watts_avg": 20.173, "energy_joules_est": 224.39, "duration_seconds": 11.123, "sample_count": 96}, "timestamp": "2026-01-25T21:09:43.315437"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8521.153, "latencies_ms": [8521.153], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Car: 1\n- Truck: 1\n- Street light: 1\n- Clouds: 1\n- Sky: 1\n- Tire: 1\n- Truck bed: 1\n- Car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24521.4, "ram_available_mb": 38319.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24605.3, "ram_available_mb": 38235.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.542}, "power_stats": {"power_gpu_soc_mean_watts": 21.944, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 71.542, "power_watts_avg": 21.944, "energy_joules_est": 187.01, "duration_seconds": 8.522, "sample_count": 72}, "timestamp": "2026-01-25T21:09:53.897305"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9931.413, "latencies_ms": [9931.413], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The red Chevrolet truck is positioned in the foreground of the image, appearing large and prominent. It is located on the left side of the frame, with other cars visible in the background. The truck is parked on a gravel surface, and there is a street lamp to the right of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24533.4, "ram_available_mb": 38307.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24595.6, "ram_available_mb": 38245.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.386}, "power_stats": {"power_gpu_soc_mean_watts": 21.236, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 70.386, "power_watts_avg": 21.236, "energy_joules_est": 210.92, "duration_seconds": 9.932, "sample_count": 83}, "timestamp": "2026-01-25T21:10:05.843071"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8941.76, "latencies_ms": [8941.76], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image features a vintage red Chevrolet truck parked on a gravel surface, likely at a car show or gathering. The truck is the main focus of the image, with a clear blue sky and a few other cars visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24518.1, "ram_available_mb": 38322.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24600.0, "ram_available_mb": 38240.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.286}, "power_stats": {"power_gpu_soc_mean_watts": 21.874, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 72.286, "power_watts_avg": 21.874, "energy_joules_est": 195.61, "duration_seconds": 8.942, "sample_count": 77}, "timestamp": "2026-01-25T21:10:16.798559"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8751.941, "latencies_ms": [8751.941], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image features a vibrant red Chevrolet truck with a shiny, reflective surface that catches the light beautifully. The weather appears to be clear with a few clouds in the sky, and the lighting suggests it might be a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24525.0, "ram_available_mb": 38315.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24588.0, "ram_available_mb": 38252.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.413}, "power_stats": {"power_gpu_soc_mean_watts": 21.744, "power_cpu_cv_mean_watts": 1.805, "power_sys_5v0_mean_watts": 9.035, "gpu_utilization_percent_mean": 71.413, "power_watts_avg": 21.744, "energy_joules_est": 190.32, "duration_seconds": 8.753, "sample_count": 75}, "timestamp": "2026-01-25T21:10:27.606835"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11115.972, "latencies_ms": [11115.972], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a black and white photograph of three cows standing behind a barbed wire fence. The cows are looking directly at the camera, giving the impression that they are posing for the picture. The fence is made of barbed wire, which is a common type of fencing used to keep livestock contained. The cows are standing in a field, which is", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24529.4, "ram_available_mb": 38311.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24530.2, "ram_available_mb": 38310.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.543}, "power_stats": {"power_gpu_soc_mean_watts": 20.93, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 69.543, "power_watts_avg": 20.93, "energy_joules_est": 232.67, "duration_seconds": 11.117, "sample_count": 94}, "timestamp": "2026-01-25T21:10:40.773356"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7831.164, "latencies_ms": [7831.164], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "cow: 3\nbarbed wire: 10\ngrass: 100%\nfield: 1\nsky: 1\nclouds: 5\ntrees: 2\nhill: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24530.2, "ram_available_mb": 38310.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24514.2, "ram_available_mb": 38326.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.742}, "power_stats": {"power_gpu_soc_mean_watts": 22.489, "power_cpu_cv_mean_watts": 1.644, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 73.742, "power_watts_avg": 22.489, "energy_joules_est": 176.13, "duration_seconds": 7.832, "sample_count": 66}, "timestamp": "2026-01-25T21:10:50.630796"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11111.365, "latencies_ms": [11111.365], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, there are three cows standing close to each other, with one facing the camera and the other two partially visible behind it. The cows are positioned behind a barbed wire fence, which is in the middle ground of the image. In the background, there is an open field that extends to the horizon, and the sky above is filled with clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24514.2, "ram_available_mb": 38326.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24538.1, "ram_available_mb": 38302.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.479}, "power_stats": {"power_gpu_soc_mean_watts": 20.967, "power_cpu_cv_mean_watts": 1.915, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.479, "power_watts_avg": 20.967, "energy_joules_est": 232.98, "duration_seconds": 11.112, "sample_count": 96}, "timestamp": "2026-01-25T21:11:03.768732"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7838.419, "latencies_ms": [7838.419], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image depicts a serene pastoral scene with three cows standing behind a barbed wire fence. The cows are looking directly at the camera, and the background shows a hilly landscape with sparse vegetation.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24538.1, "ram_available_mb": 38302.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24580.6, "ram_available_mb": 38260.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.164}, "power_stats": {"power_gpu_soc_mean_watts": 22.446, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 73.164, "power_watts_avg": 22.446, "energy_joules_est": 175.95, "duration_seconds": 7.839, "sample_count": 67}, "timestamp": "2026-01-25T21:11:13.658851"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7309.711, "latencies_ms": [7309.711], "images_per_second": 0.137, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image is a black and white photograph, which gives it a timeless and classic feel. The lighting is natural and soft, with no harsh shadows, suggesting it was taken on a cloudy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24580.6, "ram_available_mb": 38260.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 7.4, "ram_used_mb": 24594.5, "ram_available_mb": 38246.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.258}, "power_stats": {"power_gpu_soc_mean_watts": 22.452, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 9.064, "gpu_utilization_percent_mean": 72.258, "power_watts_avg": 22.452, "energy_joules_est": 164.13, "duration_seconds": 7.31, "sample_count": 62}, "timestamp": "2026-01-25T21:11:23.019759"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11114.578, "latencies_ms": [11114.578], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and cozy bedroom, bathed in soft light. Dominating the scene is a large bed, dressed in a pristine white comforter that contrasts beautifully with the warm tones of the wooden headboard and footboard. The bed is adorned with two plush pillows, inviting relaxation.\n\nTo the right", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24522.9, "ram_available_mb": 38318.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24594.5, "ram_available_mb": 38246.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.543}, "power_stats": {"power_gpu_soc_mean_watts": 20.941, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.543, "power_watts_avg": 20.941, "energy_joules_est": 232.76, "duration_seconds": 11.115, "sample_count": 94}, "timestamp": "2026-01-25T21:11:36.197225"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7396.331, "latencies_ms": [7396.331], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "bed: 1, chair: 1, armchair: 1, fireplace: 1, clock: 1, wall: 1, window: 1, floor lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24594.5, "ram_available_mb": 38246.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24592.0, "ram_available_mb": 38248.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.397}, "power_stats": {"power_gpu_soc_mean_watts": 22.496, "power_cpu_cv_mean_watts": 1.634, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 71.397, "power_watts_avg": 22.496, "energy_joules_est": 166.4, "duration_seconds": 7.397, "sample_count": 63}, "timestamp": "2026-01-25T21:11:45.618289"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7882.419, "latencies_ms": [7882.419], "images_per_second": 0.127, "prompt_tokens": 44, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The bed is positioned in the foreground on the left side of the image, while the fireplace is in the background on the right side. The chair is placed near the fireplace, closer to the viewer than the fireplace.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24517.0, "ram_available_mb": 38323.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24578.2, "ram_available_mb": 38262.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.791}, "power_stats": {"power_gpu_soc_mean_watts": 22.164, "power_cpu_cv_mean_watts": 1.71, "power_sys_5v0_mean_watts": 9.031, "gpu_utilization_percent_mean": 71.791, "power_watts_avg": 22.164, "energy_joules_est": 174.72, "duration_seconds": 7.883, "sample_count": 67}, "timestamp": "2026-01-25T21:11:55.547131"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6841.289, "latencies_ms": [6841.289], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image depicts a cozy bedroom with a large bed, a chair, and a fireplace. The room is dimly lit, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24518.2, "ram_available_mb": 38322.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24593.1, "ram_available_mb": 38247.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.5}, "power_stats": {"power_gpu_soc_mean_watts": 22.925, "power_cpu_cv_mean_watts": 1.554, "power_sys_5v0_mean_watts": 9.018, "gpu_utilization_percent_mean": 73.5, "power_watts_avg": 22.925, "energy_joules_est": 156.85, "duration_seconds": 6.842, "sample_count": 58}, "timestamp": "2026-01-25T21:12:04.413349"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9446.979, "latencies_ms": [9446.979], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The room is warmly lit with a soft glow, featuring a wooden ceiling with recessed lighting that casts a warm ambiance. The walls are adorned with a clock and a fireplace with a stone surround, adding to the cozy and inviting atmosphere of the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24520.5, "ram_available_mb": 38320.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24588.6, "ram_available_mb": 38252.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.412}, "power_stats": {"power_gpu_soc_mean_watts": 21.404, "power_cpu_cv_mean_watts": 1.817, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 70.412, "power_watts_avg": 21.404, "energy_joules_est": 202.22, "duration_seconds": 9.448, "sample_count": 80}, "timestamp": "2026-01-25T21:12:15.887723"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11100.772, "latencies_ms": [11100.772], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are three guinea fowls walking on a dry grass field. The guinea fowls are of different sizes, with one being larger and the other two smaller. They are walking in a line, with the larger one leading the way and the smaller ones following behind. The field they are walking on is covered in dry grass, and there are some bushes in", "error": null, "sys_before": {"cpu_percent": 6.5, "ram_used_mb": 24588.6, "ram_available_mb": 38252.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24534.2, "ram_available_mb": 38306.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.755}, "power_stats": {"power_gpu_soc_mean_watts": 21.013, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 69.755, "power_watts_avg": 21.013, "energy_joules_est": 233.27, "duration_seconds": 11.101, "sample_count": 94}, "timestamp": "2026-01-25T21:12:29.062051"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9179.062, "latencies_ms": [9179.062], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "1. Birds: 3\n2. Grass: 1\n3. Hill: 1\n4. Bush: 1\n5. Sky: 1\n6. Ground: 1\n7. Water: 1\n8. Tree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24534.2, "ram_available_mb": 38306.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24523.7, "ram_available_mb": 38317.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.282}, "power_stats": {"power_gpu_soc_mean_watts": 21.767, "power_cpu_cv_mean_watts": 1.761, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 72.282, "power_watts_avg": 21.767, "energy_joules_est": 199.81, "duration_seconds": 9.18, "sample_count": 78}, "timestamp": "2026-01-25T21:12:40.292148"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9828.116, "latencies_ms": [9828.116], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "In the foreground, there are three guinea fowls on a dry grassy hill. The largest bird is positioned slightly to the left of the center, while the other two are closer together, with one slightly behind the other. In the background, there are some green shrubs and a body of water visible.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24523.7, "ram_available_mb": 38317.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24591.6, "ram_available_mb": 38249.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.929}, "power_stats": {"power_gpu_soc_mean_watts": 21.225, "power_cpu_cv_mean_watts": 1.831, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 69.929, "power_watts_avg": 21.225, "energy_joules_est": 208.61, "duration_seconds": 9.829, "sample_count": 84}, "timestamp": "2026-01-25T21:12:52.168542"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6720.588, "latencies_ms": [6720.588], "images_per_second": 0.149, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "Three guinea fowls are walking across a dry grassy field with sparse vegetation. The sky is overcast, and there are some bushes and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24591.6, "ram_available_mb": 38249.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24517.9, "ram_available_mb": 38323.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.667}, "power_stats": {"power_gpu_soc_mean_watts": 23.062, "power_cpu_cv_mean_watts": 1.553, "power_sys_5v0_mean_watts": 9.015, "gpu_utilization_percent_mean": 72.667, "power_watts_avg": 23.062, "energy_joules_est": 155.0, "duration_seconds": 6.721, "sample_count": 57}, "timestamp": "2026-01-25T21:13:00.905874"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6884.288, "latencies_ms": [6884.288], "images_per_second": 0.145, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image shows three guinea fowls with speckled black feathers walking on a dry, grassy ground. The lighting is soft and diffused, suggesting an overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24517.9, "ram_available_mb": 38323.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24573.4, "ram_available_mb": 38267.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.051}, "power_stats": {"power_gpu_soc_mean_watts": 22.66, "power_cpu_cv_mean_watts": 1.636, "power_sys_5v0_mean_watts": 9.048, "gpu_utilization_percent_mean": 72.051, "power_watts_avg": 22.66, "energy_joules_est": 156.01, "duration_seconds": 6.885, "sample_count": 59}, "timestamp": "2026-01-25T21:13:09.810657"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11151.286, "latencies_ms": [11151.286], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, three people are standing in the snow, each holding ski poles and wearing skis. They appear to be posing for a picture, possibly for a skiing trip or a group photo. The skis are visible on the ground, and the snowy environment suggests that they are in a winter setting.\n\nThe three people are standing close to each other, with", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24513.3, "ram_available_mb": 38327.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24524.7, "ram_available_mb": 38316.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.766}, "power_stats": {"power_gpu_soc_mean_watts": 20.924, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.766, "power_watts_avg": 20.924, "energy_joules_est": 233.34, "duration_seconds": 11.152, "sample_count": 94}, "timestamp": "2026-01-25T21:13:22.998909"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8398.571, "latencies_ms": [8398.571], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "- Skier: 3\n- Skis: 3\n- Trees: numerous\n- Snow: covering ground\n- Snowboard: 1\n- Snowboarder: 1\n- Snow: covering ground\n- Snow: covering ground", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24524.7, "ram_available_mb": 38316.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24516.5, "ram_available_mb": 38324.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.113}, "power_stats": {"power_gpu_soc_mean_watts": 22.025, "power_cpu_cv_mean_watts": 1.703, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 72.113, "power_watts_avg": 22.025, "energy_joules_est": 184.99, "duration_seconds": 8.399, "sample_count": 71}, "timestamp": "2026-01-25T21:13:33.456174"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9891.022, "latencies_ms": [9891.022], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, there are three individuals engaged in cross-country skiing. The person on the left is positioned slightly behind the other two, moving towards the right side of the image. The background features a snowy landscape with trees covered in snow, indicating that the skiing is taking place in a wooded area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24516.5, "ram_available_mb": 38324.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24600.2, "ram_available_mb": 38240.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.94}, "power_stats": {"power_gpu_soc_mean_watts": 21.351, "power_cpu_cv_mean_watts": 1.84, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 69.94, "power_watts_avg": 21.351, "energy_joules_est": 211.2, "duration_seconds": 9.892, "sample_count": 84}, "timestamp": "2026-01-25T21:13:45.400733"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5968.468, "latencies_ms": [5968.468], "images_per_second": 0.168, "prompt_tokens": 37, "response_tokens_est": 31, "n_tiles": 16, "output_text": "Three people are skiing in a snowy forest on a sunny day. They are all wearing winter gear and holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24600.2, "ram_available_mb": 38240.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24593.7, "ram_available_mb": 38247.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.52}, "power_stats": {"power_gpu_soc_mean_watts": 23.853, "power_cpu_cv_mean_watts": 1.393, "power_sys_5v0_mean_watts": 9.035, "gpu_utilization_percent_mean": 75.52, "power_watts_avg": 23.853, "energy_joules_est": 142.38, "duration_seconds": 5.969, "sample_count": 50}, "timestamp": "2026-01-25T21:13:53.419314"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10992.026, "latencies_ms": [10992.026], "images_per_second": 0.091, "prompt_tokens": 36, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The image shows three individuals cross-country skiing on a snowy landscape with trees in the background. They are wearing winter clothing, including jackets and hats, and are equipped with ski poles and skis. The weather appears to be cold and snowy, as evidenced by the snow-covered trees and the skiers' attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24593.7, "ram_available_mb": 38247.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24594.7, "ram_available_mb": 38246.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.495}, "power_stats": {"power_gpu_soc_mean_watts": 20.934, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 69.495, "power_watts_avg": 20.934, "energy_joules_est": 230.12, "duration_seconds": 10.993, "sample_count": 93}, "timestamp": "2026-01-25T21:14:06.446422"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12302.567, "latencies_ms": [12302.567], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment on a city street, where a white bus with a blue stripe running along its side is in motion. The bus is adorned with a digital display on the front, proudly announcing \"51 CrossTown\" in bold, red letters. The bus number, \"211\", is also prominently displayed on the front. The license", "error": null, "sys_before": {"cpu_percent": 10.7, "ram_used_mb": 24522.9, "ram_available_mb": 38318.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24582.1, "ram_available_mb": 38258.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.305}, "power_stats": {"power_gpu_soc_mean_watts": 22.859, "power_cpu_cv_mean_watts": 1.77, "power_sys_5v0_mean_watts": 9.203, "gpu_utilization_percent_mean": 74.305, "power_watts_avg": 22.859, "energy_joules_est": 281.24, "duration_seconds": 12.303, "sample_count": 105}, "timestamp": "2026-01-25T21:14:20.777355"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9760.259, "latencies_ms": [9760.259], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "bus: 1, window: multiple, license plate: 1, destination sign: 1, front grill: 1, windshield wipers: 2, headlights: 2, front bumper: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24582.1, "ram_available_mb": 38258.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24516.4, "ram_available_mb": 38324.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.31}, "power_stats": {"power_gpu_soc_mean_watts": 23.802, "power_cpu_cv_mean_watts": 1.549, "power_sys_5v0_mean_watts": 9.14, "gpu_utilization_percent_mean": 77.31, "power_watts_avg": 23.802, "energy_joules_est": 232.33, "duration_seconds": 9.761, "sample_count": 84}, "timestamp": "2026-01-25T21:14:32.592067"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11060.349, "latencies_ms": [11060.349], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The bus is in the foreground of the image, with the front facing the viewer. It is positioned on the right side of the image, with the road visible in the background. The bus number '51' and destination 'Crosstown' are displayed prominently on the front of the bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24516.4, "ram_available_mb": 38324.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24605.1, "ram_available_mb": 38235.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.126}, "power_stats": {"power_gpu_soc_mean_watts": 23.209, "power_cpu_cv_mean_watts": 1.686, "power_sys_5v0_mean_watts": 9.22, "gpu_utilization_percent_mean": 75.126, "power_watts_avg": 23.209, "energy_joules_est": 256.71, "duration_seconds": 11.061, "sample_count": 95}, "timestamp": "2026-01-25T21:14:45.675036"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9359.119, "latencies_ms": [9359.119], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A bus with the number 51 and the destination \"Crosstown\" displayed on its front is driving on a city street. The bus is white with blue and green accents and has the number 61 on its side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24605.1, "ram_available_mb": 38235.8, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24578.0, "ram_available_mb": 38262.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.266}, "power_stats": {"power_gpu_soc_mean_watts": 24.094, "power_cpu_cv_mean_watts": 1.511, "power_sys_5v0_mean_watts": 9.17, "gpu_utilization_percent_mean": 77.266, "power_watts_avg": 24.094, "energy_joules_est": 225.52, "duration_seconds": 9.36, "sample_count": 79}, "timestamp": "2026-01-25T21:14:57.069308"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8139.896, "latencies_ms": [8139.896], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The bus is predominantly white with blue and green accents, and the number 51 is displayed in red on the front. The sky is partly cloudy, suggesting variable weather conditions.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24578.0, "ram_available_mb": 38262.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24627.9, "ram_available_mb": 38213.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.522}, "power_stats": {"power_gpu_soc_mean_watts": 24.426, "power_cpu_cv_mean_watts": 1.428, "power_sys_5v0_mean_watts": 9.27, "gpu_utilization_percent_mean": 77.522, "power_watts_avg": 24.426, "energy_joules_est": 198.84, "duration_seconds": 8.14, "sample_count": 69}, "timestamp": "2026-01-25T21:15:07.232929"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10817.166, "latencies_ms": [10817.166], "images_per_second": 0.092, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing in front of a white wall, exuding an air of confidence. He is dressed in a navy blue blazer, which is adorned with a white crest on the left lapel, adding a touch of elegance to his attire. The blazer is paired with a white shirt, which is complemented by a blue", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24524.2, "ram_available_mb": 38316.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24576.6, "ram_available_mb": 38264.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10124.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 67.402}, "power_stats": {"power_gpu_soc_mean_watts": 18.977, "power_cpu_cv_mean_watts": 1.976, "power_sys_5v0_mean_watts": 8.808, "gpu_utilization_percent_mean": 67.402, "power_watts_avg": 18.977, "energy_joules_est": 205.29, "duration_seconds": 10.818, "sample_count": 92}, "timestamp": "2026-01-25T21:15:20.098600"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8923.968, "latencies_ms": [8923.968], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Man: 1\n\n- Jacket: 1\n\n- Shirt: 1\n\n- Tie: 1\n\n- Skirt: 1\n\n- Pants: 1\n\n- Pocket: 1\n\n- Bag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24514.9, "ram_available_mb": 38326.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 7.8, "ram_used_mb": 24582.9, "ram_available_mb": 38258.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10150.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.5}, "power_stats": {"power_gpu_soc_mean_watts": 19.833, "power_cpu_cv_mean_watts": 2.166, "power_sys_5v0_mean_watts": 8.807, "gpu_utilization_percent_mean": 69.5, "power_watts_avg": 19.833, "energy_joules_est": 177.0, "duration_seconds": 8.925, "sample_count": 76}, "timestamp": "2026-01-25T21:15:31.075081"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8767.824, "latencies_ms": [8767.824], "images_per_second": 0.114, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The man is standing in the foreground with a bag in his left hand. The bag is hanging from his left shoulder, and he is wearing a blue blazer, a striped tie, and a grey skirt. The background consists of a white wall and a red barrier.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24529.4, "ram_available_mb": 38311.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24569.6, "ram_available_mb": 38271.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10159.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.432}, "power_stats": {"power_gpu_soc_mean_watts": 19.831, "power_cpu_cv_mean_watts": 1.845, "power_sys_5v0_mean_watts": 8.847, "gpu_utilization_percent_mean": 69.432, "power_watts_avg": 19.831, "energy_joules_est": 173.89, "duration_seconds": 8.768, "sample_count": 74}, "timestamp": "2026-01-25T21:15:41.862487"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7137.034, "latencies_ms": [7137.034], "images_per_second": 0.14, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A man is standing in a room with a red and white striped background, wearing a blue blazer, white shirt, and a striped tie. He is holding a black bag in his left hand.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24569.6, "ram_available_mb": 38271.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24570.8, "ram_available_mb": 38270.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10147.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.197}, "power_stats": {"power_gpu_soc_mean_watts": 20.907, "power_cpu_cv_mean_watts": 1.7, "power_sys_5v0_mean_watts": 8.821, "gpu_utilization_percent_mean": 71.197, "power_watts_avg": 20.907, "energy_joules_est": 149.23, "duration_seconds": 7.138, "sample_count": 61}, "timestamp": "2026-01-25T21:15:51.010471"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6753.501, "latencies_ms": [6753.501], "images_per_second": 0.148, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image shows a person in a formal outfit with a blue blazer, white shirt, and a striped tie. The person is standing indoors with a red and white striped background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24570.8, "ram_available_mb": 38270.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24587.5, "ram_available_mb": 38253.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10145.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.684}, "power_stats": {"power_gpu_soc_mean_watts": 21.032, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 8.88, "gpu_utilization_percent_mean": 69.684, "power_watts_avg": 21.032, "energy_joules_est": 142.05, "duration_seconds": 6.754, "sample_count": 57}, "timestamp": "2026-01-25T21:15:59.775045"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11126.832, "latencies_ms": [11126.832], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a power plant, bathed in a sepia-toned haze. The plant is a complex network of metal structures, with a series of large cylindrical tanks and pipes that stretch across the frame. These tanks and pipes are arranged in a grid-like pattern, creating a sense of order and precision. The perspective of the image", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 24587.5, "ram_available_mb": 38253.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24593.7, "ram_available_mb": 38247.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.17}, "power_stats": {"power_gpu_soc_mean_watts": 20.93, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 69.17, "power_watts_avg": 20.93, "energy_joules_est": 232.9, "duration_seconds": 11.128, "sample_count": 94}, "timestamp": "2026-01-25T21:16:12.935653"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10619.335, "latencies_ms": [10619.335], "images_per_second": 0.094, "prompt_tokens": 39, "response_tokens_est": 73, "n_tiles": 16, "output_text": "1. Transformer: 10\n2. Transformer: 10\n3. Transformer: 10\n4. Transformer: 10\n5. Transformer: 10\n6. Transformer: 10\n7. Transformer: 10\n8. Transformer: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24531.9, "ram_available_mb": 38309.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24596.5, "ram_available_mb": 38244.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.256}, "power_stats": {"power_gpu_soc_mean_watts": 21.288, "power_cpu_cv_mean_watts": 1.851, "power_sys_5v0_mean_watts": 8.924, "gpu_utilization_percent_mean": 71.256, "power_watts_avg": 21.288, "energy_joules_est": 226.08, "duration_seconds": 10.62, "sample_count": 90}, "timestamp": "2026-01-25T21:16:25.601746"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11200.391, "latencies_ms": [11200.391], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are multiple cylindrical objects that appear to be storage tanks or silos, arranged in a row and spaced evenly apart. They are positioned in the lower half of the image, creating a sense of depth as they recede into the background. The background features a series of vertical structures that resemble power lines or poles, extending from the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24521.1, "ram_available_mb": 38319.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24599.8, "ram_available_mb": 38241.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.885}, "power_stats": {"power_gpu_soc_mean_watts": 20.008, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.885, "power_watts_avg": 20.008, "energy_joules_est": 224.11, "duration_seconds": 11.201, "sample_count": 96}, "timestamp": "2026-01-25T21:16:38.826641"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9160.015, "latencies_ms": [9160.015], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image depicts a large industrial facility with multiple cylindrical storage tanks arranged in rows, connected by a network of pipes and cables. The facility appears to be a power plant or a chemical processing plant, with the tanks likely containing various fluids or chemicals.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24524.9, "ram_available_mb": 38316.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24593.3, "ram_available_mb": 38247.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.808}, "power_stats": {"power_gpu_soc_mean_watts": 21.905, "power_cpu_cv_mean_watts": 1.776, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 71.808, "power_watts_avg": 21.905, "energy_joules_est": 200.66, "duration_seconds": 9.161, "sample_count": 78}, "timestamp": "2026-01-25T21:16:50.010794"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10522.3, "latencies_ms": [10522.3], "images_per_second": 0.095, "prompt_tokens": 36, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image depicts a series of cylindrical objects that appear to be storage tanks or silos, arranged in a row and connected by a network of metal pipes and cables. The lighting is dim and the overall color tone is a sepia-like brown, suggesting an industrial setting with a possible dust or haze in the air.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24593.3, "ram_available_mb": 38247.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24529.6, "ram_available_mb": 38311.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.473}, "power_stats": {"power_gpu_soc_mean_watts": 21.076, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 69.473, "power_watts_avg": 21.076, "energy_joules_est": 221.78, "duration_seconds": 10.523, "sample_count": 91}, "timestamp": "2026-01-25T21:17:02.570707"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11098.812, "latencies_ms": [11098.812], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a bathroom with a white toilet positioned in the center. The floor is covered with a variety of items, including multiple pairs of shoes scattered around the room. There are at least 11 pairs of shoes, some of which are placed near the toilet, while others are spread out across the floor. \n\nIn addition to the", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24529.6, "ram_available_mb": 38311.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24532.1, "ram_available_mb": 38308.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.34}, "power_stats": {"power_gpu_soc_mean_watts": 20.87, "power_cpu_cv_mean_watts": 1.924, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.34, "power_watts_avg": 20.87, "energy_joules_est": 231.65, "duration_seconds": 11.099, "sample_count": 97}, "timestamp": "2026-01-25T21:17:15.721615"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8075.814, "latencies_ms": [8075.814], "images_per_second": 0.124, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "shoes: 12, backpacks: 2, bottles: 4, toilet: 1, plants: 2, towels: 3, books: 1, bags: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24532.1, "ram_available_mb": 38308.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24542.7, "ram_available_mb": 38298.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.853}, "power_stats": {"power_gpu_soc_mean_watts": 22.329, "power_cpu_cv_mean_watts": 1.678, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 72.853, "power_watts_avg": 22.329, "energy_joules_est": 180.34, "duration_seconds": 8.076, "sample_count": 68}, "timestamp": "2026-01-25T21:17:25.813341"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11128.116, "latencies_ms": [11128.116], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a pile of shoes and a helmet on the floor, indicating a recent activity that has concluded. The shoes are spread out near the left side of the image, while the helmet is positioned slightly to the right. In the background, there is a white toilet and a small potted plant on a stand, both", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24542.7, "ram_available_mb": 38298.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24542.6, "ram_available_mb": 38298.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.229}, "power_stats": {"power_gpu_soc_mean_watts": 21.038, "power_cpu_cv_mean_watts": 1.919, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 70.229, "power_watts_avg": 21.038, "energy_joules_est": 234.13, "duration_seconds": 11.129, "sample_count": 96}, "timestamp": "2026-01-25T21:17:38.972610"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9852.885, "latencies_ms": [9852.885], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image depicts a bathroom with a white toilet and a pile of shoes and bags scattered on the floor. There is a plant on the counter and a towel hanging on the wall. It appears that someone has been in a hurry and left their belongings in disarray.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24542.6, "ram_available_mb": 38298.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24540.6, "ram_available_mb": 38300.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.321}, "power_stats": {"power_gpu_soc_mean_watts": 21.481, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 70.321, "power_watts_avg": 21.481, "energy_joules_est": 211.66, "duration_seconds": 9.854, "sample_count": 84}, "timestamp": "2026-01-25T21:17:50.856092"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8375.381, "latencies_ms": [8375.381], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a bathroom with a white toilet and a pile of various items on the floor, including shoes and bags. The lighting is dim, and the colors are mostly muted with some red accents from the shoes.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24540.6, "ram_available_mb": 38300.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24614.7, "ram_available_mb": 38226.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.639}, "power_stats": {"power_gpu_soc_mean_watts": 21.887, "power_cpu_cv_mean_watts": 1.747, "power_sys_5v0_mean_watts": 9.03, "gpu_utilization_percent_mean": 71.639, "power_watts_avg": 21.887, "energy_joules_est": 183.33, "duration_seconds": 8.376, "sample_count": 72}, "timestamp": "2026-01-25T21:18:01.292610"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11133.807, "latencies_ms": [11133.807], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the image, a white polar bear is playing in a shallow pool of water. The bear is holding a green ball in its mouth, and there are two other balls floating nearby. The bear seems to be enjoying its time, splashing around and having fun with the balls. The scene captures the playful nature of the polar bear as it interacts with its environment.", "error": null, "sys_before": {"cpu_percent": 10.7, "ram_used_mb": 24552.8, "ram_available_mb": 38288.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24594.7, "ram_available_mb": 38246.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.948}, "power_stats": {"power_gpu_soc_mean_watts": 20.859, "power_cpu_cv_mean_watts": 1.928, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 68.948, "power_watts_avg": 20.859, "energy_joules_est": 232.25, "duration_seconds": 11.134, "sample_count": 97}, "timestamp": "2026-01-25T21:18:14.491667"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7158.82, "latencies_ms": [7158.82], "images_per_second": 0.14, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "ball: 3\nwater: multiple\npolar bear: 1\nrope: 1\nsand: multiple\nrocks: multiple\nstones: multiple\nleaves: multiple", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24532.6, "ram_available_mb": 38308.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24607.0, "ram_available_mb": 38233.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.667}, "power_stats": {"power_gpu_soc_mean_watts": 22.921, "power_cpu_cv_mean_watts": 1.609, "power_sys_5v0_mean_watts": 9.024, "gpu_utilization_percent_mean": 72.667, "power_watts_avg": 22.921, "energy_joules_est": 164.1, "duration_seconds": 7.159, "sample_count": 60}, "timestamp": "2026-01-25T21:18:23.670798"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11125.424, "latencies_ms": [11125.424], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a polar bear playing with colorful balls in the water. The bear is positioned near the left side of the image, with its paws raised above the water. The balls are scattered around the bear, with one yellow ball to the left, one green ball to the right, and one partially obscured by the bear. The background consists of rocks and a", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24535.1, "ram_available_mb": 38305.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24593.3, "ram_available_mb": 38247.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.085}, "power_stats": {"power_gpu_soc_mean_watts": 20.961, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 70.085, "power_watts_avg": 20.961, "energy_joules_est": 233.21, "duration_seconds": 11.126, "sample_count": 94}, "timestamp": "2026-01-25T21:18:36.841940"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6185.266, "latencies_ms": [6185.266], "images_per_second": 0.162, "prompt_tokens": 37, "response_tokens_est": 33, "n_tiles": 16, "output_text": "A polar bear is playing with colorful balls in a pool of water. The bear is holding a ball in its mouth and appears to be enjoying itself.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24518.3, "ram_available_mb": 38322.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24598.7, "ram_available_mb": 38242.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.077}, "power_stats": {"power_gpu_soc_mean_watts": 23.586, "power_cpu_cv_mean_watts": 1.448, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 76.077, "power_watts_avg": 23.586, "energy_joules_est": 145.9, "duration_seconds": 6.186, "sample_count": 52}, "timestamp": "2026-01-25T21:18:45.069644"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7326.018, "latencies_ms": [7326.018], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image features a polar bear in a body of water with three colorful balls: one green, one yellow, and one blue. The lighting is bright and natural, suggesting the photo was taken during the day.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24523.5, "ram_available_mb": 38317.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24517.5, "ram_available_mb": 38323.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.935}, "power_stats": {"power_gpu_soc_mean_watts": 22.423, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 9.028, "gpu_utilization_percent_mean": 71.935, "power_watts_avg": 22.423, "energy_joules_est": 164.28, "duration_seconds": 7.327, "sample_count": 62}, "timestamp": "2026-01-25T21:18:54.457695"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12457.609, "latencies_ms": [12457.609], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a diptych, split into two parts. On the left, a person is sitting on a wooden chair with their legs crossed. They are wearing blue jeans and a black skirt. The chair is positioned in front of a wooden wall, and the person's feet are resting on a white table. On the right, a person's hand is holding", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24517.5, "ram_available_mb": 38323.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24545.6, "ram_available_mb": 38295.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.99}, "power_stats": {"power_gpu_soc_mean_watts": 22.088, "power_cpu_cv_mean_watts": 1.785, "power_sys_5v0_mean_watts": 9.142, "gpu_utilization_percent_mean": 73.99, "power_watts_avg": 22.088, "energy_joules_est": 275.18, "duration_seconds": 12.458, "sample_count": 105}, "timestamp": "2026-01-25T21:19:08.956046"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8701.469, "latencies_ms": [8701.469], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "person: 1, leg: 2, chair: 1, cell phone: 1, window: 1, wooden panel: 1, denim: 1, screen: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24545.6, "ram_available_mb": 38295.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24603.0, "ram_available_mb": 38237.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.833}, "power_stats": {"power_gpu_soc_mean_watts": 24.135, "power_cpu_cv_mean_watts": 1.452, "power_sys_5v0_mean_watts": 9.121, "gpu_utilization_percent_mean": 77.833, "power_watts_avg": 24.135, "energy_joules_est": 210.02, "duration_seconds": 8.702, "sample_count": 72}, "timestamp": "2026-01-25T21:19:19.681593"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10811.048, "latencies_ms": [10811.048], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "In the left image, a person's legs are positioned in the foreground, with a wooden chair and a wooden ceiling visible in the background. In the right image, a person's hand is holding a flip phone in the foreground, with a window and a wooden wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24603.0, "ram_available_mb": 38237.9, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 24603.8, "ram_available_mb": 38237.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.247}, "power_stats": {"power_gpu_soc_mean_watts": 23.361, "power_cpu_cv_mean_watts": 1.964, "power_sys_5v0_mean_watts": 9.238, "gpu_utilization_percent_mean": 75.247, "power_watts_avg": 23.361, "energy_joules_est": 252.57, "duration_seconds": 10.812, "sample_count": 93}, "timestamp": "2026-01-25T21:19:32.527664"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 12304.286, "latencies_ms": [12304.286], "images_per_second": 0.081, "prompt_tokens": 37, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The image is a split view of a person's lower body and a hand holding a Samsung phone. The left side shows the person sitting on a wooden chair with their legs crossed, wearing blue jeans and a blue sweater. The right side shows the person's hand holding a Samsung phone with the time displayed as 11:26.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24603.8, "ram_available_mb": 38237.1, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24604.8, "ram_available_mb": 38236.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.305}, "power_stats": {"power_gpu_soc_mean_watts": 23.048, "power_cpu_cv_mean_watts": 1.724, "power_sys_5v0_mean_watts": 9.142, "gpu_utilization_percent_mean": 74.305, "power_watts_avg": 23.048, "energy_joules_est": 283.6, "duration_seconds": 12.305, "sample_count": 105}, "timestamp": "2026-01-25T21:19:46.868912"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9581.655, "latencies_ms": [9581.655], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a person sitting on a wooden chair with blue jeans, holding a Samsung flip phone with a screen displaying the time as 11:26. The room has wooden walls and a window with a view of the sky and clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24604.8, "ram_available_mb": 38236.1, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24603.7, "ram_available_mb": 38237.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_gpu_soc_mean_watts": 23.748, "power_cpu_cv_mean_watts": 1.572, "power_sys_5v0_mean_watts": 9.239, "gpu_utilization_percent_mean": 77.0, "power_watts_avg": 23.748, "energy_joules_est": 227.56, "duration_seconds": 9.582, "sample_count": 81}, "timestamp": "2026-01-25T21:19:58.470749"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12343.049, "latencies_ms": [12343.049], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility in a snowy landscape. A yellow train, vibrant against the white backdrop, is making its way down the tracks. The train is moving towards the right side of the image, leaving behind a trail of snow that glistens under the overcast sky. The tracks, also covered in snow, guide the train's journey.", "error": null, "sys_before": {"cpu_percent": 9.4, "ram_used_mb": 24516.7, "ram_available_mb": 38324.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24535.9, "ram_available_mb": 38304.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.915}, "power_stats": {"power_gpu_soc_mean_watts": 22.857, "power_cpu_cv_mean_watts": 1.791, "power_sys_5v0_mean_watts": 9.192, "gpu_utilization_percent_mean": 73.915, "power_watts_avg": 22.857, "energy_joules_est": 282.14, "duration_seconds": 12.344, "sample_count": 106}, "timestamp": "2026-01-25T21:20:12.849246"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10972.949, "latencies_ms": [10972.949], "images_per_second": 0.091, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "- Train: 1\n\n- Train tracks: 2\n\n- Trees: 10\n\n- Snow: 1\n\n- Sky: 1\n\n- Birds: 1\n\n- Train station: 1\n\n- Snow-covered ground: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24535.9, "ram_available_mb": 38304.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24606.1, "ram_available_mb": 38234.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.189}, "power_stats": {"power_gpu_soc_mean_watts": 23.37, "power_cpu_cv_mean_watts": 1.652, "power_sys_5v0_mean_watts": 9.15, "gpu_utilization_percent_mean": 76.189, "power_watts_avg": 23.37, "energy_joules_est": 256.45, "duration_seconds": 10.974, "sample_count": 95}, "timestamp": "2026-01-25T21:20:25.841175"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12325.422, "latencies_ms": [12325.422], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The train is in the foreground, moving along the tracks which are in the middle ground of the image. The trees on either side of the tracks create a natural boundary, with the left side denser and closer to the viewer, while the right side is more open and extends into the background. The sky is visible in the far background, above the trees and the train.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24606.1, "ram_available_mb": 38234.8, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24520.5, "ram_available_mb": 38320.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.217}, "power_stats": {"power_gpu_soc_mean_watts": 22.911, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 9.22, "gpu_utilization_percent_mean": 74.217, "power_watts_avg": 22.911, "energy_joules_est": 282.41, "duration_seconds": 12.326, "sample_count": 106}, "timestamp": "2026-01-25T21:20:40.183175"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7932.053, "latencies_ms": [7932.053], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A train is traveling through a snowy landscape with trees on both sides of the tracks. The sky is overcast and the ground is covered in a thick layer of snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24520.5, "ram_available_mb": 38320.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24627.0, "ram_available_mb": 38213.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 80.358}, "power_stats": {"power_gpu_soc_mean_watts": 24.77, "power_cpu_cv_mean_watts": 1.345, "power_sys_5v0_mean_watts": 9.181, "gpu_utilization_percent_mean": 80.358, "power_watts_avg": 24.77, "energy_joules_est": 196.49, "duration_seconds": 7.933, "sample_count": 67}, "timestamp": "2026-01-25T21:20:50.170531"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8573.652, "latencies_ms": [8573.652], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image depicts a snowy landscape with a train moving through the scene. The sky is overcast, and the ground is covered in a thick layer of snow, indicating a cold and possibly wintry day.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24627.0, "ram_available_mb": 38213.9, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24648.0, "ram_available_mb": 38192.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.397}, "power_stats": {"power_gpu_soc_mean_watts": 24.186, "power_cpu_cv_mean_watts": 1.481, "power_sys_5v0_mean_watts": 9.239, "gpu_utilization_percent_mean": 78.397, "power_watts_avg": 24.186, "energy_joules_est": 207.38, "duration_seconds": 8.574, "sample_count": 73}, "timestamp": "2026-01-25T21:21:00.797902"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11117.279, "latencies_ms": [11117.279], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a group of people walking through a snow-covered street. There are five people in the scene, with some of them carrying handbags. They are navigating the snowy sidewalk, which is covered in a thick layer of snow.\n\nIn the foreground, there is a fire hydrant partially buried under the snow, making it less visible. The", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24648.0, "ram_available_mb": 38192.9, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24526.9, "ram_available_mb": 38314.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.585}, "power_stats": {"power_gpu_soc_mean_watts": 20.972, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 69.585, "power_watts_avg": 20.972, "energy_joules_est": 233.17, "duration_seconds": 11.118, "sample_count": 94}, "timestamp": "2026-01-25T21:21:13.950664"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6053.174, "latencies_ms": [6053.174], "images_per_second": 0.165, "prompt_tokens": 39, "response_tokens_est": 32, "n_tiles": 16, "output_text": "- Snow: numerous piles\n- People: 5\n- Fire hydrant: 1\n- Bricks: numerous bricks in the background", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24526.9, "ram_available_mb": 38314.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24523.0, "ram_available_mb": 38317.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.863}, "power_stats": {"power_gpu_soc_mean_watts": 23.692, "power_cpu_cv_mean_watts": 1.461, "power_sys_5v0_mean_watts": 9.042, "gpu_utilization_percent_mean": 74.863, "power_watts_avg": 23.692, "energy_joules_est": 143.43, "duration_seconds": 6.054, "sample_count": 51}, "timestamp": "2026-01-25T21:21:22.037604"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11117.227, "latencies_ms": [11117.227], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large pile of snow on the left side of the image, partially covering a fire hydrant that is located on the right side. The fire hydrant is partially buried under the snow, indicating that the snowfall has been significant. In the background, there are people walking on the sidewalk, with one person in the center of the image walking towards the", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24523.0, "ram_available_mb": 38317.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24516.0, "ram_available_mb": 38324.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.798}, "power_stats": {"power_gpu_soc_mean_watts": 20.963, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 69.798, "power_watts_avg": 20.963, "energy_joules_est": 233.06, "duration_seconds": 11.118, "sample_count": 94}, "timestamp": "2026-01-25T21:21:35.182874"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6506.437, "latencies_ms": [6506.437], "images_per_second": 0.154, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The image depicts a snowy street with a large pile of snow on the side. People are walking around the pile, some of them carrying bags.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24516.0, "ram_available_mb": 38324.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24525.7, "ram_available_mb": 38315.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_gpu_soc_mean_watts": 23.382, "power_cpu_cv_mean_watts": 1.485, "power_sys_5v0_mean_watts": 9.01, "gpu_utilization_percent_mean": 75.0, "power_watts_avg": 23.382, "energy_joules_est": 152.15, "duration_seconds": 6.507, "sample_count": 55}, "timestamp": "2026-01-25T21:21:43.736933"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7537.331, "latencies_ms": [7537.331], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a large pile of white snow covering the ground and partially covering a fire hydrant. The sky is overcast, and the lighting is dim, indicating that it might be a cold and cloudy day.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24525.7, "ram_available_mb": 38315.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24520.2, "ram_available_mb": 38320.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.812}, "power_stats": {"power_gpu_soc_mean_watts": 22.263, "power_cpu_cv_mean_watts": 1.683, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 71.812, "power_watts_avg": 22.263, "energy_joules_est": 167.82, "duration_seconds": 7.538, "sample_count": 64}, "timestamp": "2026-01-25T21:21:53.312391"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11114.447, "latencies_ms": [11114.447], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene on a city street. Dominating the right side of the frame is a red and white \"No Parking\" sign, its bold colors standing out against the backdrop of the urban landscape. Just below it, a yellow \"No Stopping\" sign adds another layer of regulation to the area. These signs are affixed to a sturdy metal pole,", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24520.2, "ram_available_mb": 38320.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24576.9, "ram_available_mb": 38264.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.17}, "power_stats": {"power_gpu_soc_mean_watts": 20.98, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 69.17, "power_watts_avg": 20.98, "energy_joules_est": 233.19, "duration_seconds": 11.115, "sample_count": 94}, "timestamp": "2026-01-25T21:22:06.462605"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7291.578, "latencies_ms": [7291.578], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "tree: 1, sign: 2, pole: 2, building: 1, window: 1, staircase: 1, plant: 1, toy: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24515.3, "ram_available_mb": 38325.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24582.2, "ram_available_mb": 38258.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.274}, "power_stats": {"power_gpu_soc_mean_watts": 22.756, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 74.274, "power_watts_avg": 22.756, "energy_joules_est": 165.94, "duration_seconds": 7.292, "sample_count": 62}, "timestamp": "2026-01-25T21:22:15.817918"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11127.457, "latencies_ms": [11127.457], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a yellow sign with a black symbol of a tooth, positioned to the right of a pole that holds a 'No Parking' sign with a red arrow pointing to the right. The 'No Parking' sign is located near the center of the image, while the tooth sign is closer to the viewer. In the background, there are trees", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24527.2, "ram_available_mb": 38313.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24598.4, "ram_available_mb": 38242.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.691}, "power_stats": {"power_gpu_soc_mean_watts": 20.975, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.691, "power_watts_avg": 20.975, "energy_joules_est": 233.41, "duration_seconds": 11.128, "sample_count": 94}, "timestamp": "2026-01-25T21:22:28.971444"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10291.496, "latencies_ms": [10291.496], "images_per_second": 0.097, "prompt_tokens": 37, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image shows a street scene with a \"No Parking\" sign and a \"Tow Zone\" sign attached to a pole, indicating that parking is not allowed in this area. There is also a yellow sign with a smiley face on it, which seems to be a playful addition to the otherwise strict parking regulations.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24598.4, "ram_available_mb": 38242.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24599.1, "ram_available_mb": 38241.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.348}, "power_stats": {"power_gpu_soc_mean_watts": 21.316, "power_cpu_cv_mean_watts": 1.849, "power_sys_5v0_mean_watts": 8.924, "gpu_utilization_percent_mean": 70.348, "power_watts_avg": 21.316, "energy_joules_est": 219.39, "duration_seconds": 10.292, "sample_count": 89}, "timestamp": "2026-01-25T21:22:41.325445"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11099.221, "latencies_ms": [11099.221], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a bright, sunny day with clear skies, as evidenced by the strong shadows cast on the ground and the vibrant green leaves on the trees. The signs are mounted on metal poles, and the \"No Parking\" sign has a red circle with a blue background and a red slash through it, while the \"Tow Zone\" sign is red", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24599.1, "ram_available_mb": 38241.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24599.9, "ram_available_mb": 38241.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.432}, "power_stats": {"power_gpu_soc_mean_watts": 20.884, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.986, "gpu_utilization_percent_mean": 69.432, "power_watts_avg": 20.884, "energy_joules_est": 231.81, "duration_seconds": 11.1, "sample_count": 95}, "timestamp": "2026-01-25T21:22:54.440167"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11113.525, "latencies_ms": [11113.525], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a light brown teddy bear is the main subject. The teddy bear is wearing round glasses and has a black nose, giving it a cute and endearing appearance. It's seated on a red desk, which contrasts with its light brown fur. \n\nThe teddy bear is not just a passive observer, it", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24599.9, "ram_available_mb": 38241.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24608.6, "ram_available_mb": 38232.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.505}, "power_stats": {"power_gpu_soc_mean_watts": 20.919, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 69.505, "power_watts_avg": 20.919, "energy_joules_est": 232.5, "duration_seconds": 11.114, "sample_count": 95}, "timestamp": "2026-01-25T21:23:07.576712"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7734.255, "latencies_ms": [7734.255], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "teddy bear: 1, glasses: 1, earphones: 1, phone: 1, remote control: 1, keyboard: 1, microphone: 1, mouse: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24608.6, "ram_available_mb": 38232.3, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24598.6, "ram_available_mb": 38242.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.077}, "power_stats": {"power_gpu_soc_mean_watts": 22.541, "power_cpu_cv_mean_watts": 1.639, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 73.077, "power_watts_avg": 22.541, "energy_joules_est": 174.35, "duration_seconds": 7.735, "sample_count": 65}, "timestamp": "2026-01-25T21:23:17.331612"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11212.692, "latencies_ms": [11212.692], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a black keyboard positioned to the left of a teddy bear that is sitting behind it. The teddy bear is wearing glasses and has a heart-shaped nose, and it is placed on a red surface. To the right of the teddy bear, there is a white smartphone and a brown remote control, both of which are near the", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24528.4, "ram_available_mb": 38312.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 24597.5, "ram_available_mb": 38243.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.611}, "power_stats": {"power_gpu_soc_mean_watts": 20.877, "power_cpu_cv_mean_watts": 1.956, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.611, "power_watts_avg": 20.877, "energy_joules_est": 234.1, "duration_seconds": 11.213, "sample_count": 95}, "timestamp": "2026-01-25T21:23:30.601933"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9233.981, "latencies_ms": [9233.981], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "In the image, a teddy bear is sitting on a red surface, surrounded by various electronic devices including a keyboard, a microphone, and a cell phone. The setting appears to be a cozy and playful environment, possibly a child's room or a creative workspace.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24519.2, "ram_available_mb": 38321.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24593.7, "ram_available_mb": 38247.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.987}, "power_stats": {"power_gpu_soc_mean_watts": 21.672, "power_cpu_cv_mean_watts": 1.792, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 70.987, "power_watts_avg": 21.672, "energy_joules_est": 200.13, "duration_seconds": 9.235, "sample_count": 78}, "timestamp": "2026-01-25T21:23:41.866771"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8705.132, "latencies_ms": [8705.132], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image features a teddy bear with glasses, a white phone, and a microphone, all placed on a red surface. The lighting is dim, creating a moody atmosphere, and the teddy bear appears to be made of a soft, plush material.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24593.7, "ram_available_mb": 38247.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24594.6, "ram_available_mb": 38246.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.311}, "power_stats": {"power_gpu_soc_mean_watts": 21.826, "power_cpu_cv_mean_watts": 1.764, "power_sys_5v0_mean_watts": 9.007, "gpu_utilization_percent_mean": 71.311, "power_watts_avg": 21.826, "energy_joules_est": 190.01, "duration_seconds": 8.706, "sample_count": 74}, "timestamp": "2026-01-25T21:23:52.595990"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12318.999, "latencies_ms": [12318.999], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a skier is captured in mid-air, performing a daring trick on a snowy mountain. The skier, clad in a vibrant orange jacket and black pants, is holding two ski poles, aiding in their aerial maneuver. The mountain beneath them is blanketed in a thick layer of snow, with a few trees", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24594.6, "ram_available_mb": 38246.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24629.5, "ram_available_mb": 38211.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.057}, "power_stats": {"power_gpu_soc_mean_watts": 22.831, "power_cpu_cv_mean_watts": 1.774, "power_sys_5v0_mean_watts": 9.2, "gpu_utilization_percent_mean": 74.057, "power_watts_avg": 22.831, "energy_joules_est": 281.27, "duration_seconds": 12.32, "sample_count": 105}, "timestamp": "2026-01-25T21:24:06.974722"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8678.189, "latencies_ms": [8678.189], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "mountain: 1, ski: 2, snow: 1, tree: 1, snowboard: 1, skier: 1, snowboarder: 1, snow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24629.5, "ram_available_mb": 38211.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24641.3, "ram_available_mb": 38199.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.473}, "power_stats": {"power_gpu_soc_mean_watts": 24.316, "power_cpu_cv_mean_watts": 1.45, "power_sys_5v0_mean_watts": 9.186, "gpu_utilization_percent_mean": 77.473, "power_watts_avg": 24.316, "energy_joules_est": 211.03, "duration_seconds": 8.679, "sample_count": 74}, "timestamp": "2026-01-25T21:24:17.701295"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11171.139, "latencies_ms": [11171.139], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, a skier is captured in mid-air, performing a jump on the left side of the image. The skier is near the bottom of the frame, with snow and a clear blue sky in the background. The mountain and trees are further in the background, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24579.3, "ram_available_mb": 38261.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24646.7, "ram_available_mb": 38194.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.821}, "power_stats": {"power_gpu_soc_mean_watts": 23.238, "power_cpu_cv_mean_watts": 1.699, "power_sys_5v0_mean_watts": 9.223, "gpu_utilization_percent_mean": 74.821, "power_watts_avg": 23.238, "energy_joules_est": 259.61, "duration_seconds": 11.172, "sample_count": 95}, "timestamp": "2026-01-25T21:24:30.888892"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9625.044, "latencies_ms": [9625.044], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A skier is captured in mid-air, performing a jump on a snowy mountain slope. The date \"27 febbraio 2010\" and the event \"Tour de Sas\" are displayed at the bottom of the image.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24583.2, "ram_available_mb": 38257.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24620.5, "ram_available_mb": 38220.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.037}, "power_stats": {"power_gpu_soc_mean_watts": 23.858, "power_cpu_cv_mean_watts": 1.514, "power_sys_5v0_mean_watts": 9.132, "gpu_utilization_percent_mean": 77.037, "power_watts_avg": 23.858, "energy_joules_est": 229.65, "duration_seconds": 9.626, "sample_count": 82}, "timestamp": "2026-01-25T21:24:42.560143"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9912.698, "latencies_ms": [9912.698], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image captures a dynamic moment of a skier in mid-air, with the snow splashing around as they perform a jump. The skier is wearing an orange jacket and black pants, contrasting against the white snow and the clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24620.5, "ram_available_mb": 38220.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24642.0, "ram_available_mb": 38198.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.976}, "power_stats": {"power_gpu_soc_mean_watts": 23.597, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 9.23, "gpu_utilization_percent_mean": 75.976, "power_watts_avg": 23.597, "energy_joules_est": 233.92, "duration_seconds": 9.913, "sample_count": 84}, "timestamp": "2026-01-25T21:24:54.514923"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12349.651, "latencies_ms": [12349.651], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a dynamic scene of two surfers riding waves in the ocean. The surfer in the foreground is skillfully maneuvering a wave, while the other surfer is seen in the background, waiting for their turn. The waves are large and powerful, creating a sense of excitement and adventure. The ocean is a deep blue color, and the sky is a", "error": null, "sys_before": {"cpu_percent": 8.8, "ram_used_mb": 24578.6, "ram_available_mb": 38262.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24644.8, "ram_available_mb": 38196.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.019}, "power_stats": {"power_gpu_soc_mean_watts": 22.846, "power_cpu_cv_mean_watts": 1.774, "power_sys_5v0_mean_watts": 9.188, "gpu_utilization_percent_mean": 74.019, "power_watts_avg": 22.846, "energy_joules_est": 282.15, "duration_seconds": 12.35, "sample_count": 105}, "timestamp": "2026-01-25T21:25:08.945657"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5114.574, "latencies_ms": [5114.574], "images_per_second": 0.196, "prompt_tokens": 39, "response_tokens_est": 12, "n_tiles": 16, "output_text": "wave: 3\nsurfer: 2\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24524.5, "ram_available_mb": 38316.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24608.0, "ram_available_mb": 38232.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 86.907}, "power_stats": {"power_gpu_soc_mean_watts": 27.435, "power_cpu_cv_mean_watts": 0.735, "power_sys_5v0_mean_watts": 9.245, "gpu_utilization_percent_mean": 86.907, "power_watts_avg": 27.435, "energy_joules_est": 140.34, "duration_seconds": 5.115, "sample_count": 43}, "timestamp": "2026-01-25T21:25:16.112798"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12441.186, "latencies_ms": [12441.186], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a surfer riding a wave on the left side of the image, while another surfer is further back, closer to the horizon, on the right side. The waves are in the middle ground, with the closest wave being surfed by the surfer on the left and the further waves extending towards the background. The background is dominated by the ocean", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24526.2, "ram_available_mb": 38314.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24531.9, "ram_available_mb": 38309.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.377}, "power_stats": {"power_gpu_soc_mean_watts": 22.899, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 9.22, "gpu_utilization_percent_mean": 74.377, "power_watts_avg": 22.899, "energy_joules_est": 284.9, "duration_seconds": 12.442, "sample_count": 106}, "timestamp": "2026-01-25T21:25:30.583802"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10975.646, "latencies_ms": [10975.646], "images_per_second": 0.091, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image captures a dynamic scene of two surfers riding waves in the ocean. One surfer is skillfully navigating a large wave, while the other is waiting for their turn. The waves are powerful and the water is a deep blue, creating an exhilarating atmosphere for the surfers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24531.9, "ram_available_mb": 38309.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24539.5, "ram_available_mb": 38301.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.581}, "power_stats": {"power_gpu_soc_mean_watts": 23.43, "power_cpu_cv_mean_watts": 1.658, "power_sys_5v0_mean_watts": 9.153, "gpu_utilization_percent_mean": 75.581, "power_watts_avg": 23.43, "energy_joules_est": 257.17, "duration_seconds": 10.976, "sample_count": 93}, "timestamp": "2026-01-25T21:25:43.599006"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9718.882, "latencies_ms": [9718.882], "images_per_second": 0.103, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image captures a serene beach scene with a surfer riding a wave. The lighting is soft and warm, suggesting it might be either early morning or late afternoon. The waves are large and powerful, indicating strong ocean currents or a recent storm.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24539.5, "ram_available_mb": 38301.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24645.6, "ram_available_mb": 38195.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.427}, "power_stats": {"power_gpu_soc_mean_watts": 23.621, "power_cpu_cv_mean_watts": 1.592, "power_sys_5v0_mean_watts": 9.241, "gpu_utilization_percent_mean": 76.427, "power_watts_avg": 23.621, "energy_joules_est": 229.59, "duration_seconds": 9.72, "sample_count": 82}, "timestamp": "2026-01-25T21:25:55.377545"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11128.081, "latencies_ms": [11128.081], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a delicious pizza sitting on a plate, placed on a dining table. The pizza is topped with various ingredients, including cheese, tomatoes, and possibly onions. The table is covered with a red tablecloth, and there is a fork and a knife placed next to the pizza, ready for someone to enjoy the meal.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24558.8, "ram_available_mb": 38282.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24551.8, "ram_available_mb": 38289.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.412}, "power_stats": {"power_gpu_soc_mean_watts": 20.853, "power_cpu_cv_mean_watts": 1.924, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 69.412, "power_watts_avg": 20.853, "energy_joules_est": 232.07, "duration_seconds": 11.129, "sample_count": 97}, "timestamp": "2026-01-25T21:26:08.540922"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8926.991, "latencies_ms": [8926.991], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 58, "n_tiles": 16, "output_text": "pizza: 1\nsalt shaker: 1\npepper shaker: 1\nblue napkin: 1\nwhite napkin: 1\nblue tablecloth: 1\nred tablecloth: 1\nfork: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24551.8, "ram_available_mb": 38289.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24609.2, "ram_available_mb": 38231.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.039}, "power_stats": {"power_gpu_soc_mean_watts": 21.853, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 71.039, "power_watts_avg": 21.853, "energy_joules_est": 195.1, "duration_seconds": 8.928, "sample_count": 76}, "timestamp": "2026-01-25T21:26:19.529573"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11146.711, "latencies_ms": [11146.711], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The pizza is in the foreground, placed on a plate that is on a table covered with a red tablecloth. There is a salt shaker positioned to the left of the pizza, and a blue and white striped napkin is folded and placed to the right of the plate. The background is blurred, but it appears to be a dimly lit room", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24522.3, "ram_available_mb": 38318.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24614.4, "ram_available_mb": 38226.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.979}, "power_stats": {"power_gpu_soc_mean_watts": 20.898, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 68.979, "power_watts_avg": 20.898, "energy_joules_est": 232.96, "duration_seconds": 11.147, "sample_count": 96}, "timestamp": "2026-01-25T21:26:32.694783"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6598.331, "latencies_ms": [6598.331], "images_per_second": 0.152, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A pizza with various toppings is placed on a silver plate on a table with a red and blue tablecloth. A salt shaker is also visible on the table.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24535.9, "ram_available_mb": 38305.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24613.4, "ram_available_mb": 38227.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.018}, "power_stats": {"power_gpu_soc_mean_watts": 23.224, "power_cpu_cv_mean_watts": 1.553, "power_sys_5v0_mean_watts": 9.008, "gpu_utilization_percent_mean": 75.018, "power_watts_avg": 23.224, "energy_joules_est": 153.25, "duration_seconds": 6.599, "sample_count": 57}, "timestamp": "2026-01-25T21:26:41.330210"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10199.271, "latencies_ms": [10199.271], "images_per_second": 0.098, "prompt_tokens": 36, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The image features a pizza with a golden-brown crust, topped with melted cheese, tomato sauce, and various toppings, served on a silver plate. The setting is dimly lit with a red and blue color scheme, and there is a salt shaker and a blue napkin on the table.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24613.4, "ram_available_mb": 38227.5, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24613.7, "ram_available_mb": 38227.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.034}, "power_stats": {"power_gpu_soc_mean_watts": 21.177, "power_cpu_cv_mean_watts": 1.873, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 70.034, "power_watts_avg": 21.177, "energy_joules_est": 216.0, "duration_seconds": 10.2, "sample_count": 87}, "timestamp": "2026-01-25T21:26:53.554692"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11100.018, "latencies_ms": [11100.018], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene winter scene in a small town. Dominating the foreground is a black clock post, standing tall on a brick sidewalk. The clock face is white, adorned with black numbers and hands, marking the time. The post is situated on the left side of the image, providing a sense of depth and perspective.\n\nThe sidewalk, made", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24613.7, "ram_available_mb": 38227.2, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24613.7, "ram_available_mb": 38227.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.432}, "power_stats": {"power_gpu_soc_mean_watts": 20.912, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 69.432, "power_watts_avg": 20.912, "energy_joules_est": 232.14, "duration_seconds": 11.101, "sample_count": 95}, "timestamp": "2026-01-25T21:27:06.684766"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9390.714, "latencies_ms": [9390.714], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "1. Clock: 1\n2. Trees: 2\n3. Buildings: 3\n4. Cars: 2\n5. Snow: 1\n6. Sidewalk: 1\n7. Street: 1\n8. Sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24613.7, "ram_available_mb": 38227.2, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24635.6, "ram_available_mb": 38205.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.85}, "power_stats": {"power_gpu_soc_mean_watts": 21.687, "power_cpu_cv_mean_watts": 1.802, "power_sys_5v0_mean_watts": 9.003, "gpu_utilization_percent_mean": 70.85, "power_watts_avg": 21.687, "energy_joules_est": 203.67, "duration_seconds": 9.391, "sample_count": 80}, "timestamp": "2026-01-25T21:27:18.098355"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10061.441, "latencies_ms": [10061.441], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The clock is positioned in the foreground on the left side of the image, standing on a brick sidewalk. In the background, there is a street that runs parallel to the sidewalk, with buildings on both sides. The sky is visible in the far background, indicating that the scene is set outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24635.6, "ram_available_mb": 38205.3, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24605.8, "ram_available_mb": 38235.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.871}, "power_stats": {"power_gpu_soc_mean_watts": 21.359, "power_cpu_cv_mean_watts": 1.861, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 69.871, "power_watts_avg": 21.359, "energy_joules_est": 214.92, "duration_seconds": 10.062, "sample_count": 85}, "timestamp": "2026-01-25T21:27:30.188347"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9457.711, "latencies_ms": [9457.711], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image depicts a snowy street scene with a prominent black clock post on the left side, which is labeled \"First National Bank.\" The clock shows the time as 5:00, and the street is lined with buildings on both sides, some of which have storefronts.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24605.8, "ram_available_mb": 38235.1, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 24612.7, "ram_available_mb": 38228.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.074}, "power_stats": {"power_gpu_soc_mean_watts": 21.802, "power_cpu_cv_mean_watts": 2.309, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 72.074, "power_watts_avg": 21.802, "energy_joules_est": 206.21, "duration_seconds": 9.458, "sample_count": 81}, "timestamp": "2026-01-25T21:27:41.677957"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9699.577, "latencies_ms": [9699.577], "images_per_second": 0.103, "prompt_tokens": 36, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image features a black clock with white numerals and hands, standing on a pole with a snowy ground beneath it. The sky is clear with a gradient of blue to white, indicating either sunrise or sunset, and the buildings have a mix of brick and siding with visible windows and storefronts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24612.7, "ram_available_mb": 38228.2, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24584.9, "ram_available_mb": 38256.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.181}, "power_stats": {"power_gpu_soc_mean_watts": 21.387, "power_cpu_cv_mean_watts": 1.843, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 70.181, "power_watts_avg": 21.387, "energy_joules_est": 207.46, "duration_seconds": 9.7, "sample_count": 83}, "timestamp": "2026-01-25T21:27:53.429135"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11137.001, "latencies_ms": [11137.001], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a baseball game is in progress with a batter and a catcher in the middle of a play. The batter is holding a baseball bat and is in the process of swinging at a pitched ball. The catcher, wearing a baseball glove, is positioned behind the batter, ready to catch the ball if the batter misses.\n\nThere are several other", "error": null, "sys_before": {"cpu_percent": 10.7, "ram_used_mb": 24514.7, "ram_available_mb": 38326.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24616.0, "ram_available_mb": 38224.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.106}, "power_stats": {"power_gpu_soc_mean_watts": 20.907, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 69.106, "power_watts_avg": 20.907, "energy_joules_est": 232.86, "duration_seconds": 11.138, "sample_count": 94}, "timestamp": "2026-01-25T21:28:06.593820"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7281.781, "latencies_ms": [7281.781], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "ball: 1, player: 1, bat: 1, glove: 1, catcher: 1, uniform: 1, helmet: 1, base: 1", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24616.0, "ram_available_mb": 38224.9, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24610.0, "ram_available_mb": 38230.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.885}, "power_stats": {"power_gpu_soc_mean_watts": 22.829, "power_cpu_cv_mean_watts": 1.602, "power_sys_5v0_mean_watts": 9.033, "gpu_utilization_percent_mean": 72.885, "power_watts_avg": 22.829, "energy_joules_est": 166.25, "duration_seconds": 7.282, "sample_count": 61}, "timestamp": "2026-01-25T21:28:15.889910"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11133.529, "latencies_ms": [11133.529], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a baseball player in a blue helmet and striped uniform is swinging a bat at a baseball that is in mid-air to his left. In the background, a catcher in a red uniform and helmet is crouched down, ready to catch the ball with his glove. The field is surrounded by a green fence and there are some trees and", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24610.0, "ram_available_mb": 38230.9, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24627.7, "ram_available_mb": 38213.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.396}, "power_stats": {"power_gpu_soc_mean_watts": 20.922, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 69.396, "power_watts_avg": 20.922, "energy_joules_est": 232.95, "duration_seconds": 11.134, "sample_count": 96}, "timestamp": "2026-01-25T21:28:29.064497"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8929.475, "latencies_ms": [8929.475], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image captures a moment from a baseball game where a player in a blue and white uniform is swinging a bat at an incoming ball. The catcher, wearing a red uniform, is crouched behind the batter, ready to catch the ball with his glove.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24627.7, "ram_available_mb": 38213.2, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24627.7, "ram_available_mb": 38213.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.844}, "power_stats": {"power_gpu_soc_mean_watts": 21.725, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 71.844, "power_watts_avg": 21.725, "energy_joules_est": 194.01, "duration_seconds": 8.93, "sample_count": 77}, "timestamp": "2026-01-25T21:28:40.058408"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7765.704, "latencies_ms": [7765.704], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image captures a baseball game in progress with a player in a blue helmet and striped uniform swinging a bat at a baseball. The weather appears to be clear and sunny, casting shadows on the dirt field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24564.1, "ram_available_mb": 38276.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24614.2, "ram_available_mb": 38226.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.323}, "power_stats": {"power_gpu_soc_mean_watts": 22.233, "power_cpu_cv_mean_watts": 1.7, "power_sys_5v0_mean_watts": 9.045, "gpu_utilization_percent_mean": 72.323, "power_watts_avg": 22.233, "energy_joules_est": 172.67, "duration_seconds": 7.766, "sample_count": 65}, "timestamp": "2026-01-25T21:28:49.851520"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11142.953, "latencies_ms": [11142.953], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a brown teddy bear is comfortably seated on a brown wicker chair. The teddy bear, adorned with a red and white checkered scarf, is facing the camera, giving us a clear view of its features. The chair, with its brown wicker design, is positioned against a backdrop of a white wall. To the", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24524.0, "ram_available_mb": 38316.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24617.5, "ram_available_mb": 38223.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.66}, "power_stats": {"power_gpu_soc_mean_watts": 20.785, "power_cpu_cv_mean_watts": 1.928, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 69.66, "power_watts_avg": 20.785, "energy_joules_est": 231.62, "duration_seconds": 11.144, "sample_count": 97}, "timestamp": "2026-01-25T21:29:03.033767"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11323.938, "latencies_ms": [11323.938], "images_per_second": 0.088, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "- Chair: 1\n\n- Teddy bear: 1\n\n- Chair cushion: 1\n\n- Curtain: 1\n\n- Teddy bear's bow: 1\n\n- Teddy bear's paws: 2\n\n- Teddy bear's arms: 2\n\n- Teddy bear's legs: ", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24539.8, "ram_available_mb": 38301.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24615.3, "ram_available_mb": 38225.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.299}, "power_stats": {"power_gpu_soc_mean_watts": 21.085, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.925, "gpu_utilization_percent_mean": 70.299, "power_watts_avg": 21.085, "energy_joules_est": 238.78, "duration_seconds": 11.325, "sample_count": 97}, "timestamp": "2026-01-25T21:29:16.374277"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11170.938, "latencies_ms": [11170.938], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The teddy bear is seated on the left side of the chair, positioned in the foreground of the image. It is placed near the front of the chair, which is on the right side, and appears to be at a comfortable distance from the backrest. The chair itself is situated in the background, with its legs and armrests extending outward, creating a sense of depth", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24520.0, "ram_available_mb": 38320.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24618.0, "ram_available_mb": 38222.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.695}, "power_stats": {"power_gpu_soc_mean_watts": 20.961, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 69.695, "power_watts_avg": 20.961, "energy_joules_est": 234.17, "duration_seconds": 11.172, "sample_count": 95}, "timestamp": "2026-01-25T21:29:29.610079"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6747.577, "latencies_ms": [6747.577], "images_per_second": 0.148, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A brown teddy bear is sitting on a brown wicker chair with a red cushion. The chair is placed against a wall with a striped curtain to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24529.4, "ram_available_mb": 38311.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24589.2, "ram_available_mb": 38251.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.263}, "power_stats": {"power_gpu_soc_mean_watts": 23.201, "power_cpu_cv_mean_watts": 1.546, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 74.263, "power_watts_avg": 23.201, "energy_joules_est": 156.56, "duration_seconds": 6.748, "sample_count": 57}, "timestamp": "2026-01-25T21:29:38.416253"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8243.612, "latencies_ms": [8243.612], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image features a brown teddy bear with a red and white checkered scarf sitting on a chair with a woven rattan design and a red cushion. The lighting in the room is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24589.2, "ram_available_mb": 38251.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24513.8, "ram_available_mb": 38327.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.071}, "power_stats": {"power_gpu_soc_mean_watts": 22.023, "power_cpu_cv_mean_watts": 1.756, "power_sys_5v0_mean_watts": 9.032, "gpu_utilization_percent_mean": 71.071, "power_watts_avg": 22.023, "energy_joules_est": 181.56, "duration_seconds": 8.244, "sample_count": 70}, "timestamp": "2026-01-25T21:29:48.686177"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11103.552, "latencies_ms": [11103.552], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two snowboarders are standing on a snowy mountain, preparing to ride down the slope. They are both wearing red jackets and black pants, and one of them is holding a snowboard. The snowboarders are positioned close to each other, with one snowboarder on the left and the other on the right.\n\nThe scene is set", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 24513.8, "ram_available_mb": 38327.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24581.0, "ram_available_mb": 38259.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.862}, "power_stats": {"power_gpu_soc_mean_watts": 20.92, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 69.862, "power_watts_avg": 20.92, "energy_joules_est": 232.3, "duration_seconds": 11.104, "sample_count": 94}, "timestamp": "2026-01-25T21:30:01.819998"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7849.926, "latencies_ms": [7849.926], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "sun: 1\nmountain: 1\nperson: 2\nsnowboard: 1\nsnow: 1\nclothing: 2\ngoggles: 1\nhat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24581.0, "ram_available_mb": 38259.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24592.4, "ram_available_mb": 38248.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.806}, "power_stats": {"power_gpu_soc_mean_watts": 22.429, "power_cpu_cv_mean_watts": 1.649, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 72.806, "power_watts_avg": 22.429, "energy_joules_est": 176.08, "duration_seconds": 7.851, "sample_count": 67}, "timestamp": "2026-01-25T21:30:11.722580"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10616.991, "latencies_ms": [10616.991], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "In the foreground, there are two snowboarders standing on a snowy surface. The person on the left is wearing a red jacket and is positioned slightly behind the person on the right, who is wearing a red jacket and holding a snowboard. In the background, there are mountains and a bright sun shining in the sky.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24592.4, "ram_available_mb": 38248.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24519.0, "ram_available_mb": 38321.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.587}, "power_stats": {"power_gpu_soc_mean_watts": 21.104, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.587, "power_watts_avg": 21.104, "energy_joules_est": 224.08, "duration_seconds": 10.618, "sample_count": 92}, "timestamp": "2026-01-25T21:30:24.356193"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7847.358, "latencies_ms": [7847.358], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "Two snowboarders are standing on a snowy mountain, with the sun shining brightly in the background. They are both wearing red jackets and black pants, and one of them is holding a yellow snowboard.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24519.0, "ram_available_mb": 38321.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24519.6, "ram_available_mb": 38321.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.985}, "power_stats": {"power_gpu_soc_mean_watts": 22.362, "power_cpu_cv_mean_watts": 1.684, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 72.985, "power_watts_avg": 22.362, "energy_joules_est": 175.5, "duration_seconds": 7.848, "sample_count": 68}, "timestamp": "2026-01-25T21:30:34.256584"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7511.725, "latencies_ms": [7511.725], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image features two individuals standing on a snowy surface with a bright sun shining overhead, casting a lens flare effect. The sky is clear and blue, and the mountains in the background are covered in snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24519.6, "ram_available_mb": 38321.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24575.8, "ram_available_mb": 38265.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.667}, "power_stats": {"power_gpu_soc_mean_watts": 22.122, "power_cpu_cv_mean_watts": 1.684, "power_sys_5v0_mean_watts": 9.028, "gpu_utilization_percent_mean": 72.667, "power_watts_avg": 22.122, "energy_joules_est": 166.19, "duration_seconds": 7.512, "sample_count": 63}, "timestamp": "2026-01-25T21:30:43.790979"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11110.661, "latencies_ms": [11110.661], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a tree with a large number of apples hanging from its branches. The apples are in various stages of ripeness, with some appearing ripe and ready to be picked, while others are still green and unripe. The tree is surrounded by a field, and the scene is set in a natural environment.\n\nThere are at least 13 apples visible", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24575.8, "ram_available_mb": 38265.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24577.0, "ram_available_mb": 38263.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.287}, "power_stats": {"power_gpu_soc_mean_watts": 20.936, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 69.287, "power_watts_avg": 20.936, "energy_joules_est": 232.63, "duration_seconds": 11.111, "sample_count": 94}, "timestamp": "2026-01-25T21:30:56.951047"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7288.969, "latencies_ms": [7288.969], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "apple: 5\nleaf: 10\ntree: 1\nbranch: 10\napple tree: 1\napple: 5\napple: 1\napple: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24577.0, "ram_available_mb": 38263.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24519.0, "ram_available_mb": 38321.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.016}, "power_stats": {"power_gpu_soc_mean_watts": 22.788, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.986, "gpu_utilization_percent_mean": 74.016, "power_watts_avg": 22.788, "energy_joules_est": 166.12, "duration_seconds": 7.29, "sample_count": 61}, "timestamp": "2026-01-25T21:31:06.253889"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10879.624, "latencies_ms": [10879.624], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The apples are hanging from the branches in the foreground, with some closer to the viewer and others further away. The tree trunk is in the background, and there are more apples scattered around the tree. The apples in the foreground appear to be nearer to the viewer, while the tree trunk and other apples are farther away.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24519.0, "ram_available_mb": 38321.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24511.9, "ram_available_mb": 38329.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.753}, "power_stats": {"power_gpu_soc_mean_watts": 21.13, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 69.753, "power_watts_avg": 21.13, "energy_joules_est": 229.9, "duration_seconds": 10.88, "sample_count": 93}, "timestamp": "2026-01-25T21:31:19.152617"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7625.988, "latencies_ms": [7625.988], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image depicts a tree with a hollow trunk, surrounded by branches with red apples hanging from them. The scene appears to be in an orchard or a garden with other trees in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24511.9, "ram_available_mb": 38329.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24555.2, "ram_available_mb": 38285.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.923}, "power_stats": {"power_gpu_soc_mean_watts": 22.464, "power_cpu_cv_mean_watts": 1.657, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 72.923, "power_watts_avg": 22.464, "energy_joules_est": 171.33, "duration_seconds": 7.627, "sample_count": 65}, "timestamp": "2026-01-25T21:31:28.793087"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8991.711, "latencies_ms": [8991.711], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image features a tree with a rough, textured bark and a large, circular hole in the center. The tree is surrounded by several red apples hanging from its branches, and the background is filled with dry, brown leaves and branches, indicating a fall or autumn season.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24555.2, "ram_available_mb": 38285.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 24593.3, "ram_available_mb": 38247.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.571}, "power_stats": {"power_gpu_soc_mean_watts": 21.595, "power_cpu_cv_mean_watts": 2.132, "power_sys_5v0_mean_watts": 9.031, "gpu_utilization_percent_mean": 70.571, "power_watts_avg": 21.595, "energy_joules_est": 194.19, "duration_seconds": 8.992, "sample_count": 77}, "timestamp": "2026-01-25T21:31:39.841242"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11143.137, "latencies_ms": [11143.137], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a kitchen with two men working together to prepare food. One man is standing near the left side of the kitchen, while the other man is positioned more towards the center. They are both focused on their tasks, with one man holding a pot and the other man holding a knife.\n\nThe kitchen is well-equipped with various utensils and appliances.", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24593.3, "ram_available_mb": 38247.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24543.6, "ram_available_mb": 38297.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.074}, "power_stats": {"power_gpu_soc_mean_watts": 20.932, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.927, "gpu_utilization_percent_mean": 69.074, "power_watts_avg": 20.932, "energy_joules_est": 233.26, "duration_seconds": 11.144, "sample_count": 95}, "timestamp": "2026-01-25T21:31:53.036482"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10841.467, "latencies_ms": [10841.467], "images_per_second": 0.092, "prompt_tokens": 39, "response_tokens_est": 75, "n_tiles": 16, "output_text": "1. Chef: 2\n2. Pots and pans: 10\n3. Trays: 5\n4. Containers: 8\n5. Utensils: 4\n6. Ingredients: 3\n7. Ovens: 2\n8. Cooking equipment: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24543.6, "ram_available_mb": 38297.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24561.6, "ram_available_mb": 38279.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.839}, "power_stats": {"power_gpu_soc_mean_watts": 21.158, "power_cpu_cv_mean_watts": 1.869, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 70.839, "power_watts_avg": 21.158, "energy_joules_est": 229.4, "duration_seconds": 10.842, "sample_count": 93}, "timestamp": "2026-01-25T21:32:05.937754"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10551.563, "latencies_ms": [10551.563], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "In the foreground, a chef in a white shirt is standing near a stainless steel counter, holding a pot and preparing food. Behind him, another chef in a white shirt and blue apron is working on a cutting board. The background shows a well-equipped kitchen with various cooking utensils and appliances.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24561.6, "ram_available_mb": 38279.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24586.6, "ram_available_mb": 38254.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.868}, "power_stats": {"power_gpu_soc_mean_watts": 21.136, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 69.868, "power_watts_avg": 21.136, "energy_joules_est": 223.03, "duration_seconds": 10.552, "sample_count": 91}, "timestamp": "2026-01-25T21:32:18.533217"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8185.988, "latencies_ms": [8185.988], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "In a bustling commercial kitchen, two chefs are diligently preparing food. The chef on the left is slicing meat on a cutting board, while the chef on the right is stirring a pot on the stove.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24586.6, "ram_available_mb": 38254.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24532.0, "ram_available_mb": 38308.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.086}, "power_stats": {"power_gpu_soc_mean_watts": 22.216, "power_cpu_cv_mean_watts": 1.699, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 72.086, "power_watts_avg": 22.216, "energy_joules_est": 181.87, "duration_seconds": 8.187, "sample_count": 70}, "timestamp": "2026-01-25T21:32:28.757154"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8978.598, "latencies_ms": [8978.598], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image depicts a professional kitchen with stainless steel appliances and surfaces, giving it a clean and modern appearance. The lighting is bright and even, likely from overhead fluorescent lights, which is typical for commercial kitchens to ensure good visibility while cooking.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24532.0, "ram_available_mb": 38308.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24611.7, "ram_available_mb": 38229.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.171}, "power_stats": {"power_gpu_soc_mean_watts": 21.596, "power_cpu_cv_mean_watts": 1.797, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 71.171, "power_watts_avg": 21.596, "energy_joules_est": 193.92, "duration_seconds": 8.979, "sample_count": 76}, "timestamp": "2026-01-25T21:32:39.781615"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10690.267, "latencies_ms": [10690.267], "images_per_second": 0.094, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of motorcyclists gathered on the side of a road, with several motorcycles parked in a row. There are at least nine motorcycles visible, with some being parked closer to the foreground and others further back. A total of ten people can be seen in the scene, with some standing near the motorcycles and others walking or standing at", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24524.7, "ram_available_mb": 38316.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24584.8, "ram_available_mb": 38256.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10063.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 66.837}, "power_stats": {"power_gpu_soc_mean_watts": 18.812, "power_cpu_cv_mean_watts": 1.981, "power_sys_5v0_mean_watts": 8.786, "gpu_utilization_percent_mean": 66.837, "power_watts_avg": 18.812, "energy_joules_est": 201.12, "duration_seconds": 10.691, "sample_count": 92}, "timestamp": "2026-01-25T21:32:52.539982"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9069.192, "latencies_ms": [9069.192], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "- Motorcycles: 9\n- People: 10\n- Cars: 1\n- Clothes: 10\n- Helmets: 5\n- Jackets: 10\n- Jeans: 10\n- Boots: 10", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24521.6, "ram_available_mb": 38319.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24526.1, "ram_available_mb": 38314.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10092.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.74}, "power_stats": {"power_gpu_soc_mean_watts": 19.617, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.779, "gpu_utilization_percent_mean": 68.74, "power_watts_avg": 19.617, "energy_joules_est": 177.92, "duration_seconds": 9.07, "sample_count": 77}, "timestamp": "2026-01-25T21:33:03.657993"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10761.934, "latencies_ms": [10761.934], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are several motorcycles parked along the side of the road, with a group of people standing nearby, likely discussing or preparing for a ride. The motorcycles are positioned in a line, with the closest one being the most prominent and the others gradually receding into the background. The people are standing at varying distances from the motorcycles", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24526.1, "ram_available_mb": 38314.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24568.3, "ram_available_mb": 38272.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10098.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 67.462}, "power_stats": {"power_gpu_soc_mean_watts": 18.76, "power_cpu_cv_mean_watts": 1.977, "power_sys_5v0_mean_watts": 8.79, "gpu_utilization_percent_mean": 67.462, "power_watts_avg": 18.76, "energy_joules_est": 201.91, "duration_seconds": 10.763, "sample_count": 93}, "timestamp": "2026-01-25T21:33:16.444783"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7176.063, "latencies_ms": [7176.063], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "A group of motorcyclists are gathered on the side of a road, with some sitting on their bikes and others standing. The sky is cloudy, suggesting that it might be a cool or overcast day.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24568.3, "ram_available_mb": 38272.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24524.8, "ram_available_mb": 38316.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10089.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.59}, "power_stats": {"power_gpu_soc_mean_watts": 20.464, "power_cpu_cv_mean_watts": 1.726, "power_sys_5v0_mean_watts": 8.791, "gpu_utilization_percent_mean": 70.59, "power_watts_avg": 20.464, "energy_joules_est": 146.86, "duration_seconds": 7.177, "sample_count": 61}, "timestamp": "2026-01-25T21:33:25.656508"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10757.357, "latencies_ms": [10757.357], "images_per_second": 0.093, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a group of motorcyclists gathered on the side of a road with a cloudy sky overhead. The motorcycles are of various colors and models, including red, blue, and green. The riders are wearing helmets and jackets, and some are standing while others are seated on their bikes. The road has a yellow line painted on it,", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24524.8, "ram_available_mb": 38316.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24586.5, "ram_available_mb": 38254.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10088.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 67.576}, "power_stats": {"power_gpu_soc_mean_watts": 18.776, "power_cpu_cv_mean_watts": 1.981, "power_sys_5v0_mean_watts": 8.766, "gpu_utilization_percent_mean": 67.576, "power_watts_avg": 18.776, "energy_joules_est": 201.99, "duration_seconds": 10.758, "sample_count": 92}, "timestamp": "2026-01-25T21:33:38.460496"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11108.783, "latencies_ms": [11108.783], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of a small, single-engine propeller plane soaring through the sky. The plane, painted in a striking combination of black and white, is flying from left to right, leaving behind a trail of white smoke that contrasts with the grayish-blue backdrop of the sky. The perspective of the image is from below, looking up at the plane, giving", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 24531.5, "ram_available_mb": 38309.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24543.3, "ram_available_mb": 38297.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.358}, "power_stats": {"power_gpu_soc_mean_watts": 20.919, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 69.358, "power_watts_avg": 20.919, "energy_joules_est": 232.4, "duration_seconds": 11.109, "sample_count": 95}, "timestamp": "2026-01-25T21:33:51.593066"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7036.305, "latencies_ms": [7036.305], "images_per_second": 0.142, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "airplane: 1, cloud: multiple, smoke: 2, sky: 1, engine: 1, propeller: 1, tail: 1, wing: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24543.3, "ram_available_mb": 38297.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24601.1, "ram_available_mb": 38239.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.983}, "power_stats": {"power_gpu_soc_mean_watts": 23.002, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 9.026, "gpu_utilization_percent_mean": 72.983, "power_watts_avg": 23.002, "energy_joules_est": 161.86, "duration_seconds": 7.037, "sample_count": 59}, "timestamp": "2026-01-25T21:34:00.686728"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8997.437, "latencies_ms": [8997.437], "images_per_second": 0.111, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The airplane is flying in the foreground of the image, appearing near the viewer, while the clouds are in the background, creating a sense of depth. The airplane is positioned slightly to the left of the center of the image, leaving ample space on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24524.4, "ram_available_mb": 38316.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24583.6, "ram_available_mb": 38257.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.895}, "power_stats": {"power_gpu_soc_mean_watts": 21.653, "power_cpu_cv_mean_watts": 1.797, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 70.895, "power_watts_avg": 21.653, "energy_joules_est": 194.84, "duration_seconds": 8.998, "sample_count": 76}, "timestamp": "2026-01-25T21:34:11.728098"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7387.385, "latencies_ms": [7387.385], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A small propeller-driven aircraft is flying through a cloudy sky, leaving a trail of smoke behind it. The aircraft appears to be in motion, as it is slightly tilted to one side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24583.6, "ram_available_mb": 38257.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24594.0, "ram_available_mb": 38246.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.29}, "power_stats": {"power_gpu_soc_mean_watts": 22.726, "power_cpu_cv_mean_watts": 1.602, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 74.29, "power_watts_avg": 22.726, "energy_joules_est": 167.9, "duration_seconds": 7.388, "sample_count": 62}, "timestamp": "2026-01-25T21:34:21.131922"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7102.743, "latencies_ms": [7102.743], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image is in black and white, featuring a small propeller-driven aircraft flying through a cloudy sky. The clouds are thick and fluffy, indicating that the weather may be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24594.0, "ram_available_mb": 38246.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24582.4, "ram_available_mb": 38258.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.517}, "power_stats": {"power_gpu_soc_mean_watts": 22.574, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 9.055, "gpu_utilization_percent_mean": 72.517, "power_watts_avg": 22.574, "energy_joules_est": 160.35, "duration_seconds": 7.103, "sample_count": 60}, "timestamp": "2026-01-25T21:34:30.283752"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10884.021, "latencies_ms": [10884.021], "images_per_second": 0.092, "prompt_tokens": 24, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the image, there are three sheep standing on a grassy hillside. They are positioned close to each other, with one sheep slightly ahead of the other two. The sheep appear to be looking towards the camera, giving the impression that they are posing for a picture. The background features a beautiful mountainous landscape with a lake, adding to the picturesque setting.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24527.5, "ram_available_mb": 38313.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24597.9, "ram_available_mb": 38243.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.398}, "power_stats": {"power_gpu_soc_mean_watts": 21.015, "power_cpu_cv_mean_watts": 1.911, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 69.398, "power_watts_avg": 21.015, "energy_joules_est": 228.74, "duration_seconds": 10.885, "sample_count": 93}, "timestamp": "2026-01-25T21:34:43.222946"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6946.684, "latencies_ms": [6946.684], "images_per_second": 0.144, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "mountain: 3, sheep: 4, grass: 1, sky: 1, water: 1, rock: 1, tree: 1, valley: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24526.0, "ram_available_mb": 38314.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24615.9, "ram_available_mb": 38225.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.241}, "power_stats": {"power_gpu_soc_mean_watts": 22.886, "power_cpu_cv_mean_watts": 1.567, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 73.241, "power_watts_avg": 22.886, "energy_joules_est": 159.0, "duration_seconds": 6.947, "sample_count": 58}, "timestamp": "2026-01-25T21:34:52.193539"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11096.129, "latencies_ms": [11096.129], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there are three sheep standing on a grassy hillside with rocks scattered around. The sheep are positioned near the center of the image, with one on the left, one in the middle, and one on the right. In the background, there are rolling hills and a body of water, which appears to be a lake or a large pond. The", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24615.9, "ram_available_mb": 38225.0, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24585.7, "ram_available_mb": 38255.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.021}, "power_stats": {"power_gpu_soc_mean_watts": 20.959, "power_cpu_cv_mean_watts": 1.935, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 70.021, "power_watts_avg": 20.959, "energy_joules_est": 232.58, "duration_seconds": 11.097, "sample_count": 96}, "timestamp": "2026-01-25T21:35:05.306466"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5940.299, "latencies_ms": [5940.299], "images_per_second": 0.168, "prompt_tokens": 37, "response_tokens_est": 31, "n_tiles": 16, "output_text": "Three sheep are standing on a grassy hillside with a beautiful blue lake in the background. The landscape is mountainous and the sky is clear.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24522.2, "ram_available_mb": 38318.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24587.3, "ram_available_mb": 38253.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.286}, "power_stats": {"power_gpu_soc_mean_watts": 23.776, "power_cpu_cv_mean_watts": 1.43, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 74.286, "power_watts_avg": 23.776, "energy_joules_est": 141.25, "duration_seconds": 5.941, "sample_count": 49}, "timestamp": "2026-01-25T21:35:13.260112"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7882.535, "latencies_ms": [7882.535], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image features three sheep standing on a grassy hillside with a backdrop of rolling hills and a clear blue sky. The sheep are white and appear to be grazing on the grass, with a body of water visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24523.8, "ram_available_mb": 38317.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24583.2, "ram_available_mb": 38257.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.373}, "power_stats": {"power_gpu_soc_mean_watts": 22.051, "power_cpu_cv_mean_watts": 1.715, "power_sys_5v0_mean_watts": 9.024, "gpu_utilization_percent_mean": 71.373, "power_watts_avg": 22.051, "energy_joules_est": 173.83, "duration_seconds": 7.883, "sample_count": 67}, "timestamp": "2026-01-25T21:35:23.180815"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11122.863, "latencies_ms": [11122.863], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the image, a young woman in a wheelchair is holding a tennis racket, ready to play tennis. She is wearing a gray shirt and is positioned in the foreground of the scene. A man is also present in the background, standing and watching the woman play. The setting appears to be an indoor tennis court, with a net visible in the background.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24528.2, "ram_available_mb": 38312.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24589.9, "ram_available_mb": 38251.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.084}, "power_stats": {"power_gpu_soc_mean_watts": 20.904, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.084, "power_watts_avg": 20.904, "energy_joules_est": 232.53, "duration_seconds": 11.124, "sample_count": 95}, "timestamp": "2026-01-25T21:35:36.346143"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7420.599, "latencies_ms": [7420.599], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "wheelchair: 1, tennis racket: 1, person: 2, building: 1, wall: 1, window: 1, door: 1, chair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24526.4, "ram_available_mb": 38314.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 7.9, "ram_used_mb": 24536.3, "ram_available_mb": 38304.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.095}, "power_stats": {"power_gpu_soc_mean_watts": 22.217, "power_cpu_cv_mean_watts": 2.206, "power_sys_5v0_mean_watts": 9.025, "gpu_utilization_percent_mean": 74.095, "power_watts_avg": 22.217, "energy_joules_est": 164.88, "duration_seconds": 7.421, "sample_count": 63}, "timestamp": "2026-01-25T21:35:45.803178"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11120.892, "latencies_ms": [11120.892], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the foreground, a person in a wheelchair is holding a tennis racket, positioned near the center of the image. To the right, slightly in the background, another person is also holding a tennis racket, but their full body is not visible. The background is a nondescript wall with a window, indicating the setting is likely an indoor tennis court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24536.3, "ram_available_mb": 38304.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24602.7, "ram_available_mb": 38238.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.663}, "power_stats": {"power_gpu_soc_mean_watts": 20.893, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.663, "power_watts_avg": 20.893, "energy_joules_est": 232.36, "duration_seconds": 11.121, "sample_count": 95}, "timestamp": "2026-01-25T21:35:58.939755"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7069.608, "latencies_ms": [7069.608], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A person in a wheelchair is holding a tennis racket, ready to play tennis, with another person standing nearby. The setting appears to be an indoor tennis court or a similar facility.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24602.7, "ram_available_mb": 38238.2, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24603.0, "ram_available_mb": 38237.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.717}, "power_stats": {"power_gpu_soc_mean_watts": 22.847, "power_cpu_cv_mean_watts": 1.535, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 74.717, "power_watts_avg": 22.847, "energy_joules_est": 161.53, "duration_seconds": 7.07, "sample_count": 60}, "timestamp": "2026-01-25T21:36:08.024762"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10203.051, "latencies_ms": [10203.051], "images_per_second": 0.098, "prompt_tokens": 36, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The image features a person in a wheelchair holding a tennis racket with a purple and white design. The person is wearing a gray t-shirt with the word \"Empower\" visible on it. The background shows a building with a white wall and a blue pole, and the lighting appears to be natural daylight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24603.0, "ram_available_mb": 38237.9, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24603.7, "ram_available_mb": 38237.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.943}, "power_stats": {"power_gpu_soc_mean_watts": 21.171, "power_cpu_cv_mean_watts": 1.869, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 69.943, "power_watts_avg": 21.171, "energy_joules_est": 216.02, "duration_seconds": 10.204, "sample_count": 87}, "timestamp": "2026-01-25T21:36:20.252380"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12369.243, "latencies_ms": [12369.243], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young child is the main subject, sitting on a brown horse saddle. The child is wearing a black helmet and a pink and white checkered shirt, with blue jeans. The saddle is brown and appears to be made of leather. The child is holding onto a metal bar, possibly for support. The background is a lush green forest", "error": null, "sys_before": {"cpu_percent": 6.5, "ram_used_mb": 24533.5, "ram_available_mb": 38307.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24620.4, "ram_available_mb": 38220.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.566}, "power_stats": {"power_gpu_soc_mean_watts": 22.787, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 9.155, "gpu_utilization_percent_mean": 73.566, "power_watts_avg": 22.787, "energy_joules_est": 281.87, "duration_seconds": 12.37, "sample_count": 106}, "timestamp": "2026-01-25T21:36:34.656693"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 12671.71, "latencies_ms": [12671.71], "images_per_second": 0.079, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "object: horse, count: 1\nobject: saddle, count: 1\nobject: rider, count: 1\nobject: helmet, count: 1\nobject: plaid shirt, count: 1\nobject: jeans, count: 1\nobject: boot, count: 1\nobject: riding boots, count: ", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24620.4, "ram_available_mb": 38220.5, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24630.3, "ram_available_mb": 38210.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.426}, "power_stats": {"power_gpu_soc_mean_watts": 22.95, "power_cpu_cv_mean_watts": 1.716, "power_sys_5v0_mean_watts": 9.12, "gpu_utilization_percent_mean": 74.426, "power_watts_avg": 22.95, "energy_joules_est": 290.83, "duration_seconds": 12.672, "sample_count": 108}, "timestamp": "2026-01-25T21:36:49.387781"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12433.755, "latencies_ms": [12433.755], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The child is seated on a horse saddle that is positioned in the foreground of the image, indicating they are the main object of focus. The background is filled with greenery, suggesting the setting is outdoors, possibly in a park or a rural area. The saddle and the child are in the near foreground, while the trees and grass fill the background, creating a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24630.3, "ram_available_mb": 38210.6, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24590.3, "ram_available_mb": 38250.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.619}, "power_stats": {"power_gpu_soc_mean_watts": 22.898, "power_cpu_cv_mean_watts": 1.766, "power_sys_5v0_mean_watts": 9.198, "gpu_utilization_percent_mean": 73.619, "power_watts_avg": 22.898, "energy_joules_est": 284.72, "duration_seconds": 12.434, "sample_count": 105}, "timestamp": "2026-01-25T21:37:03.840965"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8827.161, "latencies_ms": [8827.161], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A child is riding a brown horse-shaped rocking horse in a grassy area with trees in the background. The child is wearing a plaid shirt, jeans, and brown boots.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24590.3, "ram_available_mb": 38250.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24654.5, "ram_available_mb": 38186.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.865}, "power_stats": {"power_gpu_soc_mean_watts": 24.329, "power_cpu_cv_mean_watts": 1.45, "power_sys_5v0_mean_watts": 9.169, "gpu_utilization_percent_mean": 77.865, "power_watts_avg": 24.329, "energy_joules_est": 214.77, "duration_seconds": 8.828, "sample_count": 74}, "timestamp": "2026-01-25T21:37:14.693329"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10484.998, "latencies_ms": [10484.998], "images_per_second": 0.095, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image shows a child riding a brown horse-shaped ride-on toy with a black helmet. The child is wearing a pink and white checkered shirt and blue jeans. The background is filled with green trees and the lighting appears to be natural daylight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24654.5, "ram_available_mb": 38186.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24618.8, "ram_available_mb": 38222.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.034}, "power_stats": {"power_gpu_soc_mean_watts": 23.428, "power_cpu_cv_mean_watts": 1.642, "power_sys_5v0_mean_watts": 9.239, "gpu_utilization_percent_mean": 76.034, "power_watts_avg": 23.428, "energy_joules_est": 245.66, "duration_seconds": 10.486, "sample_count": 89}, "timestamp": "2026-01-25T21:37:27.209363"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12111.72, "latencies_ms": [12111.72], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene moment at Raglan, New Zealand, where two surfers are seen riding the waves. The surfer on the left, clad in a black wetsuit, is skillfully maneuvering a white surfboard on a wave that's breaking to the right. A little further away, another surfer in a black wetsuit", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24553.7, "ram_available_mb": 38287.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24597.3, "ram_available_mb": 38243.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.163}, "power_stats": {"power_gpu_soc_mean_watts": 22.097, "power_cpu_cv_mean_watts": 1.802, "power_sys_5v0_mean_watts": 9.129, "gpu_utilization_percent_mean": 73.163, "power_watts_avg": 22.097, "energy_joules_est": 267.65, "duration_seconds": 12.112, "sample_count": 104}, "timestamp": "2026-01-25T21:37:41.379542"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6633.948, "latencies_ms": [6633.948], "images_per_second": 0.151, "prompt_tokens": 39, "response_tokens_est": 28, "n_tiles": 16, "output_text": "surfboard: 2\nsurfer: 2\nwave: multiple\nocean: single\ntext: 1\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24597.3, "ram_available_mb": 38243.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24608.8, "ram_available_mb": 38232.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 80.286}, "power_stats": {"power_gpu_soc_mean_watts": 25.173, "power_cpu_cv_mean_watts": 1.216, "power_sys_5v0_mean_watts": 9.15, "gpu_utilization_percent_mean": 80.286, "power_watts_avg": 25.173, "energy_joules_est": 167.01, "duration_seconds": 6.635, "sample_count": 56}, "timestamp": "2026-01-25T21:37:50.028261"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12046.163, "latencies_ms": [12046.163], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a surfer riding a wave on the left side of the image, while another surfer is further back, closer to the center, also riding a wave. In the background, there is a third surfer who is positioned further away from the viewer, closer to the right side of the image. The waves create a sense of depth, with", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24547.0, "ram_available_mb": 38293.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24596.2, "ram_available_mb": 38244.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.167}, "power_stats": {"power_gpu_soc_mean_watts": 22.681, "power_cpu_cv_mean_watts": 1.802, "power_sys_5v0_mean_watts": 9.15, "gpu_utilization_percent_mean": 73.167, "power_watts_avg": 22.681, "energy_joules_est": 273.23, "duration_seconds": 12.047, "sample_count": 102}, "timestamp": "2026-01-25T21:38:04.112506"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8966.954, "latencies_ms": [8966.954], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image captures a serene moment at Raglan, New Zealand, where two individuals are seen surfing on the ocean waves. The sky is clear, and the sea is calm, providing perfect conditions for the surfers.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24532.9, "ram_available_mb": 38308.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24554.8, "ram_available_mb": 38286.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.105}, "power_stats": {"power_gpu_soc_mean_watts": 23.753, "power_cpu_cv_mean_watts": 1.518, "power_sys_5v0_mean_watts": 9.117, "gpu_utilization_percent_mean": 76.105, "power_watts_avg": 23.753, "energy_joules_est": 213.01, "duration_seconds": 8.968, "sample_count": 76}, "timestamp": "2026-01-25T21:38:15.134816"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9578.674, "latencies_ms": [9578.674], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image captures a serene ocean scene with a clear blue sky and a calm sea, ideal for surfing. Two surfers, clad in black wetsuits, are skillfully riding the waves, creating a dynamic contrast against the tranquil water.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24554.8, "ram_available_mb": 38286.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24632.0, "ram_available_mb": 38208.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.21}, "power_stats": {"power_gpu_soc_mean_watts": 23.417, "power_cpu_cv_mean_watts": 1.632, "power_sys_5v0_mean_watts": 9.171, "gpu_utilization_percent_mean": 75.21, "power_watts_avg": 23.417, "energy_joules_est": 224.33, "duration_seconds": 9.58, "sample_count": 81}, "timestamp": "2026-01-25T21:38:26.731846"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11134.079, "latencies_ms": [11134.079], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a cozy kitchen scene, bathed in soft light that filters through a large window adorned with a diamond-shaped sticker. The window, a silent observer to the culinary adventures, frames a view of a bustling street outside. The kitchen itself is a testament to a busy life, with various appliances and utensils scattered", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24632.0, "ram_available_mb": 38208.9, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24528.4, "ram_available_mb": 38312.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.063}, "power_stats": {"power_gpu_soc_mean_watts": 20.895, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 69.063, "power_watts_avg": 20.895, "energy_joules_est": 232.66, "duration_seconds": 11.135, "sample_count": 95}, "timestamp": "2026-01-25T21:38:39.934078"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7506.56, "latencies_ms": [7506.56], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "stove: 1, pot: 3, pan: 2, window: 1, plant: 1, bookshelf: 1, shelf: 1, plant pot: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24528.4, "ram_available_mb": 38312.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24588.1, "ram_available_mb": 38252.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.141}, "power_stats": {"power_gpu_soc_mean_watts": 22.619, "power_cpu_cv_mean_watts": 1.639, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 73.141, "power_watts_avg": 22.619, "energy_joules_est": 169.81, "duration_seconds": 7.507, "sample_count": 64}, "timestamp": "2026-01-25T21:38:49.458760"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11120.908, "latencies_ms": [11120.908], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a small wooden stool with a potted plant on top, positioned in front of a window. The stove is located to the left of the window, and there is a shelf above it with various items. To the right of the window, there is a wooden cabinet with a few items on top, and further to the right, there is a", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24588.1, "ram_available_mb": 38252.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24614.3, "ram_available_mb": 38226.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.574}, "power_stats": {"power_gpu_soc_mean_watts": 20.97, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 69.574, "power_watts_avg": 20.97, "energy_joules_est": 233.22, "duration_seconds": 11.122, "sample_count": 94}, "timestamp": "2026-01-25T21:39:02.639579"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10559.044, "latencies_ms": [10559.044], "images_per_second": 0.095, "prompt_tokens": 37, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image depicts a cozy, dimly lit kitchen with a large window that allows natural light to filter in. On the windowsill, there is a small potted plant, and various kitchen items are scattered around the room, including a stove, a refrigerator, and shelves with various containers and utensils.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24614.3, "ram_available_mb": 38226.6, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24603.2, "ram_available_mb": 38237.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.868}, "power_stats": {"power_gpu_soc_mean_watts": 21.148, "power_cpu_cv_mean_watts": 1.866, "power_sys_5v0_mean_watts": 8.927, "gpu_utilization_percent_mean": 70.868, "power_watts_avg": 21.148, "energy_joules_est": 223.32, "duration_seconds": 10.56, "sample_count": 91}, "timestamp": "2026-01-25T21:39:15.216494"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7114.309, "latencies_ms": [7114.309], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image shows a kitchen with warm lighting and a combination of wood and metal materials. The walls are painted in a light color, and there is a large window that allows natural light to enter the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24529.6, "ram_available_mb": 38311.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24589.0, "ram_available_mb": 38251.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.983}, "power_stats": {"power_gpu_soc_mean_watts": 22.573, "power_cpu_cv_mean_watts": 1.628, "power_sys_5v0_mean_watts": 9.047, "gpu_utilization_percent_mean": 71.983, "power_watts_avg": 22.573, "energy_joules_est": 160.61, "duration_seconds": 7.115, "sample_count": 60}, "timestamp": "2026-01-25T21:39:24.372665"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11105.016, "latencies_ms": [11105.016], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a table with a variety of fruits and items arranged on it. There are three oranges placed on a plate, with one orange positioned closer to the left side of the plate and the other two on the right side. A pineapple is also present on the table, occupying a significant portion of the left side. \n\nIn addition to the fruits,", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 24528.6, "ram_available_mb": 38312.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24606.2, "ram_available_mb": 38234.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.83}, "power_stats": {"power_gpu_soc_mean_watts": 20.976, "power_cpu_cv_mean_watts": 1.925, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 69.83, "power_watts_avg": 20.976, "energy_joules_est": 232.95, "duration_seconds": 11.106, "sample_count": 94}, "timestamp": "2026-01-25T21:39:37.535294"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8736.016, "latencies_ms": [8736.016], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "Pineapple: 1\nRed incense sticks: 12\nOrange: 3\nRed cups: 6\nRed mat: 1\nWhite plate: 1\nRed candle holder: 1\nRed cloth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24524.5, "ram_available_mb": 38316.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 7.5, "ram_used_mb": 24602.6, "ram_available_mb": 38238.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.453}, "power_stats": {"power_gpu_soc_mean_watts": 21.951, "power_cpu_cv_mean_watts": 2.077, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 72.453, "power_watts_avg": 21.951, "energy_joules_est": 191.78, "duration_seconds": 8.737, "sample_count": 75}, "timestamp": "2026-01-25T21:39:48.313282"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11083.71, "latencies_ms": [11083.71], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are six red cups arranged in a row, with the cups being the closest objects to the viewer. Behind the cups, there is a pineapple on the left side and a set of oranges on the right side, both of which are further away from the viewer. The pineapple is positioned to the left of the or", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24526.0, "ram_available_mb": 38314.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24589.7, "ram_available_mb": 38251.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.885}, "power_stats": {"power_gpu_soc_mean_watts": 21.056, "power_cpu_cv_mean_watts": 1.936, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 69.885, "power_watts_avg": 21.056, "energy_joules_est": 233.39, "duration_seconds": 11.084, "sample_count": 96}, "timestamp": "2026-01-25T21:40:01.459741"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8823.625, "latencies_ms": [8823.625], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image depicts a table with a pineapple, oranges, and red cups, possibly set up for a cultural or religious event. The presence of incense sticks in a small container suggests that the scene may be related to a ritual or ceremony.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24527.6, "ram_available_mb": 38313.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24585.5, "ram_available_mb": 38255.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.203}, "power_stats": {"power_gpu_soc_mean_watts": 21.897, "power_cpu_cv_mean_watts": 1.748, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 70.203, "power_watts_avg": 21.897, "energy_joules_est": 193.23, "duration_seconds": 8.825, "sample_count": 74}, "timestamp": "2026-01-25T21:40:12.337107"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7720.373, "latencies_ms": [7720.373], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a vibrant red background with a large pineapple on the left side and a group of oranges on a white plate on the right side. The oranges are bright orange in color and appear to be fresh.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24524.5, "ram_available_mb": 38316.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24600.2, "ram_available_mb": 38240.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.727}, "power_stats": {"power_gpu_soc_mean_watts": 22.323, "power_cpu_cv_mean_watts": 1.711, "power_sys_5v0_mean_watts": 9.103, "gpu_utilization_percent_mean": 71.727, "power_watts_avg": 22.323, "energy_joules_est": 172.36, "duration_seconds": 7.721, "sample_count": 66}, "timestamp": "2026-01-25T21:40:22.118458"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11118.949, "latencies_ms": [11118.949], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is sitting at a table in a restaurant, holding a plate with a sandwich and fries. He is wearing glasses and has a beard. The restaurant appears to be busy, with other people in the background. The man is smiling, indicating that he is enjoying his meal. The table is covered with a white tablecloth, and", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24520.0, "ram_available_mb": 38320.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24540.0, "ram_available_mb": 38300.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.383}, "power_stats": {"power_gpu_soc_mean_watts": 20.964, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 69.383, "power_watts_avg": 20.964, "energy_joules_est": 233.11, "duration_seconds": 11.12, "sample_count": 94}, "timestamp": "2026-01-25T21:40:35.307978"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10857.543, "latencies_ms": [10857.543], "images_per_second": 0.092, "prompt_tokens": 39, "response_tokens_est": 75, "n_tiles": 16, "output_text": "- Man: 1\n\n- Plate with food: 1\n\n- Fries: 1 portion\n\n- Ketchup: 1 small container\n\n- Restaurant interior: Multiple objects (at least 5 visible)\n\n- People: Multiple individuals (at least 5 visible)\n\n- Clock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24540.0, "ram_available_mb": 38300.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24555.4, "ram_available_mb": 38285.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.783}, "power_stats": {"power_gpu_soc_mean_watts": 21.2, "power_cpu_cv_mean_watts": 1.846, "power_sys_5v0_mean_watts": 8.913, "gpu_utilization_percent_mean": 70.783, "power_watts_avg": 21.2, "energy_joules_est": 230.19, "duration_seconds": 10.858, "sample_count": 92}, "timestamp": "2026-01-25T21:40:48.213003"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10110.119, "latencies_ms": [10110.119], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there is a plate with a sandwich and a side of fries. The sandwich is placed towards the left side of the plate, while the fries are scattered around the bottom left corner. In the background, there are other people and tables, indicating that this scene takes place in a public dining area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24555.4, "ram_available_mb": 38285.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24544.6, "ram_available_mb": 38296.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.86}, "power_stats": {"power_gpu_soc_mean_watts": 21.283, "power_cpu_cv_mean_watts": 1.863, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 69.86, "power_watts_avg": 21.283, "energy_joules_est": 215.19, "duration_seconds": 10.111, "sample_count": 86}, "timestamp": "2026-01-25T21:41:00.341043"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5072.427, "latencies_ms": [5072.427], "images_per_second": 0.197, "prompt_tokens": 37, "response_tokens_est": 23, "n_tiles": 16, "output_text": "A man is taking a selfie in a restaurant, holding a plate with a sandwich and fries.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24544.6, "ram_available_mb": 38296.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24534.5, "ram_available_mb": 38306.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.095}, "power_stats": {"power_gpu_soc_mean_watts": 24.991, "power_cpu_cv_mean_watts": 1.22, "power_sys_5v0_mean_watts": 9.065, "gpu_utilization_percent_mean": 78.095, "power_watts_avg": 24.991, "energy_joules_est": 126.78, "duration_seconds": 5.073, "sample_count": 42}, "timestamp": "2026-01-25T21:41:07.447546"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8221.503, "latencies_ms": [8221.503], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a man indoors with warm lighting, wearing a black t-shirt and glasses. He is holding a plate with a sandwich and a side of fries, and there is a clock on the wall behind him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24534.5, "ram_available_mb": 38306.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24528.6, "ram_available_mb": 38312.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.486}, "power_stats": {"power_gpu_soc_mean_watts": 21.888, "power_cpu_cv_mean_watts": 1.751, "power_sys_5v0_mean_watts": 9.003, "gpu_utilization_percent_mean": 71.486, "power_watts_avg": 21.888, "energy_joules_est": 179.97, "duration_seconds": 8.222, "sample_count": 70}, "timestamp": "2026-01-25T21:41:17.690430"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11094.709, "latencies_ms": [11094.709], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a person walking down a sidewalk, holding an umbrella to shield themselves from the rain. The person is walking past a row of bicycles parked along the sidewalk. The bicycles are of various sizes and are positioned in a line, with some closer to the person and others further away. The person appears to be walking in the", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24528.6, "ram_available_mb": 38312.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24598.1, "ram_available_mb": 38242.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.457}, "power_stats": {"power_gpu_soc_mean_watts": 20.941, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 69.457, "power_watts_avg": 20.941, "energy_joules_est": 232.35, "duration_seconds": 11.095, "sample_count": 94}, "timestamp": "2026-01-25T21:41:30.821423"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7519.784, "latencies_ms": [7519.784], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "building: 1, window: 1, person: 1, umbrella: 1, bicycle: 2, tree: 1, railing: 1, pole: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24529.8, "ram_available_mb": 38311.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24603.2, "ram_available_mb": 38237.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.746}, "power_stats": {"power_gpu_soc_mean_watts": 22.554, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 72.746, "power_watts_avg": 22.554, "energy_joules_est": 169.62, "duration_seconds": 7.52, "sample_count": 63}, "timestamp": "2026-01-25T21:41:40.373071"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10705.791, "latencies_ms": [10705.791], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there are two bicycles leaning against a metal structure, positioned near the center of the image. In the background, there is a person walking with an umbrella, located on the left side of the image. The metal structure appears to be in the middle ground, extending from the left to the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24603.2, "ram_available_mb": 38237.7, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24585.3, "ram_available_mb": 38255.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.912}, "power_stats": {"power_gpu_soc_mean_watts": 21.11, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.912, "power_watts_avg": 21.11, "energy_joules_est": 226.01, "duration_seconds": 10.706, "sample_count": 91}, "timestamp": "2026-01-25T21:41:53.134051"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8639.229, "latencies_ms": [8639.229], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows a view from a window looking out onto a courtyard with a person walking with an umbrella, bicycles parked, and a tall metal structure. The scene appears to be in an urban area with buildings surrounding the courtyard.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24585.3, "ram_available_mb": 38255.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24599.7, "ram_available_mb": 38241.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.384}, "power_stats": {"power_gpu_soc_mean_watts": 21.903, "power_cpu_cv_mean_watts": 1.733, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 71.384, "power_watts_avg": 21.903, "energy_joules_est": 189.24, "duration_seconds": 8.64, "sample_count": 73}, "timestamp": "2026-01-25T21:42:03.804750"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6361.253, "latencies_ms": [6361.253], "images_per_second": 0.157, "prompt_tokens": 36, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The image shows a rainy day with a person walking with an umbrella. The buildings in the background have a modern design with a combination of glass and metal materials.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24599.7, "ram_available_mb": 38241.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24600.2, "ram_available_mb": 38240.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.611}, "power_stats": {"power_gpu_soc_mean_watts": 23.276, "power_cpu_cv_mean_watts": 1.52, "power_sys_5v0_mean_watts": 9.063, "gpu_utilization_percent_mean": 74.611, "power_watts_avg": 23.276, "energy_joules_est": 148.08, "duration_seconds": 6.362, "sample_count": 54}, "timestamp": "2026-01-25T21:42:12.211271"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11135.445, "latencies_ms": [11135.445], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a close-up view of a plate of spaetzle, a type of soft egg noodle or dumpling of German origin. The spaetzle are coated in a creamy white sauce, giving them a glossy appearance. The plate is white, providing a stark contrast to the creamy color of the spaetzle. In", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24600.2, "ram_available_mb": 38240.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24619.2, "ram_available_mb": 38221.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.253}, "power_stats": {"power_gpu_soc_mean_watts": 20.895, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.253, "power_watts_avg": 20.895, "energy_joules_est": 232.69, "duration_seconds": 11.136, "sample_count": 95}, "timestamp": "2026-01-25T21:42:25.391003"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7841.733, "latencies_ms": [7841.733], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "plate: 1, fork: 1, noodles: numerous, sauce: visible, plate edge: visible, noodle texture: visible, noodle color: yellow, noodle shape: spiral", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24530.7, "ram_available_mb": 38310.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24601.6, "ram_available_mb": 38239.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.106}, "power_stats": {"power_gpu_soc_mean_watts": 22.242, "power_cpu_cv_mean_watts": 1.644, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 72.106, "power_watts_avg": 22.242, "energy_joules_est": 174.43, "duration_seconds": 7.842, "sample_count": 66}, "timestamp": "2026-01-25T21:42:35.290735"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11128.454, "latencies_ms": [11128.454], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a pile of cooked noodles that are spread out and occupying the majority of the frame. In the background, slightly out of focus, there appears to be a plate with a fork resting on it. The noodles are in the near foreground, while the plate and fork are in the background, indicating that the noodles are", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24528.2, "ram_available_mb": 38312.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24599.2, "ram_available_mb": 38241.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.568}, "power_stats": {"power_gpu_soc_mean_watts": 20.972, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 69.568, "power_watts_avg": 20.972, "energy_joules_est": 233.4, "duration_seconds": 11.129, "sample_count": 95}, "timestamp": "2026-01-25T21:42:48.452912"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9076.721, "latencies_ms": [9076.721], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a close-up of a plate of cooked noodles, possibly a type of pasta dish. The noodles are coated in a creamy sauce, and the plate is placed on a table with a fork and knife in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24522.4, "ram_available_mb": 38318.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24616.5, "ram_available_mb": 38224.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.974}, "power_stats": {"power_gpu_soc_mean_watts": 21.632, "power_cpu_cv_mean_watts": 1.784, "power_sys_5v0_mean_watts": 8.923, "gpu_utilization_percent_mean": 70.974, "power_watts_avg": 21.632, "energy_joules_est": 196.36, "duration_seconds": 9.077, "sample_count": 77}, "timestamp": "2026-01-25T21:42:59.590326"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7771.711, "latencies_ms": [7771.711], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a close-up of a plate of spaghetti with a fork resting on the edge of the plate. The lighting is dim, casting a warm glow on the pasta and creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24528.0, "ram_available_mb": 38312.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24522.0, "ram_available_mb": 38318.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.773}, "power_stats": {"power_gpu_soc_mean_watts": 22.225, "power_cpu_cv_mean_watts": 1.705, "power_sys_5v0_mean_watts": 9.018, "gpu_utilization_percent_mean": 71.773, "power_watts_avg": 22.225, "energy_joules_est": 172.74, "duration_seconds": 7.772, "sample_count": 66}, "timestamp": "2026-01-25T21:43:09.388737"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11144.602, "latencies_ms": [11144.602], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a workspace, bathed in soft light filtering through a white curtain. Dominating the scene is a silver laptop, its screen alive with a vibrant green cactus wallpaper. The laptop is positioned on a white desk, which also hosts a black mouse and a black speaker, both of which are turned off. A red mouse pad", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24522.0, "ram_available_mb": 38318.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24563.8, "ram_available_mb": 38277.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.427}, "power_stats": {"power_gpu_soc_mean_watts": 20.899, "power_cpu_cv_mean_watts": 1.919, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 69.427, "power_watts_avg": 20.899, "energy_joules_est": 232.93, "duration_seconds": 11.145, "sample_count": 96}, "timestamp": "2026-01-25T21:43:22.568716"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7153.869, "latencies_ms": [7153.869], "images_per_second": 0.14, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "computer: 2, mouse: 2, keyboard: 1, monitor: 1, speaker: 1, mouse pad: 1, wall: 1, curtain: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24563.8, "ram_available_mb": 38277.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24591.7, "ram_available_mb": 38249.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.393}, "power_stats": {"power_gpu_soc_mean_watts": 22.86, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.026, "gpu_utilization_percent_mean": 73.393, "power_watts_avg": 22.86, "energy_joules_est": 163.55, "duration_seconds": 7.155, "sample_count": 61}, "timestamp": "2026-01-25T21:43:31.753927"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11130.285, "latencies_ms": [11130.285], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "On the left side of the image, there is a desktop computer with a monitor displaying a green leafy wallpaper, positioned slightly behind the laptop which is in the foreground on the right. The laptop is placed on a white desk, and there is a black computer mouse in front of it. In the background, there is a black speaker and a black mouse pad with a red and", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24519.8, "ram_available_mb": 38321.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 24599.3, "ram_available_mb": 38241.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.064}, "power_stats": {"power_gpu_soc_mean_watts": 20.967, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 70.064, "power_watts_avg": 20.967, "energy_joules_est": 233.38, "duration_seconds": 11.131, "sample_count": 94}, "timestamp": "2026-01-25T21:43:44.901679"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8191.405, "latencies_ms": [8191.405], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a workspace with a laptop open on a white desk, displaying a green leafy wallpaper. There is a computer mouse and a speaker placed on the desk, and a keyboard is visible to the left of the laptop.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24537.2, "ram_available_mb": 38303.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 7.0, "ram_used_mb": 24578.2, "ram_available_mb": 38262.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.42}, "power_stats": {"power_gpu_soc_mean_watts": 22.213, "power_cpu_cv_mean_watts": 2.2, "power_sys_5v0_mean_watts": 9.011, "gpu_utilization_percent_mean": 72.42, "power_watts_avg": 22.213, "energy_joules_est": 181.97, "duration_seconds": 8.192, "sample_count": 69}, "timestamp": "2026-01-25T21:43:55.116219"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8569.4, "latencies_ms": [8569.4], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a well-lit room with a laptop displaying a vibrant green leaf pattern on its screen. The laptop is placed on a white desk, accompanied by a black mouse and a speaker, with a window covered by white curtains in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24578.2, "ram_available_mb": 38262.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24531.8, "ram_available_mb": 38309.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.493}, "power_stats": {"power_gpu_soc_mean_watts": 21.687, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 70.493, "power_watts_avg": 21.687, "energy_joules_est": 185.86, "duration_seconds": 8.57, "sample_count": 73}, "timestamp": "2026-01-25T21:44:05.722902"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11112.273, "latencies_ms": [11112.273], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young girl is sitting on a bed in a bedroom. She is wearing a green shirt and is surrounded by a variety of items. There are several books scattered around the room, with some on the bed and others on the floor. A toy truck is also present on the bed, adding to the playful atmosphere.\n\nThe room features a bed", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24531.8, "ram_available_mb": 38309.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24611.1, "ram_available_mb": 38229.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.453}, "power_stats": {"power_gpu_soc_mean_watts": 20.9, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.453, "power_watts_avg": 20.9, "energy_joules_est": 232.27, "duration_seconds": 11.113, "sample_count": 95}, "timestamp": "2026-01-25T21:44:18.904245"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9046.599, "latencies_ms": [9046.599], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Bed: 1\n- Pillow: 2\n- Nightstand: 1\n- Lamp: 1\n- Magazines: 1\n- Toy trucks: 1\n- Toy cars: 1\n- Toy blocks: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24522.7, "ram_available_mb": 38318.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24618.5, "ram_available_mb": 38222.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.145}, "power_stats": {"power_gpu_soc_mean_watts": 21.873, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 71.145, "power_watts_avg": 21.873, "energy_joules_est": 197.89, "duration_seconds": 9.047, "sample_count": 76}, "timestamp": "2026-01-25T21:44:29.982781"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10328.219, "latencies_ms": [10328.219], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "A young girl is sitting on the right side of the bed, which is in the foreground of the image. The bed is near the center of the image and is surrounded by a wooden nightstand on the left and a lamp on the nightstand. The girl is facing away from the camera, and there is a toy car on the bed.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24618.5, "ram_available_mb": 38222.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24600.1, "ram_available_mb": 38240.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.241}, "power_stats": {"power_gpu_soc_mean_watts": 21.204, "power_cpu_cv_mean_watts": 1.86, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 70.241, "power_watts_avg": 21.204, "energy_joules_est": 219.01, "duration_seconds": 10.329, "sample_count": 87}, "timestamp": "2026-01-25T21:44:42.325242"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7060.134, "latencies_ms": [7060.134], "images_per_second": 0.142, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A young girl is sitting on a bed in a bedroom with orange walls. She is surrounded by toys and books, and there is a wooden nightstand with a lamp next to the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24600.1, "ram_available_mb": 38240.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24600.8, "ram_available_mb": 38240.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.814}, "power_stats": {"power_gpu_soc_mean_watts": 22.991, "power_cpu_cv_mean_watts": 1.574, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 74.814, "power_watts_avg": 22.991, "energy_joules_est": 162.33, "duration_seconds": 7.061, "sample_count": 59}, "timestamp": "2026-01-25T21:44:51.407061"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5307.48, "latencies_ms": [5307.48], "images_per_second": 0.188, "prompt_tokens": 36, "response_tokens_est": 27, "n_tiles": 16, "output_text": "The room has orange walls and a wooden bed frame. There is a lamp on a small wooden nightstand next to the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24600.8, "ram_available_mb": 38240.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24601.8, "ram_available_mb": 38239.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.795}, "power_stats": {"power_gpu_soc_mean_watts": 24.296, "power_cpu_cv_mean_watts": 1.347, "power_sys_5v0_mean_watts": 9.146, "gpu_utilization_percent_mean": 75.795, "power_watts_avg": 24.296, "energy_joules_est": 128.97, "duration_seconds": 5.308, "sample_count": 44}, "timestamp": "2026-01-25T21:44:58.735420"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11117.745, "latencies_ms": [11117.745], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a baseball game in progress, with a batter standing at home plate, holding a baseball bat and preparing to swing. The catcher is crouched behind the batter, wearing a baseball glove, and the umpire is standing nearby, observing the game. \n\nThere are several other people in the scene, including a player on the field and a", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24538.5, "ram_available_mb": 38302.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24618.5, "ram_available_mb": 38222.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.777}, "power_stats": {"power_gpu_soc_mean_watts": 20.947, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.978, "gpu_utilization_percent_mean": 69.777, "power_watts_avg": 20.947, "energy_joules_est": 232.9, "duration_seconds": 11.119, "sample_count": 94}, "timestamp": "2026-01-25T21:45:11.883937"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9958.447, "latencies_ms": [9958.447], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 67, "n_tiles": 16, "output_text": "1. Player: 1\n2. Baseball bat: 1\n3. Pitcher: 1\n4. Catcher: 1\n5. Umpire: 1\n6. Home plate: 1\n7. Mound: 1\n8. Grass: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24618.5, "ram_available_mb": 38222.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24584.7, "ram_available_mb": 38256.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.44}, "power_stats": {"power_gpu_soc_mean_watts": 21.514, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 71.44, "power_watts_avg": 21.514, "energy_joules_est": 214.26, "duration_seconds": 9.959, "sample_count": 84}, "timestamp": "2026-01-25T21:45:23.887724"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9741.038, "latencies_ms": [9741.038], "images_per_second": 0.103, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "In the foreground, a baseball player is standing with a bat, ready to swing. Behind the player, there is a catcher crouched down with a baseball glove, and further back, an umpire is standing with a mask on. The background shows a well-maintained grassy field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24522.9, "ram_available_mb": 38318.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24602.5, "ram_available_mb": 38238.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.506}, "power_stats": {"power_gpu_soc_mean_watts": 21.393, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 9.014, "gpu_utilization_percent_mean": 70.506, "power_watts_avg": 21.393, "energy_joules_est": 208.4, "duration_seconds": 9.742, "sample_count": 83}, "timestamp": "2026-01-25T21:45:35.663766"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9153.063, "latencies_ms": [9153.063], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image captures a moment from a baseball game, with a batter in mid-swing, a catcher crouched behind him, and an umpire observing the play. The scene is set on a well-maintained baseball field with a lush green outfield.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24534.2, "ram_available_mb": 38306.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24527.6, "ram_available_mb": 38313.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.231}, "power_stats": {"power_gpu_soc_mean_watts": 21.72, "power_cpu_cv_mean_watts": 1.776, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 71.231, "power_watts_avg": 21.72, "energy_joules_est": 198.82, "duration_seconds": 9.154, "sample_count": 78}, "timestamp": "2026-01-25T21:45:46.844549"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7889.422, "latencies_ms": [7889.422], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a baseball game in progress with a player in a red jersey swinging a bat on a field with visible dirt and grass. The lighting suggests it is a sunny day, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24527.6, "ram_available_mb": 38313.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24612.5, "ram_available_mb": 38228.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.076}, "power_stats": {"power_gpu_soc_mean_watts": 22.157, "power_cpu_cv_mean_watts": 1.699, "power_sys_5v0_mean_watts": 9.032, "gpu_utilization_percent_mean": 72.076, "power_watts_avg": 22.157, "energy_joules_est": 174.82, "duration_seconds": 7.89, "sample_count": 66}, "timestamp": "2026-01-25T21:45:56.751839"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12092.558, "latencies_ms": [12092.558], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a white cat is standing on a concrete surface, holding a small bird in its mouth. The bird appears to be a pigeon, and the cat seems to be in the process of eating it. The scene is set outdoors, with the cat and the bird being the main focus of the image.\n\nThere are several leaves scattered around the area, adding", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24522.4, "ram_available_mb": 38318.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24510.4, "ram_available_mb": 38330.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.819}, "power_stats": {"power_gpu_soc_mean_watts": 22.544, "power_cpu_cv_mean_watts": 1.804, "power_sys_5v0_mean_watts": 9.146, "gpu_utilization_percent_mean": 72.819, "power_watts_avg": 22.544, "energy_joules_est": 272.63, "duration_seconds": 12.093, "sample_count": 105}, "timestamp": "2026-01-25T21:46:10.898835"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8407.915, "latencies_ms": [8407.915], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "cat: 1, bird: 1, leaves: numerous, ground: 1, sunlight: 1, shadows: 1, feathers: numerous, bird's head: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24510.4, "ram_available_mb": 38330.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24585.4, "ram_available_mb": 38255.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.183}, "power_stats": {"power_gpu_soc_mean_watts": 24.057, "power_cpu_cv_mean_watts": 1.472, "power_sys_5v0_mean_watts": 9.156, "gpu_utilization_percent_mean": 77.183, "power_watts_avg": 24.057, "energy_joules_est": 202.28, "duration_seconds": 8.409, "sample_count": 71}, "timestamp": "2026-01-25T21:46:21.332502"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12023.72, "latencies_ms": [12023.72], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a bird with its head near the ground, close to the camera's viewpoint. Behind the bird, a cat with its head raised is positioned higher up in the image, creating a sense of depth. The background consists of a concrete surface that extends to the edges of the image, providing a stable and unchanging backdrop to the dynamic interaction", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24523.5, "ram_available_mb": 38317.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24540.5, "ram_available_mb": 38300.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.216}, "power_stats": {"power_gpu_soc_mean_watts": 22.659, "power_cpu_cv_mean_watts": 1.798, "power_sys_5v0_mean_watts": 9.186, "gpu_utilization_percent_mean": 73.216, "power_watts_avg": 22.659, "energy_joules_est": 272.46, "duration_seconds": 12.024, "sample_count": 102}, "timestamp": "2026-01-25T21:46:35.369435"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7582.239, "latencies_ms": [7582.239], "images_per_second": 0.132, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "A cat is seen eating a bird on the ground, surrounded by leaves and twigs. The scene takes place outdoors, possibly in a garden or a park.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24540.5, "ram_available_mb": 38300.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24639.9, "ram_available_mb": 38201.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.556}, "power_stats": {"power_gpu_soc_mean_watts": 24.223, "power_cpu_cv_mean_watts": 1.347, "power_sys_5v0_mean_watts": 9.117, "gpu_utilization_percent_mean": 78.556, "power_watts_avg": 24.223, "energy_joules_est": 183.68, "duration_seconds": 7.583, "sample_count": 63}, "timestamp": "2026-01-25T21:46:44.965559"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9027.279, "latencies_ms": [9027.279], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image features a cat with a white and brown coat, standing on a concrete surface with a shadow cast to the right side of the frame. The lighting is bright and appears to be natural sunlight, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 24639.9, "ram_available_mb": 38201.0, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24641.4, "ram_available_mb": 38199.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.868}, "power_stats": {"power_gpu_soc_mean_watts": 23.556, "power_cpu_cv_mean_watts": 1.586, "power_sys_5v0_mean_watts": 9.204, "gpu_utilization_percent_mean": 75.868, "power_watts_avg": 23.556, "energy_joules_est": 212.66, "duration_seconds": 9.028, "sample_count": 76}, "timestamp": "2026-01-25T21:46:56.031216"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12307.801, "latencies_ms": [12307.801], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a hand is holding a sandwich with a bite taken out of it. The sandwich is made with a white bread roll, filled with a slice of red tomato, a slice of green spinach, and a slice of white cheese. The hand holding the sandwich is positioned on the left side of the image. The background is a white countertop.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24519.3, "ram_available_mb": 38321.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24612.9, "ram_available_mb": 38228.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.856}, "power_stats": {"power_gpu_soc_mean_watts": 22.939, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 9.228, "gpu_utilization_percent_mean": 73.856, "power_watts_avg": 22.939, "energy_joules_est": 282.34, "duration_seconds": 12.308, "sample_count": 104}, "timestamp": "2026-01-25T21:47:10.387790"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8941.081, "latencies_ms": [8941.081], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "hand: 1, sandwich: 1, tomato: 1, lettuce: 1, bread: 1, knife: 1, cutting board: 1, crust: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24526.5, "ram_available_mb": 38314.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24604.1, "ram_available_mb": 38236.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.395}, "power_stats": {"power_gpu_soc_mean_watts": 24.241, "power_cpu_cv_mean_watts": 1.47, "power_sys_5v0_mean_watts": 9.158, "gpu_utilization_percent_mean": 78.395, "power_watts_avg": 24.241, "energy_joules_est": 216.76, "duration_seconds": 8.942, "sample_count": 76}, "timestamp": "2026-01-25T21:47:21.352728"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12416.635, "latencies_ms": [12416.635], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a hand holding a sandwich with a bite taken out of it, indicating it is being eaten. The sandwich is composed of a white bread roll, a layer of green spinach, a slice of red tomato, and a layer of pink meat, likely ham or roast beef. In the background, there is a blurred", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24523.9, "ram_available_mb": 38317.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24606.2, "ram_available_mb": 38234.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.019}, "power_stats": {"power_gpu_soc_mean_watts": 22.873, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 9.208, "gpu_utilization_percent_mean": 74.019, "power_watts_avg": 22.873, "energy_joules_est": 284.02, "duration_seconds": 12.417, "sample_count": 106}, "timestamp": "2026-01-25T21:47:35.787664"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8265.894, "latencies_ms": [8265.894], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A person is holding a sandwich with a bite taken out of it, and the sandwich is on a white plate. The background appears to be a kitchen counter with a stove.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24606.2, "ram_available_mb": 38234.7, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24641.4, "ram_available_mb": 38199.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.2}, "power_stats": {"power_gpu_soc_mean_watts": 24.583, "power_cpu_cv_mean_watts": 1.378, "power_sys_5v0_mean_watts": 9.174, "gpu_utilization_percent_mean": 79.2, "power_watts_avg": 24.583, "energy_joules_est": 203.22, "duration_seconds": 8.267, "sample_count": 70}, "timestamp": "2026-01-25T21:47:46.095399"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9930.432, "latencies_ms": [9930.432], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a hand holding a sandwich with a slice of tomato and a leafy green vegetable, possibly spinach, on a white plate. The lighting in the image is dim, and the focus is shallow, with the background out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24641.4, "ram_available_mb": 38199.5, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 7.5, "ram_used_mb": 24521.3, "ram_available_mb": 38319.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.25}, "power_stats": {"power_gpu_soc_mean_watts": 23.298, "power_cpu_cv_mean_watts": 2.198, "power_sys_5v0_mean_watts": 9.256, "gpu_utilization_percent_mean": 76.25, "power_watts_avg": 23.298, "energy_joules_est": 231.37, "duration_seconds": 9.931, "sample_count": 84}, "timestamp": "2026-01-25T21:47:58.041726"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11124.579, "latencies_ms": [11124.579], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two young girls are enjoying a boat ride on a sunny day. They are sitting on the back of a boat, with one girl wearing a pink shirt and the other wearing a white shirt. Both girls are wearing hats to protect themselves from the sun. The boat is sailing on a body of water, with the girls looking out towards", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24521.3, "ram_available_mb": 38319.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 24584.8, "ram_available_mb": 38256.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.351}, "power_stats": {"power_gpu_soc_mean_watts": 20.929, "power_cpu_cv_mean_watts": 1.925, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.351, "power_watts_avg": 20.929, "energy_joules_est": 232.84, "duration_seconds": 11.125, "sample_count": 94}, "timestamp": "2026-01-25T21:48:11.199749"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7721.384, "latencies_ms": [7721.384], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "boat: 1\nrope: 10\nlife preserver: 1\nperson: 2\nhat: 1\nseat: 1\nwater: 1\nsail: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24584.8, "ram_available_mb": 38256.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24578.7, "ram_available_mb": 38262.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.415}, "power_stats": {"power_gpu_soc_mean_watts": 22.4, "power_cpu_cv_mean_watts": 1.651, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 72.415, "power_watts_avg": 22.4, "energy_joules_est": 172.97, "duration_seconds": 7.722, "sample_count": 65}, "timestamp": "2026-01-25T21:48:20.964663"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11102.123, "latencies_ms": [11102.123], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "Two individuals are seated on the bow of a boat, with one person in the foreground wearing a pink top and orange pants, and the other in the background wearing a white top and blue jeans. They are positioned near the edge of the boat, which is in the foreground, while the calm blue water and the boat's mast form the background.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24525.2, "ram_available_mb": 38315.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24588.6, "ram_available_mb": 38252.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.702}, "power_stats": {"power_gpu_soc_mean_watts": 20.991, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.986, "gpu_utilization_percent_mean": 69.702, "power_watts_avg": 20.991, "energy_joules_est": 233.06, "duration_seconds": 11.103, "sample_count": 94}, "timestamp": "2026-01-25T21:48:34.091188"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7159.77, "latencies_ms": [7159.77], "images_per_second": 0.14, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "Two young girls are sitting on the bow of a sailboat, enjoying the view of the ocean. They are both wearing casual summer clothes and one of them is wearing a hat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24588.6, "ram_available_mb": 38252.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24589.1, "ram_available_mb": 38251.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.836}, "power_stats": {"power_gpu_soc_mean_watts": 22.772, "power_cpu_cv_mean_watts": 1.622, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 73.836, "power_watts_avg": 22.772, "energy_joules_est": 163.06, "duration_seconds": 7.16, "sample_count": 61}, "timestamp": "2026-01-25T21:48:43.276268"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6540.873, "latencies_ms": [6540.873], "images_per_second": 0.153, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image shows two individuals on a sailboat with a clear blue sky in the background. The boat's white sail and the bright orange lifebuoy are prominent features.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24527.2, "ram_available_mb": 38313.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24579.0, "ram_available_mb": 38261.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.582}, "power_stats": {"power_gpu_soc_mean_watts": 22.988, "power_cpu_cv_mean_watts": 1.573, "power_sys_5v0_mean_watts": 9.08, "gpu_utilization_percent_mean": 73.582, "power_watts_avg": 22.988, "energy_joules_est": 150.38, "duration_seconds": 6.542, "sample_count": 55}, "timestamp": "2026-01-25T21:48:51.860722"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11129.021, "latencies_ms": [11129.021], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a **white sheep** standing in a field. The sheep is facing the camera and appears to be looking directly at it. The field is covered in **green grass** and there are **yellow flowers** scattered throughout. In the background, there is a **stone wall**. The sheep is positioned in the center of the image, with the stone wall behind it", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 24579.0, "ram_available_mb": 38261.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24517.2, "ram_available_mb": 38323.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.2}, "power_stats": {"power_gpu_soc_mean_watts": 20.847, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 69.2, "power_watts_avg": 20.847, "energy_joules_est": 232.02, "duration_seconds": 11.13, "sample_count": 95}, "timestamp": "2026-01-25T21:49:05.064189"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6718.514, "latencies_ms": [6718.514], "images_per_second": 0.149, "prompt_tokens": 39, "response_tokens_est": 38, "n_tiles": 16, "output_text": "sheep: 1, grass: many, stones: many, flowers: yellow, green plants: many, sunlight: bright, shadows: 1, sky: not visible", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24517.2, "ram_available_mb": 38323.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24543.9, "ram_available_mb": 38297.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.661}, "power_stats": {"power_gpu_soc_mean_watts": 23.098, "power_cpu_cv_mean_watts": 1.545, "power_sys_5v0_mean_watts": 9.03, "gpu_utilization_percent_mean": 73.661, "power_watts_avg": 23.098, "energy_joules_est": 155.2, "duration_seconds": 6.719, "sample_count": 56}, "timestamp": "2026-01-25T21:49:13.825536"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11121.121, "latencies_ms": [11121.121], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The sheep is standing in the foreground of the image, facing the camera with its body positioned slightly to the left of the frame. In the background, there is a stone wall with patches of yellow lichen growing on it, indicating that the sheep is in a grassy area near a man-made structure. The sheep appears to be at a moderate distance from the wall, as", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24543.9, "ram_available_mb": 38297.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24627.5, "ram_available_mb": 38213.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.904}, "power_stats": {"power_gpu_soc_mean_watts": 20.994, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 69.904, "power_watts_avg": 20.994, "energy_joules_est": 233.49, "duration_seconds": 11.122, "sample_count": 94}, "timestamp": "2026-01-25T21:49:26.978900"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5851.68, "latencies_ms": [5851.68], "images_per_second": 0.171, "prompt_tokens": 37, "response_tokens_est": 30, "n_tiles": 16, "output_text": "A white sheep is standing in a grassy field with a stone wall in the background. The sheep appears to be looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24627.5, "ram_available_mb": 38213.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24627.4, "ram_available_mb": 38213.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.102}, "power_stats": {"power_gpu_soc_mean_watts": 23.952, "power_cpu_cv_mean_watts": 1.373, "power_sys_5v0_mean_watts": 9.03, "gpu_utilization_percent_mean": 77.102, "power_watts_avg": 23.952, "energy_joules_est": 140.17, "duration_seconds": 5.852, "sample_count": 49}, "timestamp": "2026-01-25T21:49:34.863306"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7431.025, "latencies_ms": [7431.025], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The sheep is predominantly white with some black markings on its face and legs. It is standing in a grassy field with a stone wall in the background that has patches of yellow lichen growing on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24523.7, "ram_available_mb": 38317.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24593.1, "ram_available_mb": 38247.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.333}, "power_stats": {"power_gpu_soc_mean_watts": 22.318, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 9.041, "gpu_utilization_percent_mean": 72.333, "power_watts_avg": 22.318, "energy_joules_est": 165.86, "duration_seconds": 7.432, "sample_count": 63}, "timestamp": "2026-01-25T21:49:44.325641"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11140.325, "latencies_ms": [11140.325], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing next to a large truck, which is carrying a large black pipe. The man appears to be inspecting the pipe, possibly ensuring its proper placement or checking for any issues. The truck is parked in a parking lot, and there are other cars visible in the background.\n\nThere are two other people in the scene, one", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24515.8, "ram_available_mb": 38325.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24580.7, "ram_available_mb": 38260.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.255}, "power_stats": {"power_gpu_soc_mean_watts": 20.869, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 69.255, "power_watts_avg": 20.869, "energy_joules_est": 232.5, "duration_seconds": 11.141, "sample_count": 94}, "timestamp": "2026-01-25T21:49:57.504688"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8172.151, "latencies_ms": [8172.151], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "- Truck: 1\n- Trailer: 1\n- Man: 2\n- Hose: 1\n- Container: 2\n- Tree: 1\n- Car: 1\n- Watch: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24580.7, "ram_available_mb": 38260.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24530.6, "ram_available_mb": 38310.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.391}, "power_stats": {"power_gpu_soc_mean_watts": 22.251, "power_cpu_cv_mean_watts": 1.689, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 72.391, "power_watts_avg": 22.251, "energy_joules_est": 181.85, "duration_seconds": 8.173, "sample_count": 69}, "timestamp": "2026-01-25T21:50:07.714987"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11139.724, "latencies_ms": [11139.724], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large truck with a flatbed trailer loaded with multiple black and yellow pipes. A man is standing on the trailer, pointing towards the pipes. In the background, there is another man standing on the ground, observing the scene. The truck is positioned in the center of the image, with the man on the trailer closer", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24530.6, "ram_available_mb": 38310.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24531.8, "ram_available_mb": 38309.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.621}, "power_stats": {"power_gpu_soc_mean_watts": 20.951, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.621, "power_watts_avg": 20.951, "energy_joules_est": 233.41, "duration_seconds": 11.141, "sample_count": 95}, "timestamp": "2026-01-25T21:50:20.869275"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6595.182, "latencies_ms": [6595.182], "images_per_second": 0.152, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "In the image, a man is standing next to a large truck loaded with multiple black pipes. The truck is parked in a lot with trees in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24531.8, "ram_available_mb": 38309.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24531.6, "ram_available_mb": 38309.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.054}, "power_stats": {"power_gpu_soc_mean_watts": 23.174, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 9.014, "gpu_utilization_percent_mean": 74.054, "power_watts_avg": 23.174, "energy_joules_est": 152.85, "duration_seconds": 6.596, "sample_count": 56}, "timestamp": "2026-01-25T21:50:29.504501"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6201.078, "latencies_ms": [6201.078], "images_per_second": 0.161, "prompt_tokens": 36, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A large black and blue pipe is being transported on a yellow trailer. The sky is overcast, and the ground appears to be wet, suggesting recent rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24531.6, "ram_available_mb": 38309.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24594.3, "ram_available_mb": 38246.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.075}, "power_stats": {"power_gpu_soc_mean_watts": 23.256, "power_cpu_cv_mean_watts": 1.526, "power_sys_5v0_mean_watts": 9.103, "gpu_utilization_percent_mean": 73.075, "power_watts_avg": 23.256, "energy_joules_est": 144.23, "duration_seconds": 6.202, "sample_count": 53}, "timestamp": "2026-01-25T21:50:37.759064"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11151.442, "latencies_ms": [11151.442], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a group of three giraffes walking together in a dirt field near a pond. They are all moving in the same direction, with one giraffe slightly ahead of the other two. The giraffes are positioned close to each other, creating a sense of unity and togetherness.\n\nIn addition to the giraff", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24517.6, "ram_available_mb": 38323.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24539.9, "ram_available_mb": 38301.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.175}, "power_stats": {"power_gpu_soc_mean_watts": 20.779, "power_cpu_cv_mean_watts": 1.928, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.175, "power_watts_avg": 20.779, "energy_joules_est": 231.73, "duration_seconds": 11.152, "sample_count": 97}, "timestamp": "2026-01-25T21:50:50.980242"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9843.953, "latencies_ms": [9843.953], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "1. Giraffe: 3\n2. Deer: 1\n3. Watering hole: 1\n4. Fence: 1\n5. Bushes: 1\n6. Trees: 1\n7. Rock: 1\n8. Ground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24539.9, "ram_available_mb": 38301.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24534.9, "ram_available_mb": 38306.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.06}, "power_stats": {"power_gpu_soc_mean_watts": 21.501, "power_cpu_cv_mean_watts": 1.835, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 71.06, "power_watts_avg": 21.501, "energy_joules_est": 211.67, "duration_seconds": 9.845, "sample_count": 84}, "timestamp": "2026-01-25T21:51:02.841538"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11086.043, "latencies_ms": [11086.043], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a small deer sitting near the bottom right corner of the image. In the background, there are three giraffes walking across the dirt ground, with one closer to the left side, one in the middle, and one closer to the right side of the image. The giraffes are positioned near a body of water, which is located", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24534.9, "ram_available_mb": 38306.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24576.4, "ram_available_mb": 38264.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.947}, "power_stats": {"power_gpu_soc_mean_watts": 21.06, "power_cpu_cv_mean_watts": 1.925, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 69.947, "power_watts_avg": 21.06, "energy_joules_est": 233.49, "duration_seconds": 11.087, "sample_count": 94}, "timestamp": "2026-01-25T21:51:15.947342"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7297.683, "latencies_ms": [7297.683], "images_per_second": 0.137, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "Three giraffes are walking in a line across a dirt path in a natural setting with trees and bushes in the background. A small deer is sitting on the ground near a pond.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24576.4, "ram_available_mb": 38264.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24563.3, "ram_available_mb": 38277.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.903}, "power_stats": {"power_gpu_soc_mean_watts": 22.607, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 72.903, "power_watts_avg": 22.607, "energy_joules_est": 164.99, "duration_seconds": 7.298, "sample_count": 62}, "timestamp": "2026-01-25T21:51:25.283142"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9208.483, "latencies_ms": [9208.483], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image shows three giraffes with distinct brown and white spotted patterns walking in a line across a dirt path. A small deer is sitting on the ground to the right, and there is a pond with lily pads in the foreground reflecting the trees and sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24563.3, "ram_available_mb": 38277.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24564.3, "ram_available_mb": 38276.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.41}, "power_stats": {"power_gpu_soc_mean_watts": 21.461, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 70.41, "power_watts_avg": 21.461, "energy_joules_est": 197.64, "duration_seconds": 9.209, "sample_count": 78}, "timestamp": "2026-01-25T21:51:36.550603"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11140.851, "latencies_ms": [11140.851], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large pizza sitting on a white plate, placed on a dining table. The pizza is topped with various ingredients, including mushrooms and ham. There are two wine glasses on the table, one closer to the left side and the other near the right side. A bowl can also be seen on the table, possibly containing a side dish", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24564.3, "ram_available_mb": 38276.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24527.0, "ram_available_mb": 38313.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.219}, "power_stats": {"power_gpu_soc_mean_watts": 20.85, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 69.219, "power_watts_avg": 20.85, "energy_joules_est": 232.3, "duration_seconds": 11.142, "sample_count": 96}, "timestamp": "2026-01-25T21:51:49.741064"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8412.311, "latencies_ms": [8412.311], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "table: 1\nchair: 1\nglass: 2\npizza: 1\nartichoke: 1\nham: 1\nmushrooms: 1\nbottle: 1\nperson: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24527.0, "ram_available_mb": 38313.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 8.1, "ram_used_mb": 24523.9, "ram_available_mb": 38317.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.181}, "power_stats": {"power_gpu_soc_mean_watts": 22.133, "power_cpu_cv_mean_watts": 2.342, "power_sys_5v0_mean_watts": 9.034, "gpu_utilization_percent_mean": 73.181, "power_watts_avg": 22.133, "energy_joules_est": 186.2, "duration_seconds": 8.413, "sample_count": 72}, "timestamp": "2026-01-25T21:52:00.190928"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11108.527, "latencies_ms": [11108.527], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a large pizza on a white plate, which is placed on a table covered with a white tablecloth. To the right of the pizza, there are two glasses of beer, one near the top right corner and the other slightly behind it. In the background, there are other tables and chairs, indicating that this scene takes", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24523.9, "ram_available_mb": 38317.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24530.6, "ram_available_mb": 38310.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.021}, "power_stats": {"power_gpu_soc_mean_watts": 20.957, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 70.021, "power_watts_avg": 20.957, "energy_joules_est": 232.82, "duration_seconds": 11.109, "sample_count": 94}, "timestamp": "2026-01-25T21:52:13.324042"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8178.632, "latencies_ms": [8178.632], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image depicts a cozy restaurant setting with a large pizza placed on a white plate in the foreground. There are two glasses of beer on the table, and a bowl of salad is visible in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24530.6, "ram_available_mb": 38310.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24526.4, "ram_available_mb": 38314.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.13}, "power_stats": {"power_gpu_soc_mean_watts": 22.24, "power_cpu_cv_mean_watts": 1.683, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 73.13, "power_watts_avg": 22.24, "energy_joules_est": 181.91, "duration_seconds": 8.179, "sample_count": 69}, "timestamp": "2026-01-25T21:52:23.519045"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9772.926, "latencies_ms": [9772.926], "images_per_second": 0.102, "prompt_tokens": 36, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The pizza is placed on a white plate, which contrasts with the golden-brown crust and the colorful toppings. The lighting in the room is warm and ambient, likely from the interior lights, which cast a soft glow on the pizza and the glasses of beer on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24526.4, "ram_available_mb": 38314.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24617.3, "ram_available_mb": 38223.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.253}, "power_stats": {"power_gpu_soc_mean_watts": 21.335, "power_cpu_cv_mean_watts": 1.852, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 70.253, "power_watts_avg": 21.335, "energy_joules_est": 208.52, "duration_seconds": 9.774, "sample_count": 83}, "timestamp": "2026-01-25T21:52:35.345918"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11103.751, "latencies_ms": [11103.751], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a black cat is standing on a white bathroom sink, drinking water from a silver faucet. The cat is positioned on the right side of the sink, with its head lowered to the water. The sink is located in a bathroom, and there is a bottle of hand soap on the left side of the sink. The scene captures a moment", "error": null, "sys_before": {"cpu_percent": 13.6, "ram_used_mb": 24522.0, "ram_available_mb": 38318.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24514.3, "ram_available_mb": 38326.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.266}, "power_stats": {"power_gpu_soc_mean_watts": 20.926, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.266, "power_watts_avg": 20.926, "energy_joules_est": 232.37, "duration_seconds": 11.104, "sample_count": 94}, "timestamp": "2026-01-25T21:52:48.486741"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7606.57, "latencies_ms": [7606.57], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "faucet: 1, cat: 1, sink: 1, water: 1, bottle: 1, dish: 1, dish soap: 1, label: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24514.3, "ram_available_mb": 38326.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24620.5, "ram_available_mb": 38220.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.5}, "power_stats": {"power_gpu_soc_mean_watts": 22.401, "power_cpu_cv_mean_watts": 1.633, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 72.5, "power_watts_avg": 22.401, "energy_joules_est": 170.41, "duration_seconds": 7.607, "sample_count": 64}, "timestamp": "2026-01-25T21:52:58.139172"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11117.618, "latencies_ms": [11117.618], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a black cat is positioned near the edge of a white sink, with its head close to the faucet, suggesting it is about to drink from the running water. The faucet is centrally located in the sink, and the water is flowing from it towards the cat. In the background, there is a bottle of liquid soap on the left", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24531.9, "ram_available_mb": 38309.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24596.3, "ram_available_mb": 38244.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.347}, "power_stats": {"power_gpu_soc_mean_watts": 20.922, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.347, "power_watts_avg": 20.922, "energy_joules_est": 232.62, "duration_seconds": 11.118, "sample_count": 95}, "timestamp": "2026-01-25T21:53:11.290299"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6723.268, "latencies_ms": [6723.268], "images_per_second": 0.149, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A black cat is standing on a white bathroom sink, drinking water from a silver faucet. There is a bottle of liquid soap on the edge of the sink.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24537.9, "ram_available_mb": 38303.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24628.0, "ram_available_mb": 38212.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.089}, "power_stats": {"power_gpu_soc_mean_watts": 23.185, "power_cpu_cv_mean_watts": 1.523, "power_sys_5v0_mean_watts": 8.988, "gpu_utilization_percent_mean": 74.089, "power_watts_avg": 23.185, "energy_joules_est": 155.89, "duration_seconds": 6.724, "sample_count": 56}, "timestamp": "2026-01-25T21:53:20.025408"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6787.209, "latencies_ms": [6787.209], "images_per_second": 0.147, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image shows a black cat drinking water from a silver faucet in a white sink. The lighting in the room is bright, and the walls are painted a light blue color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24628.0, "ram_available_mb": 38212.9, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24621.2, "ram_available_mb": 38219.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.534}, "power_stats": {"power_gpu_soc_mean_watts": 22.75, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 9.04, "gpu_utilization_percent_mean": 72.534, "power_watts_avg": 22.75, "energy_joules_est": 154.42, "duration_seconds": 6.788, "sample_count": 58}, "timestamp": "2026-01-25T21:53:28.859931"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12320.877, "latencies_ms": [12320.877], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man and a woman are riding a horse-drawn carriage through a puddle of water. The horse is pulling the carriage, and the man is sitting in the front while the woman is in the back. They are both wearing hats, and the man is wearing a plaid shirt. The carriage is positioned in the middle of the", "error": null, "sys_before": {"cpu_percent": 6.5, "ram_used_mb": 24533.5, "ram_available_mb": 38307.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24620.3, "ram_available_mb": 38220.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.029}, "power_stats": {"power_gpu_soc_mean_watts": 22.808, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 9.193, "gpu_utilization_percent_mean": 74.029, "power_watts_avg": 22.808, "energy_joules_est": 281.03, "duration_seconds": 12.322, "sample_count": 104}, "timestamp": "2026-01-25T21:53:43.218828"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11229.482, "latencies_ms": [11229.482], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "1. Carriage: 1\n2. Horse: 1\n3. Puddle: 1\n4. Reflection: 1\n5. Man: 2\n6. Hat: 1\n7. Wagon wheel: 2\n8. Trees: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24533.5, "ram_available_mb": 38307.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24635.7, "ram_available_mb": 38205.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.74}, "power_stats": {"power_gpu_soc_mean_watts": 23.315, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 9.137, "gpu_utilization_percent_mean": 75.74, "power_watts_avg": 23.315, "energy_joules_est": 261.83, "duration_seconds": 11.23, "sample_count": 96}, "timestamp": "2026-01-25T21:53:56.505352"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10635.048, "latencies_ms": [10635.048], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "In the foreground, there is a horse pulling a wooden cart with two people seated inside. The horse is positioned on the right side of the image, while the cart is located in the center. The background features a plowed field and a barn, with a clear blue sky above.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24635.7, "ram_available_mb": 38205.2, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24569.9, "ram_available_mb": 38271.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.187}, "power_stats": {"power_gpu_soc_mean_watts": 23.363, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 9.224, "gpu_utilization_percent_mean": 75.187, "power_watts_avg": 23.363, "energy_joules_est": 248.48, "duration_seconds": 10.636, "sample_count": 91}, "timestamp": "2026-01-25T21:54:09.187172"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9384.878, "latencies_ms": [9384.878], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "Two people are riding in a horse-drawn carriage through a puddle on a sunny day. The horse is brown and is pulling the carriage, while the riders are wearing hats and casual clothing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24569.9, "ram_available_mb": 38271.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24524.8, "ram_available_mb": 38316.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.05}, "power_stats": {"power_gpu_soc_mean_watts": 23.983, "power_cpu_cv_mean_watts": 1.506, "power_sys_5v0_mean_watts": 9.151, "gpu_utilization_percent_mean": 78.05, "power_watts_avg": 23.983, "energy_joules_est": 225.09, "duration_seconds": 9.386, "sample_count": 80}, "timestamp": "2026-01-25T21:54:20.590553"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9359.009, "latencies_ms": [9359.009], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image features a clear blue sky and a bright sun casting shadows on the ground, creating a reflection of the horse-drawn carriage on the wet surface. The carriage is made of wood and metal, with the horse being a rich brown color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24524.8, "ram_available_mb": 38316.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24586.7, "ram_available_mb": 38254.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.62}, "power_stats": {"power_gpu_soc_mean_watts": 23.857, "power_cpu_cv_mean_watts": 1.551, "power_sys_5v0_mean_watts": 9.247, "gpu_utilization_percent_mean": 76.62, "power_watts_avg": 23.857, "energy_joules_est": 223.29, "duration_seconds": 9.36, "sample_count": 79}, "timestamp": "2026-01-25T21:54:31.988828"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11090.005, "latencies_ms": [11090.005], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a bride and groom are standing together on a grassy lawn, holding an umbrella to protect themselves from the sun. The bride is wearing a white wedding dress and holding a bouquet of flowers, while the groom is dressed in a suit. They are both smiling and appear to be enjoying their special day.\n\nIn the", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24524.9, "ram_available_mb": 38316.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24571.5, "ram_available_mb": 38269.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.574}, "power_stats": {"power_gpu_soc_mean_watts": 20.998, "power_cpu_cv_mean_watts": 1.929, "power_sys_5v0_mean_watts": 9.004, "gpu_utilization_percent_mean": 69.574, "power_watts_avg": 20.998, "energy_joules_est": 232.88, "duration_seconds": 11.091, "sample_count": 94}, "timestamp": "2026-01-25T21:54:45.136114"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7749.655, "latencies_ms": [7749.655], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "umbrella: 1, bride: 1, groom: 1, flower bouquet: 1, grass: 1, building: 1, stone wall: 1, people: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24571.5, "ram_available_mb": 38269.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24589.5, "ram_available_mb": 38251.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.569}, "power_stats": {"power_gpu_soc_mean_watts": 22.452, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 72.569, "power_watts_avg": 22.452, "energy_joules_est": 174.01, "duration_seconds": 7.75, "sample_count": 65}, "timestamp": "2026-01-25T21:54:54.926517"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8928.772, "latencies_ms": [8928.772], "images_per_second": 0.112, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The bride and groom are standing close together in the foreground, with the bride on the left holding a bouquet of flowers and the groom on the right holding an umbrella. In the background, there are other people and a stone building with a red roof.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24517.7, "ram_available_mb": 38323.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24585.8, "ram_available_mb": 38255.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.868}, "power_stats": {"power_gpu_soc_mean_watts": 21.74, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 9.019, "gpu_utilization_percent_mean": 71.868, "power_watts_avg": 21.74, "energy_joules_est": 194.12, "duration_seconds": 8.929, "sample_count": 76}, "timestamp": "2026-01-25T21:55:05.917523"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7075.856, "latencies_ms": [7075.856], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A bride and groom are standing under a black and white umbrella in a garden, with the groom holding the umbrella and the bride holding a bouquet of flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24524.1, "ram_available_mb": 38316.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24518.1, "ram_available_mb": 38322.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.967}, "power_stats": {"power_gpu_soc_mean_watts": 22.792, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 8.986, "gpu_utilization_percent_mean": 72.967, "power_watts_avg": 22.792, "energy_joules_est": 161.29, "duration_seconds": 7.076, "sample_count": 60}, "timestamp": "2026-01-25T21:55:15.020415"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10500.09, "latencies_ms": [10500.09], "images_per_second": 0.095, "prompt_tokens": 36, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The image shows a bride and groom standing under a black and white umbrella. The bride is wearing a white wedding dress and holding a bouquet of flowers, while the groom is wearing a black suit. The weather appears to be overcast, as the umbrella is open and the lighting is diffused.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24518.1, "ram_available_mb": 38322.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24596.1, "ram_available_mb": 38244.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.539}, "power_stats": {"power_gpu_soc_mean_watts": 21.206, "power_cpu_cv_mean_watts": 1.871, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 70.539, "power_watts_avg": 21.206, "energy_joules_est": 222.68, "duration_seconds": 10.501, "sample_count": 89}, "timestamp": "2026-01-25T21:55:27.565840"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12147.258, "latencies_ms": [12147.258], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is sitting on a sandy beach, enjoying a sunny day. He is wearing a white shirt and brown shorts, and his feet are propped up on the sand. The man is also wearing a watch on his left wrist. \n\nA colorful kite is flying in the background, adding a vibrant touch to", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24596.1, "ram_available_mb": 38244.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24599.9, "ram_available_mb": 38241.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.01}, "power_stats": {"power_gpu_soc_mean_watts": 22.081, "power_cpu_cv_mean_watts": 1.8, "power_sys_5v0_mean_watts": 9.133, "gpu_utilization_percent_mean": 73.01, "power_watts_avg": 22.081, "energy_joules_est": 268.24, "duration_seconds": 12.148, "sample_count": 103}, "timestamp": "2026-01-25T21:55:41.778128"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9505.898, "latencies_ms": [9505.898], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "sand: numerous\nkite: 1\nperson: 2\nwatch: 1\nkite string: 4\nkite: 1\nkite string: 4\nkite: 1\nkite string: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24599.9, "ram_available_mb": 38241.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24539.5, "ram_available_mb": 38301.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.976}, "power_stats": {"power_gpu_soc_mean_watts": 22.957, "power_cpu_cv_mean_watts": 1.577, "power_sys_5v0_mean_watts": 9.087, "gpu_utilization_percent_mean": 76.976, "power_watts_avg": 22.957, "energy_joules_est": 218.24, "duration_seconds": 9.507, "sample_count": 82}, "timestamp": "2026-01-25T21:55:53.304267"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11644.078, "latencies_ms": [11644.078], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there is a person lying on the sand with their legs up, and another person is sitting behind them, holding onto a kite string. The kite is in the background, flying high in the air near the top of the image. The ocean and waves are in the far background, creating a serene backdrop for the beach scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24539.5, "ram_available_mb": 38301.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 7.8, "ram_used_mb": 24611.1, "ram_available_mb": 38229.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.798}, "power_stats": {"power_gpu_soc_mean_watts": 22.768, "power_cpu_cv_mean_watts": 2.265, "power_sys_5v0_mean_watts": 9.218, "gpu_utilization_percent_mean": 73.798, "power_watts_avg": 22.768, "energy_joules_est": 265.13, "duration_seconds": 11.645, "sample_count": 99}, "timestamp": "2026-01-25T21:56:06.983976"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8083.566, "latencies_ms": [8083.566], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A man is sitting on the beach, enjoying the sun and the waves. He is holding a kite that is flying in the air, with its string stretched out in front of him.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24611.1, "ram_available_mb": 38229.8, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24537.1, "ram_available_mb": 38303.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.159}, "power_stats": {"power_gpu_soc_mean_watts": 24.197, "power_cpu_cv_mean_watts": 1.439, "power_sys_5v0_mean_watts": 9.117, "gpu_utilization_percent_mean": 77.159, "power_watts_avg": 24.197, "energy_joules_est": 195.61, "duration_seconds": 8.084, "sample_count": 69}, "timestamp": "2026-01-25T21:56:17.119844"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10880.637, "latencies_ms": [10880.637], "images_per_second": 0.092, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image shows a sunny day at the beach with clear skies and bright sunlight casting shadows on the sand. A person is lying on the beach, wearing a white shirt and brown shorts, with a colorful kite in the background that has red, blue, green, and yellow ribbons.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24537.1, "ram_available_mb": 38303.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24651.2, "ram_available_mb": 38189.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.691}, "power_stats": {"power_gpu_soc_mean_watts": 22.272, "power_cpu_cv_mean_watts": 1.742, "power_sys_5v0_mean_watts": 9.17, "gpu_utilization_percent_mean": 73.691, "power_watts_avg": 22.272, "energy_joules_est": 242.35, "duration_seconds": 10.881, "sample_count": 94}, "timestamp": "2026-01-25T21:56:30.032893"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11137.207, "latencies_ms": [11137.207], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a brown couch positioned on the left side, a red chair on the right side, and a small black table in the center. On the table, there is a lamp providing light to the room. The room also features a television placed on the right side, and a dining table with chairs nearby. \n\nThere are", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24523.3, "ram_available_mb": 38317.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24530.0, "ram_available_mb": 38310.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.632}, "power_stats": {"power_gpu_soc_mean_watts": 20.973, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.632, "power_watts_avg": 20.973, "energy_joules_est": 233.59, "duration_seconds": 11.138, "sample_count": 95}, "timestamp": "2026-01-25T21:56:43.195180"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7176.011, "latencies_ms": [7176.011], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "couch: 1, lamp: 2, table: 1, chair: 1, window: 2, blinds: 2, wall: 1, floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24530.0, "ram_available_mb": 38310.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24588.2, "ram_available_mb": 38252.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.41}, "power_stats": {"power_gpu_soc_mean_watts": 22.735, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 72.41, "power_watts_avg": 22.735, "energy_joules_est": 163.16, "duration_seconds": 7.177, "sample_count": 61}, "timestamp": "2026-01-25T21:56:52.406383"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11115.33, "latencies_ms": [11115.33], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a brown leather couch positioned to the left, with a wooden side table and a red armchair to its right. The main objects are near the center of the image, with a small black table in the middle ground. In the background, there are two windows with wooden shutters, one on the left and one on the right, and a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24588.2, "ram_available_mb": 38252.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24602.9, "ram_available_mb": 38238.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.135}, "power_stats": {"power_gpu_soc_mean_watts": 20.988, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 69.135, "power_watts_avg": 20.988, "energy_joules_est": 233.3, "duration_seconds": 11.116, "sample_count": 96}, "timestamp": "2026-01-25T21:57:05.536305"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8144.481, "latencies_ms": [8144.481], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a brown leather couch, a black table with a lamp, and a red chair. There is a television on the right side of the room, and the windows have wooden shutters.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24525.2, "ram_available_mb": 38315.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24579.4, "ram_available_mb": 38261.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.971}, "power_stats": {"power_gpu_soc_mean_watts": 22.199, "power_cpu_cv_mean_watts": 1.717, "power_sys_5v0_mean_watts": 9.033, "gpu_utilization_percent_mean": 72.971, "power_watts_avg": 22.199, "energy_joules_est": 180.81, "duration_seconds": 8.145, "sample_count": 69}, "timestamp": "2026-01-25T21:57:15.696851"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6199.585, "latencies_ms": [6199.585], "images_per_second": 0.161, "prompt_tokens": 36, "response_tokens_est": 35, "n_tiles": 16, "output_text": "The room is dimly lit with a warm glow from the table lamps. The walls are painted in a light color, and the windows have wooden shutters.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24517.5, "ram_available_mb": 38323.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24523.6, "ram_available_mb": 38317.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.058}, "power_stats": {"power_gpu_soc_mean_watts": 23.311, "power_cpu_cv_mean_watts": 1.524, "power_sys_5v0_mean_watts": 9.159, "gpu_utilization_percent_mean": 74.058, "power_watts_avg": 23.311, "energy_joules_est": 144.54, "duration_seconds": 6.2, "sample_count": 52}, "timestamp": "2026-01-25T21:57:23.956702"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12303.129, "latencies_ms": [12303.129], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is captured in the midst of a casual outdoor meal. He's dressed in a blue t-shirt, which contrasts with the lush greenery of the park around him. The man is holding a white plate in his right hand, which is filled with a slice of cake. His left hand is holding a fork, poised to", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24523.6, "ram_available_mb": 38317.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24562.6, "ram_available_mb": 38278.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.686}, "power_stats": {"power_gpu_soc_mean_watts": 22.834, "power_cpu_cv_mean_watts": 1.792, "power_sys_5v0_mean_watts": 9.213, "gpu_utilization_percent_mean": 73.686, "power_watts_avg": 22.834, "energy_joules_est": 280.95, "duration_seconds": 12.304, "sample_count": 105}, "timestamp": "2026-01-25T21:57:38.332851"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8690.581, "latencies_ms": [8690.581], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "person: 1, plate: 1, piece of cake: 1, fork: 1, bite: 1, tree: 1, leaf: 1, grass: 1", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 24562.6, "ram_available_mb": 38278.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24562.6, "ram_available_mb": 38278.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.987}, "power_stats": {"power_gpu_soc_mean_watts": 24.278, "power_cpu_cv_mean_watts": 1.452, "power_sys_5v0_mean_watts": 9.198, "gpu_utilization_percent_mean": 78.987, "power_watts_avg": 24.278, "energy_joules_est": 211.01, "duration_seconds": 8.691, "sample_count": 75}, "timestamp": "2026-01-25T21:57:49.042075"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10604.797, "latencies_ms": [10604.797], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The person is standing in the foreground on the left side of the image, holding a plate with food on it. The background consists of trees and grass, indicating an outdoor setting. The person is positioned near the center of the image, with ample space on both the left and right sides.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24562.6, "ram_available_mb": 38278.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24563.3, "ram_available_mb": 38277.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.571}, "power_stats": {"power_gpu_soc_mean_watts": 23.413, "power_cpu_cv_mean_watts": 1.663, "power_sys_5v0_mean_watts": 9.248, "gpu_utilization_percent_mean": 75.571, "power_watts_avg": 23.413, "energy_joules_est": 248.3, "duration_seconds": 10.605, "sample_count": 91}, "timestamp": "2026-01-25T21:58:01.671014"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7043.104, "latencies_ms": [7043.104], "images_per_second": 0.142, "prompt_tokens": 37, "response_tokens_est": 29, "n_tiles": 16, "output_text": "A person is eating a piece of cake outdoors in a park-like setting with trees and grass visible in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24563.3, "ram_available_mb": 38277.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24616.4, "ram_available_mb": 38224.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 81.831}, "power_stats": {"power_gpu_soc_mean_watts": 25.393, "power_cpu_cv_mean_watts": 1.208, "power_sys_5v0_mean_watts": 9.176, "gpu_utilization_percent_mean": 81.831, "power_watts_avg": 25.393, "energy_joules_est": 178.86, "duration_seconds": 7.044, "sample_count": 59}, "timestamp": "2026-01-25T21:58:10.744076"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8898.773, "latencies_ms": [8898.773], "images_per_second": 0.112, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image shows a person holding a plate with a piece of cake, set against a backdrop of lush green trees under a clear blue sky. The lighting is natural and bright, suggesting it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24616.4, "ram_available_mb": 38224.5, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24563.5, "ram_available_mb": 38277.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.434}, "power_stats": {"power_gpu_soc_mean_watts": 24.055, "power_cpu_cv_mean_watts": 1.522, "power_sys_5v0_mean_watts": 9.288, "gpu_utilization_percent_mean": 77.434, "power_watts_avg": 24.055, "energy_joules_est": 214.08, "duration_seconds": 8.899, "sample_count": 76}, "timestamp": "2026-01-25T21:58:21.674168"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11132.313, "latencies_ms": [11132.313], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is walking alongside a horse that is carrying a large load of luggage. The horse is pulling the luggage on its back, and the man is walking beside it, possibly guiding or assisting in the journey. The luggage appears to be a mix of suitcases and backpacks, indicating that the man might be traveling or transporting", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24563.5, "ram_available_mb": 38277.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24588.3, "ram_available_mb": 38252.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.463}, "power_stats": {"power_gpu_soc_mean_watts": 20.92, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 69.463, "power_watts_avg": 20.92, "energy_joules_est": 232.91, "duration_seconds": 11.133, "sample_count": 95}, "timestamp": "2026-01-25T21:58:34.861947"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9740.512, "latencies_ms": [9740.512], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. Man: 1\n2. Horse: 1\n3. Basket: 1\n4. Luggage: 2\n5. Rope: 1\n6. Trees: 4\n7. Ground: 1\n8. Stones: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24524.6, "ram_available_mb": 38316.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24601.7, "ram_available_mb": 38239.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.494}, "power_stats": {"power_gpu_soc_mean_watts": 21.48, "power_cpu_cv_mean_watts": 1.823, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 70.494, "power_watts_avg": 21.48, "energy_joules_est": 209.24, "duration_seconds": 9.741, "sample_count": 83}, "timestamp": "2026-01-25T21:58:46.658497"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8918.995, "latencies_ms": [8918.995], "images_per_second": 0.112, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "A man is walking in the foreground, leading a horse in the background. The horse is carrying a large load on its back, including a red suitcase and a red bag. The man is walking on a dirt path, while the horse is walking on a rocky path.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24530.0, "ram_available_mb": 38310.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24616.4, "ram_available_mb": 38224.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.842}, "power_stats": {"power_gpu_soc_mean_watts": 21.566, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 70.842, "power_watts_avg": 21.566, "energy_joules_est": 192.36, "duration_seconds": 8.92, "sample_count": 76}, "timestamp": "2026-01-25T21:58:57.602543"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6748.159, "latencies_ms": [6748.159], "images_per_second": 0.148, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A man is walking with a horse carrying luggage on its back, likely on a journey or trip. The horse is walking on a dirt path with trees in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24616.4, "ram_available_mb": 38224.5, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24575.9, "ram_available_mb": 38265.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.421}, "power_stats": {"power_gpu_soc_mean_watts": 22.862, "power_cpu_cv_mean_watts": 1.58, "power_sys_5v0_mean_watts": 9.017, "gpu_utilization_percent_mean": 75.421, "power_watts_avg": 22.862, "energy_joules_est": 154.29, "duration_seconds": 6.749, "sample_count": 57}, "timestamp": "2026-01-25T21:59:06.363450"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7202.611, "latencies_ms": [7202.611], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image shows a donkey carrying a large red and grey suitcase on its back. The donkey is wearing a colorful blanket and is walking on a dirt path with trees in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24575.9, "ram_available_mb": 38265.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24594.1, "ram_available_mb": 38246.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.361}, "power_stats": {"power_gpu_soc_mean_watts": 22.518, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 9.059, "gpu_utilization_percent_mean": 72.361, "power_watts_avg": 22.518, "energy_joules_est": 162.2, "duration_seconds": 7.203, "sample_count": 61}, "timestamp": "2026-01-25T21:59:15.619012"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12137.908, "latencies_ms": [12137.908], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene night scene at a riverfront. A bridge, bathed in a soft blue glow, arches gracefully over the calm waters of the river. The bridge, with its white pillars and blue lights, stands as a beacon in the darkness. A boat, adorned with a vibrant red and green striped awning, floats", "error": null, "sys_before": {"cpu_percent": 13.0, "ram_used_mb": 24594.1, "ram_available_mb": 38246.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24595.6, "ram_available_mb": 38245.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.154}, "power_stats": {"power_gpu_soc_mean_watts": 22.087, "power_cpu_cv_mean_watts": 1.79, "power_sys_5v0_mean_watts": 9.118, "gpu_utilization_percent_mean": 73.154, "power_watts_avg": 22.087, "energy_joules_est": 268.1, "duration_seconds": 12.139, "sample_count": 104}, "timestamp": "2026-01-25T21:59:29.799971"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7994.633, "latencies_ms": [7994.633], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "people: 5, bridge: 1, boat: 1, water: 1, lights: 1, trees: 1, sky: 1, road: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24595.6, "ram_available_mb": 38245.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24624.3, "ram_available_mb": 38216.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.441}, "power_stats": {"power_gpu_soc_mean_watts": 24.193, "power_cpu_cv_mean_watts": 1.401, "power_sys_5v0_mean_watts": 9.15, "gpu_utilization_percent_mean": 78.441, "power_watts_avg": 24.193, "energy_joules_est": 193.43, "duration_seconds": 7.995, "sample_count": 68}, "timestamp": "2026-01-25T21:59:39.806299"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12074.195, "latencies_ms": [12074.195], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the foreground, there is a group of people standing on the left side of the image, near the water's edge. The main object, a bridge with blue lights, is in the background, spanning across the body of water. The boat is situated on the water, closer to the bridge than the people, and appears to be under the bridge's illumination.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24624.3, "ram_available_mb": 38216.6, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24602.9, "ram_available_mb": 38238.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.932}, "power_stats": {"power_gpu_soc_mean_watts": 22.657, "power_cpu_cv_mean_watts": 1.788, "power_sys_5v0_mean_watts": 9.151, "gpu_utilization_percent_mean": 72.932, "power_watts_avg": 22.657, "energy_joules_est": 273.58, "duration_seconds": 12.075, "sample_count": 103}, "timestamp": "2026-01-25T21:59:53.932089"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9718.791, "latencies_ms": [9718.791], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image captures a serene night scene at a river, where a bridge is illuminated with blue lights, creating a beautiful reflection on the water. A group of people are gathered on the riverbank, admiring the view and the boat passing under the bridge.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24602.9, "ram_available_mb": 38238.0, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 7.5, "ram_used_mb": 24592.4, "ram_available_mb": 38248.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.866}, "power_stats": {"power_gpu_soc_mean_watts": 23.501, "power_cpu_cv_mean_watts": 2.017, "power_sys_5v0_mean_watts": 9.195, "gpu_utilization_percent_mean": 75.866, "power_watts_avg": 23.501, "energy_joules_est": 228.42, "duration_seconds": 9.719, "sample_count": 82}, "timestamp": "2026-01-25T22:00:05.689067"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8109.18, "latencies_ms": [8109.18], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The bridge is illuminated with blue lights that create a striking contrast against the night sky. The reflection of the lights can be seen on the water's surface, adding to the visual appeal of the scene.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24592.4, "ram_available_mb": 38248.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24610.4, "ram_available_mb": 38230.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.986}, "power_stats": {"power_gpu_soc_mean_watts": 23.971, "power_cpu_cv_mean_watts": 1.52, "power_sys_5v0_mean_watts": 9.221, "gpu_utilization_percent_mean": 76.986, "power_watts_avg": 23.971, "energy_joules_est": 194.4, "duration_seconds": 8.11, "sample_count": 69}, "timestamp": "2026-01-25T22:00:15.840142"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11113.435, "latencies_ms": [11113.435], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person's foot is adorned with a vibrant pink shoe, which stands out against the backdrop of a blue bench. The shoe, characterized by a pointed toe and a small bow on the top, is firmly planted on the bench. The bench itself is painted in a striking blue color, with patches of", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24525.0, "ram_available_mb": 38315.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24580.3, "ram_available_mb": 38260.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.755}, "power_stats": {"power_gpu_soc_mean_watts": 20.996, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 69.755, "power_watts_avg": 20.996, "energy_joules_est": 233.35, "duration_seconds": 11.114, "sample_count": 94}, "timestamp": "2026-01-25T22:00:28.993901"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7390.525, "latencies_ms": [7390.525], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "shoe: 1, heel: 1, toe: 1, bow: 1, jeans: 1, blue: 1, paint: 1, bench: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24517.0, "ram_available_mb": 38323.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24594.4, "ram_available_mb": 38246.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.603}, "power_stats": {"power_gpu_soc_mean_watts": 22.535, "power_cpu_cv_mean_watts": 1.633, "power_sys_5v0_mean_watts": 9.034, "gpu_utilization_percent_mean": 72.603, "power_watts_avg": 22.535, "energy_joules_est": 166.56, "duration_seconds": 7.391, "sample_count": 63}, "timestamp": "2026-01-25T22:00:38.411959"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8210.546, "latencies_ms": [8210.546], "images_per_second": 0.122, "prompt_tokens": 44, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The pink shoe is positioned in the foreground, resting on a blue bench that occupies the middle ground of the image. The bench has a worn appearance with peeling paint, indicating it is the main object in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24517.8, "ram_available_mb": 38323.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24606.0, "ram_available_mb": 38234.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.797}, "power_stats": {"power_gpu_soc_mean_watts": 22.047, "power_cpu_cv_mean_watts": 1.746, "power_sys_5v0_mean_watts": 9.01, "gpu_utilization_percent_mean": 71.797, "power_watts_avg": 22.047, "energy_joules_est": 181.03, "duration_seconds": 8.211, "sample_count": 69}, "timestamp": "2026-01-25T22:00:48.654899"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6726.182, "latencies_ms": [6726.182], "images_per_second": 0.149, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A person is sitting on a blue bench with green and blue paint splatters. They are wearing blue jeans and pink shoes with a bow on the front.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24606.0, "ram_available_mb": 38234.9, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24606.7, "ram_available_mb": 38234.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.276}, "power_stats": {"power_gpu_soc_mean_watts": 23.049, "power_cpu_cv_mean_watts": 1.56, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 74.276, "power_watts_avg": 23.049, "energy_joules_est": 155.05, "duration_seconds": 6.727, "sample_count": 58}, "timestamp": "2026-01-25T22:00:57.410976"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8527.456, "latencies_ms": [8527.456], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a person's foot wearing a bright pink shoe with a bow on the side, placed on a blue bench with peeling green paint. The lighting appears to be natural, suggesting the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24606.7, "ram_available_mb": 38234.2, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24606.7, "ram_available_mb": 38234.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.311}, "power_stats": {"power_gpu_soc_mean_watts": 21.837, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 71.311, "power_watts_avg": 21.837, "energy_joules_est": 186.23, "duration_seconds": 8.528, "sample_count": 74}, "timestamp": "2026-01-25T22:01:07.956109"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11122.393, "latencies_ms": [11122.393], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman and a young boy are standing in a room with red walls. The woman is holding a knife in her hand, and the boy is standing next to her. They are both smiling and appear to be posing for the picture. \n\nThe room features a dining table with a cup placed on it. There are also two chairs in the scene", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24531.8, "ram_available_mb": 38309.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24598.9, "ram_available_mb": 38242.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.263}, "power_stats": {"power_gpu_soc_mean_watts": 20.939, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.263, "power_watts_avg": 20.939, "energy_joules_est": 232.9, "duration_seconds": 11.123, "sample_count": 95}, "timestamp": "2026-01-25T22:01:21.117819"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7167.86, "latencies_ms": [7167.86], "images_per_second": 0.14, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "woman: 1, knife: 1, boy: 1, table: 1, plate: 1, cup: 1, cake: 1, wall: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24522.3, "ram_available_mb": 38318.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24609.9, "ram_available_mb": 38231.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.754}, "power_stats": {"power_gpu_soc_mean_watts": 22.859, "power_cpu_cv_mean_watts": 1.575, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 74.754, "power_watts_avg": 22.859, "energy_joules_est": 163.86, "duration_seconds": 7.169, "sample_count": 61}, "timestamp": "2026-01-25T22:01:30.302485"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10327.088, "latencies_ms": [10327.088], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground, a woman is standing to the left of a table, holding a knife in her right hand. On the table, there is a red cake with a yellow design on it, a white mug, and a plate. The woman is standing near the table, and the cake is in the center of the table.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24609.9, "ram_available_mb": 38231.0, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24610.6, "ram_available_mb": 38230.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.091}, "power_stats": {"power_gpu_soc_mean_watts": 21.179, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 70.091, "power_watts_avg": 21.179, "energy_joules_est": 218.73, "duration_seconds": 10.328, "sample_count": 88}, "timestamp": "2026-01-25T22:01:42.673320"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7826.7, "latencies_ms": [7826.7], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A woman and a boy are standing in a room with red walls, and the woman is holding a knife. There is a cake on the table in front of them, suggesting that they may be celebrating a special occasion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24538.9, "ram_available_mb": 38302.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24588.5, "ram_available_mb": 38252.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.701}, "power_stats": {"power_gpu_soc_mean_watts": 22.449, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 73.701, "power_watts_avg": 22.449, "energy_joules_est": 175.72, "duration_seconds": 7.827, "sample_count": 67}, "timestamp": "2026-01-25T22:01:52.513402"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10757.756, "latencies_ms": [10757.756], "images_per_second": 0.093, "prompt_tokens": 36, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The image features a woman and a boy standing in a room with red walls. The woman is wearing a green cardigan and patterned pants, while the boy is dressed in a blue plaid shirt. The room has a wooden table with a red cake, a white mug, and a plate on it, suggesting a celebration or gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24518.1, "ram_available_mb": 38322.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24637.7, "ram_available_mb": 38203.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.815}, "power_stats": {"power_gpu_soc_mean_watts": 21.02, "power_cpu_cv_mean_watts": 1.919, "power_sys_5v0_mean_watts": 9.004, "gpu_utilization_percent_mean": 69.815, "power_watts_avg": 21.02, "energy_joules_est": 226.14, "duration_seconds": 10.758, "sample_count": 92}, "timestamp": "2026-01-25T22:02:05.296866"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11124.382, "latencies_ms": [11124.382], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a large indoor arena, where two elephants are the main subjects. The elephant on the left, with its light brown skin, stands calmly, its trunk extended towards the ground. Its companion, a gray elephant, is also on the ground, its trunk curled up in a relaxed manner. The arena", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24565.8, "ram_available_mb": 38275.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24614.9, "ram_available_mb": 38226.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.713}, "power_stats": {"power_gpu_soc_mean_watts": 20.977, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 69.713, "power_watts_avg": 20.977, "energy_joules_est": 233.37, "duration_seconds": 11.125, "sample_count": 94}, "timestamp": "2026-01-25T22:02:18.448868"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10069.787, "latencies_ms": [10069.787], "images_per_second": 0.099, "prompt_tokens": 39, "response_tokens_est": 68, "n_tiles": 16, "output_text": "1. Elephant: 2\n2. Barrier: 1\n3. Camera: 1\n4. Lighting equipment: 1\n5. Folding chair: 1\n6. Trunk: 2\n7. Stroller: 1\n8. Cone: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24614.9, "ram_available_mb": 38226.0, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24603.6, "ram_available_mb": 38237.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.094}, "power_stats": {"power_gpu_soc_mean_watts": 21.343, "power_cpu_cv_mean_watts": 1.842, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 70.094, "power_watts_avg": 21.343, "energy_joules_est": 214.93, "duration_seconds": 10.07, "sample_count": 85}, "timestamp": "2026-01-25T22:02:30.560495"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11128.107, "latencies_ms": [11128.107], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a brown elephant standing on a patterned mat, with a camera on a tripod positioned in front of it, suggesting that the elephant is the main subject of the photo. In the background, there is a grey elephant standing behind the brown one, and a person is visible on the left side, partially obscured by the brown", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24603.6, "ram_available_mb": 38237.3, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24634.3, "ram_available_mb": 38206.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.684}, "power_stats": {"power_gpu_soc_mean_watts": 20.909, "power_cpu_cv_mean_watts": 1.934, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.684, "power_watts_avg": 20.909, "energy_joules_est": 232.69, "duration_seconds": 11.129, "sample_count": 95}, "timestamp": "2026-01-25T22:02:43.710420"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11281.731, "latencies_ms": [11281.731], "images_per_second": 0.089, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In an indoor arena with a high ceiling and metal beams, two elephants are standing side by side, with a person standing to the left of the lighter-colored elephant and another person standing to the right of the darker-colored elephant. The arena is equipped with bleachers and a camera on a tripod is position", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24534.0, "ram_available_mb": 38306.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24625.8, "ram_available_mb": 38215.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.677}, "power_stats": {"power_gpu_soc_mean_watts": 21.042, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.926, "gpu_utilization_percent_mean": 69.677, "power_watts_avg": 21.042, "energy_joules_est": 237.41, "duration_seconds": 11.283, "sample_count": 96}, "timestamp": "2026-01-25T22:02:57.035958"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11200.056, "latencies_ms": [11200.056], "images_per_second": 0.089, "prompt_tokens": 36, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The image features two elephants, one with a light brown skin and the other with a darker grey skin, standing in an indoor arena with a patterned floor. The lighting is artificial, with spotlights directed towards the elephants, and the arena has a concrete ceiling with metal beams and a series of empty bleachers in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24625.8, "ram_available_mb": 38215.1, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24521.6, "ram_available_mb": 38319.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.021}, "power_stats": {"power_gpu_soc_mean_watts": 19.972, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 70.021, "power_watts_avg": 19.972, "energy_joules_est": 223.7, "duration_seconds": 11.201, "sample_count": 96}, "timestamp": "2026-01-25T22:03:10.264949"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11128.862, "latencies_ms": [11128.862], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a thrilling moment of a horse race on a beach. Two jockeys, clad in white helmets and uniforms, are seen riding their horses in a race. The horses are galloping on the sandy beach, with the ocean in the background. The scene is set in black and white, adding a timeless and classic feel to the", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24521.6, "ram_available_mb": 38319.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24598.3, "ram_available_mb": 38242.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.255}, "power_stats": {"power_gpu_soc_mean_watts": 20.998, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.255, "power_watts_avg": 20.998, "energy_joules_est": 233.7, "duration_seconds": 11.13, "sample_count": 94}, "timestamp": "2026-01-25T22:03:23.435770"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7287.108, "latencies_ms": [7287.108], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "horse: 2, jockey: 2, beach: 1, water: 1, sky: 1, clouds: 1, sand: 1, ripples: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24528.1, "ram_available_mb": 38312.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24523.9, "ram_available_mb": 38317.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.159}, "power_stats": {"power_gpu_soc_mean_watts": 22.63, "power_cpu_cv_mean_watts": 1.64, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 73.159, "power_watts_avg": 22.63, "energy_joules_est": 164.92, "duration_seconds": 7.288, "sample_count": 63}, "timestamp": "2026-01-25T22:03:32.763686"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11113.285, "latencies_ms": [11113.285], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two jockeys riding horses, positioned close to each other and moving towards the right side of the image. The background features a vast, open landscape that appears to be a beach, with the ocean extending to the horizon. The horses and jockeys are in the near foreground, while the beach and ocean are in the far background,", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24523.9, "ram_available_mb": 38317.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24621.2, "ram_available_mb": 38219.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.385}, "power_stats": {"power_gpu_soc_mean_watts": 20.936, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.385, "power_watts_avg": 20.936, "energy_joules_est": 232.68, "duration_seconds": 11.114, "sample_count": 96}, "timestamp": "2026-01-25T22:03:45.926739"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9399.974, "latencies_ms": [9399.974], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image captures a dynamic scene of two jockeys riding horses on a beach. The horses are galloping along the wet sand, leaving a trail of splashes behind them, suggesting a recent rain or a high tide that has left the beach temporarily submerged.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24621.2, "ram_available_mb": 38219.7, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24574.1, "ram_available_mb": 38266.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.827}, "power_stats": {"power_gpu_soc_mean_watts": 21.633, "power_cpu_cv_mean_watts": 1.8, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 70.827, "power_watts_avg": 21.633, "energy_joules_est": 203.36, "duration_seconds": 9.401, "sample_count": 81}, "timestamp": "2026-01-25T22:03:57.385047"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7353.787, "latencies_ms": [7353.787], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image is a sepia-toned photograph, giving it a vintage or timeless feel. The lighting is soft and diffused, with no harsh shadows, suggesting an overcast day.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24574.1, "ram_available_mb": 38266.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 8.1, "ram_used_mb": 24519.8, "ram_available_mb": 38321.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.175}, "power_stats": {"power_gpu_soc_mean_watts": 22.125, "power_cpu_cv_mean_watts": 2.181, "power_sys_5v0_mean_watts": 9.075, "gpu_utilization_percent_mean": 72.175, "power_watts_avg": 22.125, "energy_joules_est": 162.72, "duration_seconds": 7.354, "sample_count": 63}, "timestamp": "2026-01-25T22:04:06.754256"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11101.013, "latencies_ms": [11101.013], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is standing in a snowy landscape, talking on his cell phone. He is wearing a black jacket and a black hat, which contrasts with the white snow around him. The man appears to be engaged in a conversation, as he holds the phone to his ear with his left hand. The background is filled with trees, adding a sense of depth and", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 24519.8, "ram_available_mb": 38321.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24578.2, "ram_available_mb": 38262.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.84}, "power_stats": {"power_gpu_soc_mean_watts": 20.955, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 69.84, "power_watts_avg": 20.955, "energy_joules_est": 232.64, "duration_seconds": 11.102, "sample_count": 94}, "timestamp": "2026-01-25T22:04:19.887363"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9636.22, "latencies_ms": [9636.22], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "1. Boy: 1\n2. Jacket: 1\n3. Glove: 1\n4. Hat: 1\n5. Goggles: 1\n6. Cell phone: 1\n7. Trees: 1\n8. Snow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24578.2, "ram_available_mb": 38262.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24582.5, "ram_available_mb": 38258.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.768}, "power_stats": {"power_gpu_soc_mean_watts": 21.582, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 70.768, "power_watts_avg": 21.582, "energy_joules_est": 207.98, "duration_seconds": 9.637, "sample_count": 82}, "timestamp": "2026-01-25T22:04:31.570462"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 6068.06, "latencies_ms": [6068.06], "images_per_second": 0.165, "prompt_tokens": 44, "response_tokens_est": 33, "n_tiles": 16, "output_text": "The person is standing in the foreground with a snowy landscape in the background. The trees are behind the person, and the ground is covered in snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24582.5, "ram_available_mb": 38258.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24573.6, "ram_available_mb": 38267.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.451}, "power_stats": {"power_gpu_soc_mean_watts": 23.411, "power_cpu_cv_mean_watts": 1.468, "power_sys_5v0_mean_watts": 9.1, "gpu_utilization_percent_mean": 73.451, "power_watts_avg": 23.411, "energy_joules_est": 142.07, "duration_seconds": 6.069, "sample_count": 51}, "timestamp": "2026-01-25T22:04:39.687585"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7512.427, "latencies_ms": [7512.427], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A person is standing in a snowy area, wearing a black jacket and a goggles on their head. They are holding a cell phone to their ear and appear to be talking on the phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24516.9, "ram_available_mb": 38323.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24571.4, "ram_available_mb": 38269.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.453}, "power_stats": {"power_gpu_soc_mean_watts": 22.57, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 73.453, "power_watts_avg": 22.57, "energy_joules_est": 169.57, "duration_seconds": 7.513, "sample_count": 64}, "timestamp": "2026-01-25T22:04:49.214988"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6574.131, "latencies_ms": [6574.131], "images_per_second": 0.152, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The person in the image is wearing a black jacket and a blue and black goggle on their head. The background shows a snowy landscape with trees and a blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24571.4, "ram_available_mb": 38269.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24610.8, "ram_available_mb": 38230.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.982}, "power_stats": {"power_gpu_soc_mean_watts": 23.089, "power_cpu_cv_mean_watts": 1.572, "power_sys_5v0_mean_watts": 9.071, "gpu_utilization_percent_mean": 72.982, "power_watts_avg": 23.089, "energy_joules_est": 151.8, "duration_seconds": 6.575, "sample_count": 55}, "timestamp": "2026-01-25T22:04:57.821566"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11128.973, "latencies_ms": [11128.973], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene camping scene at sunset. A green tent is pitched on a grassy field, with a motorcycle parked in front of it. The motorcycle, with its black seat and handlebars, is equipped with a large black bag and a smaller black bag attached to the back. The motorcycle is facing towards the right side of the image,", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 24520.8, "ram_available_mb": 38320.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24599.9, "ram_available_mb": 38241.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.284}, "power_stats": {"power_gpu_soc_mean_watts": 20.859, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 69.284, "power_watts_avg": 20.859, "energy_joules_est": 232.15, "duration_seconds": 11.13, "sample_count": 95}, "timestamp": "2026-01-25T22:05:11.009400"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7270.617, "latencies_ms": [7270.617], "images_per_second": 0.138, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "tent: 1, motorcycle: 1, trees: numerous, sun: 1, sky: 1, grass: 1, forest: 1, camping gear: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24599.9, "ram_available_mb": 38241.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24623.1, "ram_available_mb": 38217.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.082}, "power_stats": {"power_gpu_soc_mean_watts": 22.677, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 74.082, "power_watts_avg": 22.677, "energy_joules_est": 164.89, "duration_seconds": 7.271, "sample_count": 61}, "timestamp": "2026-01-25T22:05:20.310482"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8564.793, "latencies_ms": [8564.793], "images_per_second": 0.117, "prompt_tokens": 44, "response_tokens_est": 56, "n_tiles": 16, "output_text": "A motorcycle is parked in the foreground on the left side of the image, while a green tent is situated in the background on the right side. The motorcycle is positioned closer to the camera than the tent, creating a sense of depth in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24528.1, "ram_available_mb": 38312.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24602.9, "ram_available_mb": 38238.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.5}, "power_stats": {"power_gpu_soc_mean_watts": 21.732, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 70.5, "power_watts_avg": 21.732, "energy_joules_est": 186.14, "duration_seconds": 8.565, "sample_count": 74}, "timestamp": "2026-01-25T22:05:30.937585"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6813.766, "latencies_ms": [6813.766], "images_per_second": 0.147, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A motorcycle is parked in a field with a green tent nearby, suggesting a camping trip. The sun is setting in the background, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24527.0, "ram_available_mb": 38313.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24605.4, "ram_available_mb": 38235.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.931}, "power_stats": {"power_gpu_soc_mean_watts": 23.02, "power_cpu_cv_mean_watts": 1.567, "power_sys_5v0_mean_watts": 9.025, "gpu_utilization_percent_mean": 73.931, "power_watts_avg": 23.02, "energy_joules_est": 156.88, "duration_seconds": 6.815, "sample_count": 58}, "timestamp": "2026-01-25T22:05:39.774453"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8330.226, "latencies_ms": [8330.226], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image features a serene outdoor setting with a green tent and a motorcycle parked on dry grass. The lighting suggests it's either dawn or dusk, with the sun low in the sky casting a warm glow and long shadows.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24605.4, "ram_available_mb": 38235.5, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24606.4, "ram_available_mb": 38234.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.917}, "power_stats": {"power_gpu_soc_mean_watts": 21.793, "power_cpu_cv_mean_watts": 1.746, "power_sys_5v0_mean_watts": 8.981, "gpu_utilization_percent_mean": 70.917, "power_watts_avg": 21.793, "energy_joules_est": 181.55, "duration_seconds": 8.331, "sample_count": 72}, "timestamp": "2026-01-25T22:05:50.123315"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11101.294, "latencies_ms": [11101.294], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment frozen in time, featuring a vintage steam locomotive, numbered 67371, pulling a passenger train along a railway track. The locomotive, painted in a striking black, is adorned with a white number and a red emblem on its side, adding a touch of color to its otherwise monochrome appearance. The train", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24606.4, "ram_available_mb": 38234.5, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24619.3, "ram_available_mb": 38221.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.979}, "power_stats": {"power_gpu_soc_mean_watts": 20.996, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 69.979, "power_watts_avg": 20.996, "energy_joules_est": 233.1, "duration_seconds": 11.102, "sample_count": 94}, "timestamp": "2026-01-25T22:06:03.255454"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9280.906, "latencies_ms": [9280.906], "images_per_second": 0.108, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Train: 1\n\n- Train car: 2\n\n- Train engine: 1\n\n- Train tracks: 1\n\n- Station platform: 1\n\n- Station sign: 1\n\n- People: 4\n\n- Smoke: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24557.5, "ram_available_mb": 38283.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24584.2, "ram_available_mb": 38256.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.848}, "power_stats": {"power_gpu_soc_mean_watts": 21.711, "power_cpu_cv_mean_watts": 1.774, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 71.848, "power_watts_avg": 21.711, "energy_joules_est": 201.51, "duration_seconds": 9.282, "sample_count": 79}, "timestamp": "2026-01-25T22:06:14.547952"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11115.172, "latencies_ms": [11115.172], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a vintage steam locomotive with the number 67371 prominently displayed on its front. It is situated on the railway tracks, positioned between the platform and the station buildings in the background. A group of people is standing on the platform to the right of the locomotive, suggesting they are waiting to board or have just disemb", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24584.2, "ram_available_mb": 38256.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24584.2, "ram_available_mb": 38256.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.674}, "power_stats": {"power_gpu_soc_mean_watts": 20.87, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 69.674, "power_watts_avg": 20.87, "energy_joules_est": 231.99, "duration_seconds": 11.116, "sample_count": 95}, "timestamp": "2026-01-25T22:06:27.720328"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9400.04, "latencies_ms": [9400.04], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image depicts a vintage black and white scene of a steam locomotive with the number 67371 on its front, pulling a series of passenger cars at a station. There are several people standing on the platform, likely waiting to board or disembark the train.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24584.2, "ram_available_mb": 38256.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24614.9, "ram_available_mb": 38226.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.633}, "power_stats": {"power_gpu_soc_mean_watts": 21.576, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 70.633, "power_watts_avg": 21.576, "energy_joules_est": 202.83, "duration_seconds": 9.401, "sample_count": 79}, "timestamp": "2026-01-25T22:06:39.134575"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9346.422, "latencies_ms": [9346.422], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image is a black and white photograph, indicating it was taken during a time when color photography was not available or commonly used. The lighting is soft and diffused, with no harsh shadows, suggesting an overcast day or a time of day when the sun was not at its peak.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24528.9, "ram_available_mb": 38312.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24600.2, "ram_available_mb": 38240.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.296}, "power_stats": {"power_gpu_soc_mean_watts": 21.363, "power_cpu_cv_mean_watts": 1.839, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 70.296, "power_watts_avg": 21.363, "energy_joules_est": 199.68, "duration_seconds": 9.347, "sample_count": 81}, "timestamp": "2026-01-25T22:06:50.539046"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11146.584, "latencies_ms": [11146.584], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a bustling street scene in Japan, bathed in the stark contrast of black and white. The perspective is from a low angle, looking upwards towards the sky, giving a sense of the towering structures that dominate the scene. These buildings, constructed from concrete and steel, reach towards the sky, their facades adorned with numerous signs and banners", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24526.6, "ram_available_mb": 38314.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24546.9, "ram_available_mb": 38294.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.25}, "power_stats": {"power_gpu_soc_mean_watts": 20.895, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.25, "power_watts_avg": 20.895, "energy_joules_est": 232.92, "duration_seconds": 11.147, "sample_count": 96}, "timestamp": "2026-01-25T22:07:03.726133"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7603.099, "latencies_ms": [7603.099], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "sign: 20, building: 2, window: 15, sky: 1, wire: 5, banner: 10, text: 30, character: 25", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24546.9, "ram_available_mb": 38294.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24614.7, "ram_available_mb": 38226.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.246}, "power_stats": {"power_gpu_soc_mean_watts": 22.561, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 73.246, "power_watts_avg": 22.561, "energy_joules_est": 171.55, "duration_seconds": 7.604, "sample_count": 65}, "timestamp": "2026-01-25T22:07:13.384479"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10102.968, "latencies_ms": [10102.968], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image shows a dense cluster of signs hanging from above, with the signs appearing in the foreground and filling the space. The signs are closely packed together, creating a sense of depth as they recede into the background. The signs are positioned at various heights and angles, creating a complex and layered spatial relationship.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24527.7, "ram_available_mb": 38313.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24617.3, "ram_available_mb": 38223.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.188}, "power_stats": {"power_gpu_soc_mean_watts": 21.365, "power_cpu_cv_mean_watts": 1.879, "power_sys_5v0_mean_watts": 9.011, "gpu_utilization_percent_mean": 70.188, "power_watts_avg": 21.365, "energy_joules_est": 215.86, "duration_seconds": 10.104, "sample_count": 85}, "timestamp": "2026-01-25T22:07:25.505194"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7847.535, "latencies_ms": [7847.535], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image depicts a bustling urban scene with numerous signs hanging from above, likely indicating shops or businesses. The signs are densely packed and create a sense of a busy marketplace or commercial area.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24545.4, "ram_available_mb": 38295.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24637.7, "ram_available_mb": 38203.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.561}, "power_stats": {"power_gpu_soc_mean_watts": 22.389, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 73.561, "power_watts_avg": 22.389, "energy_joules_est": 175.71, "duration_seconds": 7.848, "sample_count": 66}, "timestamp": "2026-01-25T22:07:35.389012"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8432.998, "latencies_ms": [8432.998], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image is a black and white photograph, showcasing a dense array of hanging signs with Chinese characters. The signs are suspended from above by a complex network of wires and cables, creating a web-like structure that spans across the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24534.0, "ram_available_mb": 38306.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24619.8, "ram_available_mb": 38221.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.718}, "power_stats": {"power_gpu_soc_mean_watts": 21.85, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 9.003, "gpu_utilization_percent_mean": 71.718, "power_watts_avg": 21.85, "energy_joules_est": 184.27, "duration_seconds": 8.434, "sample_count": 71}, "timestamp": "2026-01-25T22:07:45.859968"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11114.182, "latencies_ms": [11114.182], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is sitting on a concrete ledge by the water, enjoying the view. He is wearing blue shorts and appears to be relaxed. The scene also features a blue bench, which is positioned in front of the man. The bench has a sign on it that reads \"Closed,\" indicating that it is not in use. The man is", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 24532.8, "ram_available_mb": 38308.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24618.9, "ram_available_mb": 38222.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.617}, "power_stats": {"power_gpu_soc_mean_watts": 20.904, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 69.617, "power_watts_avg": 20.904, "energy_joules_est": 232.34, "duration_seconds": 11.115, "sample_count": 94}, "timestamp": "2026-01-25T22:07:59.033648"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8425.962, "latencies_ms": [8425.962], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "- Man: 1\n- Bench: 1\n- Sign: 2\n- Sticker: 1\n- Grass: 1\n- Dirt: 1\n- Plants: 1\n- Path: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24618.9, "ram_available_mb": 38222.0, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 24532.0, "ram_available_mb": 38308.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.486}, "power_stats": {"power_gpu_soc_mean_watts": 22.052, "power_cpu_cv_mean_watts": 2.085, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 72.486, "power_watts_avg": 22.052, "energy_joules_est": 185.83, "duration_seconds": 8.427, "sample_count": 72}, "timestamp": "2026-01-25T22:08:09.478202"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10776.429, "latencies_ms": [10776.429], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "A man is lying on a concrete barrier near the water's edge, which is in the background. The barrier is positioned in the foreground, and there is a 'CLOSED' sign in the foreground to the left of the barrier. The grassy area is in the foreground, and the water is in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24532.0, "ram_available_mb": 38308.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24531.0, "ram_available_mb": 38309.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.516}, "power_stats": {"power_gpu_soc_mean_watts": 20.385, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 70.516, "power_watts_avg": 20.385, "energy_joules_est": 219.69, "duration_seconds": 10.777, "sample_count": 91}, "timestamp": "2026-01-25T22:08:22.279277"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7864.27, "latencies_ms": [7864.27], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A person is lying on a concrete barrier by the water, with a closed sign for the Lakefront Trail in the foreground. The grassy area appears to be a park or recreational area near a body of water.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24531.0, "ram_available_mb": 38309.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24620.6, "ram_available_mb": 38220.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.896}, "power_stats": {"power_gpu_soc_mean_watts": 22.324, "power_cpu_cv_mean_watts": 1.667, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 72.896, "power_watts_avg": 22.324, "energy_joules_est": 175.58, "duration_seconds": 7.865, "sample_count": 67}, "timestamp": "2026-01-25T22:08:32.202940"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7198.181, "latencies_ms": [7198.181], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A man is lying on a concrete barrier by the water, wearing blue shorts and black shoes. The barrier is blue with white text that reads \"Lakefront Trail Closed\".", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24620.6, "ram_available_mb": 38220.3, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24620.3, "ram_available_mb": 38220.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.55}, "power_stats": {"power_gpu_soc_mean_watts": 22.484, "power_cpu_cv_mean_watts": 1.628, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 72.55, "power_watts_avg": 22.484, "energy_joules_est": 161.86, "duration_seconds": 7.199, "sample_count": 60}, "timestamp": "2026-01-25T22:08:41.442510"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11144.582, "latencies_ms": [11144.582], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the serene expanse of a lush green field, a brown horse and a white dog share a moment of tranquility. The horse, adorned with a black halter, stands majestically on the right side of the image. Its coat, a rich shade of brown, contrasts beautifully with the verdant surroundings. \n\nOn the left", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24531.7, "ram_available_mb": 38309.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24623.5, "ram_available_mb": 38217.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.537}, "power_stats": {"power_gpu_soc_mean_watts": 20.857, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 69.537, "power_watts_avg": 20.857, "energy_joules_est": 232.46, "duration_seconds": 11.145, "sample_count": 95}, "timestamp": "2026-01-25T22:08:54.660648"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7191.806, "latencies_ms": [7191.806], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "horse: 1, dog: 1, grass: many, sky: 1, trees: many, field: 1, sun: 1, horse's mane: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24536.6, "ram_available_mb": 38304.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24547.0, "ram_available_mb": 38293.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.377}, "power_stats": {"power_gpu_soc_mean_watts": 22.854, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 74.377, "power_watts_avg": 22.854, "energy_joules_est": 164.38, "duration_seconds": 7.192, "sample_count": 61}, "timestamp": "2026-01-25T22:09:03.907452"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9748.014, "latencies_ms": [9748.014], "images_per_second": 0.103, "prompt_tokens": 44, "response_tokens_est": 66, "n_tiles": 16, "output_text": "In the foreground, there is a golden retriever dog standing in the grass, positioned to the left side of the image. Behind the dog, on the right side, is a brown horse with a black halter. The background features a clear blue sky and a distant landscape with trees and greenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24547.0, "ram_available_mb": 38293.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24614.7, "ram_available_mb": 38226.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.244}, "power_stats": {"power_gpu_soc_mean_watts": 21.385, "power_cpu_cv_mean_watts": 1.826, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 71.244, "power_watts_avg": 21.385, "energy_joules_est": 208.48, "duration_seconds": 9.749, "sample_count": 82}, "timestamp": "2026-01-25T22:09:15.673616"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8563.036, "latencies_ms": [8563.036], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "In a serene field, a brown horse and a golden retriever are standing close to each other, both looking towards the camera. The horse is wearing a bridle, and the dog appears to be enjoying the sunny day outdoors.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24614.7, "ram_available_mb": 38226.2, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24640.9, "ram_available_mb": 38200.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.792}, "power_stats": {"power_gpu_soc_mean_watts": 22.016, "power_cpu_cv_mean_watts": 1.718, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 72.792, "power_watts_avg": 22.016, "energy_joules_est": 188.54, "duration_seconds": 8.564, "sample_count": 72}, "timestamp": "2026-01-25T22:09:26.259741"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7480.993, "latencies_ms": [7480.993], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image features a golden retriever and a chestnut-colored horse standing in a field of tall green grass. The lighting is bright and natural, suggesting the photo was taken on a sunny day.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24528.8, "ram_available_mb": 38312.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24616.3, "ram_available_mb": 38224.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.703}, "power_stats": {"power_gpu_soc_mean_watts": 22.407, "power_cpu_cv_mean_watts": 1.67, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 72.703, "power_watts_avg": 22.407, "energy_joules_est": 167.64, "duration_seconds": 7.482, "sample_count": 64}, "timestamp": "2026-01-25T22:09:35.805715"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11126.805, "latencies_ms": [11126.805], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people playing volleyball in a gymnasium. There are at least 13 people visible in the scene, with some players actively participating in the game and others standing or waiting for the ball. The volleyball is in the air, and the players are focused on the game.\n\nThe gymnasium has a blue floor, and the walls are", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 24521.9, "ram_available_mb": 38318.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24609.5, "ram_available_mb": 38231.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.255}, "power_stats": {"power_gpu_soc_mean_watts": 20.947, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 69.255, "power_watts_avg": 20.947, "energy_joules_est": 233.09, "duration_seconds": 11.127, "sample_count": 94}, "timestamp": "2026-01-25T22:09:48.978781"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9638.929, "latencies_ms": [9638.929], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "- Volleyball: 1\n\n- Players: 15\n\n- Net: 1\n\n- Gym floor: 1\n\n- Wall: 1\n\n- Ceiling: 1\n\n- Basketball hoop: 1\n\n- Basketball: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24537.6, "ram_available_mb": 38303.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24542.0, "ram_available_mb": 38298.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.463}, "power_stats": {"power_gpu_soc_mean_watts": 21.546, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.923, "gpu_utilization_percent_mean": 71.463, "power_watts_avg": 21.546, "energy_joules_est": 207.69, "duration_seconds": 9.64, "sample_count": 82}, "timestamp": "2026-01-25T22:10:00.658262"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11117.719, "latencies_ms": [11117.719], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a player in a blue outfit is preparing to serve the volleyball, which is positioned near the top right corner of the image. The players are spread out across the court, with some in the background near the net and others further away, indicating they are engaged in a game or practice session. The court lines are clearly visible, creating a sense of depth", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24542.0, "ram_available_mb": 38298.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24613.4, "ram_available_mb": 38227.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.621}, "power_stats": {"power_gpu_soc_mean_watts": 20.917, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 69.621, "power_watts_avg": 20.917, "energy_joules_est": 232.56, "duration_seconds": 11.118, "sample_count": 95}, "timestamp": "2026-01-25T22:10:13.794704"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6837.603, "latencies_ms": [6837.603], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image depicts a group of people in a gymnasium playing volleyball. The players are wearing blue and green uniforms, and the court is marked with white lines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24613.4, "ram_available_mb": 38227.5, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24644.1, "ram_available_mb": 38196.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.912}, "power_stats": {"power_gpu_soc_mean_watts": 22.959, "power_cpu_cv_mean_watts": 1.545, "power_sys_5v0_mean_watts": 9.015, "gpu_utilization_percent_mean": 72.912, "power_watts_avg": 22.959, "energy_joules_est": 157.0, "duration_seconds": 6.838, "sample_count": 57}, "timestamp": "2026-01-25T22:10:22.654257"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7400.49, "latencies_ms": [7400.49], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows an indoor volleyball court with a blue floor and white boundary lines. The lighting is artificial, coming from ceiling fixtures, and the walls are adorned with brown tiles.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24644.1, "ram_available_mb": 38196.8, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24645.3, "ram_available_mb": 38195.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.71}, "power_stats": {"power_gpu_soc_mean_watts": 22.617, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 9.074, "gpu_utilization_percent_mean": 72.71, "power_watts_avg": 22.617, "energy_joules_est": 167.4, "duration_seconds": 7.401, "sample_count": 62}, "timestamp": "2026-01-25T22:10:32.071582"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11151.309, "latencies_ms": [11151.309], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large herd of zebras and wildebeests grazing in a grassy field. There are at least nine zebras and four wildebeests visible in the scene. The zebras are spread out across the field, with some standing closer to the wildebeests. The wildebeests are grazing on the grass, while the z", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24565.2, "ram_available_mb": 38275.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24566.0, "ram_available_mb": 38274.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.126}, "power_stats": {"power_gpu_soc_mean_watts": 20.906, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 69.126, "power_watts_avg": 20.906, "energy_joules_est": 233.14, "duration_seconds": 11.152, "sample_count": 95}, "timestamp": "2026-01-25T22:10:45.264252"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8864.556, "latencies_ms": [8864.556], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 57, "n_tiles": 16, "output_text": "zebras: 5, wildebeests: 3, flamingos: 100, birds: 0, elephants: 0, giraffes: 0, lions: 0, crocodiles: 0", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24566.0, "ram_available_mb": 38274.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24625.4, "ram_available_mb": 38215.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.693}, "power_stats": {"power_gpu_soc_mean_watts": 21.852, "power_cpu_cv_mean_watts": 1.762, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 71.693, "power_watts_avg": 21.852, "energy_joules_est": 193.72, "duration_seconds": 8.865, "sample_count": 75}, "timestamp": "2026-01-25T22:10:56.188110"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11135.821, "latencies_ms": [11135.821], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are several zebras and wildebeests grazing on the grass, with one zebra standing slightly apart from the rest. In the background, there is a large herd of flamingos gathered in the water, creating a striking contrast between the two groups. The zebras and wildebeests are closer to the viewer, while the", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24535.3, "ram_available_mb": 38305.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24605.0, "ram_available_mb": 38235.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.74}, "power_stats": {"power_gpu_soc_mean_watts": 20.896, "power_cpu_cv_mean_watts": 1.915, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 69.74, "power_watts_avg": 20.896, "energy_joules_est": 232.71, "duration_seconds": 11.136, "sample_count": 96}, "timestamp": "2026-01-25T22:11:09.355572"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9182.216, "latencies_ms": [9182.216], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image depicts a serene scene of a grassy field where a group of zebras and wildebeests are grazing. In the background, there is a large herd of flamingos gathered in the water, creating a picturesque and diverse wildlife view.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24535.8, "ram_available_mb": 38305.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24542.6, "ram_available_mb": 38298.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.13}, "power_stats": {"power_gpu_soc_mean_watts": 21.638, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 8.967, "gpu_utilization_percent_mean": 71.13, "power_watts_avg": 21.638, "energy_joules_est": 198.7, "duration_seconds": 9.183, "sample_count": 77}, "timestamp": "2026-01-25T22:11:20.570063"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7434.122, "latencies_ms": [7434.122], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image depicts a serene landscape with a clear sky and a body of water in the background. The foreground shows a group of zebras and wildebeests grazing on a grassy field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24542.6, "ram_available_mb": 38298.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24621.5, "ram_available_mb": 38219.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.698}, "power_stats": {"power_gpu_soc_mean_watts": 22.364, "power_cpu_cv_mean_watts": 1.659, "power_sys_5v0_mean_watts": 9.033, "gpu_utilization_percent_mean": 72.698, "power_watts_avg": 22.364, "energy_joules_est": 166.27, "duration_seconds": 7.435, "sample_count": 63}, "timestamp": "2026-01-25T22:11:30.042183"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11138.817, "latencies_ms": [11138.817], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two cats with orange and white fur sitting on a wooden deck. The cat on the left is looking at its reflection in a mirror, while the cat on the right is looking at the other cat. The deck is made of wooden planks, and the cats are positioned in such a way that they are facing each other. The cat on the left is", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 24531.4, "ram_available_mb": 38309.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24520.3, "ram_available_mb": 38320.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.196}, "power_stats": {"power_gpu_soc_mean_watts": 20.818, "power_cpu_cv_mean_watts": 1.92, "power_sys_5v0_mean_watts": 8.931, "gpu_utilization_percent_mean": 69.196, "power_watts_avg": 20.818, "energy_joules_est": 231.9, "duration_seconds": 11.14, "sample_count": 97}, "timestamp": "2026-01-25T22:11:43.204994"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7596.825, "latencies_ms": [7596.825], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "cat: 2\nreflection: 1\nwindow: 1\ndeck: 1\nrain: 1\nrain droplets: 1\nmirror: 1\nrainbow: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24520.3, "ram_available_mb": 38320.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24604.0, "ram_available_mb": 38236.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.484}, "power_stats": {"power_gpu_soc_mean_watts": 22.728, "power_cpu_cv_mean_watts": 1.658, "power_sys_5v0_mean_watts": 9.005, "gpu_utilization_percent_mean": 73.484, "power_watts_avg": 22.728, "energy_joules_est": 172.68, "duration_seconds": 7.598, "sample_count": 64}, "timestamp": "2026-01-25T22:11:52.853147"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11108.564, "latencies_ms": [11108.564], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The two cats are positioned in the foreground, with one cat appearing larger and more dominant, while the other is smaller and seems to be looking up at the larger cat. They are standing on a wooden surface that extends into the background, which is blurred and indistinct. The larger cat is closer to the camera, making it appear more prominent in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24544.0, "ram_available_mb": 38296.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24545.8, "ram_available_mb": 38295.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.617}, "power_stats": {"power_gpu_soc_mean_watts": 20.984, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.617, "power_watts_avg": 20.984, "energy_joules_est": 233.12, "duration_seconds": 11.109, "sample_count": 94}, "timestamp": "2026-01-25T22:12:05.983074"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7190.026, "latencies_ms": [7190.026], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "Two orange and white cats are sitting on a wooden deck, looking at their reflections in a mirror. The deck appears to be outdoors and the cats seem curious about their own image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24545.8, "ram_available_mb": 38295.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 8.6, "ram_used_mb": 24630.1, "ram_available_mb": 38210.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.361}, "power_stats": {"power_gpu_soc_mean_watts": 22.75, "power_cpu_cv_mean_watts": 2.239, "power_sys_5v0_mean_watts": 9.068, "gpu_utilization_percent_mean": 74.361, "power_watts_avg": 22.75, "energy_joules_est": 163.59, "duration_seconds": 7.191, "sample_count": 61}, "timestamp": "2026-01-25T22:12:15.213792"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10847.918, "latencies_ms": [10847.918], "images_per_second": 0.092, "prompt_tokens": 36, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The image features two cats with a vibrant orange and white coat, sitting on a wooden surface that appears to be a deck or a similar structure. The lighting is soft and diffused, suggesting an overcast day or a shaded area, and the wooden planks have a weathered look with some discoloration, indicating exposure to the elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24560.0, "ram_available_mb": 38280.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24611.1, "ram_available_mb": 38229.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.181}, "power_stats": {"power_gpu_soc_mean_watts": 20.053, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 70.181, "power_watts_avg": 20.053, "energy_joules_est": 217.55, "duration_seconds": 10.849, "sample_count": 94}, "timestamp": "2026-01-25T22:12:28.102481"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11129.532, "latencies_ms": [11129.532], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a serene lake with several boats docked at a pier. There are at least five boats visible, with some closer to the shore and others further out in the water. The boats vary in size and design, adding to the picturesque scene.\n\nIn addition to the boats, there are a few people scattered around the area, possibly enjoying the view or attending to", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24527.0, "ram_available_mb": 38313.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24544.6, "ram_available_mb": 38296.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.453}, "power_stats": {"power_gpu_soc_mean_watts": 20.91, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.453, "power_watts_avg": 20.91, "energy_joules_est": 232.74, "duration_seconds": 11.13, "sample_count": 95}, "timestamp": "2026-01-25T22:12:41.256979"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6832.906, "latencies_ms": [6832.906], "images_per_second": 0.146, "prompt_tokens": 39, "response_tokens_est": 39, "n_tiles": 16, "output_text": "boat: 5, building: 1, tree: many, mountain: 1, water: 1, lamp post: 1, grass: many, road: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24544.6, "ram_available_mb": 38296.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24569.2, "ram_available_mb": 38271.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.448}, "power_stats": {"power_gpu_soc_mean_watts": 22.903, "power_cpu_cv_mean_watts": 1.56, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 73.448, "power_watts_avg": 22.903, "energy_joules_est": 156.51, "duration_seconds": 6.834, "sample_count": 58}, "timestamp": "2026-01-25T22:12:50.138706"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9553.144, "latencies_ms": [9553.144], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "In the foreground, there is a street lamp on the left side of the image, with a tree in front of it. The boats are docked in the water, with some closer to the shore and others further away. The buildings and trees are in the background, with the mountains visible in the far distance.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24569.2, "ram_available_mb": 38271.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24596.2, "ram_available_mb": 38244.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.37}, "power_stats": {"power_gpu_soc_mean_watts": 21.427, "power_cpu_cv_mean_watts": 1.834, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 70.37, "power_watts_avg": 21.427, "energy_joules_est": 204.72, "duration_seconds": 9.554, "sample_count": 81}, "timestamp": "2026-01-25T22:13:01.734241"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7430.809, "latencies_ms": [7430.809], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image depicts a serene lakeside scene with several boats docked at a pier. In the background, there are buildings and lush greenery, creating a picturesque and peaceful atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24596.2, "ram_available_mb": 38244.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24596.9, "ram_available_mb": 38244.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.438}, "power_stats": {"power_gpu_soc_mean_watts": 22.285, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 74.438, "power_watts_avg": 22.285, "energy_joules_est": 165.61, "duration_seconds": 7.431, "sample_count": 64}, "timestamp": "2026-01-25T22:13:11.202828"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7192.495, "latencies_ms": [7192.495], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image shows a body of water with several boats docked along the shore. The sky is overcast, and the overall lighting is dim, suggesting it might be an early morning or a cloudy day.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24596.9, "ram_available_mb": 38244.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24541.9, "ram_available_mb": 38299.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.508}, "power_stats": {"power_gpu_soc_mean_watts": 22.535, "power_cpu_cv_mean_watts": 1.648, "power_sys_5v0_mean_watts": 9.044, "gpu_utilization_percent_mean": 72.508, "power_watts_avg": 22.535, "energy_joules_est": 162.1, "duration_seconds": 7.193, "sample_count": 61}, "timestamp": "2026-01-25T22:13:20.412212"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11128.422, "latencies_ms": [11128.422], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photo, a man is standing on a street, holding a bicycle. He is wearing a towel around his neck, suggesting he might have been cycling in the heat. The street is lined with shops, and a woman is visible in the background, holding an umbrella, possibly to shield herself from the sun or rain. The man is", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24541.9, "ram_available_mb": 38299.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24608.8, "ram_available_mb": 38232.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.372}, "power_stats": {"power_gpu_soc_mean_watts": 20.901, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 69.372, "power_watts_avg": 20.901, "energy_joules_est": 232.61, "duration_seconds": 11.129, "sample_count": 94}, "timestamp": "2026-01-25T22:13:33.565076"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7495.088, "latencies_ms": [7495.088], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "bicycle: 1, building: 2, umbrella: 1, person: 2, sign: 3, window: 1, door: 1, street light: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24530.3, "ram_available_mb": 38310.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24614.1, "ram_available_mb": 38226.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.5}, "power_stats": {"power_gpu_soc_mean_watts": 22.478, "power_cpu_cv_mean_watts": 1.652, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 71.5, "power_watts_avg": 22.478, "energy_joules_est": 168.49, "duration_seconds": 7.496, "sample_count": 64}, "timestamp": "2026-01-25T22:13:43.117560"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10333.81, "latencies_ms": [10333.81], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground, a man is standing with a bicycle, positioned to the right of the frame. The bicycle is in the middle ground, with a shop to its left and another person holding an umbrella to its right. In the background, there are more shops and a person walking, indicating a street scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24539.0, "ram_available_mb": 38301.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24606.2, "ram_available_mb": 38234.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.25}, "power_stats": {"power_gpu_soc_mean_watts": 21.193, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 70.25, "power_watts_avg": 21.193, "energy_joules_est": 219.02, "duration_seconds": 10.334, "sample_count": 88}, "timestamp": "2026-01-25T22:13:55.502223"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7966.27, "latencies_ms": [7966.27], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A man is standing on a street with a bicycle, holding an umbrella, and appears to be waiting or looking for something. The setting is a street with shops and signs in the background, suggesting a commercial area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24542.8, "ram_available_mb": 38298.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24559.6, "ram_available_mb": 38281.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.97}, "power_stats": {"power_gpu_soc_mean_watts": 22.187, "power_cpu_cv_mean_watts": 1.655, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 71.97, "power_watts_avg": 22.187, "energy_joules_est": 176.76, "duration_seconds": 7.967, "sample_count": 67}, "timestamp": "2026-01-25T22:14:05.490704"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7241.753, "latencies_ms": [7241.753], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image is in black and white, featuring a man with an umbrella and a bicycle. The weather appears to be overcast, as the sky is grey and the overall lighting is dim.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24559.6, "ram_available_mb": 38281.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24602.8, "ram_available_mb": 38238.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.548}, "power_stats": {"power_gpu_soc_mean_watts": 21.962, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 72.548, "power_watts_avg": 21.962, "energy_joules_est": 159.06, "duration_seconds": 7.242, "sample_count": 62}, "timestamp": "2026-01-25T22:14:14.780155"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12296.355, "latencies_ms": [12296.355], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a market stall with a large number of bananas hanging from the ceiling. There are several bunches of bananas suspended in the air, with some bunches closer to the front and others further back. The bananas are displayed in various positions, creating an appealing and abundant presentation. The stall appears to be a fruit stand, and the bananas", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24532.6, "ram_available_mb": 38308.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24605.6, "ram_available_mb": 38235.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.077}, "power_stats": {"power_gpu_soc_mean_watts": 22.873, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 9.21, "gpu_utilization_percent_mean": 74.077, "power_watts_avg": 22.873, "energy_joules_est": 281.27, "duration_seconds": 12.297, "sample_count": 104}, "timestamp": "2026-01-25T22:14:29.103616"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10963.169, "latencies_ms": [10963.169], "images_per_second": 0.091, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "banana: 100\nbanana: 100\nbanana: 100\nbanana: 100\nbanana: 100\nbanana: 100\nbanana: 100\nbanana: 100", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24605.6, "ram_available_mb": 38235.3, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24641.2, "ram_available_mb": 38199.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.684}, "power_stats": {"power_gpu_soc_mean_watts": 23.393, "power_cpu_cv_mean_watts": 1.648, "power_sys_5v0_mean_watts": 9.157, "gpu_utilization_percent_mean": 75.684, "power_watts_avg": 23.393, "energy_joules_est": 256.48, "duration_seconds": 10.964, "sample_count": 95}, "timestamp": "2026-01-25T22:14:42.124613"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11956.664, "latencies_ms": [11956.664], "images_per_second": 0.084, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The bananas are hanging in bunches from the ceiling, with some bunches closer to the foreground and others further in the background. The bunches are spread out across the ceiling, creating a sense of depth in the image. The bananas in the foreground appear larger and more detailed, while those in the background appear smaller and less distinct.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24641.2, "ram_available_mb": 38199.7, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24532.1, "ram_available_mb": 38308.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.176}, "power_stats": {"power_gpu_soc_mean_watts": 22.985, "power_cpu_cv_mean_watts": 1.751, "power_sys_5v0_mean_watts": 9.196, "gpu_utilization_percent_mean": 74.176, "power_watts_avg": 22.985, "energy_joules_est": 274.84, "duration_seconds": 11.957, "sample_count": 102}, "timestamp": "2026-01-25T22:14:56.120889"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8019.464, "latencies_ms": [8019.464], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image depicts a market scene with a bunch of bananas hanging from the ceiling. The bananas are displayed in a way that they are easily visible to customers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24532.1, "ram_available_mb": 38308.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24614.5, "ram_available_mb": 38226.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.391}, "power_stats": {"power_gpu_soc_mean_watts": 24.645, "power_cpu_cv_mean_watts": 1.398, "power_sys_5v0_mean_watts": 9.197, "gpu_utilization_percent_mean": 79.391, "power_watts_avg": 24.645, "energy_joules_est": 197.66, "duration_seconds": 8.02, "sample_count": 69}, "timestamp": "2026-01-25T22:15:06.170871"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8131.259, "latencies_ms": [8131.259], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The bananas are hanging in bunches and appear to be ripe and ready to eat. The lighting in the image is bright and natural, suggesting that it was taken during the day.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24614.5, "ram_available_mb": 38226.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24575.5, "ram_available_mb": 38265.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.725}, "power_stats": {"power_gpu_soc_mean_watts": 24.33, "power_cpu_cv_mean_watts": 1.445, "power_sys_5v0_mean_watts": 9.254, "gpu_utilization_percent_mean": 77.725, "power_watts_avg": 24.33, "energy_joules_est": 197.85, "duration_seconds": 8.132, "sample_count": 69}, "timestamp": "2026-01-25T22:15:16.325882"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11111.654, "latencies_ms": [11111.654], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene rural landscape, dominated by a green field that stretches out to meet a backdrop of majestic mountains. A train, painted in vibrant hues of red and green, is the focal point of the scene. The train, composed of a locomotive and several carriages, is in motion, traveling from the left to the right", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24575.5, "ram_available_mb": 38265.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24536.9, "ram_available_mb": 38304.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.229}, "power_stats": {"power_gpu_soc_mean_watts": 20.904, "power_cpu_cv_mean_watts": 1.935, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.229, "power_watts_avg": 20.904, "energy_joules_est": 232.29, "duration_seconds": 11.112, "sample_count": 96}, "timestamp": "2026-01-25T22:15:29.476368"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7609.594, "latencies_ms": [7609.594], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "Train: 1\nCar: 2\nHouse: 1\nTrees: 1\nPower line: 1\nMountain: 1\nHouse: 1\nHouse: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24536.9, "ram_available_mb": 38304.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24587.8, "ram_available_mb": 38253.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.344}, "power_stats": {"power_gpu_soc_mean_watts": 22.553, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 73.344, "power_watts_avg": 22.553, "energy_joules_est": 171.63, "duration_seconds": 7.61, "sample_count": 64}, "timestamp": "2026-01-25T22:15:39.098477"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10583.939, "latencies_ms": [10583.939], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The train is positioned in the foreground of the image, moving from left to right across the frame. It is situated on a track that cuts through a grassy field, which occupies the lower portion of the image. In the background, there are mountains and a clear sky, which provide a scenic backdrop to the train's journey.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24587.8, "ram_available_mb": 38253.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24541.5, "ram_available_mb": 38299.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.489}, "power_stats": {"power_gpu_soc_mean_watts": 21.074, "power_cpu_cv_mean_watts": 1.877, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 69.489, "power_watts_avg": 21.074, "energy_joules_est": 223.06, "duration_seconds": 10.585, "sample_count": 90}, "timestamp": "2026-01-25T22:15:51.703879"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6600.318, "latencies_ms": [6600.318], "images_per_second": 0.152, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A green and white train with red carriages is traveling through a lush green field with mountains in the background. The sky is blue with a few clouds scattered across it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24541.5, "ram_available_mb": 38299.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24530.1, "ram_available_mb": 38310.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.339}, "power_stats": {"power_gpu_soc_mean_watts": 23.276, "power_cpu_cv_mean_watts": 1.53, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 74.339, "power_watts_avg": 23.276, "energy_joules_est": 153.64, "duration_seconds": 6.601, "sample_count": 56}, "timestamp": "2026-01-25T22:16:00.333985"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6211.79, "latencies_ms": [6211.79], "images_per_second": 0.161, "prompt_tokens": 36, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A green and white train with red carriages is on tracks through a grassy field. The sky is blue with some clouds, and there are mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24530.1, "ram_available_mb": 38310.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24589.0, "ram_available_mb": 38251.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.231}, "power_stats": {"power_gpu_soc_mean_watts": 23.321, "power_cpu_cv_mean_watts": 1.548, "power_sys_5v0_mean_watts": 9.088, "gpu_utilization_percent_mean": 74.231, "power_watts_avg": 23.321, "energy_joules_est": 144.88, "duration_seconds": 6.212, "sample_count": 52}, "timestamp": "2026-01-25T22:16:08.563455"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11139.541, "latencies_ms": [11139.541], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment on a beach where a man is standing with his back to the camera, arms raised in a flying pose. He is dressed in a dark shirt, khaki shorts, and a hat, suggesting a casual, possibly warm day. The ocean is visible in the background with waves crashing onto the shore, indicating a coastal setting. A colorful", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 24589.0, "ram_available_mb": 38251.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 8.3, "ram_used_mb": 24569.2, "ram_available_mb": 38271.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.484}, "power_stats": {"power_gpu_soc_mean_watts": 20.825, "power_cpu_cv_mean_watts": 2.289, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 69.484, "power_watts_avg": 20.825, "energy_joules_est": 231.99, "duration_seconds": 11.14, "sample_count": 95}, "timestamp": "2026-01-25T22:16:21.746245"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6950.929, "latencies_ms": [6950.929], "images_per_second": 0.144, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "person: 1, kite: 1, chair: 1, sand: numerous, water: multiple waves, sky: clear, sun: not visible, beach: entire visible area", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24569.2, "ram_available_mb": 38271.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24533.6, "ram_available_mb": 38307.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.186}, "power_stats": {"power_gpu_soc_mean_watts": 22.985, "power_cpu_cv_mean_watts": 1.568, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 74.186, "power_watts_avg": 22.985, "energy_joules_est": 159.78, "duration_seconds": 6.952, "sample_count": 59}, "timestamp": "2026-01-25T22:16:30.757994"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7314.81, "latencies_ms": [7314.81], "images_per_second": 0.137, "prompt_tokens": 44, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The man is standing in the foreground on the left side of the image, near the edge of the water. The kite is in the background, flying high in the sky on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24533.6, "ram_available_mb": 38307.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24596.6, "ram_available_mb": 38244.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.242}, "power_stats": {"power_gpu_soc_mean_watts": 22.54, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 9.04, "gpu_utilization_percent_mean": 72.242, "power_watts_avg": 22.54, "energy_joules_est": 164.89, "duration_seconds": 7.315, "sample_count": 62}, "timestamp": "2026-01-25T22:16:40.094925"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8283.239, "latencies_ms": [8283.239], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A person is standing on a sandy beach with their arms raised, possibly celebrating or enjoying the moment. The ocean is visible in the background with waves coming in, and there is a colorful kite flying in the sky to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24534.8, "ram_available_mb": 38306.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24605.9, "ram_available_mb": 38235.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.159, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 22.159, "energy_joules_est": 183.56, "duration_seconds": 8.284, "sample_count": 70}, "timestamp": "2026-01-25T22:16:50.426825"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7214.911, "latencies_ms": [7214.911], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image shows a person standing on a sandy beach with the ocean waves in the background. The sky is partly cloudy, and the person is wearing a dark shirt and khaki shorts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24535.7, "ram_available_mb": 38305.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24616.8, "ram_available_mb": 38224.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.213}, "power_stats": {"power_gpu_soc_mean_watts": 22.483, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.037, "gpu_utilization_percent_mean": 72.213, "power_watts_avg": 22.483, "energy_joules_est": 162.23, "duration_seconds": 7.216, "sample_count": 61}, "timestamp": "2026-01-25T22:16:59.699088"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11141.469, "latencies_ms": [11141.469], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene garden scene, where two terracotta pots cradle a vibrant green plant. The pots, with their rustic charm, are nestled on a bed of rich, dark soil. The plant, a lush green, is a mix of leafy greens and broccoli, its leaves a deep green and its head a l", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24528.4, "ram_available_mb": 38312.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24550.9, "ram_available_mb": 38290.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.271}, "power_stats": {"power_gpu_soc_mean_watts": 20.864, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.94, "gpu_utilization_percent_mean": 69.271, "power_watts_avg": 20.864, "energy_joules_est": 232.47, "duration_seconds": 11.142, "sample_count": 96}, "timestamp": "2026-01-25T22:17:12.866876"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8285.61, "latencies_ms": [8285.61], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "- Broccoli plant: 2\n- Leafy greens: 2\n- Plants: 2\n- Pot: 2\n- Soil: 2\n- Flower: 1\n- Bed: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24550.9, "ram_available_mb": 38290.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24526.8, "ram_available_mb": 38314.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.886}, "power_stats": {"power_gpu_soc_mean_watts": 22.227, "power_cpu_cv_mean_watts": 1.71, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 71.886, "power_watts_avg": 22.227, "energy_joules_est": 184.18, "duration_seconds": 8.286, "sample_count": 70}, "timestamp": "2026-01-25T22:17:23.198633"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11142.968, "latencies_ms": [11142.968], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two terracotta pots placed close to each other, each containing a plant with large green leaves. The plants appear to be in the early stages of growth, with small clusters of broccoli forming at the top of the leaves. The background is a sandy ground, and there is a small pink object, possibly a flower, located in the top", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24526.8, "ram_available_mb": 38314.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24523.9, "ram_available_mb": 38317.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.505}, "power_stats": {"power_gpu_soc_mean_watts": 20.919, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.505, "power_watts_avg": 20.919, "energy_joules_est": 233.11, "duration_seconds": 11.144, "sample_count": 95}, "timestamp": "2026-01-25T22:17:36.369801"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10186.982, "latencies_ms": [10186.982], "images_per_second": 0.098, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image shows a gardening scene with two terracotta pots containing leafy green plants, likely kale or a similar variety, growing in a sandy or loamy soil. The plants are healthy and appear to be well-maintained, suggesting they are being cared for in a garden or a controlled environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24523.9, "ram_available_mb": 38317.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24525.3, "ram_available_mb": 38315.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.267}, "power_stats": {"power_gpu_soc_mean_watts": 21.362, "power_cpu_cv_mean_watts": 1.839, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 70.267, "power_watts_avg": 21.362, "energy_joules_est": 217.63, "duration_seconds": 10.188, "sample_count": 86}, "timestamp": "2026-01-25T22:17:48.567504"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8973.848, "latencies_ms": [8973.848], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image shows a garden with two large, healthy green plants with large leaves, possibly kale or a similar leafy green, growing in terracotta pots. The lighting is natural and appears to be daylight, suggesting the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24525.3, "ram_available_mb": 38315.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24615.2, "ram_available_mb": 38225.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.091}, "power_stats": {"power_gpu_soc_mean_watts": 21.59, "power_cpu_cv_mean_watts": 1.805, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 71.091, "power_watts_avg": 21.59, "energy_joules_est": 193.76, "duration_seconds": 8.974, "sample_count": 77}, "timestamp": "2026-01-25T22:17:59.563686"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11126.576, "latencies_ms": [11126.576], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young boy is sitting on a brown and white horse, while an older man stands nearby, holding the horse's reins. The boy appears to be enjoying the experience of riding the horse, and the older man is likely the horse's owner or trainer. \n\nThe scene takes place in front of a house, with a chair visible on the", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24521.8, "ram_available_mb": 38319.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24540.5, "ram_available_mb": 38300.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.484}, "power_stats": {"power_gpu_soc_mean_watts": 20.848, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 69.484, "power_watts_avg": 20.848, "energy_joules_est": 231.98, "duration_seconds": 11.127, "sample_count": 95}, "timestamp": "2026-01-25T22:18:12.746487"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7053.475, "latencies_ms": [7053.475], "images_per_second": 0.142, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "boy: 1, horse: 1, rope: 1, building: 2, window: 2, chair: 1, person: 2, ground: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24540.5, "ram_available_mb": 38300.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24525.5, "ram_available_mb": 38315.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.517}, "power_stats": {"power_gpu_soc_mean_watts": 22.801, "power_cpu_cv_mean_watts": 1.609, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 73.517, "power_watts_avg": 22.801, "energy_joules_est": 160.84, "duration_seconds": 7.054, "sample_count": 60}, "timestamp": "2026-01-25T22:18:21.822214"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10974.367, "latencies_ms": [10974.367], "images_per_second": 0.091, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, a young boy is seated on a small brown and white pony, positioned near the center of the image. To the right, an older man stands holding the pony's reins, appearing to guide or assist the boy. The background features a red building with white trim and windows, providing a vivid backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24525.5, "ram_available_mb": 38315.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24614.1, "ram_available_mb": 38226.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.602}, "power_stats": {"power_gpu_soc_mean_watts": 20.137, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 70.602, "power_watts_avg": 20.137, "energy_joules_est": 221.0, "duration_seconds": 10.975, "sample_count": 93}, "timestamp": "2026-01-25T22:18:34.818865"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6849.731, "latencies_ms": [6849.731], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A young boy is sitting on a small pony while an older man holds the reins and guides the pony. They are standing in front of a red building with white windows.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24527.2, "ram_available_mb": 38313.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24611.5, "ram_available_mb": 38229.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.086}, "power_stats": {"power_gpu_soc_mean_watts": 23.047, "power_cpu_cv_mean_watts": 1.54, "power_sys_5v0_mean_watts": 8.985, "gpu_utilization_percent_mean": 74.086, "power_watts_avg": 23.047, "energy_joules_est": 157.88, "duration_seconds": 6.85, "sample_count": 58}, "timestamp": "2026-01-25T22:18:43.723861"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8309.499, "latencies_ms": [8309.499], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image features a bright and sunny day with clear skies, casting shadows on the ground. The main colors in the scene are the red of the building, the blue of the man's shirt, and the brown and white of the horse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24611.5, "ram_available_mb": 38229.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24612.8, "ram_available_mb": 38228.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.557}, "power_stats": {"power_gpu_soc_mean_watts": 21.958, "power_cpu_cv_mean_watts": 1.751, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 71.557, "power_watts_avg": 21.958, "energy_joules_est": 182.47, "duration_seconds": 8.31, "sample_count": 70}, "timestamp": "2026-01-25T22:18:54.065008"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11138.094, "latencies_ms": [11138.094], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young boy is walking through a field of blue flowers. He is wearing a striped shirt and blue jeans, and he is holding a teddy bear in his hand. The boy is walking on a dirt path, and the field of blue flowers is on his left side. The boy is looking at the flowers, seemingly admiring their beauty. The", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24539.4, "ram_available_mb": 38301.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24527.2, "ram_available_mb": 38313.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.34}, "power_stats": {"power_gpu_soc_mean_watts": 20.874, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 69.34, "power_watts_avg": 20.874, "energy_joules_est": 232.51, "duration_seconds": 11.139, "sample_count": 94}, "timestamp": "2026-01-25T22:19:07.244865"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8512.856, "latencies_ms": [8512.856], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "child: 1, striped shirt: 1, blue jeans: 1, brown teddy bear: 1, blue flowers: numerous, green plants: numerous, dirt path: 1, sunlight: visible, trees: visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24527.2, "ram_available_mb": 38313.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24546.0, "ram_available_mb": 38294.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.875}, "power_stats": {"power_gpu_soc_mean_watts": 21.91, "power_cpu_cv_mean_watts": 1.724, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 71.875, "power_watts_avg": 21.91, "energy_joules_est": 186.53, "duration_seconds": 8.514, "sample_count": 72}, "timestamp": "2026-01-25T22:19:17.782033"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10688.985, "latencies_ms": [10688.985], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The boy is standing in the foreground on a dirt path, facing away from the camera, with a field of blue flowers to his left and a blurred background that suggests a wooded area. The boy is positioned to the right of the frame, and the flowers are densely packed, creating a vibrant backdrop to his figure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24546.0, "ram_available_mb": 38294.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24616.4, "ram_available_mb": 38224.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.813}, "power_stats": {"power_gpu_soc_mean_watts": 21.027, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 69.813, "power_watts_avg": 21.027, "energy_joules_est": 224.77, "duration_seconds": 10.69, "sample_count": 91}, "timestamp": "2026-01-25T22:19:30.497559"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7735.04, "latencies_ms": [7735.04], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A young child is walking through a field of bluebells, holding a teddy bear in one hand. The child is wearing a striped shirt and blue jeans, and is walking on a dirt path.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24616.4, "ram_available_mb": 38224.5, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24600.6, "ram_available_mb": 38240.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.258}, "power_stats": {"power_gpu_soc_mean_watts": 22.442, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 73.258, "power_watts_avg": 22.442, "energy_joules_est": 173.6, "duration_seconds": 7.736, "sample_count": 66}, "timestamp": "2026-01-25T22:19:40.260540"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7328.232, "latencies_ms": [7328.232], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image features a child in a field of vibrant blue flowers under bright sunlight. The child is wearing a striped blue and white shirt and blue jeans, holding a brown teddy bear.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24518.9, "ram_available_mb": 38322.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24615.7, "ram_available_mb": 38225.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.841}, "power_stats": {"power_gpu_soc_mean_watts": 22.33, "power_cpu_cv_mean_watts": 1.665, "power_sys_5v0_mean_watts": 9.038, "gpu_utilization_percent_mean": 71.841, "power_watts_avg": 22.33, "energy_joules_est": 163.65, "duration_seconds": 7.329, "sample_count": 63}, "timestamp": "2026-01-25T22:19:49.638339"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11116.195, "latencies_ms": [11116.195], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a single orange placed on the side of a road, with a parking lot in the background. The orange is positioned in the center of the scene, drawing attention to itself. The parking lot is filled with various cars, including a white car and a black car, parked in different spots. The cars are of different sizes and colors, creating a diverse and busy", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24527.3, "ram_available_mb": 38313.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24585.4, "ram_available_mb": 38255.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.316}, "power_stats": {"power_gpu_soc_mean_watts": 20.928, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 69.316, "power_watts_avg": 20.928, "energy_joules_est": 232.66, "duration_seconds": 11.117, "sample_count": 95}, "timestamp": "2026-01-25T22:20:02.818428"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9626.048, "latencies_ms": [9626.048], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "1. Orange: 1\n2. Cars: 10\n3. Trees: 4\n4. Parking lot: 1\n5. White line: 1\n6. Road: 1\n7. Clouds: 1\n8. Sky: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24585.4, "ram_available_mb": 38255.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24594.6, "ram_available_mb": 38246.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.58}, "power_stats": {"power_gpu_soc_mean_watts": 21.472, "power_cpu_cv_mean_watts": 1.8, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 71.58, "power_watts_avg": 21.472, "energy_joules_est": 206.7, "duration_seconds": 9.627, "sample_count": 81}, "timestamp": "2026-01-25T22:20:14.479553"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11144.899, "latencies_ms": [11144.899], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The main object, an orange, is positioned in the foreground on the left side of the image, standing out against the asphalt. In the background, there is a parking lot with multiple cars, some of which are parked while others are in motion, indicating a distance from the orange. The sky is visible above the cars, suggesting the orange is on a higher elevation", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24594.6, "ram_available_mb": 38246.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 8.1, "ram_used_mb": 24612.3, "ram_available_mb": 38228.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.716}, "power_stats": {"power_gpu_soc_mean_watts": 20.949, "power_cpu_cv_mean_watts": 2.407, "power_sys_5v0_mean_watts": 9.004, "gpu_utilization_percent_mean": 69.716, "power_watts_avg": 20.949, "energy_joules_est": 233.49, "duration_seconds": 11.146, "sample_count": 95}, "timestamp": "2026-01-25T22:20:27.643325"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7184.005, "latencies_ms": [7184.005], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "An orange is placed on the side of a road, with cars parked in the background. The sky is overcast, and the road appears to be in a parking lot or a similar area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24612.3, "ram_available_mb": 38228.6, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24612.3, "ram_available_mb": 38228.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.918}, "power_stats": {"power_gpu_soc_mean_watts": 22.81, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 73.918, "power_watts_avg": 22.81, "energy_joules_est": 163.88, "duration_seconds": 7.185, "sample_count": 61}, "timestamp": "2026-01-25T22:20:36.885071"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7878.408, "latencies_ms": [7878.408], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image features a single, large, and round orange placed on a gray asphalt surface. The lighting is soft and diffused, suggesting an overcast day, and the background shows a parking lot with various cars and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24612.3, "ram_available_mb": 38228.6, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24623.0, "ram_available_mb": 38217.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.636}, "power_stats": {"power_gpu_soc_mean_watts": 22.114, "power_cpu_cv_mean_watts": 1.705, "power_sys_5v0_mean_watts": 9.035, "gpu_utilization_percent_mean": 71.636, "power_watts_avg": 22.114, "energy_joules_est": 174.24, "duration_seconds": 7.879, "sample_count": 66}, "timestamp": "2026-01-25T22:20:46.796567"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12312.682, "latencies_ms": [12312.682], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seated at a wooden table, wearing a gray suit and a white shirt. He is smiling and has a bowl of food in front of him. On the table, there are two beer bottles, one of which is half-empty. A set of keys is also present on the table. The background is a plain white wall.", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24531.4, "ram_available_mb": 38309.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24530.2, "ram_available_mb": 38310.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.076}, "power_stats": {"power_gpu_soc_mean_watts": 22.847, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 9.191, "gpu_utilization_percent_mean": 74.076, "power_watts_avg": 22.847, "energy_joules_est": 281.32, "duration_seconds": 12.313, "sample_count": 105}, "timestamp": "2026-01-25T22:21:01.173625"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8835.478, "latencies_ms": [8835.478], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "bowl: 1, bottle: 2, keys: 5, remote control: 1, wallet: 1, pen: 1, glass: 1, spoon: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24530.2, "ram_available_mb": 38310.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24567.9, "ram_available_mb": 38273.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.56}, "power_stats": {"power_gpu_soc_mean_watts": 24.266, "power_cpu_cv_mean_watts": 1.442, "power_sys_5v0_mean_watts": 9.15, "gpu_utilization_percent_mean": 78.56, "power_watts_avg": 24.266, "energy_joules_est": 214.42, "duration_seconds": 8.836, "sample_count": 75}, "timestamp": "2026-01-25T22:21:12.034222"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9731.013, "latencies_ms": [9731.013], "images_per_second": 0.103, "prompt_tokens": 44, "response_tokens_est": 55, "n_tiles": 16, "output_text": "In the foreground, there is a wooden table with a bowl in the center, a bottle of beer to the right, and a set of keys to the left. The background features a plain wall and a partial view of another person's arm.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24567.9, "ram_available_mb": 38273.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24567.9, "ram_available_mb": 38273.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.012}, "power_stats": {"power_gpu_soc_mean_watts": 23.62, "power_cpu_cv_mean_watts": 1.602, "power_sys_5v0_mean_watts": 9.237, "gpu_utilization_percent_mean": 76.012, "power_watts_avg": 23.62, "energy_joules_est": 229.86, "duration_seconds": 9.732, "sample_count": 82}, "timestamp": "2026-01-25T22:21:23.782608"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9160.439, "latencies_ms": [9160.439], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A man is seated at a table with a bowl of food in front of him, accompanied by two bottles of beer. The setting appears to be a casual dining environment, possibly a restaurant or a home.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24567.9, "ram_available_mb": 38273.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24630.8, "ram_available_mb": 38210.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.506}, "power_stats": {"power_gpu_soc_mean_watts": 24.023, "power_cpu_cv_mean_watts": 1.505, "power_sys_5v0_mean_watts": 9.161, "gpu_utilization_percent_mean": 77.506, "power_watts_avg": 24.023, "energy_joules_est": 220.08, "duration_seconds": 9.161, "sample_count": 79}, "timestamp": "2026-01-25T22:21:34.991563"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9588.928, "latencies_ms": [9588.928], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a man sitting at a table with a bowl in front of him, two beer bottles to his right, and a glass of beer next to the bottles. The table is wooden and the man is wearing a grey suit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24568.8, "ram_available_mb": 38272.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24614.7, "ram_available_mb": 38226.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.691}, "power_stats": {"power_gpu_soc_mean_watts": 23.691, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 9.25, "gpu_utilization_percent_mean": 75.691, "power_watts_avg": 23.691, "energy_joules_est": 227.19, "duration_seconds": 9.59, "sample_count": 81}, "timestamp": "2026-01-25T22:21:46.627827"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11133.905, "latencies_ms": [11133.905], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a neatly made bed in a bedroom. The bed is covered with white sheets and pillows, creating a clean and inviting atmosphere. A chair is placed next to the bed, providing a comfortable spot for relaxation or reading. A window can be seen in the background, allowing natural light to enter the room.\n\nIn addition to the bed and chair, there is", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24527.7, "ram_available_mb": 38313.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24547.3, "ram_available_mb": 38293.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.436}, "power_stats": {"power_gpu_soc_mean_watts": 20.918, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 69.436, "power_watts_avg": 20.918, "energy_joules_est": 232.91, "duration_seconds": 11.135, "sample_count": 94}, "timestamp": "2026-01-25T22:21:59.785399"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8258.894, "latencies_ms": [8258.894], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "bed: 1\npillow: 4\nchair: 1\ndesk: 1\nphone: 1\nlamp: 1\nwindow: 1\nframe: 1\ncarpet: 1", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24547.3, "ram_available_mb": 38293.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24556.9, "ram_available_mb": 38284.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.478}, "power_stats": {"power_gpu_soc_mean_watts": 21.459, "power_cpu_cv_mean_watts": 1.683, "power_sys_5v0_mean_watts": 8.9, "gpu_utilization_percent_mean": 73.478, "power_watts_avg": 21.459, "energy_joules_est": 177.24, "duration_seconds": 8.26, "sample_count": 69}, "timestamp": "2026-01-25T22:22:10.070420"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9896.267, "latencies_ms": [9896.267], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The bed is positioned in the foreground of the image, taking up a significant portion of the space. It is placed against a wall with a window above it, allowing natural light to enter the room. The desk and chair are located to the left of the bed, creating a workspace adjacent to the sleeping area.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24556.9, "ram_available_mb": 38284.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24594.5, "ram_available_mb": 38246.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.265}, "power_stats": {"power_gpu_soc_mean_watts": 21.315, "power_cpu_cv_mean_watts": 1.848, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 70.265, "power_watts_avg": 21.315, "energy_joules_est": 210.95, "duration_seconds": 9.897, "sample_count": 83}, "timestamp": "2026-01-25T22:22:21.978472"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8502.152, "latencies_ms": [8502.152], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a neatly made bed in a small, well-lit room with a window on the wall. The bed is adorned with white pillows and a white comforter, and there is a chair and a desk nearby.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24594.5, "ram_available_mb": 38246.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24571.6, "ram_available_mb": 38269.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.111}, "power_stats": {"power_gpu_soc_mean_watts": 21.944, "power_cpu_cv_mean_watts": 1.741, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 71.111, "power_watts_avg": 21.944, "energy_joules_est": 186.58, "duration_seconds": 8.503, "sample_count": 72}, "timestamp": "2026-01-25T22:22:32.508730"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7787.268, "latencies_ms": [7787.268], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The room is well-lit with natural light coming from the window, which shows a clear sky outside. The bed is neatly made with white linens, and there is a white desk with a phone and some items on it.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 24571.6, "ram_available_mb": 38269.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24625.5, "ram_available_mb": 38215.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.045}, "power_stats": {"power_gpu_soc_mean_watts": 22.116, "power_cpu_cv_mean_watts": 1.717, "power_sys_5v0_mean_watts": 9.061, "gpu_utilization_percent_mean": 72.045, "power_watts_avg": 22.116, "energy_joules_est": 172.24, "duration_seconds": 7.788, "sample_count": 66}, "timestamp": "2026-01-25T22:22:42.343129"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11132.934, "latencies_ms": [11132.934], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of the image, a trio of plush toys are gathered, each with its own unique charm. The bear, a symbol of warmth and comfort, is adorned with a vibrant green hat and a red scarf, adding a pop of color to its soft brown fur. It sits next to a snowman, who is dressed in a black top", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24530.3, "ram_available_mb": 38310.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24544.0, "ram_available_mb": 38296.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.84}, "power_stats": {"power_gpu_soc_mean_watts": 20.861, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 69.84, "power_watts_avg": 20.861, "energy_joules_est": 232.26, "duration_seconds": 11.134, "sample_count": 94}, "timestamp": "2026-01-25T22:22:55.510825"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8499.196, "latencies_ms": [8499.196], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "bear: 2, coca cola hat: 1, snowman: 1, berlin tag: 1, red scarf: 1, green hat: 1, orange background: 1, blue floral pattern: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24544.0, "ram_available_mb": 38296.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24537.0, "ram_available_mb": 38303.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.781}, "power_stats": {"power_gpu_soc_mean_watts": 22.065, "power_cpu_cv_mean_watts": 1.733, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 71.781, "power_watts_avg": 22.065, "energy_joules_est": 187.55, "duration_seconds": 8.5, "sample_count": 73}, "timestamp": "2026-01-25T22:23:06.057846"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11188.31, "latencies_ms": [11188.31], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a teddy bear wearing a green hat and a red scarf, sitting next to a stuffed bear wearing a red hat and a green scarf. In the background, there is a snowman wearing a black hat and a red scarf, sitting next to a teddy bear wearing a green hat and a red scarf. The snow", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24537.0, "ram_available_mb": 38303.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24623.4, "ram_available_mb": 38217.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.677}, "power_stats": {"power_gpu_soc_mean_watts": 20.96, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 69.677, "power_watts_avg": 20.96, "energy_joules_est": 234.53, "duration_seconds": 11.189, "sample_count": 96}, "timestamp": "2026-01-25T22:23:19.268363"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8650.344, "latencies_ms": [8650.344], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "In the image, a group of stuffed animals, including a teddy bear, a snowman, and a bear, are sitting together on a blue surface. They are wearing festive hats and scarves, suggesting a holiday or celebratory theme.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24536.6, "ram_available_mb": 38304.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24544.6, "ram_available_mb": 38296.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.151}, "power_stats": {"power_gpu_soc_mean_watts": 21.956, "power_cpu_cv_mean_watts": 1.728, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 71.151, "power_watts_avg": 21.956, "energy_joules_est": 189.94, "duration_seconds": 8.651, "sample_count": 73}, "timestamp": "2026-01-25T22:23:29.978074"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8566.11, "latencies_ms": [8566.11], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image features a plush teddy bear wearing a green hat and a red scarf, sitting next to a snowman plush toy wearing a red hat and a green hat. The background has a floral pattern with orange and pink colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24544.6, "ram_available_mb": 38296.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24551.0, "ram_available_mb": 38289.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.153}, "power_stats": {"power_gpu_soc_mean_watts": 21.55, "power_cpu_cv_mean_watts": 1.752, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 72.153, "power_watts_avg": 21.55, "energy_joules_est": 184.61, "duration_seconds": 8.567, "sample_count": 72}, "timestamp": "2026-01-25T22:23:40.577686"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12325.072, "latencies_ms": [12325.072], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a clear glass bowl filled with a group of oranges. The bowl is placed on a table, and the oranges are arranged in a circular pattern, with some oranges overlapping each other. The oranges are bright orange in color, and the bowl is transparent, allowing the oranges to be clearly visible. The table has a textured surface, and", "error": null, "sys_before": {"cpu_percent": 6.5, "ram_used_mb": 24551.0, "ram_available_mb": 38289.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24552.8, "ram_available_mb": 38288.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.17}, "power_stats": {"power_gpu_soc_mean_watts": 22.793, "power_cpu_cv_mean_watts": 1.791, "power_sys_5v0_mean_watts": 9.213, "gpu_utilization_percent_mean": 74.17, "power_watts_avg": 22.793, "energy_joules_est": 280.94, "duration_seconds": 12.326, "sample_count": 106}, "timestamp": "2026-01-25T22:23:54.960866"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4326.139, "latencies_ms": [4326.139], "images_per_second": 0.231, "prompt_tokens": 39, "response_tokens_est": 5, "n_tiles": 16, "output_text": "orange: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24552.8, "ram_available_mb": 38288.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24574.3, "ram_available_mb": 38266.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_gpu_soc_mean_watts": 28.553, "power_cpu_cv_mean_watts": 0.522, "power_sys_5v0_mean_watts": 9.276, "gpu_utilization_percent_mean": 91.0, "power_watts_avg": 28.553, "energy_joules_est": 123.54, "duration_seconds": 4.327, "sample_count": 36}, "timestamp": "2026-01-25T22:24:01.327957"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12399.659, "latencies_ms": [12399.659], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The oranges are clustered together in the foreground of the image, with some appearing closer to the viewer and others further away, creating a sense of depth. The bowl is positioned centrally and is the main focus, with the oranges filling the bowl's interior. The background is a textured surface that provides contrast to the smooth bowl and the gloss", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24574.3, "ram_available_mb": 38266.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24575.5, "ram_available_mb": 38265.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.613}, "power_stats": {"power_gpu_soc_mean_watts": 22.894, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 9.199, "gpu_utilization_percent_mean": 73.613, "power_watts_avg": 22.894, "energy_joules_est": 283.89, "duration_seconds": 12.4, "sample_count": 106}, "timestamp": "2026-01-25T22:24:15.757691"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8585.252, "latencies_ms": [8585.252], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A bowl filled with oranges is placed on a textured surface, possibly a table or countertop. The oranges are arranged neatly in the bowl, with some overlapping each other.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24575.5, "ram_available_mb": 38265.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 7.6, "ram_used_mb": 24579.8, "ram_available_mb": 38261.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.041}, "power_stats": {"power_gpu_soc_mean_watts": 24.457, "power_cpu_cv_mean_watts": 2.096, "power_sys_5v0_mean_watts": 9.245, "gpu_utilization_percent_mean": 79.041, "power_watts_avg": 24.457, "energy_joules_est": 209.99, "duration_seconds": 8.586, "sample_count": 73}, "timestamp": "2026-01-25T22:24:26.386055"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8486.585, "latencies_ms": [8486.585], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The oranges are a vibrant orange color, indicating they are ripe and likely sweet. They are placed in a clear glass bowl, which reflects light and highlights their glossy texture.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24579.8, "ram_available_mb": 38261.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24642.5, "ram_available_mb": 38198.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.014}, "power_stats": {"power_gpu_soc_mean_watts": 24.229, "power_cpu_cv_mean_watts": 1.479, "power_sys_5v0_mean_watts": 9.251, "gpu_utilization_percent_mean": 77.014, "power_watts_avg": 24.229, "energy_joules_est": 205.64, "duration_seconds": 8.487, "sample_count": 72}, "timestamp": "2026-01-25T22:24:36.898231"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12081.021, "latencies_ms": [12081.021], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a surfer is skillfully riding a large wave in the ocean. The surfer, clad in a black wetsuit, is positioned on a white surfboard. The wave, a magnificent shade of blue, is breaking to the right, creating a dramatic scene. The sky above is a uniform gray, suggesting an overcast day. The", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24642.5, "ram_available_mb": 38198.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24642.8, "ram_available_mb": 38198.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.752}, "power_stats": {"power_gpu_soc_mean_watts": 22.54, "power_cpu_cv_mean_watts": 1.8, "power_sys_5v0_mean_watts": 9.163, "gpu_utilization_percent_mean": 72.752, "power_watts_avg": 22.54, "energy_joules_est": 272.32, "duration_seconds": 12.082, "sample_count": 105}, "timestamp": "2026-01-25T22:24:51.031280"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8525.101, "latencies_ms": [8525.101], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "wave: 1\nsurfer: 1\nwater spray: multiple\nocean: 1\nsky: 1\nclouds: 1\nsand: 1\nboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24537.5, "ram_available_mb": 38303.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24546.8, "ram_available_mb": 38294.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.181}, "power_stats": {"power_gpu_soc_mean_watts": 24.058, "power_cpu_cv_mean_watts": 1.485, "power_sys_5v0_mean_watts": 9.151, "gpu_utilization_percent_mean": 77.181, "power_watts_avg": 24.058, "energy_joules_est": 205.12, "duration_seconds": 8.526, "sample_count": 72}, "timestamp": "2026-01-25T22:25:01.600928"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11834.713, "latencies_ms": [11834.713], "images_per_second": 0.084, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The surfer is positioned near the foreground, riding a wave that is closer to the viewer than the horizon. The wave is large and forms the central focus of the image, with the surfer appearing small in comparison to the wave's size. The background is the ocean extending to the horizon, which appears far away and is less detailed than the wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24546.8, "ram_available_mb": 38294.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24620.7, "ram_available_mb": 38220.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.03}, "power_stats": {"power_gpu_soc_mean_watts": 22.742, "power_cpu_cv_mean_watts": 1.772, "power_sys_5v0_mean_watts": 9.174, "gpu_utilization_percent_mean": 73.03, "power_watts_avg": 22.742, "energy_joules_est": 269.17, "duration_seconds": 11.836, "sample_count": 101}, "timestamp": "2026-01-25T22:25:15.490780"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6434.842, "latencies_ms": [6434.842], "images_per_second": 0.155, "prompt_tokens": 37, "response_tokens_est": 26, "n_tiles": 16, "output_text": "A surfer is riding a large wave in the ocean. The sky is overcast and the water is choppy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24533.8, "ram_available_mb": 38307.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24531.3, "ram_available_mb": 38309.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 81.667}, "power_stats": {"power_gpu_soc_mean_watts": 25.285, "power_cpu_cv_mean_watts": 1.156, "power_sys_5v0_mean_watts": 9.201, "gpu_utilization_percent_mean": 81.667, "power_watts_avg": 25.285, "energy_joules_est": 162.72, "duration_seconds": 6.435, "sample_count": 54}, "timestamp": "2026-01-25T22:25:23.984094"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7802.301, "latencies_ms": [7802.301], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The surfer is riding a large wave with a deep blue color and white foam at the crest. The sky is overcast, suggesting a cloudy day with no direct sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24531.3, "ram_available_mb": 38309.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24535.1, "ram_available_mb": 38305.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.106}, "power_stats": {"power_gpu_soc_mean_watts": 24.05, "power_cpu_cv_mean_watts": 1.468, "power_sys_5v0_mean_watts": 9.225, "gpu_utilization_percent_mean": 77.106, "power_watts_avg": 24.05, "energy_joules_est": 187.66, "duration_seconds": 7.803, "sample_count": 66}, "timestamp": "2026-01-25T22:25:33.840043"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11129.014, "latencies_ms": [11129.014], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a cat is sitting on a couch, looking at a laptop screen. The cat appears to be focused on the screen, possibly watching something interesting or playing with the laptop. The laptop is placed on the couch, and the cat is positioned in front of it, occupying a significant portion of the scene. The cat's presence adds a playful and cozy", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24535.1, "ram_available_mb": 38305.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24540.6, "ram_available_mb": 38300.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.505}, "power_stats": {"power_gpu_soc_mean_watts": 20.918, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.505, "power_watts_avg": 20.918, "energy_joules_est": 232.81, "duration_seconds": 11.13, "sample_count": 95}, "timestamp": "2026-01-25T22:25:46.996273"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7425.136, "latencies_ms": [7425.136], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "couch: 1\nlaptop: 1\nscreen: 1\ntwitter: 1\nweb page: 1\nsearch bar: 1\nkeyword: 1\ncat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24540.6, "ram_available_mb": 38300.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24529.5, "ram_available_mb": 38311.4, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.698}, "power_stats": {"power_gpu_soc_mean_watts": 22.61, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 73.698, "power_watts_avg": 22.61, "energy_joules_est": 167.9, "duration_seconds": 7.426, "sample_count": 63}, "timestamp": "2026-01-25T22:25:56.449182"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8646.523, "latencies_ms": [8646.523], "images_per_second": 0.116, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The cat is positioned in the foreground on the left side of the image, with its body facing the laptop screen which is in the background on the right side. The laptop is placed on a surface that appears to be a couch or a similar piece of furniture.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24529.5, "ram_available_mb": 38311.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24545.6, "ram_available_mb": 38295.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.189}, "power_stats": {"power_gpu_soc_mean_watts": 21.75, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 9.003, "gpu_utilization_percent_mean": 71.189, "power_watts_avg": 21.75, "energy_joules_est": 188.08, "duration_seconds": 8.647, "sample_count": 74}, "timestamp": "2026-01-25T22:26:07.134653"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8048.678, "latencies_ms": [8048.678], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A cat is sitting on a couch with its head resting on a laptop, appearing to be using the Twitter website. The laptop is open and the screen is visible, showing the Twitter homepage with the search bar and the Twitter logo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24545.6, "ram_available_mb": 38295.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24562.0, "ram_available_mb": 38278.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.676}, "power_stats": {"power_gpu_soc_mean_watts": 22.261, "power_cpu_cv_mean_watts": 1.701, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 71.676, "power_watts_avg": 22.261, "energy_joules_est": 179.19, "duration_seconds": 8.05, "sample_count": 68}, "timestamp": "2026-01-25T22:26:17.203055"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5308.567, "latencies_ms": [5308.567], "images_per_second": 0.188, "prompt_tokens": 36, "response_tokens_est": 27, "n_tiles": 16, "output_text": "The image shows a cat with a white and brown coat sitting in front of a laptop. The laptop screen displays the Twitter website.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24562.0, "ram_available_mb": 38278.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24613.0, "ram_available_mb": 38227.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.667}, "power_stats": {"power_gpu_soc_mean_watts": 24.198, "power_cpu_cv_mean_watts": 1.388, "power_sys_5v0_mean_watts": 9.142, "gpu_utilization_percent_mean": 75.667, "power_watts_avg": 24.198, "energy_joules_est": 128.47, "duration_seconds": 5.309, "sample_count": 45}, "timestamp": "2026-01-25T22:26:24.528365"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12077.557, "latencies_ms": [12077.557], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of horses in a field, with some of them standing close together and others scattered around. There are at least five horses visible in the scene, with one horse standing alone in the background. The horses are of various sizes, and they appear to be enjoying their time in the field.\n\nIn the background, there is a house, which suggests that the horses are", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 24539.6, "ram_available_mb": 38301.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24529.8, "ram_available_mb": 38311.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.68}, "power_stats": {"power_gpu_soc_mean_watts": 22.622, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 9.161, "gpu_utilization_percent_mean": 72.68, "power_watts_avg": 22.622, "energy_joules_est": 273.23, "duration_seconds": 12.078, "sample_count": 103}, "timestamp": "2026-01-25T22:26:38.667410"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10542.864, "latencies_ms": [10542.864], "images_per_second": 0.095, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Horses: 5\n2. Hay: 1\n3. Grass: 1\n4. Fence: 1\n5. House: 1\n6. Trees: 1\n7. Power lines: 1\n8. Sky: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24529.8, "ram_available_mb": 38311.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24547.8, "ram_available_mb": 38293.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.2}, "power_stats": {"power_gpu_soc_mean_watts": 23.192, "power_cpu_cv_mean_watts": 1.655, "power_sys_5v0_mean_watts": 9.108, "gpu_utilization_percent_mean": 75.2, "power_watts_avg": 23.192, "energy_joules_est": 244.52, "duration_seconds": 10.543, "sample_count": 90}, "timestamp": "2026-01-25T22:26:51.231284"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9508.448, "latencies_ms": [9508.448], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 56, "n_tiles": 16, "output_text": "In the foreground, there is a group of horses standing close together, with one horse slightly ahead of the others. In the background, there is a house and trees. The horses are in the near foreground, while the house and trees are in the far background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24547.8, "ram_available_mb": 38293.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24522.3, "ram_available_mb": 38318.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.988}, "power_stats": {"power_gpu_soc_mean_watts": 23.311, "power_cpu_cv_mean_watts": 1.631, "power_sys_5v0_mean_watts": 9.192, "gpu_utilization_percent_mean": 74.988, "power_watts_avg": 23.311, "energy_joules_est": 221.67, "duration_seconds": 9.509, "sample_count": 82}, "timestamp": "2026-01-25T22:27:02.795956"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8969.391, "latencies_ms": [8969.391], "images_per_second": 0.111, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A group of horses, including a foal, are standing in a grassy field with hay scattered around. The horses are of different colors, including brown and black, and they appear to be grazing or resting in the field.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24522.3, "ram_available_mb": 38318.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24616.7, "ram_available_mb": 38224.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.329}, "power_stats": {"power_gpu_soc_mean_watts": 23.771, "power_cpu_cv_mean_watts": 1.517, "power_sys_5v0_mean_watts": 9.126, "gpu_utilization_percent_mean": 76.329, "power_watts_avg": 23.771, "energy_joules_est": 213.23, "duration_seconds": 8.97, "sample_count": 76}, "timestamp": "2026-01-25T22:27:13.802055"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7580.103, "latencies_ms": [7580.103], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image shows a group of horses in a field with a mix of brown and black colors. The lighting is natural and appears to be daylight, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24536.5, "ram_available_mb": 38304.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24612.1, "ram_available_mb": 38228.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.547}, "power_stats": {"power_gpu_soc_mean_watts": 24.257, "power_cpu_cv_mean_watts": 1.432, "power_sys_5v0_mean_watts": 9.219, "gpu_utilization_percent_mean": 77.547, "power_watts_avg": 24.257, "energy_joules_est": 183.89, "duration_seconds": 7.581, "sample_count": 64}, "timestamp": "2026-01-25T22:27:23.424582"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11115.184, "latencies_ms": [11115.184], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seen surfing on a wave in the ocean. He is wearing a black wetsuit and is skillfully riding the wave on a white surfboard. The surfer is positioned in the center of the image, with the wave beneath him and the ocean surrounding him. The background features a cliff, adding a scenic element to the", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24532.0, "ram_available_mb": 38308.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24534.1, "ram_available_mb": 38306.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.621}, "power_stats": {"power_gpu_soc_mean_watts": 20.971, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 69.621, "power_watts_avg": 20.971, "energy_joules_est": 233.11, "duration_seconds": 11.116, "sample_count": 95}, "timestamp": "2026-01-25T22:27:36.603298"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7294.159, "latencies_ms": [7294.159], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "surfboard: 1, water: multiple, surfer: 1, wave: 1, cliff: 1, sky: 1, sand: 1, wind: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24534.1, "ram_available_mb": 38306.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24547.8, "ram_available_mb": 38293.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.295}, "power_stats": {"power_gpu_soc_mean_watts": 22.596, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 72.295, "power_watts_avg": 22.596, "energy_joules_est": 164.83, "duration_seconds": 7.295, "sample_count": 61}, "timestamp": "2026-01-25T22:27:45.938198"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 6670.54, "latencies_ms": [6670.54], "images_per_second": 0.15, "prompt_tokens": 44, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The surfer is in the foreground, riding a wave that is breaking near the shore. The cliff is in the background, located far from the surfer's position.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24547.8, "ram_available_mb": 38293.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24595.0, "ram_available_mb": 38245.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.071}, "power_stats": {"power_gpu_soc_mean_watts": 23.005, "power_cpu_cv_mean_watts": 1.587, "power_sys_5v0_mean_watts": 9.066, "gpu_utilization_percent_mean": 73.071, "power_watts_avg": 23.005, "energy_joules_est": 153.47, "duration_seconds": 6.671, "sample_count": 56}, "timestamp": "2026-01-25T22:27:54.664746"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6307.58, "latencies_ms": [6307.58], "images_per_second": 0.159, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "A man in a black wetsuit is surfing on a wave in the ocean. The background shows a rocky cliff and a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24533.3, "ram_available_mb": 38307.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24565.1, "ram_available_mb": 38275.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.019}, "power_stats": {"power_gpu_soc_mean_watts": 23.303, "power_cpu_cv_mean_watts": 1.473, "power_sys_5v0_mean_watts": 9.012, "gpu_utilization_percent_mean": 74.019, "power_watts_avg": 23.303, "energy_joules_est": 147.0, "duration_seconds": 6.308, "sample_count": 53}, "timestamp": "2026-01-25T22:28:03.032674"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6726.429, "latencies_ms": [6726.429], "images_per_second": 0.149, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The surfer is wearing a black wetsuit and is riding a wave on a yellow surfboard. The water is a deep blue color and the sky is cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24565.1, "ram_available_mb": 38275.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24604.3, "ram_available_mb": 38236.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.839}, "power_stats": {"power_gpu_soc_mean_watts": 22.585, "power_cpu_cv_mean_watts": 1.559, "power_sys_5v0_mean_watts": 9.021, "gpu_utilization_percent_mean": 73.839, "power_watts_avg": 22.585, "energy_joules_est": 151.93, "duration_seconds": 6.727, "sample_count": 56}, "timestamp": "2026-01-25T22:28:11.774413"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11135.552, "latencies_ms": [11135.552], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a collection of carved pumpkins on display, each with a unique design. There are three pumpkins in total, with one large pumpkin in the center and two smaller ones on either side. The large pumpkin has a face carved into it, while the smaller pumpkins have various designs, including a cat and a dog.\n\nIn addition", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24540.9, "ram_available_mb": 38300.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 8.1, "ram_used_mb": 24625.6, "ram_available_mb": 38215.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.411}, "power_stats": {"power_gpu_soc_mean_watts": 20.896, "power_cpu_cv_mean_watts": 1.981, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 69.411, "power_watts_avg": 20.896, "energy_joules_est": 232.7, "duration_seconds": 11.136, "sample_count": 95}, "timestamp": "2026-01-25T22:28:24.955899"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4977.657, "latencies_ms": [4977.657], "images_per_second": 0.201, "prompt_tokens": 39, "response_tokens_est": 22, "n_tiles": 16, "output_text": "pumpkin: 3, flower: 2, drawing: 8, figure: 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24625.6, "ram_available_mb": 38215.3, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24648.3, "ram_available_mb": 38192.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.78}, "power_stats": {"power_gpu_soc_mean_watts": 25.101, "power_cpu_cv_mean_watts": 1.543, "power_sys_5v0_mean_watts": 9.091, "gpu_utilization_percent_mean": 79.78, "power_watts_avg": 25.101, "energy_joules_est": 124.96, "duration_seconds": 4.978, "sample_count": 41}, "timestamp": "2026-01-25T22:28:31.952844"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11131.585, "latencies_ms": [11131.585], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large pumpkin with a carved face and a smaller pumpkin with a carved face nearby. To the right, there is a vase filled with pink and white flowers. The vase is placed on top of the smaller pumpkin, creating a layered effect. In the background, there are various drawings and posters on", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24648.3, "ram_available_mb": 38192.6, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24554.1, "ram_available_mb": 38286.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.362}, "power_stats": {"power_gpu_soc_mean_watts": 20.956, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 69.362, "power_watts_avg": 20.956, "energy_joules_est": 233.29, "duration_seconds": 11.132, "sample_count": 94}, "timestamp": "2026-01-25T22:28:45.107803"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8393.081, "latencies_ms": [8393.081], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a collection of carved pumpkins on display, likely for a Halloween event. There are three pumpkins in the foreground, each with a unique carving, and a vase with pink flowers in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24554.1, "ram_available_mb": 38286.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24638.9, "ram_available_mb": 38202.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.958}, "power_stats": {"power_gpu_soc_mean_watts": 22.001, "power_cpu_cv_mean_watts": 1.713, "power_sys_5v0_mean_watts": 8.999, "gpu_utilization_percent_mean": 71.958, "power_watts_avg": 22.001, "energy_joules_est": 184.67, "duration_seconds": 8.394, "sample_count": 72}, "timestamp": "2026-01-25T22:28:55.545993"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9520.957, "latencies_ms": [9520.957], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image features a collection of carved pumpkins with various designs, including a smiling face and a bat, set against a backdrop of a bookshelf filled with books. The pumpkins are illuminated from within, casting a warm glow and highlighting the intricate carvings.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24542.9, "ram_available_mb": 38298.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24623.4, "ram_available_mb": 38217.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.975}, "power_stats": {"power_gpu_soc_mean_watts": 21.485, "power_cpu_cv_mean_watts": 1.847, "power_sys_5v0_mean_watts": 9.004, "gpu_utilization_percent_mean": 70.975, "power_watts_avg": 21.485, "energy_joules_est": 204.57, "duration_seconds": 9.522, "sample_count": 80}, "timestamp": "2026-01-25T22:29:07.117070"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12135.004, "latencies_ms": [12135.004], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a small, cozy bathroom bathed in a soft, warm light. Dominating the left side of the frame is a pristine white sink, its surface gleaming under the light. Above it, a mirror with a silver frame reflects the room, adding depth to the space. \n\nOn the right, a white door stands closed, its handle", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24561.6, "ram_available_mb": 38279.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24663.9, "ram_available_mb": 38177.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11579.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.32}, "power_stats": {"power_gpu_soc_mean_watts": 22.687, "power_cpu_cv_mean_watts": 1.776, "power_sys_5v0_mean_watts": 9.14, "gpu_utilization_percent_mean": 73.32, "power_watts_avg": 22.687, "energy_joules_est": 275.32, "duration_seconds": 12.136, "sample_count": 103}, "timestamp": "2026-01-25T22:29:21.308517"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8706.215, "latencies_ms": [8706.215], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "wall: 1\nmirror: 1\nshelf: 1\nfaucet: 1\nsink: 1\npipe: 1\ndoor: 1\nbags: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24533.6, "ram_available_mb": 38307.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24555.3, "ram_available_mb": 38285.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11613.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.466}, "power_stats": {"power_gpu_soc_mean_watts": 24.051, "power_cpu_cv_mean_watts": 1.492, "power_sys_5v0_mean_watts": 9.143, "gpu_utilization_percent_mean": 77.466, "power_watts_avg": 24.051, "energy_joules_est": 209.41, "duration_seconds": 8.707, "sample_count": 73}, "timestamp": "2026-01-25T22:29:32.059962"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9983.375, "latencies_ms": [9983.375], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "In the foreground, there is a white sink with a silver faucet on the left side, and a white door on the right side. In the background, there is a small black shelf with a few items on it, and a white wall with some stickers on it.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24555.3, "ram_available_mb": 38285.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24557.7, "ram_available_mb": 38283.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11624.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.452}, "power_stats": {"power_gpu_soc_mean_watts": 23.266, "power_cpu_cv_mean_watts": 1.659, "power_sys_5v0_mean_watts": 9.199, "gpu_utilization_percent_mean": 74.452, "power_watts_avg": 23.266, "energy_joules_est": 232.29, "duration_seconds": 9.984, "sample_count": 84}, "timestamp": "2026-01-25T22:29:44.079633"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7807.712, "latencies_ms": [7807.712], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image shows a small, cluttered bathroom with a white sink, a mirror, and a white door. There is a black bag on the floor next to the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24557.7, "ram_available_mb": 38283.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24551.7, "ram_available_mb": 38289.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11608.7, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.045}, "power_stats": {"power_gpu_soc_mean_watts": 24.294, "power_cpu_cv_mean_watts": 1.408, "power_sys_5v0_mean_watts": 9.149, "gpu_utilization_percent_mean": 79.045, "power_watts_avg": 24.294, "energy_joules_est": 189.69, "duration_seconds": 7.808, "sample_count": 66}, "timestamp": "2026-01-25T22:29:53.918349"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11635.297, "latencies_ms": [11635.297], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The image shows an interior space with a white sink and a white door. The lighting appears to be artificial, and the walls are a light color, possibly white or a light shade of beige. There is a small shelf with a few items on it, including a blue container and a white container. A black bag is also visible on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24551.7, "ram_available_mb": 38289.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24620.9, "ram_available_mb": 38220.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11606.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.434}, "power_stats": {"power_gpu_soc_mean_watts": 22.767, "power_cpu_cv_mean_watts": 1.776, "power_sys_5v0_mean_watts": 9.168, "gpu_utilization_percent_mean": 73.434, "power_watts_avg": 22.767, "energy_joules_est": 264.92, "duration_seconds": 11.636, "sample_count": 99}, "timestamp": "2026-01-25T22:30:07.613304"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11123.002, "latencies_ms": [11123.002], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young child is sitting on a bed, engrossed in watching something on a laptop. The child is positioned in the center of the bed, with the laptop placed in front of them. The laptop screen is turned on, and the child appears to be focused on the content displayed. The scene captures a moment of the child's leisure time, as", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 24534.0, "ram_available_mb": 38306.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24610.3, "ram_available_mb": 38230.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.632}, "power_stats": {"power_gpu_soc_mean_watts": 20.9, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 69.632, "power_watts_avg": 20.9, "energy_joules_est": 232.48, "duration_seconds": 11.124, "sample_count": 95}, "timestamp": "2026-01-25T22:30:20.764149"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7309.823, "latencies_ms": [7309.823], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "laptop: 1, child: 1, bed: 1, pillow: 1, blanket: 1, wall: 1, window: 1, curtain: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24536.7, "ram_available_mb": 38304.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24603.3, "ram_available_mb": 38237.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.935}, "power_stats": {"power_gpu_soc_mean_watts": 22.103, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 73.935, "power_watts_avg": 22.103, "energy_joules_est": 161.58, "duration_seconds": 7.31, "sample_count": 62}, "timestamp": "2026-01-25T22:30:30.090671"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9099.646, "latencies_ms": [9099.646], "images_per_second": 0.11, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The laptop is positioned on the left side of the image, slightly in the foreground, while the child is seated on the right side of the image, closer to the viewer. The background is a plain, light-colored wall that provides a neutral backdrop for the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24541.3, "ram_available_mb": 38299.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24534.8, "ram_available_mb": 38306.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.872}, "power_stats": {"power_gpu_soc_mean_watts": 21.584, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 70.872, "power_watts_avg": 21.584, "energy_joules_est": 196.42, "duration_seconds": 9.1, "sample_count": 78}, "timestamp": "2026-01-25T22:30:41.238356"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5622.451, "latencies_ms": [5622.451], "images_per_second": 0.178, "prompt_tokens": 37, "response_tokens_est": 28, "n_tiles": 16, "output_text": "A young child is sitting on a bed with a laptop in front of them. The child appears to be focused on the laptop screen.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24534.8, "ram_available_mb": 38306.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24551.9, "ram_available_mb": 38289.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.872}, "power_stats": {"power_gpu_soc_mean_watts": 24.08, "power_cpu_cv_mean_watts": 1.354, "power_sys_5v0_mean_watts": 9.056, "gpu_utilization_percent_mean": 73.872, "power_watts_avg": 24.08, "energy_joules_est": 135.4, "duration_seconds": 5.623, "sample_count": 47}, "timestamp": "2026-01-25T22:30:48.913895"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8339.697, "latencies_ms": [8339.697], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image is in black and white, with the laptop being the only object in color, creating a stark contrast against the white background. The lighting is soft and diffused, coming from the left side, casting gentle shadows to the right of the objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24551.9, "ram_available_mb": 38289.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24615.4, "ram_available_mb": 38225.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.761}, "power_stats": {"power_gpu_soc_mean_watts": 21.829, "power_cpu_cv_mean_watts": 1.754, "power_sys_5v0_mean_watts": 9.025, "gpu_utilization_percent_mean": 70.761, "power_watts_avg": 21.829, "energy_joules_est": 182.06, "duration_seconds": 8.34, "sample_count": 71}, "timestamp": "2026-01-25T22:30:59.294680"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12107.96, "latencies_ms": [12107.96], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is skiing down a snowy mountain slope. He is wearing a brown jacket and is in the process of making a turn on his skis. The skier is leaning to the right, indicating that he is turning right. The snowy mountain slope is surrounded by trees, creating a picturesque winter landscape. The skier appears to be enjoying", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24545.1, "ram_available_mb": 38295.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24634.6, "ram_available_mb": 38206.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.76}, "power_stats": {"power_gpu_soc_mean_watts": 22.457, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 9.141, "gpu_utilization_percent_mean": 72.76, "power_watts_avg": 22.457, "energy_joules_est": 271.93, "duration_seconds": 12.109, "sample_count": 104}, "timestamp": "2026-01-25T22:31:13.436845"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 12345.3, "latencies_ms": [12345.3], "images_per_second": 0.081, "prompt_tokens": 39, "response_tokens_est": 80, "n_tiles": 16, "output_text": "object: ski, count: 2\nobject: snow, count: unlimited\nobject: trees, count: unlimited\nobject: skier, count: 1\nobject: snow pants, count: 1\nobject: gloves, count: 1\nobject: goggles, count: 1\nobject: jacket, count: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24547.8, "ram_available_mb": 38293.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24569.3, "ram_available_mb": 38271.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.491}, "power_stats": {"power_gpu_soc_mean_watts": 22.673, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 9.078, "gpu_utilization_percent_mean": 73.491, "power_watts_avg": 22.673, "energy_joules_est": 279.92, "duration_seconds": 12.346, "sample_count": 106}, "timestamp": "2026-01-25T22:31:27.843013"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10716.085, "latencies_ms": [10716.085], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The skier is in the foreground, moving downhill on the snow-covered slope. The trees in the background are also covered in snow, indicating that the skier is at a higher elevation than the trees. The skier is closer to the camera than the trees, making them appear larger in the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24569.3, "ram_available_mb": 38271.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24559.8, "ram_available_mb": 38281.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.826}, "power_stats": {"power_gpu_soc_mean_watts": 22.998, "power_cpu_cv_mean_watts": 1.737, "power_sys_5v0_mean_watts": 9.181, "gpu_utilization_percent_mean": 73.826, "power_watts_avg": 22.998, "energy_joules_est": 246.46, "duration_seconds": 10.717, "sample_count": 92}, "timestamp": "2026-01-25T22:31:40.594304"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8520.703, "latencies_ms": [8520.703], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A skier is seen in action, carving through the snow on a mountain slope. The skier is wearing a brown jacket, blue pants, and a blue helmet with goggles.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24559.8, "ram_available_mb": 38281.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24570.3, "ram_available_mb": 38270.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.192}, "power_stats": {"power_gpu_soc_mean_watts": 24.023, "power_cpu_cv_mean_watts": 1.497, "power_sys_5v0_mean_watts": 9.134, "gpu_utilization_percent_mean": 77.192, "power_watts_avg": 24.023, "energy_joules_est": 204.71, "duration_seconds": 8.521, "sample_count": 73}, "timestamp": "2026-01-25T22:31:51.137587"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7564.249, "latencies_ms": [7564.249], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The skier is wearing a brown jacket and blue helmet, and is surrounded by snow-covered trees. The snow is falling gently, creating a beautiful winter scene.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24570.3, "ram_available_mb": 38270.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24649.9, "ram_available_mb": 38191.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.359}, "power_stats": {"power_gpu_soc_mean_watts": 24.311, "power_cpu_cv_mean_watts": 1.458, "power_sys_5v0_mean_watts": 9.232, "gpu_utilization_percent_mean": 77.359, "power_watts_avg": 24.311, "energy_joules_est": 183.91, "duration_seconds": 7.565, "sample_count": 64}, "timestamp": "2026-01-25T22:32:00.750220"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11110.543, "latencies_ms": [11110.543], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene hotel room bathed in soft light. Dominating the scene is a large bed, dressed in a pristine white comforter and adorned with two pillows, one a vibrant shade of green and the other a soothing blue. The bed is positioned against a wall, which is punctuated by a window draped", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24553.0, "ram_available_mb": 38287.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24621.1, "ram_available_mb": 38219.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.758}, "power_stats": {"power_gpu_soc_mean_watts": 20.993, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.948, "gpu_utilization_percent_mean": 69.758, "power_watts_avg": 20.993, "energy_joules_est": 233.26, "duration_seconds": 11.111, "sample_count": 95}, "timestamp": "2026-01-25T22:32:13.889958"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7825.887, "latencies_ms": [7825.887], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "bed: 1\nlamp: 2\nchair: 1\nsofa: 1\nwindow: 1\ncurtain: 1\npillow: 3\nbedspread: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24540.9, "ram_available_mb": 38300.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24558.8, "ram_available_mb": 38282.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.118}, "power_stats": {"power_gpu_soc_mean_watts": 22.371, "power_cpu_cv_mean_watts": 1.684, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 73.118, "power_watts_avg": 22.371, "energy_joules_est": 175.09, "duration_seconds": 7.827, "sample_count": 68}, "timestamp": "2026-01-25T22:32:23.780029"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11115.447, "latencies_ms": [11115.447], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The bed is in the foreground of the image, occupying the right side and extending from the bottom to the top of the frame. The chair is positioned in the background, to the left of the bed, and is smaller in size compared to the bed. The window is in the far background, behind the chair, and the curtains are drawn back, allowing natural light to fill", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24558.8, "ram_available_mb": 38282.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 24567.0, "ram_available_mb": 38273.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.191}, "power_stats": {"power_gpu_soc_mean_watts": 20.997, "power_cpu_cv_mean_watts": 2.377, "power_sys_5v0_mean_watts": 9.037, "gpu_utilization_percent_mean": 70.191, "power_watts_avg": 20.997, "energy_joules_est": 233.4, "duration_seconds": 11.116, "sample_count": 94}, "timestamp": "2026-01-25T22:32:36.918943"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9518.3, "latencies_ms": [9518.3], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image depicts a well-lit hotel room with a large bed in the center, flanked by two nightstands with lamps on them. There is a blue armchair and a suitcase on the left side of the room, suggesting that someone is staying in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24567.0, "ram_available_mb": 38273.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24628.5, "ram_available_mb": 38212.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.642}, "power_stats": {"power_gpu_soc_mean_watts": 21.547, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 70.642, "power_watts_avg": 21.547, "energy_joules_est": 205.1, "duration_seconds": 9.519, "sample_count": 81}, "timestamp": "2026-01-25T22:32:48.482545"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7207.217, "latencies_ms": [7207.217], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The room is well-lit with natural light coming from the window, which has white curtains. The bed has a white comforter and is adorned with two blue and two green pillows.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24628.5, "ram_available_mb": 38212.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24621.2, "ram_available_mb": 38219.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.525}, "power_stats": {"power_gpu_soc_mean_watts": 22.432, "power_cpu_cv_mean_watts": 1.647, "power_sys_5v0_mean_watts": 9.038, "gpu_utilization_percent_mean": 72.525, "power_watts_avg": 22.432, "energy_joules_est": 161.69, "duration_seconds": 7.208, "sample_count": 61}, "timestamp": "2026-01-25T22:32:57.733715"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10426.146, "latencies_ms": [10426.146], "images_per_second": 0.096, "prompt_tokens": 24, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image captures a snowy mountain slope with a group of people skiing and snowboarding. A skier is in the process of jumping over a red pipe, while other skiers and snowboarders are scattered across the slope. The scene is lively and full of action, with people enjoying the winter sports on the mountain.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24621.2, "ram_available_mb": 38219.7, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24622.2, "ram_available_mb": 38218.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.899}, "power_stats": {"power_gpu_soc_mean_watts": 20.158, "power_cpu_cv_mean_watts": 1.898, "power_sys_5v0_mean_watts": 8.931, "gpu_utilization_percent_mean": 70.899, "power_watts_avg": 20.158, "energy_joules_est": 210.19, "duration_seconds": 10.427, "sample_count": 89}, "timestamp": "2026-01-25T22:33:10.189150"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11281.312, "latencies_ms": [11281.312], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "object: skiers, count: 10\nobject: ski lifts, count: 2\nobject: snowboarders, count: 1\nobject: snow, count: uncountable\nobject: ski equipment, count: 1\nobject: ski tracks, count: uncountable\nobject: ski poles, count: 1\nobject: snow-cover", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24535.2, "ram_available_mb": 38305.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24557.5, "ram_available_mb": 38283.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.622}, "power_stats": {"power_gpu_soc_mean_watts": 21.0, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 70.622, "power_watts_avg": 21.0, "energy_joules_est": 236.92, "duration_seconds": 11.282, "sample_count": 98}, "timestamp": "2026-01-25T22:33:23.488702"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11128.971, "latencies_ms": [11128.971], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large red pipe that extends from the left side of the image towards the center, where a person is seen skiing down it. The background features a snowy slope with multiple skiers at various distances, some closer and others further away, creating a sense of depth. The sky is clear and occupies the upper part of the image, providing a vast,", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24557.5, "ram_available_mb": 38283.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24588.1, "ram_available_mb": 38252.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.649}, "power_stats": {"power_gpu_soc_mean_watts": 20.897, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.96, "gpu_utilization_percent_mean": 69.649, "power_watts_avg": 20.897, "energy_joules_est": 232.58, "duration_seconds": 11.13, "sample_count": 94}, "timestamp": "2026-01-25T22:33:36.645333"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10113.621, "latencies_ms": [10113.621], "images_per_second": 0.099, "prompt_tokens": 37, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image captures a dynamic scene at a ski resort where a skier is in the midst of performing a jump over a red, curved obstacle. Several other skiers can be seen in the background, some on the slopes and others in the air, enjoying the snowy terrain under a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24588.1, "ram_available_mb": 38252.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24577.4, "ram_available_mb": 38263.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.989}, "power_stats": {"power_gpu_soc_mean_watts": 21.276, "power_cpu_cv_mean_watts": 1.831, "power_sys_5v0_mean_watts": 8.919, "gpu_utilization_percent_mean": 70.989, "power_watts_avg": 21.276, "energy_joules_est": 215.19, "duration_seconds": 10.114, "sample_count": 87}, "timestamp": "2026-01-25T22:33:48.788404"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9207.899, "latencies_ms": [9207.899], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image captures a bright and sunny day at a ski resort with clear blue skies. The snow-covered slopes are bustling with skiers and snowboarders, and the main feature is a large red metal rail that they are using for jumps and tricks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24577.4, "ram_available_mb": 38263.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24613.1, "ram_available_mb": 38227.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.679}, "power_stats": {"power_gpu_soc_mean_watts": 21.497, "power_cpu_cv_mean_watts": 1.822, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 70.679, "power_watts_avg": 21.497, "energy_joules_est": 197.96, "duration_seconds": 9.209, "sample_count": 78}, "timestamp": "2026-01-25T22:34:00.054375"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11090.062, "latencies_ms": [11090.062], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene of urban street art. Dominating the left side of the frame is a wall, its surface a canvas for a graffiti mural. The mural is a riot of colors, with the word \"CHAKA\" boldly painted in white. The letters are large and prominent, drawing the eye immediately. \n\nTo the right", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24541.4, "ram_available_mb": 38299.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24605.8, "ram_available_mb": 38235.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.505}, "power_stats": {"power_gpu_soc_mean_watts": 20.881, "power_cpu_cv_mean_watts": 1.934, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 69.505, "power_watts_avg": 20.881, "energy_joules_est": 231.58, "duration_seconds": 11.091, "sample_count": 95}, "timestamp": "2026-01-25T22:34:13.175272"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6056.246, "latencies_ms": [6056.246], "images_per_second": 0.165, "prompt_tokens": 39, "response_tokens_est": 32, "n_tiles": 16, "output_text": "1. Graffiti: multiple instances\n2. Parking meter: 1\n3. Wall: large surface area covered with graffiti", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24543.9, "ram_available_mb": 38296.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24520.6, "ram_available_mb": 38320.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.412}, "power_stats": {"power_gpu_soc_mean_watts": 23.709, "power_cpu_cv_mean_watts": 1.46, "power_sys_5v0_mean_watts": 9.031, "gpu_utilization_percent_mean": 74.412, "power_watts_avg": 23.709, "energy_joules_est": 143.6, "duration_seconds": 6.057, "sample_count": 51}, "timestamp": "2026-01-25T22:34:21.253739"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10047.179, "latencies_ms": [10047.179], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The graffiti-covered wall serves as the background, with various tags and designs in the foreground. A metal pole with a parking meter is positioned in the foreground, slightly to the right of the center of the image. The pole is closer to the viewer than the wall, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24520.6, "ram_available_mb": 38320.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24535.4, "ram_available_mb": 38305.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.588}, "power_stats": {"power_gpu_soc_mean_watts": 21.299, "power_cpu_cv_mean_watts": 1.856, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 70.588, "power_watts_avg": 21.299, "energy_joules_est": 214.01, "duration_seconds": 10.048, "sample_count": 85}, "timestamp": "2026-01-25T22:34:33.347886"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8550.593, "latencies_ms": [8550.593], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a graffiti-covered wall with a metal gate in the background. A metal pole with a parking meter is in the foreground, and the graffiti includes the phrase \"THE ONE N' ONLY CHAKA.\"", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24535.4, "ram_available_mb": 38305.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24551.0, "ram_available_mb": 38289.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.111}, "power_stats": {"power_gpu_soc_mean_watts": 22.005, "power_cpu_cv_mean_watts": 1.724, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 71.111, "power_watts_avg": 22.005, "energy_joules_est": 188.17, "duration_seconds": 8.551, "sample_count": 72}, "timestamp": "2026-01-25T22:34:43.938144"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8263.908, "latencies_ms": [8263.908], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image features a wall covered in vibrant graffiti art with a mix of colors including black, white, red, and blue. A metal gate with vertical bars is partially visible in the background, and the wall appears to be made of concrete.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24551.0, "ram_available_mb": 38289.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24617.9, "ram_available_mb": 38223.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.183}, "power_stats": {"power_gpu_soc_mean_watts": 22.005, "power_cpu_cv_mean_watts": 1.726, "power_sys_5v0_mean_watts": 9.017, "gpu_utilization_percent_mean": 71.183, "power_watts_avg": 22.005, "energy_joules_est": 181.86, "duration_seconds": 8.265, "sample_count": 71}, "timestamp": "2026-01-25T22:34:54.260618"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11176.028, "latencies_ms": [11176.028], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility and adventure in the vast expanse of the ocean. A surfer, clad in a black wetsuit, is skillfully riding a wave. The surfer is crouched on a white surfboard, leaning into the wave with a sense of balance and control. The wave, a beautiful shade of blue,", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24527.7, "ram_available_mb": 38313.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24527.1, "ram_available_mb": 38313.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.863}, "power_stats": {"power_gpu_soc_mean_watts": 20.362, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.931, "gpu_utilization_percent_mean": 69.863, "power_watts_avg": 20.362, "energy_joules_est": 227.58, "duration_seconds": 11.177, "sample_count": 95}, "timestamp": "2026-01-25T22:35:07.509691"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8176.16, "latencies_ms": [8176.16], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "water: numerous\nsurfboard: 1\nwave: 1\nfoam: 1\nsurfer: 1\ndark area: 1\nlight area: 1\nshadow: 1\ncolor contrast: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24527.1, "ram_available_mb": 38313.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24561.7, "ram_available_mb": 38279.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.072}, "power_stats": {"power_gpu_soc_mean_watts": 22.258, "power_cpu_cv_mean_watts": 1.671, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 73.072, "power_watts_avg": 22.258, "energy_joules_est": 182.0, "duration_seconds": 8.177, "sample_count": 69}, "timestamp": "2026-01-25T22:35:17.698338"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11116.78, "latencies_ms": [11116.78], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The surfer is positioned on the right side of the image, riding a wave that is breaking towards the left. The wave is in the foreground, while the vast expanse of the ocean extends into the background, creating a sense of depth. The surfer appears small in comparison to the wave, emphasizing the wave's size and the distance between the surfer and the vie", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24561.7, "ram_available_mb": 38279.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24551.8, "ram_available_mb": 38289.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.716}, "power_stats": {"power_gpu_soc_mean_watts": 20.99, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.716, "power_watts_avg": 20.99, "energy_joules_est": 233.35, "duration_seconds": 11.117, "sample_count": 95}, "timestamp": "2026-01-25T22:35:30.833918"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6838.429, "latencies_ms": [6838.429], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A surfer is riding a wave in the deep blue ocean. The wave is breaking to the right, and the surfer is skillfully maneuvering the surfboard.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24551.8, "ram_available_mb": 38289.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24551.0, "ram_available_mb": 38289.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.259}, "power_stats": {"power_gpu_soc_mean_watts": 23.09, "power_cpu_cv_mean_watts": 1.546, "power_sys_5v0_mean_watts": 9.004, "gpu_utilization_percent_mean": 74.259, "power_watts_avg": 23.09, "energy_joules_est": 157.91, "duration_seconds": 6.839, "sample_count": 58}, "timestamp": "2026-01-25T22:35:39.686904"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5954.656, "latencies_ms": [5954.656], "images_per_second": 0.168, "prompt_tokens": 36, "response_tokens_est": 33, "n_tiles": 16, "output_text": "The surfer is riding a wave in the deep blue ocean under a clear sky. The wave is a bright white as it breaks around the surfer.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24551.0, "ram_available_mb": 38289.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24631.9, "ram_available_mb": 38209.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.1}, "power_stats": {"power_gpu_soc_mean_watts": 23.59, "power_cpu_cv_mean_watts": 1.538, "power_sys_5v0_mean_watts": 9.134, "gpu_utilization_percent_mean": 74.1, "power_watts_avg": 23.59, "energy_joules_est": 140.49, "duration_seconds": 5.955, "sample_count": 50}, "timestamp": "2026-01-25T22:35:47.683156"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11118.058, "latencies_ms": [11118.058], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a yellow double-decker bus parked at a bus stop on a city street. The bus is stopped next to a bus stop shelter, and a man is standing nearby, possibly waiting to board the bus. There are several other people in the scene, some of whom are standing near the bus stop, while others are walking or standing further away.\n\nIn the background,", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24631.9, "ram_available_mb": 38209.0, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24632.1, "ram_available_mb": 38208.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.585}, "power_stats": {"power_gpu_soc_mean_watts": 20.927, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.946, "gpu_utilization_percent_mean": 69.585, "power_watts_avg": 20.927, "energy_joules_est": 232.68, "duration_seconds": 11.119, "sample_count": 94}, "timestamp": "2026-01-25T22:36:00.867988"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7197.675, "latencies_ms": [7197.675], "images_per_second": 0.139, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "bus: 1, person: 2, flower: multiple, building: 1, sign: 1, bus stop: 1, license plate: 1, flower pot: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24632.1, "ram_available_mb": 38208.8, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24616.8, "ram_available_mb": 38224.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.77}, "power_stats": {"power_gpu_soc_mean_watts": 22.836, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 74.77, "power_watts_avg": 22.836, "energy_joules_est": 164.38, "duration_seconds": 7.198, "sample_count": 61}, "timestamp": "2026-01-25T22:36:10.092257"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10478.277, "latencies_ms": [10478.277], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The bus is in the foreground, parked on the side of the road, with a clear view of its front and destination sign. In the background, there is a bus stop shelter with a person standing in front of it, and a building can be seen further back. The bus is closer to the camera than the bus stop shelter and the building.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24544.9, "ram_available_mb": 38296.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24653.2, "ram_available_mb": 38187.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.011}, "power_stats": {"power_gpu_soc_mean_watts": 21.194, "power_cpu_cv_mean_watts": 1.872, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 70.011, "power_watts_avg": 21.194, "energy_joules_est": 222.09, "duration_seconds": 10.479, "sample_count": 89}, "timestamp": "2026-01-25T22:36:22.616739"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7181.822, "latencies_ms": [7181.822], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A double-decker bus is parked at a bus stop with a man waiting to board. The bus displays the route number 11 and the destination \"Lyham & St Rines\".", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24529.5, "ram_available_mb": 38311.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 7.4, "ram_used_mb": 24645.0, "ram_available_mb": 38195.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.787}, "power_stats": {"power_gpu_soc_mean_watts": 22.808, "power_cpu_cv_mean_watts": 1.648, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 72.787, "power_watts_avg": 22.808, "energy_joules_est": 163.82, "duration_seconds": 7.182, "sample_count": 61}, "timestamp": "2026-01-25T22:36:31.850868"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9258.79, "latencies_ms": [9258.79], "images_per_second": 0.108, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image features a vibrant yellow double-decker bus with the destination \"Lyham & St Rnes\" displayed on its front. The bus is parked at a bus stop with a blue shelter, and the weather appears to be overcast with a wet ground, suggesting recent rain.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24558.2, "ram_available_mb": 38282.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 24639.9, "ram_available_mb": 38201.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.577}, "power_stats": {"power_gpu_soc_mean_watts": 21.639, "power_cpu_cv_mean_watts": 2.182, "power_sys_5v0_mean_watts": 9.016, "gpu_utilization_percent_mean": 71.577, "power_watts_avg": 21.639, "energy_joules_est": 200.36, "duration_seconds": 9.259, "sample_count": 78}, "timestamp": "2026-01-25T22:36:43.128548"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11147.332, "latencies_ms": [11147.332], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of serenity in the sky, featuring a small yellow and blue biplane with the registration number SP-AWF. The biplane, with its vibrant colors, is soaring through the sky, its wings spread wide as it ascends into the air. The tail of the biplane is painted in a striking red color, adding a pop of color", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24639.9, "ram_available_mb": 38201.0, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24568.2, "ram_available_mb": 38272.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.947}, "power_stats": {"power_gpu_soc_mean_watts": 20.825, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.939, "gpu_utilization_percent_mean": 68.947, "power_watts_avg": 20.825, "energy_joules_est": 232.16, "duration_seconds": 11.148, "sample_count": 95}, "timestamp": "2026-01-25T22:36:56.331884"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7730.86, "latencies_ms": [7730.86], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "airplane: 1, wing: 2, propeller: 1, tail: 1, engine: 1, landing gear: 2, fuselage: 1, tail fin: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24568.2, "ram_available_mb": 38272.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24533.0, "ram_available_mb": 38307.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.446}, "power_stats": {"power_gpu_soc_mean_watts": 22.385, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 72.446, "power_watts_avg": 22.385, "energy_joules_est": 173.07, "duration_seconds": 7.732, "sample_count": 65}, "timestamp": "2026-01-25T22:37:06.096811"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9991.934, "latencies_ms": [9991.934], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The airplane is in the foreground, flying against a backdrop of a cloudy sky. It is positioned slightly to the left of the center of the image, with its wings spread wide and its nose pointed upwards. The clouds in the background appear to be at a distance, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24533.0, "ram_available_mb": 38307.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24564.6, "ram_available_mb": 38276.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.271}, "power_stats": {"power_gpu_soc_mean_watts": 21.32, "power_cpu_cv_mean_watts": 1.861, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 70.271, "power_watts_avg": 21.32, "energy_joules_est": 213.04, "duration_seconds": 9.993, "sample_count": 85}, "timestamp": "2026-01-25T22:37:18.119515"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9367.698, "latencies_ms": [9367.698], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "A yellow and blue biplane with the registration SP-AWF is captured in mid-flight against a backdrop of a cloudy sky. The aircraft is equipped with a single propeller and appears to be in good condition, suggesting it may be used for recreational flying or training purposes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24564.6, "ram_available_mb": 38276.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24652.0, "ram_available_mb": 38188.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.259}, "power_stats": {"power_gpu_soc_mean_watts": 21.553, "power_cpu_cv_mean_watts": 1.805, "power_sys_5v0_mean_watts": 8.97, "gpu_utilization_percent_mean": 71.259, "power_watts_avg": 21.553, "energy_joules_est": 201.92, "duration_seconds": 9.368, "sample_count": 81}, "timestamp": "2026-01-25T22:37:29.531934"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8282.939, "latencies_ms": [8282.939], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The airplane in the image is a small, single-engine, propeller-driven aircraft with a yellow and red color scheme. It is flying in a cloudy sky with its landing gear extended, indicating that it may be preparing for landing.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24652.0, "ram_available_mb": 38188.9, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24639.2, "ram_available_mb": 38201.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.042}, "power_stats": {"power_gpu_soc_mean_watts": 21.955, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 9.038, "gpu_utilization_percent_mean": 72.042, "power_watts_avg": 21.955, "energy_joules_est": 181.86, "duration_seconds": 8.284, "sample_count": 71}, "timestamp": "2026-01-25T22:37:39.877987"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11109.868, "latencies_ms": [11109.868], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment from an airplane's perspective, just before it begins its descent. The airplane's wing is prominently visible in the foreground, with its blue and white colors standing out against the clear blue sky. The wing's design and structure are clearly visible, indicating the plane's readiness for landing.\n\nBelow the wing, a", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24541.2, "ram_available_mb": 38299.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24552.2, "ram_available_mb": 38288.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.453}, "power_stats": {"power_gpu_soc_mean_watts": 20.924, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 9.003, "gpu_utilization_percent_mean": 69.453, "power_watts_avg": 20.924, "energy_joules_est": 232.48, "duration_seconds": 11.11, "sample_count": 95}, "timestamp": "2026-01-25T22:37:53.041932"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11312.952, "latencies_ms": [11312.952], "images_per_second": 0.088, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "- Cars: numerous, exact count not visible\n- Parking lot: large, exact area not visible\n- Trees: scattered throughout, exact count not visible\n- Buildings: several, exact number not visible\n- Airport: visible in the background, exact size not visible\n- Clouds: scattered across the sky, exact number not visible\n- Runway: visible in the", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24552.2, "ram_available_mb": 38288.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24619.3, "ram_available_mb": 38221.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.876}, "power_stats": {"power_gpu_soc_mean_watts": 21.013, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.91, "gpu_utilization_percent_mean": 69.876, "power_watts_avg": 21.013, "energy_joules_est": 237.73, "duration_seconds": 11.314, "sample_count": 97}, "timestamp": "2026-01-25T22:38:06.408725"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11146.168, "latencies_ms": [11146.168], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The airplane wing is in the foreground on the left side of the image, indicating it is closer to the viewer than the parking lot and buildings in the background. The parking lot is situated in the middle ground, occupying a large portion of the image, while the buildings are in the background, with one prominent building to the left of the parking lot and another to the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24557.4, "ram_available_mb": 38283.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24633.6, "ram_available_mb": 38207.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.663}, "power_stats": {"power_gpu_soc_mean_watts": 20.853, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 69.663, "power_watts_avg": 20.853, "energy_joules_est": 232.45, "duration_seconds": 11.147, "sample_count": 95}, "timestamp": "2026-01-25T22:38:19.575843"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9877.629, "latencies_ms": [9877.629], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image captures an aerial view of a bustling airport parking lot filled with numerous cars, with a large airplane wing visible in the foreground. The scene is set against a backdrop of a cityscape with buildings and green fields, indicating that the airport is located near urban areas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24530.1, "ram_available_mb": 38310.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24544.1, "ram_available_mb": 38296.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.417}, "power_stats": {"power_gpu_soc_mean_watts": 21.397, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.929, "gpu_utilization_percent_mean": 71.417, "power_watts_avg": 21.397, "energy_joules_est": 211.37, "duration_seconds": 9.878, "sample_count": 84}, "timestamp": "2026-01-25T22:38:31.508822"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6526.542, "latencies_ms": [6526.542], "images_per_second": 0.153, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image shows a clear day with a bright blue sky and a few scattered clouds. The sunlight is casting shadows on the ground, indicating it is either morning or late afternoon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24544.1, "ram_available_mb": 38296.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24522.8, "ram_available_mb": 38318.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.679}, "power_stats": {"power_gpu_soc_mean_watts": 22.92, "power_cpu_cv_mean_watts": 1.58, "power_sys_5v0_mean_watts": 9.068, "gpu_utilization_percent_mean": 73.679, "power_watts_avg": 22.92, "energy_joules_est": 149.6, "duration_seconds": 6.527, "sample_count": 56}, "timestamp": "2026-01-25T22:38:40.065175"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11129.501, "latencies_ms": [11129.501], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is holding a pink flip phone in their hand. The phone is turned on, displaying a picture of a woman. The person is sitting on a couch, and there is a cup on the couch next to them. Another person is also present in the scene, sitting on a chair nearby. The overall atmosphere suggests a casual, relaxed setting.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24522.8, "ram_available_mb": 38318.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24619.2, "ram_available_mb": 38221.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.211}, "power_stats": {"power_gpu_soc_mean_watts": 20.916, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.953, "gpu_utilization_percent_mean": 69.211, "power_watts_avg": 20.916, "energy_joules_est": 232.8, "duration_seconds": 11.13, "sample_count": 95}, "timestamp": "2026-01-25T22:38:53.245566"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7376.162, "latencies_ms": [7376.162], "images_per_second": 0.136, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "person: 2, hand: 2, remote control: 1, cup: 1, couch: 1, blanket: 1, dog: 1, bone: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24547.7, "ram_available_mb": 38293.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24557.1, "ram_available_mb": 38283.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.548}, "power_stats": {"power_gpu_soc_mean_watts": 22.681, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 9.008, "gpu_utilization_percent_mean": 73.548, "power_watts_avg": 22.681, "energy_joules_est": 167.31, "duration_seconds": 7.377, "sample_count": 62}, "timestamp": "2026-01-25T22:39:02.645294"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9991.287, "latencies_ms": [9991.287], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, a person is holding a pink flip phone with a picture of a girl on the screen. Behind them, another person is seated on a couch, partially visible, with a white cup on a table nearby. The setting appears to be a living room with a casual, relaxed atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24557.1, "ram_available_mb": 38283.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24637.5, "ram_available_mb": 38203.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.238}, "power_stats": {"power_gpu_soc_mean_watts": 21.393, "power_cpu_cv_mean_watts": 1.864, "power_sys_5v0_mean_watts": 8.997, "gpu_utilization_percent_mean": 70.238, "power_watts_avg": 21.393, "energy_joules_est": 213.76, "duration_seconds": 9.992, "sample_count": 84}, "timestamp": "2026-01-25T22:39:14.670565"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8463.673, "latencies_ms": [8463.673], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "A person is holding a pink flip phone with a picture of a girl on the screen, while another person's hand is visible in the background. The scene appears to be in a living room with a couch and a cup on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24533.8, "ram_available_mb": 38307.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24636.1, "ram_available_mb": 38204.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.507}, "power_stats": {"power_gpu_soc_mean_watts": 21.538, "power_cpu_cv_mean_watts": 1.692, "power_sys_5v0_mean_watts": 8.932, "gpu_utilization_percent_mean": 73.507, "power_watts_avg": 21.538, "energy_joules_est": 182.31, "duration_seconds": 8.465, "sample_count": 71}, "timestamp": "2026-01-25T22:39:25.160945"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7542.594, "latencies_ms": [7542.594], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a person holding a pink flip phone with a screen displaying a cartoon character. The phone is held in a dimly lit room with a red object and a white cup on a table in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24636.1, "ram_available_mb": 38204.8, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24636.8, "ram_available_mb": 38204.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.297}, "power_stats": {"power_gpu_soc_mean_watts": 22.327, "power_cpu_cv_mean_watts": 1.677, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 72.297, "power_watts_avg": 22.327, "energy_joules_est": 168.42, "duration_seconds": 7.543, "sample_count": 64}, "timestamp": "2026-01-25T22:39:34.724082"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11145.827, "latencies_ms": [11145.827], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a dry savanna, two zebras stand in a field of tall, dry grass. The zebra on the left, with its black and white stripes, is facing the camera, its gaze as sharp as its stripes. Its companion, on the right, is turned away, its attention seemingly caught by something in the distance. The field they inhab", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24636.8, "ram_available_mb": 38204.1, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24658.3, "ram_available_mb": 38182.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.453}, "power_stats": {"power_gpu_soc_mean_watts": 20.86, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.926, "gpu_utilization_percent_mean": 69.453, "power_watts_avg": 20.86, "energy_joules_est": 232.51, "duration_seconds": 11.146, "sample_count": 95}, "timestamp": "2026-01-25T22:39:47.908162"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7621.699, "latencies_ms": [7621.699], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "zebra: 2, grass: many, trees: scattered, bushes: scattered, sky: visible, mountains: in the distance, sun: shining, shadows: visible, wild animals: 2", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24527.9, "ram_available_mb": 38313.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24634.9, "ram_available_mb": 38206.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.641}, "power_stats": {"power_gpu_soc_mean_watts": 22.396, "power_cpu_cv_mean_watts": 1.639, "power_sys_5v0_mean_watts": 9.004, "gpu_utilization_percent_mean": 73.641, "power_watts_avg": 22.396, "energy_joules_est": 170.71, "duration_seconds": 7.623, "sample_count": 64}, "timestamp": "2026-01-25T22:39:57.566511"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10425.142, "latencies_ms": [10425.142], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The two zebras are positioned in the foreground of the image, standing in a field of tall, dry grass. They are relatively close to each other, with one zebra slightly ahead of the other. In the background, there are trees and shrubs, indicating that the zebras are in a natural habitat with vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24634.9, "ram_available_mb": 38206.0, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24635.9, "ram_available_mb": 38205.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.207}, "power_stats": {"power_gpu_soc_mean_watts": 20.374, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.935, "gpu_utilization_percent_mean": 71.207, "power_watts_avg": 20.374, "energy_joules_est": 212.42, "duration_seconds": 10.426, "sample_count": 87}, "timestamp": "2026-01-25T22:40:10.007875"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7513.073, "latencies_ms": [7513.073], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "Two zebras are standing in a field of tall, dry grass with trees and bushes in the background. The zebras appear to be in a natural habitat, possibly a savannah or grassland.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24548.9, "ram_available_mb": 38292.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24564.9, "ram_available_mb": 38276.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.453}, "power_stats": {"power_gpu_soc_mean_watts": 22.559, "power_cpu_cv_mean_watts": 1.633, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 72.453, "power_watts_avg": 22.559, "energy_joules_est": 169.5, "duration_seconds": 7.514, "sample_count": 64}, "timestamp": "2026-01-25T22:40:19.540038"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7880.173, "latencies_ms": [7880.173], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image features two zebras standing in a field of tall, dry grass with a backdrop of trees and shrubs. The lighting is bright and natural, indicating that the photo was taken during the day in a sunny environment.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24564.9, "ram_available_mb": 38276.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24519.7, "ram_available_mb": 38321.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.239}, "power_stats": {"power_gpu_soc_mean_watts": 22.015, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 9.013, "gpu_utilization_percent_mean": 72.239, "power_watts_avg": 22.015, "energy_joules_est": 173.5, "duration_seconds": 7.881, "sample_count": 67}, "timestamp": "2026-01-25T22:40:29.455075"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11149.439, "latencies_ms": [11149.439], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seen walking into the ocean with a yellow surfboard under his arm. He is wearing black shorts and appears to be heading towards the waves. The ocean is a deep blue color, and the waves are white and foamy. The sky is clear and blue, indicating a sunny day. The man seems to be preparing to surf, as", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24519.7, "ram_available_mb": 38321.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 9.1, "ram_used_mb": 24534.0, "ram_available_mb": 38306.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.196}, "power_stats": {"power_gpu_soc_mean_watts": 20.788, "power_cpu_cv_mean_watts": 2.403, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 69.196, "power_watts_avg": 20.788, "energy_joules_est": 231.79, "duration_seconds": 11.15, "sample_count": 97}, "timestamp": "2026-01-25T22:40:42.649975"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6965.722, "latencies_ms": [6965.722], "images_per_second": 0.144, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "person: 1, surfboard: 1, wave: multiple, ocean: 1, sky: 1, sun: 1, sand: 1, water: multiple", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24534.0, "ram_available_mb": 38306.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24561.7, "ram_available_mb": 38279.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.237}, "power_stats": {"power_gpu_soc_mean_watts": 22.994, "power_cpu_cv_mean_watts": 1.575, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 74.237, "power_watts_avg": 22.994, "energy_joules_est": 160.18, "duration_seconds": 6.966, "sample_count": 59}, "timestamp": "2026-01-25T22:40:51.663477"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9303.17, "latencies_ms": [9303.17], "images_per_second": 0.107, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground on the left side of the image, walking towards the right side where the waves are breaking in the background. The waves appear closer to the viewer and are more detailed, while the surfer and the sandy beach are in the background and less detailed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24561.7, "ram_available_mb": 38279.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24625.3, "ram_available_mb": 38215.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.709}, "power_stats": {"power_gpu_soc_mean_watts": 21.586, "power_cpu_cv_mean_watts": 1.84, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 70.709, "power_watts_avg": 21.586, "energy_joules_est": 200.83, "duration_seconds": 9.304, "sample_count": 79}, "timestamp": "2026-01-25T22:41:02.987944"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7087.451, "latencies_ms": [7087.451], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A man is seen walking into the ocean with a yellow surfboard, likely preparing to surf. The waves are crashing onto the shore, creating a beautiful scene for the surfer.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24625.3, "ram_available_mb": 38215.6, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24550.4, "ram_available_mb": 38290.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.25}, "power_stats": {"power_gpu_soc_mean_watts": 22.774, "power_cpu_cv_mean_watts": 1.568, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 74.25, "power_watts_avg": 22.774, "energy_joules_est": 161.42, "duration_seconds": 7.088, "sample_count": 60}, "timestamp": "2026-01-25T22:41:12.131592"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8430.061, "latencies_ms": [8430.061], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image features a person holding a bright yellow surfboard, standing in the shallow water of a beach. The sky is clear and the sunlight is reflecting off the water's surface, creating a serene and inviting atmosphere for surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24550.4, "ram_available_mb": 38290.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24626.1, "ram_available_mb": 38214.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.634}, "power_stats": {"power_gpu_soc_mean_watts": 21.874, "power_cpu_cv_mean_watts": 1.749, "power_sys_5v0_mean_watts": 9.017, "gpu_utilization_percent_mean": 71.634, "power_watts_avg": 21.874, "energy_joules_est": 184.41, "duration_seconds": 8.431, "sample_count": 71}, "timestamp": "2026-01-25T22:41:22.603105"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11131.962, "latencies_ms": [11131.962], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a black and white cow stands on a sandy beach. The cow is positioned in the center of the frame, facing the camera. The beach is sandy, and the ocean can be seen in the background. The cow appears to be calmly standing on the beach, with no signs of distress or discomfort. The image captures a unique and serene", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24537.5, "ram_available_mb": 38303.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24551.0, "ram_available_mb": 38289.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.083}, "power_stats": {"power_gpu_soc_mean_watts": 20.826, "power_cpu_cv_mean_watts": 1.919, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 69.083, "power_watts_avg": 20.826, "energy_joules_est": 231.85, "duration_seconds": 11.133, "sample_count": 96}, "timestamp": "2026-01-25T22:41:35.782985"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6736.858, "latencies_ms": [6736.858], "images_per_second": 0.148, "prompt_tokens": 39, "response_tokens_est": 38, "n_tiles": 16, "output_text": "cow: 1, rock: 1, sand: many, water: many, sky: not visible, grass: not visible, tree: not visible, cloud: not visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24551.0, "ram_available_mb": 38289.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24613.6, "ram_available_mb": 38227.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.351}, "power_stats": {"power_gpu_soc_mean_watts": 23.066, "power_cpu_cv_mean_watts": 1.553, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 73.351, "power_watts_avg": 23.066, "energy_joules_est": 155.41, "duration_seconds": 6.737, "sample_count": 57}, "timestamp": "2026-01-25T22:41:44.582018"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10447.692, "latencies_ms": [10447.692], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The cow is positioned in the foreground of the image, standing on the sandy ground. The background consists of a vast expanse of water, which appears to be a beach or a shoreline. The cow is near the water's edge, with a small rock or piece of debris visible in the foreground to its left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24526.7, "ram_available_mb": 38314.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24564.1, "ram_available_mb": 38276.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.831}, "power_stats": {"power_gpu_soc_mean_watts": 21.146, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 69.831, "power_watts_avg": 21.146, "energy_joules_est": 220.94, "duration_seconds": 10.448, "sample_count": 89}, "timestamp": "2026-01-25T22:41:57.068149"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6410.852, "latencies_ms": [6410.852], "images_per_second": 0.156, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A black and white image features a cow standing on a sandy beach with the ocean in the background. The cow appears to be calmly enjoying the beach environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24564.1, "ram_available_mb": 38276.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24527.8, "ram_available_mb": 38313.1, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.074}, "power_stats": {"power_gpu_soc_mean_watts": 23.1, "power_cpu_cv_mean_watts": 1.468, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 74.074, "power_watts_avg": 23.1, "energy_joules_est": 148.11, "duration_seconds": 6.412, "sample_count": 54}, "timestamp": "2026-01-25T22:42:05.504043"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11115.772, "latencies_ms": [11115.772], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a black and white photograph featuring a cow standing on a beach. The cow is predominantly black with white patches on its face and legs. The lighting in the image is natural and appears to be coming from the side, casting shadows on the cow and the sand. The beach appears to be wet, possibly due to the tide, and there is a small rock", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24527.8, "ram_available_mb": 38313.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24614.6, "ram_available_mb": 38226.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.404}, "power_stats": {"power_gpu_soc_mean_watts": 20.911, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 69.404, "power_watts_avg": 20.911, "energy_joules_est": 232.46, "duration_seconds": 11.116, "sample_count": 94}, "timestamp": "2026-01-25T22:42:18.642023"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12176.896, "latencies_ms": [12176.896], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photograph, a woman is the central figure, standing on a snowy mountain. She is dressed in a white blouse adorned with black floral embellishments and black pants. Her feet are clad in black boots, and she is holding a pair of skis in her left hand. In her right hand, she holds a ski pole.", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 24528.9, "ram_available_mb": 38312.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24550.6, "ram_available_mb": 38290.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11579.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.524}, "power_stats": {"power_gpu_soc_mean_watts": 22.239, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 9.129, "gpu_utilization_percent_mean": 73.524, "power_watts_avg": 22.239, "energy_joules_est": 270.82, "duration_seconds": 12.178, "sample_count": 103}, "timestamp": "2026-01-25T22:42:32.851170"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9807.689, "latencies_ms": [9807.689], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "woman: 1, ski poles: 2, skis: 2, snow: multiple patches, trees: multiple, snow pants: 1, white blouse: 1, black pants: 1, black shoes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24550.6, "ram_available_mb": 38290.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24630.7, "ram_available_mb": 38210.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11613.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.627}, "power_stats": {"power_gpu_soc_mean_watts": 23.517, "power_cpu_cv_mean_watts": 1.587, "power_sys_5v0_mean_watts": 9.128, "gpu_utilization_percent_mean": 76.627, "power_watts_avg": 23.517, "energy_joules_est": 230.66, "duration_seconds": 9.808, "sample_count": 83}, "timestamp": "2026-01-25T22:42:44.673794"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10983.73, "latencies_ms": [10983.73], "images_per_second": 0.091, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The skier is positioned in the foreground, standing on the snow with skis and poles in hand. The background features a mountainous landscape with trees, suggesting a snowy, outdoor setting. The skier appears to be at a distance from the viewer, with the mountain and trees further away in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24630.7, "ram_available_mb": 38210.2, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24589.7, "ram_available_mb": 38251.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11624.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.742}, "power_stats": {"power_gpu_soc_mean_watts": 22.799, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 9.195, "gpu_utilization_percent_mean": 73.742, "power_watts_avg": 22.799, "energy_joules_est": 250.43, "duration_seconds": 10.984, "sample_count": 93}, "timestamp": "2026-01-25T22:42:57.701824"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9680.771, "latencies_ms": [9680.771], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "A woman is standing in a snowy landscape, holding ski poles and dressed in winter attire, suggesting she is ready for skiing. The background features a cloudy sky and pine trees, indicating a mountainous or forested area suitable for skiing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24527.8, "ram_available_mb": 38313.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24562.8, "ram_available_mb": 38278.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11608.7, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.53}, "power_stats": {"power_gpu_soc_mean_watts": 23.572, "power_cpu_cv_mean_watts": 1.602, "power_sys_5v0_mean_watts": 9.125, "gpu_utilization_percent_mean": 76.53, "power_watts_avg": 23.572, "energy_joules_est": 228.22, "duration_seconds": 9.682, "sample_count": 83}, "timestamp": "2026-01-25T22:43:09.405537"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8851.605, "latencies_ms": [8851.605], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image is in black and white, featuring a person dressed in winter attire, standing in a snowy landscape. The lighting appears to be natural, coming from the side, casting shadows to the right of the person and objects.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24562.8, "ram_available_mb": 38278.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24642.0, "ram_available_mb": 38198.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11606.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.747}, "power_stats": {"power_gpu_soc_mean_watts": 23.583, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 9.189, "gpu_utilization_percent_mean": 75.747, "power_watts_avg": 23.583, "energy_joules_est": 208.76, "duration_seconds": 8.852, "sample_count": 75}, "timestamp": "2026-01-25T22:43:20.317275"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11143.648, "latencies_ms": [11143.648], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a dog is standing on a sandy beach, holding a yellow frisbee in its mouth. The dog appears to be a medium-sized breed, possibly a collie, with a white and gray coat. The beach is sandy and the ocean can be seen in the background, with waves crashing onto the shore. There are a few people visible in", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24518.2, "ram_available_mb": 38322.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24525.0, "ram_available_mb": 38315.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 68.905}, "power_stats": {"power_gpu_soc_mean_watts": 20.86, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.975, "gpu_utilization_percent_mean": 68.905, "power_watts_avg": 20.86, "energy_joules_est": 232.47, "duration_seconds": 11.144, "sample_count": 95}, "timestamp": "2026-01-25T22:43:33.516326"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7067.664, "latencies_ms": [7067.664], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "dog: 1, frisbee: 1, sand: numerous, water: multiple, waves: multiple, beach: 1, people: 1, rock formation: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24525.0, "ram_available_mb": 38315.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24618.6, "ram_available_mb": 38222.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.817}, "power_stats": {"power_gpu_soc_mean_watts": 22.921, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 73.817, "power_watts_avg": 22.921, "energy_joules_est": 162.01, "duration_seconds": 7.068, "sample_count": 60}, "timestamp": "2026-01-25T22:43:42.626535"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10225.54, "latencies_ms": [10225.54], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "In the foreground, there is a dog holding a yellow frisbee in its mouth. The dog is positioned on the sandy beach, which is in the middle ground of the image. In the background, there is the ocean with waves crashing onto the shore, and a rock formation can be seen further out in the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24530.1, "ram_available_mb": 38310.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24637.9, "ram_available_mb": 38203.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.169}, "power_stats": {"power_gpu_soc_mean_watts": 21.17, "power_cpu_cv_mean_watts": 1.89, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 70.169, "power_watts_avg": 21.17, "energy_joules_est": 216.49, "duration_seconds": 10.226, "sample_count": 89}, "timestamp": "2026-01-25T22:43:54.868053"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6781.229, "latencies_ms": [6781.229], "images_per_second": 0.147, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A dog is standing on a sandy beach holding a yellow frisbee in its mouth. The ocean can be seen in the background with a small island visible in the distance.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24637.9, "ram_available_mb": 38203.0, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24647.4, "ram_available_mb": 38193.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.93}, "power_stats": {"power_gpu_soc_mean_watts": 22.529, "power_cpu_cv_mean_watts": 1.532, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 73.93, "power_watts_avg": 22.529, "energy_joules_est": 152.79, "duration_seconds": 6.782, "sample_count": 57}, "timestamp": "2026-01-25T22:44:03.706579"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7194.508, "latencies_ms": [7194.508], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The dog is holding a yellow frisbee on a sandy beach with turquoise water and a cloudy sky in the background. The beach appears to be deserted with no people in sight.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24585.7, "ram_available_mb": 38255.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24625.3, "ram_available_mb": 38215.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.164}, "power_stats": {"power_gpu_soc_mean_watts": 22.584, "power_cpu_cv_mean_watts": 1.654, "power_sys_5v0_mean_watts": 9.036, "gpu_utilization_percent_mean": 73.164, "power_watts_avg": 22.584, "energy_joules_est": 162.5, "duration_seconds": 7.195, "sample_count": 61}, "timestamp": "2026-01-25T22:44:12.916156"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11126.482, "latencies_ms": [11126.482], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a group of people in a kitchen setting. There are at least eight individuals visible, with some standing and others sitting. The kitchen is equipped with a stainless steel refrigerator, a stove, and various cooking utensils and containers. The walls are made of concrete, and there is a window on the right side of the image. The", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24530.1, "ram_available_mb": 38310.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24539.2, "ram_available_mb": 38301.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.354}, "power_stats": {"power_gpu_soc_mean_watts": 20.895, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 69.354, "power_watts_avg": 20.895, "energy_joules_est": 232.5, "duration_seconds": 11.127, "sample_count": 96}, "timestamp": "2026-01-25T22:44:26.095820"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7729.692, "latencies_ms": [7729.692], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "pot: 1, bowl: 1, refrigerator: 1, box: 1, container: 1, cutting board: 1, utensil: 1, container: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24539.2, "ram_available_mb": 38301.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24617.6, "ram_available_mb": 38223.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.508}, "power_stats": {"power_gpu_soc_mean_watts": 22.443, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 71.508, "power_watts_avg": 22.443, "energy_joules_est": 173.49, "duration_seconds": 7.73, "sample_count": 65}, "timestamp": "2026-01-25T22:44:35.874077"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11093.361, "latencies_ms": [11093.361], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large pot on a stove with a lid, and to the left, there is a cutting board with a knife and some food items. In the background, there is a refrigerator and a group of people standing around it. The people are positioned in a way that suggests they are gathered around the refrigerator, possibly for a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24529.1, "ram_available_mb": 38311.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24540.7, "ram_available_mb": 38300.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.084}, "power_stats": {"power_gpu_soc_mean_watts": 20.996, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 70.084, "power_watts_avg": 20.996, "energy_joules_est": 232.93, "duration_seconds": 11.094, "sample_count": 95}, "timestamp": "2026-01-25T22:44:49.023415"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10076.923, "latencies_ms": [10076.923], "images_per_second": 0.099, "prompt_tokens": 37, "response_tokens_est": 68, "n_tiles": 16, "output_text": "A group of people, including at least one in military uniform, are gathered in a kitchen with a refrigerator, stove, and various kitchen items, suggesting a communal or social event. The setting appears to be a makeshift or temporary kitchen, possibly in a non-residential or outdoor area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24540.7, "ram_available_mb": 38300.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24524.3, "ram_available_mb": 38316.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.259}, "power_stats": {"power_gpu_soc_mean_watts": 21.403, "power_cpu_cv_mean_watts": 1.814, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 71.259, "power_watts_avg": 21.403, "energy_joules_est": 215.69, "duration_seconds": 10.078, "sample_count": 85}, "timestamp": "2026-01-25T22:45:01.143615"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6986.658, "latencies_ms": [6986.658], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image shows a group of people in a kitchen with concrete walls and a metal refrigerator. The lighting is artificial, and the materials visible include metal, wood, and various kitchen items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24524.3, "ram_available_mb": 38316.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24613.7, "ram_available_mb": 38227.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.458}, "power_stats": {"power_gpu_soc_mean_watts": 22.633, "power_cpu_cv_mean_watts": 1.622, "power_sys_5v0_mean_watts": 9.048, "gpu_utilization_percent_mean": 72.458, "power_watts_avg": 22.633, "energy_joules_est": 158.14, "duration_seconds": 6.987, "sample_count": 59}, "timestamp": "2026-01-25T22:45:10.187924"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11113.226, "latencies_ms": [11113.226], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and well-organized bathroom. Dominating the scene is a pristine white toilet, its lid closed, standing against a wall adorned with beige tiles. Above the toilet, a silver towel rack is mounted, holding two neatly folded white towels. The rack is positioned above", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24525.2, "ram_available_mb": 38315.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24633.3, "ram_available_mb": 38207.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.842}, "power_stats": {"power_gpu_soc_mean_watts": 20.872, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 69.842, "power_watts_avg": 20.872, "energy_joules_est": 231.97, "duration_seconds": 11.114, "sample_count": 95}, "timestamp": "2026-01-25T22:45:23.358671"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11349.701, "latencies_ms": [11349.701], "images_per_second": 0.088, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "toilet: 1, toilet paper: 1, toilet brush: 1, toilet paper holder: 1, toilet paper roll: 1, toilet paper: 1, toilet paper: 1, toilet paper: 1, toilet paper: 1, toilet paper: ", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24546.4, "ram_available_mb": 38294.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24533.9, "ram_available_mb": 38307.0, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.629}, "power_stats": {"power_gpu_soc_mean_watts": 20.987, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.91, "gpu_utilization_percent_mean": 70.629, "power_watts_avg": 20.987, "energy_joules_est": 238.21, "duration_seconds": 11.35, "sample_count": 97}, "timestamp": "2026-01-25T22:45:36.736837"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11185.391, "latencies_ms": [11185.391], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a toilet positioned near the bottom right corner, with a toilet paper roll and a toilet brush placed to its left. Behind the toilet, there is a wall-mounted shelf with two white towels on top and several bottles of toiletries arranged below. The shelf", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24533.9, "ram_available_mb": 38307.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24525.2, "ram_available_mb": 38315.7, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.894}, "power_stats": {"power_gpu_soc_mean_watts": 20.981, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 69.894, "power_watts_avg": 20.981, "energy_joules_est": 234.69, "duration_seconds": 11.186, "sample_count": 94}, "timestamp": "2026-01-25T22:45:49.945212"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7530.943, "latencies_ms": [7530.943], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows a small, well-lit bathroom with a toilet and a shelf above it. The shelf is filled with various bottles, possibly containing toiletries or cleaning products.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24525.2, "ram_available_mb": 38315.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24544.9, "ram_available_mb": 38296.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.359}, "power_stats": {"power_gpu_soc_mean_watts": 22.581, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.974, "gpu_utilization_percent_mean": 73.359, "power_watts_avg": 22.581, "energy_joules_est": 170.07, "duration_seconds": 7.532, "sample_count": 64}, "timestamp": "2026-01-25T22:45:59.488505"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10597.044, "latencies_ms": [10597.044], "images_per_second": 0.094, "prompt_tokens": 36, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image shows a bathroom with a beige color scheme, featuring a toilet, a towel rack with folded white towels, and a shelf with various bottles. The lighting appears to be artificial, coming from a ceiling light, and the materials used in the bathroom are primarily tiles and ceramics.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24544.9, "ram_available_mb": 38296.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24622.9, "ram_available_mb": 38218.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.167}, "power_stats": {"power_gpu_soc_mean_watts": 21.134, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.951, "gpu_utilization_percent_mean": 70.167, "power_watts_avg": 21.134, "energy_joules_est": 223.97, "duration_seconds": 10.598, "sample_count": 90}, "timestamp": "2026-01-25T22:46:12.106987"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11128.714, "latencies_ms": [11128.714], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman and a dog are sitting in the passenger seat of a car. The woman is wearing a green hat, and the dog is also wearing a green hat. They are both looking out the window, possibly observing something interesting. The car has a shiny chrome mirror, which is reflecting the image of the woman and the dog. The car's side", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24535.9, "ram_available_mb": 38305.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24608.8, "ram_available_mb": 38232.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.453}, "power_stats": {"power_gpu_soc_mean_watts": 20.868, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.453, "power_watts_avg": 20.868, "energy_joules_est": 232.25, "duration_seconds": 11.13, "sample_count": 95}, "timestamp": "2026-01-25T22:46:25.272838"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9166.267, "latencies_ms": [9166.267], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "1. Car: 1\n2. Mirror: 1\n3. Window: 1\n4. Shamrock: 1\n5. Dog: 1\n6. Hat: 1\n7. Tree: 1\n8. Person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24538.6, "ram_available_mb": 38302.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24618.0, "ram_available_mb": 38222.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.359}, "power_stats": {"power_gpu_soc_mean_watts": 21.674, "power_cpu_cv_mean_watts": 1.792, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 71.359, "power_watts_avg": 21.674, "energy_joules_est": 198.68, "duration_seconds": 9.167, "sample_count": 78}, "timestamp": "2026-01-25T22:46:36.483883"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10559.985, "latencies_ms": [10559.985], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "In the foreground, there is a reflection of a person and a dog in the side mirror of a vehicle. The person is seated in the driver's seat, wearing a green hat, and the dog is looking out of the window. The background is blurred, but it appears to be a street scene with trees and possibly other vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24531.0, "ram_available_mb": 38309.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24541.3, "ram_available_mb": 38299.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.989}, "power_stats": {"power_gpu_soc_mean_watts": 21.12, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 69.989, "power_watts_avg": 21.12, "energy_joules_est": 223.04, "duration_seconds": 10.561, "sample_count": 90}, "timestamp": "2026-01-25T22:46:49.059322"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7747.199, "latencies_ms": [7747.199], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A person is sitting in a vehicle with a dog, both wearing green hats, likely celebrating St. Patrick's Day. The side mirror of the vehicle has a green shamrock decoration attached to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24541.3, "ram_available_mb": 38299.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24656.1, "ram_available_mb": 38184.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.6}, "power_stats": {"power_gpu_soc_mean_watts": 22.44, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 71.6, "power_watts_avg": 22.44, "energy_joules_est": 173.86, "duration_seconds": 7.748, "sample_count": 65}, "timestamp": "2026-01-25T22:46:58.824822"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6654.675, "latencies_ms": [6654.675], "images_per_second": 0.15, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image shows a reflection of a person and a dog wearing a green hat in a vehicle's side mirror. The mirror has a green shamrock sticker attached to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24577.8, "ram_available_mb": 38263.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24645.7, "ram_available_mb": 38195.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.054}, "power_stats": {"power_gpu_soc_mean_watts": 22.839, "power_cpu_cv_mean_watts": 1.573, "power_sys_5v0_mean_watts": 9.051, "gpu_utilization_percent_mean": 73.054, "power_watts_avg": 22.839, "energy_joules_est": 152.0, "duration_seconds": 6.655, "sample_count": 56}, "timestamp": "2026-01-25T22:47:07.520046"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11132.164, "latencies_ms": [11132.164], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a large elephant standing in a pool of water, surrounded by rocks. The elephant appears to be enjoying its time in the water, possibly cooling off or playing. There are several people in the background, watching the elephant and enjoying the scene. Some of the people are standing closer to the elephant, while others are further", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 24645.7, "ram_available_mb": 38195.2, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24524.7, "ram_available_mb": 38316.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.5}, "power_stats": {"power_gpu_soc_mean_watts": 20.889, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 69.5, "power_watts_avg": 20.889, "energy_joules_est": 232.55, "duration_seconds": 11.133, "sample_count": 94}, "timestamp": "2026-01-25T22:47:20.679501"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9192.137, "latencies_ms": [9192.137], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "1. Elephant: 1\n2. Rock: 3\n3. Water: 2\n4. Fence: 1\n5. Log: 1\n6. Bush: 1\n7. Tree: 1\n8. People: 3", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24524.7, "ram_available_mb": 38316.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24626.6, "ram_available_mb": 38214.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.873}, "power_stats": {"power_gpu_soc_mean_watts": 21.676, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 71.873, "power_watts_avg": 21.676, "energy_joules_est": 199.26, "duration_seconds": 9.193, "sample_count": 79}, "timestamp": "2026-01-25T22:47:31.906778"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7782.147, "latencies_ms": [7782.147], "images_per_second": 0.128, "prompt_tokens": 44, "response_tokens_est": 49, "n_tiles": 16, "output_text": "In the foreground, there is a large elephant standing in a pool of water. To the left of the elephant, there is a large rock. In the background, there are several people walking around the enclosure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24626.6, "ram_available_mb": 38214.3, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24532.0, "ram_available_mb": 38308.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.477}, "power_stats": {"power_gpu_soc_mean_watts": 22.267, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 9.019, "gpu_utilization_percent_mean": 72.477, "power_watts_avg": 22.267, "energy_joules_est": 173.3, "duration_seconds": 7.783, "sample_count": 65}, "timestamp": "2026-01-25T22:47:41.710358"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6402.738, "latencies_ms": [6402.738], "images_per_second": 0.156, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "In the image, there is an elephant standing in a pool of water at a zoo. People are standing in the background, observing the elephant.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24532.0, "ram_available_mb": 38308.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24531.1, "ram_available_mb": 38309.8, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.547}, "power_stats": {"power_gpu_soc_mean_watts": 23.432, "power_cpu_cv_mean_watts": 1.481, "power_sys_5v0_mean_watts": 8.987, "gpu_utilization_percent_mean": 73.547, "power_watts_avg": 23.432, "energy_joules_est": 150.04, "duration_seconds": 6.403, "sample_count": 53}, "timestamp": "2026-01-25T22:47:50.138431"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6856.126, "latencies_ms": [6856.126], "images_per_second": 0.146, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The elephant is standing in a shallow pool of water surrounded by rocks. The pool is located in a sandy area with some greenery and a fallen tree trunk in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24531.1, "ram_available_mb": 38309.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24533.7, "ram_available_mb": 38307.2, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.345}, "power_stats": {"power_gpu_soc_mean_watts": 22.811, "power_cpu_cv_mean_watts": 1.616, "power_sys_5v0_mean_watts": 9.065, "gpu_utilization_percent_mean": 73.345, "power_watts_avg": 22.811, "energy_joules_est": 156.41, "duration_seconds": 6.857, "sample_count": 58}, "timestamp": "2026-01-25T22:47:59.018633"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11159.549, "latencies_ms": [11159.549], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a group of four people is standing on a snow-covered slope, posing for a picture. They are all wearing skis and holding ski poles, indicating that they are skiing enthusiasts. The group consists of two men and two women, with the men standing on the left side and the women on the right side. The snowy mountain in the", "error": null, "sys_before": {"cpu_percent": 11.5, "ram_used_mb": 24533.7, "ram_available_mb": 38307.2, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24628.5, "ram_available_mb": 38212.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.302}, "power_stats": {"power_gpu_soc_mean_watts": 20.799, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.952, "gpu_utilization_percent_mean": 69.302, "power_watts_avg": 20.799, "energy_joules_est": 232.12, "duration_seconds": 11.16, "sample_count": 96}, "timestamp": "2026-01-25T22:48:12.253119"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9071.94, "latencies_ms": [9071.94], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Skis: 4\n\n- Ski poles: 8\n\n- Jackets: 4\n\n- Gloves: 4\n\n- Boots: 4\n\n- Skis in the background: 1\n\n- Trees: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24541.3, "ram_available_mb": 38299.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24642.9, "ram_available_mb": 38198.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.053}, "power_stats": {"power_gpu_soc_mean_watts": 21.753, "power_cpu_cv_mean_watts": 1.755, "power_sys_5v0_mean_watts": 8.93, "gpu_utilization_percent_mean": 71.053, "power_watts_avg": 21.753, "energy_joules_est": 197.36, "duration_seconds": 9.073, "sample_count": 76}, "timestamp": "2026-01-25T22:48:23.341151"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8913.539, "latencies_ms": [8913.539], "images_per_second": 0.112, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "In the foreground, there are four skiers standing close together on a snowy slope. They are positioned near the base of a mountain range that stretches into the background. The skiers are in the near foreground, while the mountain range is in the far background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24642.9, "ram_available_mb": 38198.0, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24547.1, "ram_available_mb": 38293.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.75}, "power_stats": {"power_gpu_soc_mean_watts": 21.636, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 8.995, "gpu_utilization_percent_mean": 70.75, "power_watts_avg": 21.636, "energy_joules_est": 192.87, "duration_seconds": 8.914, "sample_count": 76}, "timestamp": "2026-01-25T22:48:34.301974"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6830.436, "latencies_ms": [6830.436], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "Four skiers are standing on a snowy slope with a mountain in the background. They are all wearing ski gear and holding ski poles, ready to ski down the mountain.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24547.1, "ram_available_mb": 38293.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 9.1, "ram_used_mb": 24639.3, "ram_available_mb": 38201.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.719}, "power_stats": {"power_gpu_soc_mean_watts": 23.023, "power_cpu_cv_mean_watts": 1.89, "power_sys_5v0_mean_watts": 9.049, "gpu_utilization_percent_mean": 72.719, "power_watts_avg": 23.023, "energy_joules_est": 157.27, "duration_seconds": 6.831, "sample_count": 57}, "timestamp": "2026-01-25T22:48:43.162403"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7776.453, "latencies_ms": [7776.453], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image shows a group of four skiers on a snowy mountain under clear blue skies. They are wearing winter clothing in bright colors like blue, black, and white, and the snow appears fresh and powdery.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24639.3, "ram_available_mb": 38201.6, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24657.7, "ram_available_mb": 38183.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.8}, "power_stats": {"power_gpu_soc_mean_watts": 22.181, "power_cpu_cv_mean_watts": 1.719, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 71.8, "power_watts_avg": 22.181, "energy_joules_est": 172.5, "duration_seconds": 7.777, "sample_count": 65}, "timestamp": "2026-01-25T22:48:52.963173"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11112.308, "latencies_ms": [11112.308], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person's hand is holding a black iPhone with a white keyboard. The phone is displaying a photo of a snowy landscape with trees and a house. The time displayed on the phone is 9:45. The person's fingers are visible on the left side of the phone, and the thumb is on the right side. The phone is resting on a", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24537.3, "ram_available_mb": 38303.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24637.6, "ram_available_mb": 38203.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.809}, "power_stats": {"power_gpu_soc_mean_watts": 20.887, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 69.809, "power_watts_avg": 20.887, "energy_joules_est": 232.12, "duration_seconds": 11.113, "sample_count": 94}, "timestamp": "2026-01-25T22:49:06.111003"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7326.123, "latencies_ms": [7326.123], "images_per_second": 0.136, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "hand: 1\nphone: 1\nkeyboard: 1\ndigital clock: 1\ndate: 1\ntime: 1\nlocation: 1\nimage: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24565.7, "ram_available_mb": 38275.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24589.8, "ram_available_mb": 38251.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.387}, "power_stats": {"power_gpu_soc_mean_watts": 22.725, "power_cpu_cv_mean_watts": 1.576, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 73.387, "power_watts_avg": 22.725, "energy_joules_est": 166.5, "duration_seconds": 7.327, "sample_count": 62}, "timestamp": "2026-01-25T22:49:15.490872"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10164.593, "latencies_ms": [10164.593], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The phone is held in the foreground, with the screen displaying an image of a snowy scene. The background is less distinct but appears to be a wooden surface, possibly a table or desk. The person's fingers are positioned near the bottom of the phone, suggesting they are either holding it or about to interact with it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24589.8, "ram_available_mb": 38251.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24573.7, "ram_available_mb": 38267.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.047}, "power_stats": {"power_gpu_soc_mean_watts": 21.284, "power_cpu_cv_mean_watts": 1.854, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 71.047, "power_watts_avg": 21.284, "energy_joules_est": 216.36, "duration_seconds": 10.165, "sample_count": 86}, "timestamp": "2026-01-25T22:49:27.709796"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7538.133, "latencies_ms": [7538.133], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A person is holding a smartphone in their hand, displaying a photo of a snowy landscape on the screen. The phone's screen is turned on, and the time is displayed as 9:45.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24573.7, "ram_available_mb": 38267.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24593.3, "ram_available_mb": 38247.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.968}, "power_stats": {"power_gpu_soc_mean_watts": 22.574, "power_cpu_cv_mean_watts": 1.602, "power_sys_5v0_mean_watts": 8.996, "gpu_utilization_percent_mean": 71.968, "power_watts_avg": 22.574, "energy_joules_est": 170.18, "duration_seconds": 7.539, "sample_count": 63}, "timestamp": "2026-01-25T22:49:37.278848"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8605.216, "latencies_ms": [8605.216], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a person holding a black smartphone with a reflective screen, displaying a snowy scene with a time stamp of 9:45. The phone's keyboard is visible, and the person's fingers are gently cradling the device.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24593.3, "ram_available_mb": 38247.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24650.3, "ram_available_mb": 38190.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.425}, "power_stats": {"power_gpu_soc_mean_watts": 21.869, "power_cpu_cv_mean_watts": 1.745, "power_sys_5v0_mean_watts": 8.982, "gpu_utilization_percent_mean": 71.425, "power_watts_avg": 21.869, "energy_joules_est": 188.2, "duration_seconds": 8.606, "sample_count": 73}, "timestamp": "2026-01-25T22:49:47.906795"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12305.092, "latencies_ms": [12305.092], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene on Denver's Road Home, a homeless shelter in Denver, Colorado. The main focus is a red and blue donation box, standing tall on a gray pole. The box is adorned with a white label that reads \"Denver's Road Home\" and \"Campaign to End Homelessness\", indicating its purpose and the cause", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24528.0, "ram_available_mb": 38312.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24573.8, "ram_available_mb": 38267.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.234}, "power_stats": {"power_gpu_soc_mean_watts": 22.843, "power_cpu_cv_mean_watts": 1.785, "power_sys_5v0_mean_watts": 9.213, "gpu_utilization_percent_mean": 74.234, "power_watts_avg": 22.843, "energy_joules_est": 281.1, "duration_seconds": 12.306, "sample_count": 107}, "timestamp": "2026-01-25T22:50:02.239380"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 12538.222, "latencies_ms": [12538.222], "images_per_second": 0.08, "prompt_tokens": 39, "response_tokens_est": 78, "n_tiles": 16, "output_text": "1. Denver's Road Home donation box: 1\n2. Donate Here sign: 1\n3. Campaign to End Homelessness sign: 1\n4. Parking sign: 1\n5. Trees: multiple\n6. Bushes: multiple\n7. Sidewalk: 1\n8. Street: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24573.8, "ram_available_mb": 38267.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24640.2, "ram_available_mb": 38200.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.925}, "power_stats": {"power_gpu_soc_mean_watts": 23.023, "power_cpu_cv_mean_watts": 1.737, "power_sys_5v0_mean_watts": 9.142, "gpu_utilization_percent_mean": 74.925, "power_watts_avg": 23.023, "energy_joules_est": 288.68, "duration_seconds": 12.539, "sample_count": 107}, "timestamp": "2026-01-25T22:50:16.819767"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7911.833, "latencies_ms": [7911.833], "images_per_second": 0.126, "prompt_tokens": 44, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The parking meter is located on the right side of the image, near the foreground. The sign below it is in the background, slightly to the left of the parking meter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24640.2, "ram_available_mb": 38200.7, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24640.5, "ram_available_mb": 38200.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.94}, "power_stats": {"power_gpu_soc_mean_watts": 24.591, "power_cpu_cv_mean_watts": 1.405, "power_sys_5v0_mean_watts": 9.282, "gpu_utilization_percent_mean": 78.94, "power_watts_avg": 24.591, "energy_joules_est": 194.57, "duration_seconds": 7.912, "sample_count": 67}, "timestamp": "2026-01-25T22:50:26.783908"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10863.956, "latencies_ms": [10863.956], "images_per_second": 0.092, "prompt_tokens": 37, "response_tokens_est": 63, "n_tiles": 16, "output_text": "A red and black donation box is placed on a sidewalk next to a sign that reads \"Campaign to End Homelessness.\" The box is located on Denver's Road Home, indicating that it is likely a place where people can donate to support the homelessness initiative.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24640.5, "ram_available_mb": 38200.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24543.7, "ram_available_mb": 38297.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.011}, "power_stats": {"power_gpu_soc_mean_watts": 23.479, "power_cpu_cv_mean_watts": 1.611, "power_sys_5v0_mean_watts": 9.134, "gpu_utilization_percent_mean": 76.011, "power_watts_avg": 23.479, "energy_joules_est": 255.09, "duration_seconds": 10.865, "sample_count": 92}, "timestamp": "2026-01-25T22:50:39.665348"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11255.34, "latencies_ms": [11255.34], "images_per_second": 0.089, "prompt_tokens": 36, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image features a red and black parking meter with a white label that reads \"DENVER'S ROAD HOME\" and a sign below it that says \"CAMPAIGN TO END HOMELESSNESS.\" The parking meter is located on a sidewalk next to a concrete wall and some greenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24543.7, "ram_available_mb": 38297.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24636.4, "ram_available_mb": 38204.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.216}, "power_stats": {"power_gpu_soc_mean_watts": 23.132, "power_cpu_cv_mean_watts": 1.73, "power_sys_5v0_mean_watts": 9.222, "gpu_utilization_percent_mean": 74.216, "power_watts_avg": 23.132, "energy_joules_est": 260.37, "duration_seconds": 11.256, "sample_count": 97}, "timestamp": "2026-01-25T22:50:52.934089"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11107.273, "latencies_ms": [11107.273], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of zebras grazing in a grassy field. There are four zebras in total, with one zebra prominently in the foreground, and the other three zebras positioned further back in the scene. The zebras are standing on a lush green field, which provides them with ample space to graze and roam", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24636.4, "ram_available_mb": 38204.5, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24552.5, "ram_available_mb": 38288.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.823}, "power_stats": {"power_gpu_soc_mean_watts": 20.946, "power_cpu_cv_mean_watts": 1.927, "power_sys_5v0_mean_watts": 8.957, "gpu_utilization_percent_mean": 69.823, "power_watts_avg": 20.946, "energy_joules_est": 232.67, "duration_seconds": 11.108, "sample_count": 96}, "timestamp": "2026-01-25T22:51:06.083418"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4360.802, "latencies_ms": [4360.802], "images_per_second": 0.229, "prompt_tokens": 39, "response_tokens_est": 17, "n_tiles": 16, "output_text": "zebra: 4\ngrass: many\ntrees: many\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24552.5, "ram_available_mb": 38288.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24536.3, "ram_available_mb": 38304.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 80.568}, "power_stats": {"power_gpu_soc_mean_watts": 26.145, "power_cpu_cv_mean_watts": 1.104, "power_sys_5v0_mean_watts": 9.176, "gpu_utilization_percent_mean": 80.568, "power_watts_avg": 26.145, "energy_joules_est": 114.03, "duration_seconds": 4.361, "sample_count": 37}, "timestamp": "2026-01-25T22:51:12.482376"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11103.593, "latencies_ms": [11103.593], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a zebra grazing on the left side of the image, while the other two zebras are positioned further back, with one on the right side and the other partially visible on the far left. The zebra in the foreground is closer to the viewer, making it appear larger, while the other zebras are farther away,", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24536.3, "ram_available_mb": 38304.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24642.7, "ram_available_mb": 38198.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.385}, "power_stats": {"power_gpu_soc_mean_watts": 20.931, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 9.005, "gpu_utilization_percent_mean": 69.385, "power_watts_avg": 20.931, "energy_joules_est": 232.42, "duration_seconds": 11.104, "sample_count": 96}, "timestamp": "2026-01-25T22:51:25.628942"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7153.732, "latencies_ms": [7153.732], "images_per_second": 0.14, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A group of zebras are grazing in a grassy field with trees and bushes in the background. The zebras are standing close together and appear to be enjoying their meal.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24555.5, "ram_available_mb": 38285.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24558.9, "ram_available_mb": 38282.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.639}, "power_stats": {"power_gpu_soc_mean_watts": 22.819, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 9.0, "gpu_utilization_percent_mean": 73.639, "power_watts_avg": 22.819, "energy_joules_est": 163.26, "duration_seconds": 7.154, "sample_count": 61}, "timestamp": "2026-01-25T22:51:34.812620"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6976.032, "latencies_ms": [6976.032], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The zebras are grazing in a field with dry, golden grass under bright sunlight. The background is filled with green shrubs and trees, indicating a savannah or grassland habitat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24558.9, "ram_available_mb": 38282.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24638.0, "ram_available_mb": 38202.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.051}, "power_stats": {"power_gpu_soc_mean_watts": 22.705, "power_cpu_cv_mean_watts": 1.609, "power_sys_5v0_mean_watts": 9.043, "gpu_utilization_percent_mean": 73.051, "power_watts_avg": 22.705, "energy_joules_est": 158.41, "duration_seconds": 6.977, "sample_count": 59}, "timestamp": "2026-01-25T22:51:43.812262"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11125.596, "latencies_ms": [11125.596], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is skillfully riding a wave on a surfboard. He is wearing a black wetsuit, which is designed to provide thermal insulation and protection from the elements while surfing. The surfboard, a crucial piece of equipment for this sport, is visible beneath him. The wave he is riding is a beautiful shade of green", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 24638.0, "ram_available_mb": 38202.9, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24664.7, "ram_available_mb": 38176.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.432}, "power_stats": {"power_gpu_soc_mean_watts": 20.845, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 9.008, "gpu_utilization_percent_mean": 69.432, "power_watts_avg": 20.845, "energy_joules_est": 231.93, "duration_seconds": 11.126, "sample_count": 95}, "timestamp": "2026-01-25T22:51:57.001770"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8164.833, "latencies_ms": [8164.833], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "water: numerous\nsurfboard: 1\nwetsuit: 1\nwave: 1\nsurfer: 1\nfoam: numerous\nspray: numerous\nsunlight: 1\nsea: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24534.2, "ram_available_mb": 38306.7, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24527.6, "ram_available_mb": 38313.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.676}, "power_stats": {"power_gpu_soc_mean_watts": 22.169, "power_cpu_cv_mean_watts": 1.698, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 72.676, "power_watts_avg": 22.169, "energy_joules_est": 181.02, "duration_seconds": 8.166, "sample_count": 71}, "timestamp": "2026-01-25T22:52:07.198405"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8659.793, "latencies_ms": [8659.793], "images_per_second": 0.115, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is breaking to the right of the frame. The wave originates in the background and extends towards the left side of the image, creating a dynamic spatial relationship between the surfer and the moving water.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 24527.6, "ram_available_mb": 38313.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24544.0, "ram_available_mb": 38296.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.973}, "power_stats": {"power_gpu_soc_mean_watts": 21.808, "power_cpu_cv_mean_watts": 1.767, "power_sys_5v0_mean_watts": 8.989, "gpu_utilization_percent_mean": 70.973, "power_watts_avg": 21.808, "energy_joules_est": 188.87, "duration_seconds": 8.66, "sample_count": 73}, "timestamp": "2026-01-25T22:52:17.886001"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7271.002, "latencies_ms": [7271.002], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A surfer in a black wetsuit is skillfully riding a wave in the ocean. The surfer is crouched on a surfboard, navigating the powerful force of the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24544.0, "ram_available_mb": 38296.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24641.4, "ram_available_mb": 38199.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.475}, "power_stats": {"power_gpu_soc_mean_watts": 22.669, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 8.991, "gpu_utilization_percent_mean": 72.475, "power_watts_avg": 22.669, "energy_joules_est": 164.84, "duration_seconds": 7.272, "sample_count": 61}, "timestamp": "2026-01-25T22:52:27.198479"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7445.728, "latencies_ms": [7445.728], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The surfer is wearing a black wetsuit that contrasts with the bright blue of the ocean water. The sunlight is reflecting off the water's surface, creating a sparkling effect on the wave.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24641.4, "ram_available_mb": 38199.5, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24558.5, "ram_available_mb": 38282.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.047}, "power_stats": {"power_gpu_soc_mean_watts": 22.311, "power_cpu_cv_mean_watts": 1.677, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 72.047, "power_watts_avg": 22.311, "energy_joules_est": 166.14, "duration_seconds": 7.446, "sample_count": 64}, "timestamp": "2026-01-25T22:52:36.679958"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11135.379, "latencies_ms": [11135.379], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene moment on a snowy mountain. Two skiers, clad in white jackets and helmets, are seen in the foreground. The skier on the left is holding a pair of skis, ready to descend the slope, while the one on the right is adjusting their backpack. The sun is setting, casting a warm glow on", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 24558.5, "ram_available_mb": 38282.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 24585.8, "ram_available_mb": 38255.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.347}, "power_stats": {"power_gpu_soc_mean_watts": 20.917, "power_cpu_cv_mean_watts": 2.344, "power_sys_5v0_mean_watts": 8.984, "gpu_utilization_percent_mean": 69.347, "power_watts_avg": 20.917, "energy_joules_est": 232.93, "duration_seconds": 11.136, "sample_count": 95}, "timestamp": "2026-01-25T22:52:49.856262"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7930.589, "latencies_ms": [7930.589], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "person: 2, ski poles: 2, backpacks: 2, ski boots: 2, ski gear: 2, ski tracks: 2, snow: 2, sun: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24585.8, "ram_available_mb": 38255.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24655.0, "ram_available_mb": 38185.9, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.382}, "power_stats": {"power_gpu_soc_mean_watts": 22.253, "power_cpu_cv_mean_watts": 1.678, "power_sys_5v0_mean_watts": 8.986, "gpu_utilization_percent_mean": 72.382, "power_watts_avg": 22.253, "energy_joules_est": 176.49, "duration_seconds": 7.931, "sample_count": 68}, "timestamp": "2026-01-25T22:52:59.822930"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11132.385, "latencies_ms": [11132.385], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person standing on skis, holding ski poles, and wearing a backpack, positioned near the bottom right of the image. Another person is seen in the background, also on skis, standing further up the slope towards the left side of the image. The sun is positioned in the background, slightly to the left, casting light on the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24583.1, "ram_available_mb": 38257.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24646.4, "ram_available_mb": 38194.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.49}, "power_stats": {"power_gpu_soc_mean_watts": 20.889, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 69.49, "power_watts_avg": 20.889, "energy_joules_est": 232.56, "duration_seconds": 11.133, "sample_count": 96}, "timestamp": "2026-01-25T22:53:12.975079"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11288.58, "latencies_ms": [11288.58], "images_per_second": 0.089, "prompt_tokens": 37, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The image captures a serene moment on a snowy mountain peak during what appears to be either sunrise or sunset, with the sun casting a warm glow on the snow-covered landscape. Two skiers are seen, one preparing to ski down the slope while the other stands with skis and poles, possibly taking a break or assessing the path ahead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24524.2, "ram_available_mb": 38316.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24536.4, "ram_available_mb": 38304.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.969}, "power_stats": {"power_gpu_soc_mean_watts": 20.935, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.924, "gpu_utilization_percent_mean": 69.969, "power_watts_avg": 20.935, "energy_joules_est": 236.34, "duration_seconds": 11.289, "sample_count": 97}, "timestamp": "2026-01-25T22:53:26.287497"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10314.799, "latencies_ms": [10314.799], "images_per_second": 0.097, "prompt_tokens": 36, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image captures a serene winter scene with a clear sky and the sun setting behind a mountain peak, casting a warm glow and creating a silhouette of a skier. The skier, dressed in white, is seen holding skis and poles, with a backpack on, suggesting a day of adventure in the snow.", "error": null, "sys_before": {"cpu_percent": 27.3, "ram_used_mb": 24536.4, "ram_available_mb": 38304.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24539.0, "ram_available_mb": 38301.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.989}, "power_stats": {"power_gpu_soc_mean_watts": 21.175, "power_cpu_cv_mean_watts": 1.873, "power_sys_5v0_mean_watts": 9.006, "gpu_utilization_percent_mean": 69.989, "power_watts_avg": 21.175, "energy_joules_est": 218.43, "duration_seconds": 10.315, "sample_count": 87}, "timestamp": "2026-01-25T22:53:38.628923"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11127.366, "latencies_ms": [11127.366], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a baseball game in progress, with a batter standing at home plate, holding a baseball bat and preparing to swing. The catcher is crouched behind the batter, wearing a baseball glove and ready to catch the ball. The umpire is also present, closely observing the game.\n\nIn the background, there are several people watching the game,", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24539.0, "ram_available_mb": 38301.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24619.6, "ram_available_mb": 38221.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.702}, "power_stats": {"power_gpu_soc_mean_watts": 20.904, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 69.702, "power_watts_avg": 20.904, "energy_joules_est": 232.62, "duration_seconds": 11.128, "sample_count": 94}, "timestamp": "2026-01-25T22:53:51.804714"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10080.397, "latencies_ms": [10080.397], "images_per_second": 0.099, "prompt_tokens": 39, "response_tokens_est": 68, "n_tiles": 16, "output_text": "1. Baseball players: 4\n2. Baseball bat: 1\n3. Baseball glove: 1\n4. Baseball field: 1\n5. Catcher: 1\n6. Umpire: 1\n7. Spectators: 3\n8. Cars: 3", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24619.6, "ram_available_mb": 38221.3, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24642.8, "ram_available_mb": 38198.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.141}, "power_stats": {"power_gpu_soc_mean_watts": 21.332, "power_cpu_cv_mean_watts": 1.837, "power_sys_5v0_mean_watts": 8.944, "gpu_utilization_percent_mean": 71.141, "power_watts_avg": 21.332, "energy_joules_est": 215.05, "duration_seconds": 10.081, "sample_count": 85}, "timestamp": "2026-01-25T22:54:03.910527"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9572.491, "latencies_ms": [9572.491], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "In the foreground, a baseball player is standing near the home plate, holding a bat and ready to swing. Behind the batter, the catcher is crouched behind home plate, wearing a baseball glove. In the background, there are spectators seated on chairs, watching the game.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24642.8, "ram_available_mb": 38198.1, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24665.5, "ram_available_mb": 38175.4, "ram_percent": 39.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.341}, "power_stats": {"power_gpu_soc_mean_watts": 21.376, "power_cpu_cv_mean_watts": 1.846, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 70.341, "power_watts_avg": 21.376, "energy_joules_est": 204.63, "duration_seconds": 9.573, "sample_count": 82}, "timestamp": "2026-01-25T22:54:15.536880"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8179.403, "latencies_ms": [8179.403], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image captures a moment from a baseball game, with a batter in a red shirt and white pants at the plate, ready to swing. In the background, there are spectators seated on chairs, watching the game unfold.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 24665.5, "ram_available_mb": 38175.4, "ram_percent": 39.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24573.1, "ram_available_mb": 38267.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.145}, "power_stats": {"power_gpu_soc_mean_watts": 22.218, "power_cpu_cv_mean_watts": 1.689, "power_sys_5v0_mean_watts": 8.964, "gpu_utilization_percent_mean": 72.145, "power_watts_avg": 22.218, "energy_joules_est": 181.74, "duration_seconds": 8.18, "sample_count": 69}, "timestamp": "2026-01-25T22:54:25.743879"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8759.704, "latencies_ms": [8759.704], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image shows a sunny day with clear skies, as evidenced by the bright lighting and shadows cast on the ground. The players are wearing helmets and gloves, suggesting the game is being played in a casual, possibly informal setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24573.1, "ram_available_mb": 38267.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24648.5, "ram_available_mb": 38192.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.919}, "power_stats": {"power_gpu_soc_mean_watts": 21.693, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 9.018, "gpu_utilization_percent_mean": 70.919, "power_watts_avg": 21.693, "energy_joules_est": 190.04, "duration_seconds": 8.76, "sample_count": 74}, "timestamp": "2026-01-25T22:54:36.543322"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12310.538, "latencies_ms": [12310.538], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a restaurant setting. Dominating the foreground is a tall glass filled with a creamy, frothy beverage, topped with a generous dollop of whipped cream. The glass, with its clear design, is placed on a black table. Adjacent to the glass, there's a white plate holding a slice of cake.", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 24648.4, "ram_available_mb": 38192.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24546.5, "ram_available_mb": 38294.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.571}, "power_stats": {"power_gpu_soc_mean_watts": 22.829, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 9.198, "gpu_utilization_percent_mean": 73.571, "power_watts_avg": 22.829, "energy_joules_est": 281.05, "duration_seconds": 12.311, "sample_count": 105}, "timestamp": "2026-01-25T22:54:50.928170"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11093.076, "latencies_ms": [11093.076], "images_per_second": 0.09, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "- Glass: 1\n\n- Whipped cream: 1\n\n- Ice cream: 1\n\n- Fork: 2\n\n- Knife: 2\n\n- Napkin: 1\n\n- Cake: 1\n\n- Table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24546.5, "ram_available_mb": 38294.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24639.6, "ram_available_mb": 38201.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.021}, "power_stats": {"power_gpu_soc_mean_watts": 23.364, "power_cpu_cv_mean_watts": 1.648, "power_sys_5v0_mean_watts": 9.154, "gpu_utilization_percent_mean": 76.021, "power_watts_avg": 23.364, "energy_joules_est": 259.19, "duration_seconds": 11.094, "sample_count": 94}, "timestamp": "2026-01-25T22:55:04.067657"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12457.338, "latencies_ms": [12457.338], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a slice of cake on a white plate, positioned near the center of the image. Behind the cake, slightly to the left, is a tall glass filled with a creamy beverage, which is the main object in the image. The background features a dining area with people seated at tables, and the floor extends into the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24559.4, "ram_available_mb": 38281.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24542.0, "ram_available_mb": 38298.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.402}, "power_stats": {"power_gpu_soc_mean_watts": 22.868, "power_cpu_cv_mean_watts": 1.773, "power_sys_5v0_mean_watts": 9.167, "gpu_utilization_percent_mean": 73.402, "power_watts_avg": 22.868, "energy_joules_est": 284.89, "duration_seconds": 12.458, "sample_count": 107}, "timestamp": "2026-01-25T22:55:18.549048"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8366.272, "latencies_ms": [8366.272], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "In the image, there is a dessert table with a slice of cake and a milkshake on it. The table is in a restaurant with people sitting at the tables in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24542.0, "ram_available_mb": 38298.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24514.3, "ram_available_mb": 38326.6, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.38}, "power_stats": {"power_gpu_soc_mean_watts": 24.501, "power_cpu_cv_mean_watts": 1.393, "power_sys_5v0_mean_watts": 9.187, "gpu_utilization_percent_mean": 79.38, "power_watts_avg": 24.501, "energy_joules_est": 205.0, "duration_seconds": 8.367, "sample_count": 71}, "timestamp": "2026-01-25T22:55:28.974452"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9676.914, "latencies_ms": [9676.914], "images_per_second": 0.103, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image features a clear glass of a creamy, frothy beverage with a white straw, placed on a dark wooden table. Beside it is a slice of cake on a white plate, with a fork and knife laid out for eating.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24514.3, "ram_available_mb": 38326.6, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24633.1, "ram_available_mb": 38207.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 75.904}, "power_stats": {"power_gpu_soc_mean_watts": 23.675, "power_cpu_cv_mean_watts": 1.587, "power_sys_5v0_mean_watts": 9.229, "gpu_utilization_percent_mean": 75.904, "power_watts_avg": 23.675, "energy_joules_est": 229.11, "duration_seconds": 9.677, "sample_count": 83}, "timestamp": "2026-01-25T22:55:40.683564"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11103.893, "latencies_ms": [11103.893], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large, multi-tiered wedding cake with white frosting and blue accents, placed on a dining table. The cake is adorned with white flowers and has a chandelier hanging above it, adding elegance to the scene. The table is surrounded by numerous chairs, indicating that it is set up for a wedding", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 24534.5, "ram_available_mb": 38306.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24552.9, "ram_available_mb": 38288.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.734}, "power_stats": {"power_gpu_soc_mean_watts": 20.931, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.734, "power_watts_avg": 20.931, "energy_joules_est": 232.43, "duration_seconds": 11.105, "sample_count": 94}, "timestamp": "2026-01-25T22:55:53.832727"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7306.757, "latencies_ms": [7306.757], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "wedding cake: 3 tiers, chairs: multiple, tables: multiple, decorations: multiple, flowers: multiple, candles: multiple, guests: multiple, venue: large room", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24552.9, "ram_available_mb": 38288.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24635.8, "ram_available_mb": 38205.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.967}, "power_stats": {"power_gpu_soc_mean_watts": 22.721, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 72.967, "power_watts_avg": 22.721, "energy_joules_est": 166.03, "duration_seconds": 7.307, "sample_count": 61}, "timestamp": "2026-01-25T22:56:03.160664"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9617.099, "latencies_ms": [9617.099], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The wedding cake is the central object in the foreground, placed on a table covered with a blue cloth. In the background, there are multiple round tables set up for guests, with chairs arranged around them. A chandelier hangs from the ceiling, providing light to the entire scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24635.8, "ram_available_mb": 38205.1, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24652.2, "ram_available_mb": 38188.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.667}, "power_stats": {"power_gpu_soc_mean_watts": 21.428, "power_cpu_cv_mean_watts": 1.848, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 70.667, "power_watts_avg": 21.428, "energy_joules_est": 206.09, "duration_seconds": 9.618, "sample_count": 81}, "timestamp": "2026-01-25T22:56:14.802066"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8742.173, "latencies_ms": [8742.173], "images_per_second": 0.114, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image showcases a beautifully decorated wedding cake placed on a table covered with a blue cloth. The cake is adorned with white and blue frosting, and is surrounded by elegant chairs and tables set up for a wedding reception.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24652.2, "ram_available_mb": 38188.7, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24653.4, "ram_available_mb": 38187.5, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.432}, "power_stats": {"power_gpu_soc_mean_watts": 21.911, "power_cpu_cv_mean_watts": 1.753, "power_sys_5v0_mean_watts": 8.963, "gpu_utilization_percent_mean": 71.432, "power_watts_avg": 21.911, "energy_joules_est": 191.56, "duration_seconds": 8.743, "sample_count": 74}, "timestamp": "2026-01-25T22:56:25.576284"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9478.302, "latencies_ms": [9478.302], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The wedding cake is adorned with white and blue frosting, featuring intricate designs and a large white floral decoration on top. The room is elegantly decorated with a chandelier hanging from the ceiling, providing a warm and inviting atmosphere for the event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24653.4, "ram_available_mb": 38187.5, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24636.3, "ram_available_mb": 38204.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.713}, "power_stats": {"power_gpu_soc_mean_watts": 21.506, "power_cpu_cv_mean_watts": 1.837, "power_sys_5v0_mean_watts": 8.973, "gpu_utilization_percent_mean": 70.713, "power_watts_avg": 21.506, "energy_joules_est": 203.85, "duration_seconds": 9.479, "sample_count": 80}, "timestamp": "2026-01-25T22:56:37.087918"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11121.579, "latencies_ms": [11121.579], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing in a kitchen, holding a plate with a piece of food on it. She is wearing a blue and red sweater and is in the process of serving the food. The kitchen is equipped with a stove, an oven, and a refrigerator. There are several bottles and jars on the countertop, and a bow", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 24531.1, "ram_available_mb": 38309.8, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 8.3, "ram_used_mb": 24522.6, "ram_available_mb": 38318.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.137}, "power_stats": {"power_gpu_soc_mean_watts": 20.922, "power_cpu_cv_mean_watts": 2.002, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 69.137, "power_watts_avg": 20.922, "energy_joules_est": 232.7, "duration_seconds": 11.122, "sample_count": 95}, "timestamp": "2026-01-25T22:56:50.272890"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7402.888, "latencies_ms": [7402.888], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "pot: 2, plate: 1, spoon: 1, knife: 0, bowl: 0, bottle: 0, can: 0, jar: 0", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24522.6, "ram_available_mb": 38318.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24553.2, "ram_available_mb": 38287.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.635}, "power_stats": {"power_gpu_soc_mean_watts": 22.649, "power_cpu_cv_mean_watts": 1.919, "power_sys_5v0_mean_watts": 9.02, "gpu_utilization_percent_mean": 72.635, "power_watts_avg": 22.649, "energy_joules_est": 167.68, "duration_seconds": 7.404, "sample_count": 63}, "timestamp": "2026-01-25T22:56:59.713971"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10384.73, "latencies_ms": [10384.73], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground, a person is standing in front of a stove, holding a plate with food. The stove is on the left side of the image, and there are various containers and items on the shelves in the background. The person is closer to the camera than the shelves, indicating they are in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24553.2, "ram_available_mb": 38287.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24598.2, "ram_available_mb": 38242.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.809}, "power_stats": {"power_gpu_soc_mean_watts": 21.236, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 69.809, "power_watts_avg": 21.236, "energy_joules_est": 220.55, "duration_seconds": 10.385, "sample_count": 89}, "timestamp": "2026-01-25T22:57:12.115639"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9853.145, "latencies_ms": [9853.145], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 66, "n_tiles": 16, "output_text": "A person is standing in a kitchen, wearing a blue and red sweater, and is holding a plate with food on it. There are two large pots on the stove, and various items are displayed on the wall, including a poster with a cartoon character and a shelf with jars and containers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24598.2, "ram_available_mb": 38242.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24586.3, "ram_available_mb": 38254.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.155}, "power_stats": {"power_gpu_soc_mean_watts": 21.552, "power_cpu_cv_mean_watts": 1.821, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 71.155, "power_watts_avg": 21.552, "energy_joules_est": 212.37, "duration_seconds": 9.854, "sample_count": 84}, "timestamp": "2026-01-25T22:57:24.007461"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6295.626, "latencies_ms": [6295.626], "images_per_second": 0.159, "prompt_tokens": 36, "response_tokens_est": 35, "n_tiles": 16, "output_text": "The image shows a person in a blue and red sweater standing in a kitchen. The kitchen has a white stove and a shelf with various items on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24586.3, "ram_available_mb": 38254.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24563.1, "ram_available_mb": 38277.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.519}, "power_stats": {"power_gpu_soc_mean_watts": 22.97, "power_cpu_cv_mean_watts": 1.527, "power_sys_5v0_mean_watts": 9.047, "gpu_utilization_percent_mean": 73.519, "power_watts_avg": 22.97, "energy_joules_est": 144.63, "duration_seconds": 6.296, "sample_count": 54}, "timestamp": "2026-01-25T22:57:32.322118"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11104.622, "latencies_ms": [11104.622], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing in a dirt field, holding a rope that is attached to a white horse. The woman is wearing a pink shirt and black pants, and she is also wearing brown boots. The horse is positioned to the left of the woman, and it appears to be looking towards her. The background of the image features a f", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 24563.1, "ram_available_mb": 38277.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 24647.2, "ram_available_mb": 38193.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.6}, "power_stats": {"power_gpu_soc_mean_watts": 20.933, "power_cpu_cv_mean_watts": 1.926, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 69.6, "power_watts_avg": 20.933, "energy_joules_est": 232.47, "duration_seconds": 11.105, "sample_count": 95}, "timestamp": "2026-01-25T22:57:45.477054"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7527.904, "latencies_ms": [7527.904], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "horse: 1, woman: 1, rope: 2, fence: 2, trees: 2, ground: 1, boot: 2, pink shirt: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24543.3, "ram_available_mb": 38297.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24645.1, "ram_available_mb": 38195.8, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.54}, "power_stats": {"power_gpu_soc_mean_watts": 22.518, "power_cpu_cv_mean_watts": 1.672, "power_sys_5v0_mean_watts": 8.971, "gpu_utilization_percent_mean": 72.54, "power_watts_avg": 22.518, "energy_joules_est": 169.53, "duration_seconds": 7.529, "sample_count": 63}, "timestamp": "2026-01-25T22:57:55.071566"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9460.07, "latencies_ms": [9460.07], "images_per_second": 0.106, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "In the foreground, a woman is standing on the left side of the image, holding a rope that extends towards the horse in the background. The horse is positioned on the right side of the image, with a wooden fence behind it, creating a clear spatial separation between the woman and the horse.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24566.5, "ram_available_mb": 38274.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24646.9, "ram_available_mb": 38194.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.915}, "power_stats": {"power_gpu_soc_mean_watts": 21.517, "power_cpu_cv_mean_watts": 1.851, "power_sys_5v0_mean_watts": 8.983, "gpu_utilization_percent_mean": 70.915, "power_watts_avg": 21.517, "energy_joules_est": 203.57, "duration_seconds": 9.461, "sample_count": 82}, "timestamp": "2026-01-25T22:58:06.557019"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7488.774, "latencies_ms": [7488.774], "images_per_second": 0.134, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A woman in a pink shirt and black pants is holding a white rope in an outdoor setting with a white horse nearby. The woman appears to be leading the horse or preparing to do so.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24550.9, "ram_available_mb": 38290.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24558.0, "ram_available_mb": 38282.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.828}, "power_stats": {"power_gpu_soc_mean_watts": 22.74, "power_cpu_cv_mean_watts": 1.633, "power_sys_5v0_mean_watts": 8.998, "gpu_utilization_percent_mean": 73.828, "power_watts_avg": 22.74, "energy_joules_est": 170.31, "duration_seconds": 7.489, "sample_count": 64}, "timestamp": "2026-01-25T22:58:16.085983"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7228.205, "latencies_ms": [7228.205], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image features a woman in a pink shirt and black pants standing in a dirt field with a white horse nearby. The lighting is natural and bright, suggesting it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24558.0, "ram_available_mb": 38282.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24631.7, "ram_available_mb": 38209.2, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.855}, "power_stats": {"power_gpu_soc_mean_watts": 22.596, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 9.068, "gpu_utilization_percent_mean": 71.855, "power_watts_avg": 22.596, "energy_joules_est": 163.34, "duration_seconds": 7.229, "sample_count": 62}, "timestamp": "2026-01-25T22:58:25.365896"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11157.988, "latencies_ms": [11157.988], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a busy city street with multiple cars and a bus driving down the road. There are at least 11 cars visible, with some closer to the foreground and others further away. A bus can be seen in the middle of the scene, and a truck is also present on the right side of the street. \n\nThere are several traffic lights throughout the scene,", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24543.0, "ram_available_mb": 38297.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24565.3, "ram_available_mb": 38275.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.567}, "power_stats": {"power_gpu_soc_mean_watts": 20.747, "power_cpu_cv_mean_watts": 1.932, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 69.567, "power_watts_avg": 20.747, "energy_joules_est": 231.51, "duration_seconds": 11.159, "sample_count": 97}, "timestamp": "2026-01-25T22:58:38.554613"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9308.84, "latencies_ms": [9308.84], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Cars: 10\n- Buildings: 5\n- Traffic lights: 4\n- Signs: 3\n- Trees: 2\n- Pedestrians: 1\n- Buses: 1\n- Bicycles: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24565.3, "ram_available_mb": 38275.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24570.3, "ram_available_mb": 38270.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.443}, "power_stats": {"power_gpu_soc_mean_watts": 21.668, "power_cpu_cv_mean_watts": 1.784, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 71.443, "power_watts_avg": 21.668, "energy_joules_est": 201.72, "duration_seconds": 9.309, "sample_count": 79}, "timestamp": "2026-01-25T22:58:49.876948"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10713.219, "latencies_ms": [10713.219], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The traffic sign is positioned in the foreground on the right side of the image, indicating its importance for drivers. In the background, there are multiple buildings that appear to be part of a cityscape, with the traffic lights and vehicles situated between them. The trees lining the street are in the midground, providing a natural element to the urban environment.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24570.3, "ram_available_mb": 38270.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24595.6, "ram_available_mb": 38245.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.956}, "power_stats": {"power_gpu_soc_mean_watts": 21.127, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.968, "gpu_utilization_percent_mean": 69.956, "power_watts_avg": 21.127, "energy_joules_est": 226.35, "duration_seconds": 10.714, "sample_count": 91}, "timestamp": "2026-01-25T22:59:02.622558"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8974.739, "latencies_ms": [8974.739], "images_per_second": 0.111, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image depicts a busy city street with multiple vehicles, including cars and a bus, driving in both directions. There are traffic lights and street signs, such as a yellow warning sign for a speed camera, indicating that this is an urban area with traffic regulations in place.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24595.6, "ram_available_mb": 38245.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24576.9, "ram_available_mb": 38264.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.25}, "power_stats": {"power_gpu_soc_mean_watts": 21.755, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 71.25, "power_watts_avg": 21.755, "energy_joules_est": 195.26, "duration_seconds": 8.975, "sample_count": 76}, "timestamp": "2026-01-25T22:59:13.652212"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10058.656, "latencies_ms": [10058.656], "images_per_second": 0.099, "prompt_tokens": 36, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image depicts a city street scene with a mix of vehicles, including cars and a bus, traveling on a wet road, suggesting recent rain. The lighting is overcast, with a gray sky, and the overall color palette is muted with blues, grays, and the yellow of a traffic sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24576.9, "ram_available_mb": 38264.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24629.8, "ram_available_mb": 38211.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.141}, "power_stats": {"power_gpu_soc_mean_watts": 21.326, "power_cpu_cv_mean_watts": 1.856, "power_sys_5v0_mean_watts": 8.979, "gpu_utilization_percent_mean": 70.141, "power_watts_avg": 21.326, "energy_joules_est": 214.52, "duration_seconds": 10.059, "sample_count": 85}, "timestamp": "2026-01-25T22:59:25.723170"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11141.035, "latencies_ms": [11141.035], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is standing in a bathroom, facing a stainless steel toilet. The toilet is positioned in the center of the image, with the person's feet visible in the foreground. The bathroom features white tiles on the walls, providing a clean and modern aesthetic. A blue toilet brush is hanging", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24629.8, "ram_available_mb": 38211.1, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24585.5, "ram_available_mb": 38255.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.67}, "power_stats": {"power_gpu_soc_mean_watts": 20.909, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.67, "power_watts_avg": 20.909, "energy_joules_est": 232.96, "duration_seconds": 11.142, "sample_count": 94}, "timestamp": "2026-01-25T22:59:38.896880"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11315.188, "latencies_ms": [11315.188], "images_per_second": 0.088, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "object: 1 toilet, count: 1\nobject: 1 toilet brush, count: 1\nobject: 1 toilet paper, count: 1\nobject: 1 toilet brush holder, count: 1\nobject: 1 toilet paper holder, count: 1\nobject: 1 toilet", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24523.8, "ram_available_mb": 38317.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24577.0, "ram_available_mb": 38263.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.167}, "power_stats": {"power_gpu_soc_mean_watts": 20.98, "power_cpu_cv_mean_watts": 1.881, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 70.167, "power_watts_avg": 20.98, "energy_joules_est": 237.41, "duration_seconds": 11.316, "sample_count": 96}, "timestamp": "2026-01-25T22:59:52.263431"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8890.019, "latencies_ms": [8890.019], "images_per_second": 0.112, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The toilet is located in the foreground of the image, with a person standing in front of it. The person is wearing black shoes and is positioned near the toilet. In the background, there is a stainless steel pipe running along the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24577.0, "ram_available_mb": 38263.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24589.0, "ram_available_mb": 38251.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.04}, "power_stats": {"power_gpu_soc_mean_watts": 21.676, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 9.001, "gpu_utilization_percent_mean": 71.04, "power_watts_avg": 21.676, "energy_joules_est": 192.71, "duration_seconds": 8.891, "sample_count": 75}, "timestamp": "2026-01-25T23:00:03.215673"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6831.562, "latencies_ms": [6831.562], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A person is standing in front of a stainless steel toilet in a public restroom. The toilet has a blue toilet brush attached to the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24589.0, "ram_available_mb": 38251.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24570.8, "ram_available_mb": 38270.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.897}, "power_stats": {"power_gpu_soc_mean_watts": 23.091, "power_cpu_cv_mean_watts": 1.56, "power_sys_5v0_mean_watts": 9.02, "gpu_utilization_percent_mean": 74.897, "power_watts_avg": 23.091, "energy_joules_est": 157.76, "duration_seconds": 6.832, "sample_count": 58}, "timestamp": "2026-01-25T23:00:12.098850"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7530.865, "latencies_ms": [7530.865], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a metallic toilet with a shiny, reflective surface, set against a backdrop of beige tiles. A blue plunger is visible to the left of the toilet.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24570.8, "ram_available_mb": 38270.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24581.3, "ram_available_mb": 38259.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.312}, "power_stats": {"power_gpu_soc_mean_watts": 22.371, "power_cpu_cv_mean_watts": 1.683, "power_sys_5v0_mean_watts": 9.038, "gpu_utilization_percent_mean": 72.312, "power_watts_avg": 22.371, "energy_joules_est": 168.49, "duration_seconds": 7.532, "sample_count": 64}, "timestamp": "2026-01-25T23:00:21.656215"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11104.114, "latencies_ms": [11104.114], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a room with a wooden floor, where a pink bicycle is prominently displayed. The bicycle is parked in the middle of the room, surrounded by other bicycles of various sizes and colors. The room appears to be a bike shop or a bike rental facility, as there are multiple bicycles on display for customers to choose from", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 24581.3, "ram_available_mb": 38259.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24552.4, "ram_available_mb": 38288.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.547}, "power_stats": {"power_gpu_soc_mean_watts": 20.888, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.965, "gpu_utilization_percent_mean": 69.547, "power_watts_avg": 20.888, "energy_joules_est": 231.96, "duration_seconds": 11.105, "sample_count": 95}, "timestamp": "2026-01-25T23:00:34.831664"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8182.045, "latencies_ms": [8182.045], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "bicycle: 10, wall: 1, floor: 1, flowers: 1, handlebar: 1, seat: 1, pedals: 1, kickstand: 1, chain: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24552.4, "ram_available_mb": 38288.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24568.1, "ram_available_mb": 38272.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.929}, "power_stats": {"power_gpu_soc_mean_watts": 22.233, "power_cpu_cv_mean_watts": 1.722, "power_sys_5v0_mean_watts": 8.956, "gpu_utilization_percent_mean": 72.929, "power_watts_avg": 22.233, "energy_joules_est": 181.92, "duration_seconds": 8.183, "sample_count": 70}, "timestamp": "2026-01-25T23:00:45.068264"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11172.535, "latencies_ms": [11172.535], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a pink bicycle with a decorative flower basket on the handlebars, positioned in the center of the image. To the right, there is a row of other bicycles, including a black one and a blue one, which are parked against the wall in the background. The floor is made of light-colored wood planks,", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24568.1, "ram_available_mb": 38272.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 24567.4, "ram_available_mb": 38273.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.705}, "power_stats": {"power_gpu_soc_mean_watts": 20.968, "power_cpu_cv_mean_watts": 2.369, "power_sys_5v0_mean_watts": 9.007, "gpu_utilization_percent_mean": 69.705, "power_watts_avg": 20.968, "energy_joules_est": 234.28, "duration_seconds": 11.173, "sample_count": 95}, "timestamp": "2026-01-25T23:00:58.286226"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7766.741, "latencies_ms": [7766.741], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a bicycle shop with a variety of bicycles on display. A pink bicycle with a flower basket is the focal point of the image, and it is placed in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24567.4, "ram_available_mb": 38273.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24580.5, "ram_available_mb": 38260.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.8}, "power_stats": {"power_gpu_soc_mean_watts": 22.548, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 8.966, "gpu_utilization_percent_mean": 73.8, "power_watts_avg": 22.548, "energy_joules_est": 175.14, "duration_seconds": 7.767, "sample_count": 65}, "timestamp": "2026-01-25T23:01:08.085644"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8384.378, "latencies_ms": [8384.378], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image features a collection of bicycles, predominantly in shades of pink and black, displayed indoors on a wooden floor. The lighting in the room is bright and natural, suggesting daylight coming from a window out of view.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24580.5, "ram_available_mb": 38260.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24641.6, "ram_available_mb": 38199.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.014}, "power_stats": {"power_gpu_soc_mean_watts": 21.981, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 8.994, "gpu_utilization_percent_mean": 72.014, "power_watts_avg": 21.981, "energy_joules_est": 184.31, "duration_seconds": 8.385, "sample_count": 71}, "timestamp": "2026-01-25T23:01:18.533356"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11142.638, "latencies_ms": [11142.638], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a dry savanna, a majestic giraffe stands tall and proud. Its body, a beautiful canvas of brown and white spots, contrasts with the surrounding greenery. The giraffe's long neck stretches upwards, reaching for the sky, while its legs are firmly planted on the ground. It faces the right side of the image", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24529.5, "ram_available_mb": 38311.4, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24522.0, "ram_available_mb": 38318.9, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.237}, "power_stats": {"power_gpu_soc_mean_watts": 20.804, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.237, "power_watts_avg": 20.804, "energy_joules_est": 231.83, "duration_seconds": 11.143, "sample_count": 97}, "timestamp": "2026-01-25T23:01:31.729858"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6833.667, "latencies_ms": [6833.667], "images_per_second": 0.146, "prompt_tokens": 39, "response_tokens_est": 39, "n_tiles": 16, "output_text": "giraffe: 1, grass: many, trees: 3, bushes: many, sky: 1, clouds: many, ground: dry, water: 0", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24522.0, "ram_available_mb": 38318.9, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24572.8, "ram_available_mb": 38268.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.069}, "power_stats": {"power_gpu_soc_mean_watts": 23.013, "power_cpu_cv_mean_watts": 1.553, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 73.069, "power_watts_avg": 23.013, "energy_joules_est": 157.28, "duration_seconds": 6.834, "sample_count": 58}, "timestamp": "2026-01-25T23:01:40.586436"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10872.964, "latencies_ms": [10872.964], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The giraffe is standing in the foreground of the image, with its body facing towards the right side of the frame. In the background, there are trees and shrubs scattered across the landscape, with a large tree visible to the right of the giraffe. The giraffe appears to be in a natural habitat, possibly a savannah or grassland.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24572.8, "ram_available_mb": 38268.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24529.4, "ram_available_mb": 38311.5, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.596}, "power_stats": {"power_gpu_soc_mean_watts": 21.015, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.959, "gpu_utilization_percent_mean": 69.596, "power_watts_avg": 21.015, "energy_joules_est": 228.51, "duration_seconds": 10.874, "sample_count": 94}, "timestamp": "2026-01-25T23:01:53.493292"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6819.459, "latencies_ms": [6819.459], "images_per_second": 0.147, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A giraffe is standing in a dry grassland with sparse trees in the background. The sky is overcast, and the giraffe appears to be looking off into the distance.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24529.4, "ram_available_mb": 38311.5, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24638.5, "ram_available_mb": 38202.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.362}, "power_stats": {"power_gpu_soc_mean_watts": 22.903, "power_cpu_cv_mean_watts": 1.574, "power_sys_5v0_mean_watts": 9.043, "gpu_utilization_percent_mean": 73.362, "power_watts_avg": 22.903, "energy_joules_est": 156.2, "duration_seconds": 6.82, "sample_count": 58}, "timestamp": "2026-01-25T23:02:02.349399"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7582.878, "latencies_ms": [7582.878], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The giraffe has a pattern of brown spots on its body, and it is standing in a dry grassland with a cloudy sky above. The lighting is soft, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24534.6, "ram_available_mb": 38306.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24632.6, "ram_available_mb": 38208.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.046}, "power_stats": {"power_gpu_soc_mean_watts": 21.897, "power_cpu_cv_mean_watts": 1.706, "power_sys_5v0_mean_watts": 9.002, "gpu_utilization_percent_mean": 72.046, "power_watts_avg": 21.897, "energy_joules_est": 166.05, "duration_seconds": 7.583, "sample_count": 65}, "timestamp": "2026-01-25T23:02:11.965111"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11242.911, "latencies_ms": [11242.911], "images_per_second": 0.089, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a young girl sitting on a luggage cart in a parking lot, surrounded by various cars. She is holding a paper and appears to be enjoying her time. The girl is accompanied by a child sitting on her lap, who is also on the luggage cart. \n\nThere are several cars parked around the area, with some closer to the foreground", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24528.9, "ram_available_mb": 38312.0, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24562.4, "ram_available_mb": 38278.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.958}, "power_stats": {"power_gpu_soc_mean_watts": 19.992, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.889, "gpu_utilization_percent_mean": 70.958, "power_watts_avg": 19.992, "energy_joules_est": 224.78, "duration_seconds": 11.244, "sample_count": 95}, "timestamp": "2026-01-25T23:02:25.256797"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9072.208, "latencies_ms": [9072.208], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Cars: 10\n- Cart: 1\n- Luggage: 1\n- Children: 2\n- Parking lot: 1\n- Store: 1\n- Parking sign: 1\n- Parking lot sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24562.4, "ram_available_mb": 38278.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24539.7, "ram_available_mb": 38301.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.87}, "power_stats": {"power_gpu_soc_mean_watts": 21.636, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 70.87, "power_watts_avg": 21.636, "energy_joules_est": 196.3, "duration_seconds": 9.073, "sample_count": 77}, "timestamp": "2026-01-25T23:02:36.349133"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9550.476, "latencies_ms": [9550.476], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "In the foreground, there is a black car with a child sitting on a suitcase. The child is positioned in the middle of the image, surrounded by other cars parked in a lot. The background shows a Budget Bay store sign and more cars, indicating that the parking lot is quite full.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24539.7, "ram_available_mb": 38301.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24614.8, "ram_available_mb": 38226.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.22}, "power_stats": {"power_gpu_soc_mean_watts": 21.425, "power_cpu_cv_mean_watts": 1.831, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 70.22, "power_watts_avg": 21.425, "energy_joules_est": 204.63, "duration_seconds": 9.551, "sample_count": 82}, "timestamp": "2026-01-25T23:02:47.922398"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8512.376, "latencies_ms": [8512.376], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "A young child is sitting on a suitcase in a parking lot, with a Budget Bay 17-18 sign visible in the background. The parking lot is filled with various cars and a truck, indicating a busy shopping area.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24527.8, "ram_available_mb": 38313.1, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24543.7, "ram_available_mb": 38297.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.903}, "power_stats": {"power_gpu_soc_mean_watts": 22.067, "power_cpu_cv_mean_watts": 1.73, "power_sys_5v0_mean_watts": 8.962, "gpu_utilization_percent_mean": 71.903, "power_watts_avg": 22.067, "energy_joules_est": 187.86, "duration_seconds": 8.513, "sample_count": 72}, "timestamp": "2026-01-25T23:02:58.490979"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6313.439, "latencies_ms": [6313.439], "images_per_second": 0.158, "prompt_tokens": 36, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The image shows a wet parking lot with various cars and a yellow taxi. The sky is overcast, and the ground appears to be wet, suggesting recent rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24543.7, "ram_available_mb": 38297.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24532.6, "ram_available_mb": 38308.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.453}, "power_stats": {"power_gpu_soc_mean_watts": 23.242, "power_cpu_cv_mean_watts": 1.533, "power_sys_5v0_mean_watts": 9.085, "gpu_utilization_percent_mean": 73.453, "power_watts_avg": 23.242, "energy_joules_est": 146.75, "duration_seconds": 6.314, "sample_count": 53}, "timestamp": "2026-01-25T23:03:06.818178"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12134.413, "latencies_ms": [12134.413], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a city bus driving down a street in a small town. The bus is yellow and blue, and it is positioned in the middle of the road. There are several other vehicles on the street, including a truck and a car. \n\nIn addition to the vehicles, there are multiple people visible in the scene. Some of them are walking on the sidewalk, while", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 24532.6, "ram_available_mb": 38308.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24540.3, "ram_available_mb": 38300.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11525.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.99}, "power_stats": {"power_gpu_soc_mean_watts": 22.049, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 9.133, "gpu_utilization_percent_mean": 72.99, "power_watts_avg": 22.049, "energy_joules_est": 267.57, "duration_seconds": 12.135, "sample_count": 103}, "timestamp": "2026-01-25T23:03:20.982671"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7753.541, "latencies_ms": [7753.541], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 38, "n_tiles": 16, "output_text": "bus: 1, van: 1, building: 3, tree: many, flower: many, grass: many, bus stop: 1, car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24540.3, "ram_available_mb": 38300.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24526.6, "ram_available_mb": 38314.3, "ram_percent": 39.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11559.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 79.076}, "power_stats": {"power_gpu_soc_mean_watts": 24.418, "power_cpu_cv_mean_watts": 1.377, "power_sys_5v0_mean_watts": 9.128, "gpu_utilization_percent_mean": 79.076, "power_watts_avg": 24.418, "energy_joules_est": 189.35, "duration_seconds": 7.754, "sample_count": 66}, "timestamp": "2026-01-25T23:03:30.783993"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10476.476, "latencies_ms": [10476.476], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "A yellow and blue bus with the word 'citylink' on the front is in the foreground, driving on the right side of the road. In the background, there are houses on the left and a hill with trees on the right. The bus is closer to the viewer than the houses and the hill.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24526.6, "ram_available_mb": 38314.3, "ram_percent": 39.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24539.9, "ram_available_mb": 38301.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11570.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.809}, "power_stats": {"power_gpu_soc_mean_watts": 23.084, "power_cpu_cv_mean_watts": 1.705, "power_sys_5v0_mean_watts": 9.159, "gpu_utilization_percent_mean": 73.809, "power_watts_avg": 23.084, "energy_joules_est": 241.85, "duration_seconds": 10.477, "sample_count": 89}, "timestamp": "2026-01-25T23:03:43.292797"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8561.72, "latencies_ms": [8561.72], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A blue and yellow bus with the word \"citylink\" on the front is driving down a street in a small town. There are houses on the left side of the street and a hill with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24539.9, "ram_available_mb": 38301.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24545.4, "ram_available_mb": 38295.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11554.8, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.958}, "power_stats": {"power_gpu_soc_mean_watts": 23.978, "power_cpu_cv_mean_watts": 1.457, "power_sys_5v0_mean_watts": 9.127, "gpu_utilization_percent_mean": 77.958, "power_watts_avg": 23.978, "energy_joules_est": 205.31, "duration_seconds": 8.562, "sample_count": 72}, "timestamp": "2026-01-25T23:03:53.879347"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8144.374, "latencies_ms": [8144.374], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A blue and yellow bus with the word \"citylink\" on the front is driving down the road. The sky is overcast and the trees in the background are bare, indicating it might be fall or winter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24545.4, "ram_available_mb": 38295.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24646.8, "ram_available_mb": 38194.1, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11552.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.783}, "power_stats": {"power_gpu_soc_mean_watts": 23.969, "power_cpu_cv_mean_watts": 1.485, "power_sys_5v0_mean_watts": 9.199, "gpu_utilization_percent_mean": 76.783, "power_watts_avg": 23.969, "energy_joules_est": 195.23, "duration_seconds": 8.145, "sample_count": 69}, "timestamp": "2026-01-25T23:04:04.043857"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11126.984, "latencies_ms": [11126.984], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene beach scene with a large bird, possibly a pelican, standing on a rock near the water. The bird is positioned on the right side of the image, with its wings spread out, as if it is about to take flight. The beach is lined with several chairs, providing a relaxing atmosphere for visitors.\n\nIn the background, there", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 24568.2, "ram_available_mb": 38272.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24559.2, "ram_available_mb": 38281.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.734}, "power_stats": {"power_gpu_soc_mean_watts": 20.961, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 69.734, "power_watts_avg": 20.961, "energy_joules_est": 233.25, "duration_seconds": 11.128, "sample_count": 94}, "timestamp": "2026-01-25T23:04:17.202417"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9518.945, "latencies_ms": [9518.945], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Bird: 1\n2. Water: 1\n3. Sand: 1\n4. Rocks: 1\n5. Trees: 1\n6. Mountains: 1\n7. Umbrellas: 1\n8. Chairs: 1", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24559.2, "ram_available_mb": 38281.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24573.3, "ram_available_mb": 38267.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.805}, "power_stats": {"power_gpu_soc_mean_watts": 21.43, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 8.943, "gpu_utilization_percent_mean": 70.805, "power_watts_avg": 21.43, "energy_joules_est": 204.0, "duration_seconds": 9.52, "sample_count": 82}, "timestamp": "2026-01-25T23:04:28.772780"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9999.355, "latencies_ms": [9999.355], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a rocky outcrop with a bird perched on top, positioned near the center of the image. The beach stretches out in the middle ground, leading to the calm sea in the background. The hills are situated far in the distance, creating a sense of depth in the landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24573.3, "ram_available_mb": 38267.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24573.8, "ram_available_mb": 38267.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.44}, "power_stats": {"power_gpu_soc_mean_watts": 21.263, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 8.976, "gpu_utilization_percent_mean": 70.44, "power_watts_avg": 21.263, "energy_joules_est": 212.63, "duration_seconds": 10.0, "sample_count": 84}, "timestamp": "2026-01-25T23:04:40.790614"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11173.158, "latencies_ms": [11173.158], "images_per_second": 0.09, "prompt_tokens": 37, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The image depicts a serene beach scene with a bird perched on a rock in the foreground. The bird appears to be a pelican, and it is looking out towards the sea. The beach is lined with thatched umbrellas and chairs, and there are mountains in the background. The sky is cloudy, and the sea is calm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24573.8, "ram_available_mb": 38267.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24590.1, "ram_available_mb": 38250.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.438}, "power_stats": {"power_gpu_soc_mean_watts": 21.038, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.934, "gpu_utilization_percent_mean": 69.438, "power_watts_avg": 21.038, "energy_joules_est": 235.07, "duration_seconds": 11.174, "sample_count": 96}, "timestamp": "2026-01-25T23:04:53.986290"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11156.823, "latencies_ms": [11156.823], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a seagull perched on a rocky outcrop with a backdrop of a cloudy sky and a calm sea. The colors in the image are predominantly blue and grey, with the blue representing the sea and sky, and the grey representing the clouds. The lighting is natural and soft, with the sun shining through the clouds, creating a peaceful", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24590.1, "ram_available_mb": 38250.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 24589.8, "ram_available_mb": 38251.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.135}, "power_stats": {"power_gpu_soc_mean_watts": 20.851, "power_cpu_cv_mean_watts": 2.49, "power_sys_5v0_mean_watts": 9.022, "gpu_utilization_percent_mean": 69.135, "power_watts_avg": 20.851, "energy_joules_est": 232.65, "duration_seconds": 11.158, "sample_count": 96}, "timestamp": "2026-01-25T23:05:07.204922"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11132.671, "latencies_ms": [11132.671], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is comfortably seated in a gray recliner chair. He is wearing a blue plaid shirt and glasses, and he's holding a brown paper bag in his left hand. His right hand is holding a black remote control. The chair is positioned against a white wall, and there's a window in the background. The man", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24589.8, "ram_available_mb": 38251.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24577.4, "ram_available_mb": 38263.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.411}, "power_stats": {"power_gpu_soc_mean_watts": 20.848, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 69.411, "power_watts_avg": 20.848, "energy_joules_est": 232.11, "duration_seconds": 11.133, "sample_count": 95}, "timestamp": "2026-01-25T23:05:20.383729"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9753.798, "latencies_ms": [9753.798], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "- Chair: 1\n\n- Paper bag: 1\n\n- Remote control: 2\n\n- Paper: 1\n\n- Table: 1\n\n- Couch: 1\n\n- Blanket: 1\n\n- Pillow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24577.4, "ram_available_mb": 38263.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24545.8, "ram_available_mb": 38295.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.585}, "power_stats": {"power_gpu_soc_mean_watts": 21.544, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 8.935, "gpu_utilization_percent_mean": 70.585, "power_watts_avg": 21.544, "energy_joules_est": 210.15, "duration_seconds": 9.754, "sample_count": 82}, "timestamp": "2026-01-25T23:05:32.165885"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8254.175, "latencies_ms": [8254.175], "images_per_second": 0.121, "prompt_tokens": 44, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The person is seated in a chair that is positioned in the foreground of the image. The chair is placed against a wall in the background, and there is a remote control on the right side of the image, near the person's lap.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24545.8, "ram_available_mb": 38295.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24571.3, "ram_available_mb": 38269.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.352}, "power_stats": {"power_gpu_soc_mean_watts": 21.384, "power_cpu_cv_mean_watts": 1.743, "power_sys_5v0_mean_watts": 8.993, "gpu_utilization_percent_mean": 71.352, "power_watts_avg": 21.384, "energy_joules_est": 176.52, "duration_seconds": 8.255, "sample_count": 71}, "timestamp": "2026-01-25T23:05:42.432346"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6940.187, "latencies_ms": [6940.187], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A person is sitting in a gray recliner chair, holding a brown paper bag in one hand and a remote control in the other, possibly preparing to watch television or eat snacks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24571.3, "ram_available_mb": 38269.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24624.2, "ram_available_mb": 38216.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.153}, "power_stats": {"power_gpu_soc_mean_watts": 22.697, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.99, "gpu_utilization_percent_mean": 74.153, "power_watts_avg": 22.697, "energy_joules_est": 157.53, "duration_seconds": 6.941, "sample_count": 59}, "timestamp": "2026-01-25T23:05:51.407410"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6530.478, "latencies_ms": [6530.478], "images_per_second": 0.153, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The person is wearing a blue and white checkered shirt and is seated in a gray recliner. There is a remote control on the armrest of the chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24624.2, "ram_available_mb": 38216.7, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24630.3, "ram_available_mb": 38210.6, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.727}, "power_stats": {"power_gpu_soc_mean_watts": 23.106, "power_cpu_cv_mean_watts": 1.573, "power_sys_5v0_mean_watts": 9.088, "gpu_utilization_percent_mean": 73.727, "power_watts_avg": 23.106, "energy_joules_est": 150.91, "duration_seconds": 6.531, "sample_count": 55}, "timestamp": "2026-01-25T23:05:59.961805"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11116.241, "latencies_ms": [11116.241], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a man is captured in the midst of a powerful tennis swing. He's dressed in an orange shirt and black shorts, his athletic form poised in anticipation of the next move. His blue and white tennis racket is extended towards the right side of the image, ready to strike the yellow tennis ball that's suspended in the air", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 24568.7, "ram_available_mb": 38272.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24663.2, "ram_available_mb": 38177.7, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.505}, "power_stats": {"power_gpu_soc_mean_watts": 20.944, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 69.505, "power_watts_avg": 20.944, "energy_joules_est": 232.83, "duration_seconds": 11.117, "sample_count": 95}, "timestamp": "2026-01-25T23:06:13.119451"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9986.957, "latencies_ms": [9986.957], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 67, "n_tiles": 16, "output_text": "- tennis ball: 1\n\n- tennis racket: 1\n\n- tennis court: 1\n\n- tennis net: 1\n\n- tennis player: 1\n\n- orange shirt: 1\n\n- black shorts: 1\n\n- white socks: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24593.0, "ram_available_mb": 38247.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24643.6, "ram_available_mb": 38197.3, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10673.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.107}, "power_stats": {"power_gpu_soc_mean_watts": 21.428, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.927, "gpu_utilization_percent_mean": 70.107, "power_watts_avg": 21.428, "energy_joules_est": 214.01, "duration_seconds": 9.988, "sample_count": 84}, "timestamp": "2026-01-25T23:06:25.135044"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7145.558, "latencies_ms": [7145.558], "images_per_second": 0.14, "prompt_tokens": 44, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The tennis player is positioned in the foreground on the left side of the image, near the net which is in the middle ground. The background shows the tennis court extending to the edges of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24571.7, "ram_available_mb": 38269.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24636.9, "ram_available_mb": 38204.0, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10683.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.344}, "power_stats": {"power_gpu_soc_mean_watts": 22.704, "power_cpu_cv_mean_watts": 1.647, "power_sys_5v0_mean_watts": 9.031, "gpu_utilization_percent_mean": 73.344, "power_watts_avg": 22.704, "energy_joules_est": 162.25, "duration_seconds": 7.146, "sample_count": 61}, "timestamp": "2026-01-25T23:06:34.335175"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7090.964, "latencies_ms": [7090.964], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A tennis player in an orange shirt and black shorts is in the middle of a forehand swing, hitting a yellow tennis ball with a blue and white racket on a green tennis court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24636.9, "ram_available_mb": 38204.0, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24604.5, "ram_available_mb": 38236.4, "ram_percent": 39.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10669.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.25}, "power_stats": {"power_gpu_soc_mean_watts": 22.775, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 9.007, "gpu_utilization_percent_mean": 73.25, "power_watts_avg": 22.775, "energy_joules_est": 161.51, "duration_seconds": 7.092, "sample_count": 60}, "timestamp": "2026-01-25T23:06:43.458241"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7151.741, "latencies_ms": [7151.741], "images_per_second": 0.14, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The tennis player is wearing an orange shirt and a matching cap, and the court appears to be a vibrant green. The lighting seems natural, suggesting the photo was taken during the day.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24604.5, "ram_available_mb": 38236.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24679.3, "ram_available_mb": 38161.6, "ram_percent": 39.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10667.3, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.967}, "power_stats": {"power_gpu_soc_mean_watts": 22.629, "power_cpu_cv_mean_watts": 1.628, "power_sys_5v0_mean_watts": 9.049, "gpu_utilization_percent_mean": 72.967, "power_watts_avg": 22.629, "energy_joules_est": 161.85, "duration_seconds": 7.152, "sample_count": 60}, "timestamp": "2026-01-25T23:06:52.657101"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12328.255, "latencies_ms": [12328.255], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a cozy, compact kitchen bathed in soft light. The walls, painted a cheerful yellow, contrast with the white ceiling, creating a warm and inviting atmosphere. A black floor mat lies in the center of the room, adding a touch of elegance to the space.\n\nOn the left, a white countertop houses a sink and a stove,", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24617.5, "ram_available_mb": 38223.4, "ram_percent": 39.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24588.3, "ram_available_mb": 38252.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.81}, "power_stats": {"power_gpu_soc_mean_watts": 22.821, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 9.189, "gpu_utilization_percent_mean": 73.81, "power_watts_avg": 22.821, "energy_joules_est": 281.36, "duration_seconds": 12.329, "sample_count": 105}, "timestamp": "2026-01-25T23:07:07.019633"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9976.888, "latencies_ms": [9976.888], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Shelf: 2\n- Canister: 4\n- Stove: 1\n- Oven: 1\n- Countertop: 1\n- Chair: 1\n- Door: 1\n- Mat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24588.3, "ram_available_mb": 38252.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24592.0, "ram_available_mb": 38248.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.471}, "power_stats": {"power_gpu_soc_mean_watts": 23.776, "power_cpu_cv_mean_watts": 1.569, "power_sys_5v0_mean_watts": 9.151, "gpu_utilization_percent_mean": 77.471, "power_watts_avg": 23.776, "energy_joules_est": 237.23, "duration_seconds": 9.978, "sample_count": 85}, "timestamp": "2026-01-25T23:07:19.046072"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12416.737, "latencies_ms": [12416.737], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The kitchen area is in the foreground of the image, with the stove and sink positioned close to each other. In the background, there is a doorway leading to another room, and a fire extinguisher is mounted on the wall to the right of the stove. The ceiling lights are positioned above the kitchen, providing illumination to the space.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24592.0, "ram_available_mb": 38248.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24575.2, "ram_available_mb": 38265.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.028}, "power_stats": {"power_gpu_soc_mean_watts": 22.894, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 9.207, "gpu_utilization_percent_mean": 74.028, "power_watts_avg": 22.894, "energy_joules_est": 284.28, "duration_seconds": 12.417, "sample_count": 106}, "timestamp": "2026-01-25T23:07:33.501710"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9526.6, "latencies_ms": [9526.6], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a small, compact kitchen with yellow walls and wooden accents. It appears to be a galley or a small kitchenette on a boat, with a sink, stove, and various kitchen utensils and appliances.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24575.2, "ram_available_mb": 38265.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24569.4, "ram_available_mb": 38271.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.963}, "power_stats": {"power_gpu_soc_mean_watts": 23.96, "power_cpu_cv_mean_watts": 1.523, "power_sys_5v0_mean_watts": 9.122, "gpu_utilization_percent_mean": 77.963, "power_watts_avg": 23.96, "energy_joules_est": 228.27, "duration_seconds": 9.527, "sample_count": 81}, "timestamp": "2026-01-25T23:07:45.066640"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8259.172, "latencies_ms": [8259.172], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The kitchen is predominantly yellow with wooden cabinets and a red fire extinguisher on the wall. The lighting is bright, coming from a skylight and additional ceiling lights.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24569.4, "ram_available_mb": 38271.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24591.4, "ram_available_mb": 38249.5, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.071}, "power_stats": {"power_gpu_soc_mean_watts": 24.343, "power_cpu_cv_mean_watts": 1.447, "power_sys_5v0_mean_watts": 9.267, "gpu_utilization_percent_mean": 78.071, "power_watts_avg": 24.343, "energy_joules_est": 201.07, "duration_seconds": 8.26, "sample_count": 70}, "timestamp": "2026-01-25T23:07:55.386159"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11148.973, "latencies_ms": [11148.973], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a delicious sandwich takes the spotlight. It's a classic sub sandwich, generously filled with layers of sliced turkey, ham, and roast beef. The sandwich is nestled between two slices of white bread, and it's topped with a layer of fresh lettuce and a slice of ripe tom", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24591.4, "ram_available_mb": 38249.5, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24547.8, "ram_available_mb": 38293.1, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.242}, "power_stats": {"power_gpu_soc_mean_watts": 20.886, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.954, "gpu_utilization_percent_mean": 69.242, "power_watts_avg": 20.886, "energy_joules_est": 232.87, "duration_seconds": 11.15, "sample_count": 95}, "timestamp": "2026-01-25T23:08:08.569447"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9963.014, "latencies_ms": [9963.014], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 67, "n_tiles": 16, "output_text": "- Bread: 2 slices\n- Lettuce: 2 leaves\n- Tomato: 1 slice\n- Ham: 2 slices\n- Pickles: 2 pickles\n- Pepper: 1 pepper\n- Plate: 1 plate\n- Computer: 1 computer", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24547.8, "ram_available_mb": 38293.1, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24551.0, "ram_available_mb": 38289.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.524}, "power_stats": {"power_gpu_soc_mean_watts": 21.487, "power_cpu_cv_mean_watts": 1.821, "power_sys_5v0_mean_watts": 8.955, "gpu_utilization_percent_mean": 70.524, "power_watts_avg": 21.487, "energy_joules_est": 214.09, "duration_seconds": 9.964, "sample_count": 84}, "timestamp": "2026-01-25T23:08:20.552750"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9811.307, "latencies_ms": [9811.307], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The sandwich is in the foreground, placed on a white paper plate which is on a wooden table. In the background, there is a computer monitor and a keyboard, suggesting that the setting is an office or a workspace. The sandwich is positioned to the left of the monitor and in front of the keyboard.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24551.0, "ram_available_mb": 38289.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24546.5, "ram_available_mb": 38294.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 70.494}, "power_stats": {"power_gpu_soc_mean_watts": 21.375, "power_cpu_cv_mean_watts": 1.843, "power_sys_5v0_mean_watts": 8.977, "gpu_utilization_percent_mean": 70.494, "power_watts_avg": 21.375, "energy_joules_est": 209.73, "duration_seconds": 9.812, "sample_count": 83}, "timestamp": "2026-01-25T23:08:32.376880"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8502.815, "latencies_ms": [8502.815], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "A sandwich is placed on a white paper plate on a wooden table, with a computer monitor and keyboard in the background. The sandwich appears to be a multi-layered sub with various ingredients, and there is a pickle on the side.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24546.5, "ram_available_mb": 38294.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24551.9, "ram_available_mb": 38289.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 71.889}, "power_stats": {"power_gpu_soc_mean_watts": 22.004, "power_cpu_cv_mean_watts": 1.741, "power_sys_5v0_mean_watts": 8.972, "gpu_utilization_percent_mean": 71.889, "power_watts_avg": 22.004, "energy_joules_est": 187.11, "duration_seconds": 8.503, "sample_count": 72}, "timestamp": "2026-01-25T23:08:42.921588"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6965.373, "latencies_ms": [6965.373], "images_per_second": 0.144, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The sandwich is placed on a white paper plate, which is on a wooden table. The lighting in the room is dim, with the primary light source coming from the computer screen in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24551.9, "ram_available_mb": 38289.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24549.6, "ram_available_mb": 38291.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.458}, "power_stats": {"power_gpu_soc_mean_watts": 22.835, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 9.077, "gpu_utilization_percent_mean": 72.458, "power_watts_avg": 22.835, "energy_joules_est": 159.07, "duration_seconds": 6.966, "sample_count": 59}, "timestamp": "2026-01-25T23:08:51.900863"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11164.455, "latencies_ms": [11164.455], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a desk with a computer setup, including two monitors, a keyboard, and a mouse. The monitors are positioned side by side, with one on the left and the other on the right. The keyboard is placed in front of the monitors, and the mouse is located in front of the keyboard. \n\nThere are also two cell phones on the des", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 24549.6, "ram_available_mb": 38291.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 9.1, "ram_used_mb": 24589.3, "ram_available_mb": 38251.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10621.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.24}, "power_stats": {"power_gpu_soc_mean_watts": 20.83, "power_cpu_cv_mean_watts": 2.323, "power_sys_5v0_mean_watts": 8.969, "gpu_utilization_percent_mean": 69.24, "power_watts_avg": 20.83, "energy_joules_est": 232.57, "duration_seconds": 11.165, "sample_count": 96}, "timestamp": "2026-01-25T23:09:05.096911"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8627.449, "latencies_ms": [8627.449], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Computer monitor: 2\n- Keyboard: 1\n- Computer mouse: 1\n- Cell phone: 2\n- Tablet: 1\n- Wire: 5\n- Screen: 2\n- Monitor: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24589.3, "ram_available_mb": 38251.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24583.7, "ram_available_mb": 38257.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10651.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 72.425}, "power_stats": {"power_gpu_soc_mean_watts": 22.038, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.958, "gpu_utilization_percent_mean": 72.425, "power_watts_avg": 22.038, "energy_joules_est": 190.15, "duration_seconds": 8.628, "sample_count": 73}, "timestamp": "2026-01-25T23:09:15.760685"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11112.327, "latencies_ms": [11112.327], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a workspace with two computer monitors positioned side by side, creating a near-far spatial relationship as the left monitor is slightly closer to the viewer than the right one. In the foreground, there is a white keyboard and a white mouse, both of which are in front of the monitors. In the background, there are various electronic devices such as a smartphone", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 24583.7, "ram_available_mb": 38257.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24559.7, "ram_available_mb": 38281.2, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10660.0, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.681}, "power_stats": {"power_gpu_soc_mean_watts": 20.949, "power_cpu_cv_mean_watts": 1.917, "power_sys_5v0_mean_watts": 8.992, "gpu_utilization_percent_mean": 69.681, "power_watts_avg": 20.949, "energy_joules_est": 232.8, "duration_seconds": 11.113, "sample_count": 94}, "timestamp": "2026-01-25T23:09:28.893122"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8202.684, "latencies_ms": [8202.684], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image depicts a workspace with a dual monitor computer setup, a keyboard, a mouse, and a smartphone. The monitors are turned on and displaying various web pages, suggesting that the user is working or browsing the internet.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 24559.7, "ram_available_mb": 38281.2, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24585.2, "ram_available_mb": 38255.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10645.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.13}, "power_stats": {"power_gpu_soc_mean_watts": 22.232, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 9.007, "gpu_utilization_percent_mean": 73.13, "power_watts_avg": 22.232, "energy_joules_est": 182.38, "duration_seconds": 8.203, "sample_count": 69}, "timestamp": "2026-01-25T23:09:39.116838"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11145.371, "latencies_ms": [11145.371], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The image shows a desk with two computer monitors, one of which is turned on displaying a webpage with a red and white color scheme. The other monitor is off, and there is a white keyboard and a white mouse in front of it. The desk is illuminated by ambient light, and there are various cables and a small smartphone on the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24585.2, "ram_available_mb": 38255.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 24579.5, "ram_available_mb": 38261.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 10643.6, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 69.84}, "power_stats": {"power_gpu_soc_mean_watts": 19.945, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.901, "gpu_utilization_percent_mean": 69.84, "power_watts_avg": 19.945, "energy_joules_est": 222.31, "duration_seconds": 11.146, "sample_count": 94}, "timestamp": "2026-01-25T23:09:52.279342"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12412.909, "latencies_ms": [12412.909], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The image captures a group of people lying on the floor in a bathroom, with their heads and upper bodies visible. They are all smiling and appear to be having a good time. The bathroom has a white toilet, and the floor is tiled. The people are spread out across the floor, with some closer to the toilet and others further away.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 24579.5, "ram_available_mb": 38261.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24567.5, "ram_available_mb": 38273.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.726}, "power_stats": {"power_gpu_soc_mean_watts": 22.633, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 9.169, "gpu_utilization_percent_mean": 73.726, "power_watts_avg": 22.633, "energy_joules_est": 280.96, "duration_seconds": 12.414, "sample_count": 106}, "timestamp": "2026-01-25T23:10:06.724782"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9631.487, "latencies_ms": [9631.487], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "person: 10, hand: 10, tile: 100, floor: 100, wall: 100, reflection: 100, camera: 1, toilet: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 24567.5, "ram_available_mb": 38273.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24565.2, "ram_available_mb": 38275.7, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 77.951}, "power_stats": {"power_gpu_soc_mean_watts": 23.944, "power_cpu_cv_mean_watts": 1.518, "power_sys_5v0_mean_watts": 9.129, "gpu_utilization_percent_mean": 77.951, "power_watts_avg": 23.944, "energy_joules_est": 230.63, "duration_seconds": 9.632, "sample_count": 81}, "timestamp": "2026-01-25T23:10:18.377407"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12574.52, "latencies_ms": [12574.52], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a group of people lying on the ground in the foreground, with their heads and upper bodies visible. They are positioned in a way that creates a mosaic effect with the tiled floor, which extends into the background. The perspective is from above, looking down on the scene, and the people appear to be at different distances from the camera, with some closer and others", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 24565.2, "ram_available_mb": 38275.7, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24569.1, "ram_available_mb": 38271.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.215}, "power_stats": {"power_gpu_soc_mean_watts": 22.149, "power_cpu_cv_mean_watts": 1.793, "power_sys_5v0_mean_watts": 9.129, "gpu_utilization_percent_mean": 74.215, "power_watts_avg": 22.149, "energy_joules_est": 278.53, "duration_seconds": 12.575, "sample_count": 107}, "timestamp": "2026-01-25T23:10:32.988119"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10270.294, "latencies_ms": [10270.294], "images_per_second": 0.097, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image shows a group of people lying on the floor in a public restroom, with their heads and upper bodies visible through the gaps between the tiles. They appear to be playfully posing for a photo, with some of them making funny faces and gestures.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 24569.1, "ram_available_mb": 38271.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24566.1, "ram_available_mb": 38274.8, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 76.455}, "power_stats": {"power_gpu_soc_mean_watts": 23.647, "power_cpu_cv_mean_watts": 1.602, "power_sys_5v0_mean_watts": 9.134, "gpu_utilization_percent_mean": 76.455, "power_watts_avg": 23.647, "energy_joules_est": 242.88, "duration_seconds": 10.271, "sample_count": 88}, "timestamp": "2026-01-25T23:10:45.277990"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11828.276, "latencies_ms": [11828.276], "images_per_second": 0.085, "prompt_tokens": 36, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image shows a group of people lying on the floor with their heads down, creating a pattern that resembles a mosaic. The lighting is bright and artificial, illuminating the scene with a clear and even distribution of light. The floor is tiled, and the people are wearing casual clothing, suggesting an indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24566.1, "ram_available_mb": 38274.8, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24574.9, "ram_available_mb": 38266.0, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.26}, "power_stats": {"power_gpu_soc_mean_watts": 23.061, "power_cpu_cv_mean_watts": 1.734, "power_sys_5v0_mean_watts": 9.212, "gpu_utilization_percent_mean": 74.26, "power_watts_avg": 23.061, "energy_joules_est": 272.79, "duration_seconds": 11.829, "sample_count": 100}, "timestamp": "2026-01-25T23:10:59.118655"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12338.632, "latencies_ms": [12338.632], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a small gray bird is perched on a tree branch. The bird is facing to the left, and its beak is slightly open. The branch it's perched on is brown and has a small yellow tag on it. The background is a blurred green, suggesting a lush environment. The bird's feathers are a mix of gray and white,", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 24574.9, "ram_available_mb": 38266.0, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24557.3, "ram_available_mb": 38283.6, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11882.1, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 73.991}, "power_stats": {"power_gpu_soc_mean_watts": 22.827, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 9.19, "gpu_utilization_percent_mean": 73.991, "power_watts_avg": 22.827, "energy_joules_est": 281.67, "duration_seconds": 12.339, "sample_count": 106}, "timestamp": "2026-01-25T23:11:13.508488"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8047.027, "latencies_ms": [8047.027], "images_per_second": 0.124, "prompt_tokens": 39, "response_tokens_est": 38, "n_tiles": 16, "output_text": "bird: 1, branch: 2, leaves: multiple, background: multiple, light: multiple, bokeh: multiple, sky: 1, tree: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24557.3, "ram_available_mb": 38283.6, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24585.6, "ram_available_mb": 38255.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11917.4, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 80.221}, "power_stats": {"power_gpu_soc_mean_watts": 24.743, "power_cpu_cv_mean_watts": 1.36, "power_sys_5v0_mean_watts": 9.178, "gpu_utilization_percent_mean": 80.221, "power_watts_avg": 24.743, "energy_joules_est": 199.12, "duration_seconds": 8.048, "sample_count": 68}, "timestamp": "2026-01-25T23:11:23.583159"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12443.429, "latencies_ms": [12443.429], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The bird is perched on a branch in the foreground of the image, with a blurred green background that suggests a dense foliage environment. The bird is positioned slightly to the left of the center of the image, and there is a clear space between it and the background, indicating it is the main subject of the photo. The branch it is perched on is in the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24585.6, "ram_available_mb": 38255.3, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24544.0, "ram_available_mb": 38296.9, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11927.9, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.095}, "power_stats": {"power_gpu_soc_mean_watts": 22.87, "power_cpu_cv_mean_watts": 1.774, "power_sys_5v0_mean_watts": 9.176, "gpu_utilization_percent_mean": 74.095, "power_watts_avg": 22.87, "energy_joules_est": 284.6, "duration_seconds": 12.444, "sample_count": 105}, "timestamp": "2026-01-25T23:11:38.057305"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8955.626, "latencies_ms": [8955.626], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "A small bird is perched on a branch with a blurred green background, suggesting a natural, outdoor setting. The bird appears to be looking around, possibly observing its surroundings or searching for food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24544.0, "ram_available_mb": 38296.9, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24545.5, "ram_available_mb": 38295.4, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11911.5, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 78.187}, "power_stats": {"power_gpu_soc_mean_watts": 24.243, "power_cpu_cv_mean_watts": 1.479, "power_sys_5v0_mean_watts": 9.18, "gpu_utilization_percent_mean": 78.187, "power_watts_avg": 24.243, "energy_joules_est": 217.13, "duration_seconds": 8.956, "sample_count": 75}, "timestamp": "2026-01-25T23:11:49.054938"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9938.715, "latencies_ms": [9938.715], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The bird is perched on a branch with a backdrop of green foliage, suggesting a natural, outdoor setting. The lighting in the image is soft and diffused, with no harsh shadows, indicating an overcast day or a shaded area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24545.5, "ram_available_mb": 38295.4, "ram_percent": 39.1}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24590.6, "ram_available_mb": 38250.3, "ram_percent": 39.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 17658.0, "gpu_max_mem_alloc_mb": 11909.2, "gpu_max_mem_reserved_mb": 17658.0, "gpu_utilization_percent": 74.893}, "power_stats": {"power_gpu_soc_mean_watts": 23.612, "power_cpu_cv_mean_watts": 1.621, "power_sys_5v0_mean_watts": 9.232, "gpu_utilization_percent_mean": 74.893, "power_watts_avg": 23.612, "energy_joules_est": 234.69, "duration_seconds": 9.939, "sample_count": 84}, "timestamp": "2026-01-25T23:12:01.043922"}
