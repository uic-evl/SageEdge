{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11595.091, "latencies_ms": [11595.091], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a woman standing in a spacious living room with yellow walls. She is positioned near a dining table, which is surrounded by several chairs. The room is well-lit, with a television placed on the left side of the room. A potted plant can be seen on the right side of the room, adding a touch of greenery to the space.\n", "error": null, "sys_before": {"cpu_percent": 7.9, "ram_used_mb": 19241.9, "ram_available_mb": 43599.0, "ram_percent": 30.6}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 19276.2, "ram_available_mb": 43564.7, "ram_percent": 30.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 11762.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 11762.0, "gpu_utilization_percent": 69.929}, "power_stats": {"power_gpu_soc_mean_watts": 19.74, "power_cpu_cv_mean_watts": 1.942, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 69.929, "power_watts_avg": 19.74, "energy_joules_est": 228.9, "duration_seconds": 11.596, "sample_count": 98}, "timestamp": "2026-01-26T09:01:02.861556"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8649.255, "latencies_ms": [8649.255], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "- Television: 1\n- Chair: 6\n- Table: 1\n- Sofa: 1\n- Cabinet: 2\n- Window: 3\n- Plant: 2\n- Rug: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 19276.2, "ram_available_mb": 43564.7, "ram_percent": 30.7}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 19286.3, "ram_available_mb": 43554.6, "ram_percent": 30.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 11762.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 11762.0, "gpu_utilization_percent": 72.014}, "power_stats": {"power_gpu_soc_mean_watts": 20.519, "power_cpu_cv_mean_watts": 1.57, "power_sys_5v0_mean_watts": 8.385, "gpu_utilization_percent_mean": 72.014, "power_watts_avg": 20.519, "energy_joules_est": 177.49, "duration_seconds": 8.65, "sample_count": 73}, "timestamp": "2026-01-26T09:01:13.531590"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11709.513, "latencies_ms": [11709.513], "images_per_second": 0.085, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a television set on the left side of the room, with a DVD player and a red vase in front of it. The right side of the room features a dining area with a table and chairs, and a woman is standing near the kitchen area in the background. The room has a large window on the right side, allowing natural light to enter and", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 19286.3, "ram_available_mb": 43554.6, "ram_percent": 30.7}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 19288.4, "ram_available_mb": 43552.5, "ram_percent": 30.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 11762.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 11762.0, "gpu_utilization_percent": 69.108}, "power_stats": {"power_gpu_soc_mean_watts": 19.117, "power_cpu_cv_mean_watts": 2.043, "power_sys_5v0_mean_watts": 8.502, "gpu_utilization_percent_mean": 69.108, "power_watts_avg": 19.117, "energy_joules_est": 223.86, "duration_seconds": 11.71, "sample_count": 102}, "timestamp": "2026-01-26T09:01:27.273844"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9138.175, "latencies_ms": [9138.175], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image depicts a warm and inviting living room with yellow walls and hardwood flooring. A person is standing near the kitchen area, which has a refrigerator, cabinets, and a countertop with a vase of flowers on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 19288.4, "ram_available_mb": 43552.5, "ram_percent": 30.7}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 19293.5, "ram_available_mb": 43547.4, "ram_percent": 30.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 11762.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 11762.0, "gpu_utilization_percent": 71.658}, "power_stats": {"power_gpu_soc_mean_watts": 20.297, "power_cpu_cv_mean_watts": 1.577, "power_sys_5v0_mean_watts": 8.422, "gpu_utilization_percent_mean": 71.658, "power_watts_avg": 20.297, "energy_joules_est": 185.49, "duration_seconds": 9.139, "sample_count": 79}, "timestamp": "2026-01-26T09:01:38.459093"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8774.844, "latencies_ms": [8774.844], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The room has a warm and inviting atmosphere with yellow walls and wooden flooring. There is a mix of modern and traditional elements, such as a flat-screen TV and a chandelier, as well as a wooden dining table and chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 19293.5, "ram_available_mb": 43547.4, "ram_percent": 30.7}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 19282.5, "ram_available_mb": 43558.4, "ram_percent": 30.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 11762.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 11762.0, "gpu_utilization_percent": 71.133}, "power_stats": {"power_gpu_soc_mean_watts": 20.097, "power_cpu_cv_mean_watts": 1.73, "power_sys_5v0_mean_watts": 8.471, "gpu_utilization_percent_mean": 71.133, "power_watts_avg": 20.097, "energy_joules_est": 176.36, "duration_seconds": 8.776, "sample_count": 75}, "timestamp": "2026-01-26T09:01:49.245952"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12715.742, "latencies_ms": [12715.742], "images_per_second": 0.079, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility featuring a brown bear in its natural habitat. The bear, with its fur glistening in the sunlight, is sitting on a bed of lush green grass. Its head is slightly tilted to the left, and its mouth is open, revealing its sharp teeth. The bear's eyes are focused on something in the distance,", "error": null, "sys_before": {"cpu_percent": 9.7, "ram_used_mb": 19282.5, "ram_available_mb": 43558.4, "ram_percent": 30.7}, "sys_after": {"cpu_percent": 8.5, "ram_used_mb": 19393.6, "ram_available_mb": 43447.3, "ram_percent": 30.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 12544.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 12544.0, "gpu_utilization_percent": 71.064}, "power_stats": {"power_gpu_soc_mean_watts": 21.061, "power_cpu_cv_mean_watts": 2.084, "power_sys_5v0_mean_watts": 8.742, "gpu_utilization_percent_mean": 71.064, "power_watts_avg": 21.061, "energy_joules_est": 267.82, "duration_seconds": 12.716, "sample_count": 109}, "timestamp": "2026-01-26T09:02:04.026212"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5292.874, "latencies_ms": [5292.874], "images_per_second": 0.189, "prompt_tokens": 39, "response_tokens_est": 11, "n_tiles": 16, "output_text": "grass: numerous\nbear: 1\n", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 19393.6, "ram_available_mb": 43447.3, "ram_percent": 30.9}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20241.5, "ram_available_mb": 42599.4, "ram_percent": 32.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 13334.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 13334.0, "gpu_utilization_percent": 85.523}, "power_stats": {"power_gpu_soc_mean_watts": 25.524, "power_cpu_cv_mean_watts": 0.846, "power_sys_5v0_mean_watts": 8.36, "gpu_utilization_percent_mean": 85.523, "power_watts_avg": 25.524, "energy_joules_est": 135.11, "duration_seconds": 5.294, "sample_count": 44}, "timestamp": "2026-01-26T09:02:11.372678"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10517.507, "latencies_ms": [10517.507], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The bear is positioned in the foreground of the image, appearing large and prominent. The background is a natural setting with grass, which is less detailed and appears smaller in comparison to the bear. The bear is facing the camera, making it the main object of focus in the image.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 20241.5, "ram_available_mb": 42599.4, "ram_percent": 32.2}, "sys_after": {"cpu_percent": 10.6, "ram_used_mb": 20959.2, "ram_available_mb": 41881.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.921}, "power_stats": {"power_gpu_soc_mean_watts": 22.083, "power_cpu_cv_mean_watts": 2.264, "power_sys_5v0_mean_watts": 8.74, "gpu_utilization_percent_mean": 74.921, "power_watts_avg": 22.083, "energy_joules_est": 232.27, "duration_seconds": 10.518, "sample_count": 89}, "timestamp": "2026-01-26T09:02:23.927699"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9480.791, "latencies_ms": [9480.791], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image shows a close-up of a bear's face, with its fur appearing soft and well-groomed. The bear is sitting in a grassy area, with some patches of dirt visible on its face.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20959.2, "ram_available_mb": 41881.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 10.2, "ram_used_mb": 21042.8, "ram_available_mb": 41798.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.812}, "power_stats": {"power_gpu_soc_mean_watts": 22.835, "power_cpu_cv_mean_watts": 2.194, "power_sys_5v0_mean_watts": 8.72, "gpu_utilization_percent_mean": 76.812, "power_watts_avg": 22.835, "energy_joules_est": 216.51, "duration_seconds": 9.481, "sample_count": 80}, "timestamp": "2026-01-26T09:02:35.437171"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9002.574, "latencies_ms": [9002.574], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The bear in the image has a thick, golden-brown fur coat that appears soft and well-groomed. The lighting is natural and bright, suggesting that the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20980.1, "ram_available_mb": 41860.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 11.6, "ram_used_mb": 21003.7, "ram_available_mb": 41837.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.312}, "power_stats": {"power_gpu_soc_mean_watts": 22.75, "power_cpu_cv_mean_watts": 2.482, "power_sys_5v0_mean_watts": 8.8, "gpu_utilization_percent_mean": 75.312, "power_watts_avg": 22.75, "energy_joules_est": 204.82, "duration_seconds": 9.003, "sample_count": 77}, "timestamp": "2026-01-26T09:02:46.459427"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12481.999, "latencies_ms": [12481.999], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a cozy bedroom with a large bed occupying most of the space. The bed is covered with a blue comforter, and there is a window above it, allowing natural light to enter the room. A bookshelf is situated next to the bed, filled with numerous books of various sizes. \n\nIn addition to the bed and bookshelf, there is", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21004.2, "ram_available_mb": 41836.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 20951.8, "ram_available_mb": 41889.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.679}, "power_stats": {"power_gpu_soc_mean_watts": 21.559, "power_cpu_cv_mean_watts": 2.237, "power_sys_5v0_mean_watts": 8.839, "gpu_utilization_percent_mean": 72.679, "power_watts_avg": 21.559, "energy_joules_est": 269.11, "duration_seconds": 12.483, "sample_count": 106}, "timestamp": "2026-01-26T09:03:00.987170"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9412.469, "latencies_ms": [9412.469], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "Bed: 1\nBookshelf: 1\nPlants: 2\nBooks: numerous\nBasket: 1\nMirror: 1\nDresser: 1\nWindow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20951.8, "ram_available_mb": 41889.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20948.7, "ram_available_mb": 41892.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.722}, "power_stats": {"power_gpu_soc_mean_watts": 22.831, "power_cpu_cv_mean_watts": 1.486, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 77.722, "power_watts_avg": 22.831, "energy_joules_est": 214.91, "duration_seconds": 9.413, "sample_count": 79}, "timestamp": "2026-01-26T09:03:12.410173"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10650.795, "latencies_ms": [10650.795], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The bed is in the foreground on the left side of the image, with a wooden dresser and mirror behind it. The bookshelf is in the background on the right side, filled with books and plants. The window is in the middle ground, allowing natural light to enter the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20948.7, "ram_available_mb": 41892.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20947.0, "ram_available_mb": 41893.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.022}, "power_stats": {"power_gpu_soc_mean_watts": 22.136, "power_cpu_cv_mean_watts": 1.651, "power_sys_5v0_mean_watts": 8.768, "gpu_utilization_percent_mean": 74.022, "power_watts_avg": 22.136, "energy_joules_est": 235.78, "duration_seconds": 10.651, "sample_count": 90}, "timestamp": "2026-01-26T09:03:25.101592"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11020.125, "latencies_ms": [11020.125], "images_per_second": 0.091, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image shows a cozy bedroom with a large bed covered in blue comforter, a wooden dresser with a mirror, a window with a view of greenery outside, and a bookshelf filled with books. The room is well-lit with natural light coming in through the window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20947.0, "ram_available_mb": 41893.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20939.0, "ram_available_mb": 41901.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.538}, "power_stats": {"power_gpu_soc_mean_watts": 22.188, "power_cpu_cv_mean_watts": 1.628, "power_sys_5v0_mean_watts": 8.707, "gpu_utilization_percent_mean": 73.538, "power_watts_avg": 22.188, "energy_joules_est": 244.53, "duration_seconds": 11.021, "sample_count": 93}, "timestamp": "2026-01-26T09:03:38.155447"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9611.043, "latencies_ms": [9611.043], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The room is filled with natural light from the large window, which shows a view of lush green trees outside. The bed is covered with a blue comforter, and there is a wooden bookshelf filled with various books and decorative items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20939.0, "ram_available_mb": 41901.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20933.1, "ram_available_mb": 41907.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.802}, "power_stats": {"power_gpu_soc_mean_watts": 22.556, "power_cpu_cv_mean_watts": 1.563, "power_sys_5v0_mean_watts": 8.75, "gpu_utilization_percent_mean": 75.802, "power_watts_avg": 22.556, "energy_joules_est": 216.8, "duration_seconds": 9.612, "sample_count": 81}, "timestamp": "2026-01-26T09:03:49.804582"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11665.804, "latencies_ms": [11665.804], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene day at a street corner. Dominating the scene is a stop sign, its octagonal shape and bold red color standing out against the backdrop of a clear blue sky dotted with fluffy white clouds. The sign, mounted on a sturdy black pole, is positioned on the right side of the frame, its white letters spelling out", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 20933.1, "ram_available_mb": 41907.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20959.0, "ram_available_mb": 41881.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.899}, "power_stats": {"power_gpu_soc_mean_watts": 19.114, "power_cpu_cv_mean_watts": 1.898, "power_sys_5v0_mean_watts": 8.574, "gpu_utilization_percent_mean": 68.899, "power_watts_avg": 19.114, "energy_joules_est": 222.99, "duration_seconds": 11.666, "sample_count": 99}, "timestamp": "2026-01-26T09:04:03.529728"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7620.459, "latencies_ms": [7620.459], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "stop sign: 1, tree: 5, car: 1, building: 1, bush: 3, bench: 1, street: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20959.0, "ram_available_mb": 41881.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20960.7, "ram_available_mb": 41880.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.594}, "power_stats": {"power_gpu_soc_mean_watts": 21.284, "power_cpu_cv_mean_watts": 1.558, "power_sys_5v0_mean_watts": 8.478, "gpu_utilization_percent_mean": 72.594, "power_watts_avg": 21.284, "energy_joules_est": 162.21, "duration_seconds": 7.621, "sample_count": 64}, "timestamp": "2026-01-26T09:04:13.169330"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10802.407, "latencies_ms": [10802.407], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The stop sign is positioned in the foreground on the right side of the image, clearly visible against the sky. In the background, there is a parking lot with a car and a building, indicating that the stop sign is near a roadway intersection. The trees and bushes are further in the background, suggesting a suburban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20960.7, "ram_available_mb": 41880.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20960.6, "ram_available_mb": 41880.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.772}, "power_stats": {"power_gpu_soc_mean_watts": 19.584, "power_cpu_cv_mean_watts": 1.838, "power_sys_5v0_mean_watts": 8.547, "gpu_utilization_percent_mean": 70.772, "power_watts_avg": 19.584, "energy_joules_est": 211.57, "duration_seconds": 10.803, "sample_count": 92}, "timestamp": "2026-01-26T09:04:25.996849"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9803.508, "latencies_ms": [9803.508], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image shows a red stop sign with the word \"STOP\" in white letters, mounted on a metal pole at an intersection. The sign is partially obscured by a tree, and there is a vehicle in the background, suggesting that the photo was taken in a suburban area with traffic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20964.0, "ram_available_mb": 41876.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20939.4, "ram_available_mb": 41901.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.024}, "power_stats": {"power_gpu_soc_mean_watts": 20.143, "power_cpu_cv_mean_watts": 1.732, "power_sys_5v0_mean_watts": 8.528, "gpu_utilization_percent_mean": 71.024, "power_watts_avg": 20.143, "energy_joules_est": 197.48, "duration_seconds": 9.804, "sample_count": 83}, "timestamp": "2026-01-26T09:04:37.848697"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8262.42, "latencies_ms": [8262.42], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The stop sign in the image is octagonal and red with white lettering, indicating a command to stop. It is mounted on a metal pole and surrounded by greenery, with a clear blue sky and some clouds in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20939.4, "ram_available_mb": 41901.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20928.6, "ram_available_mb": 41912.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.814}, "power_stats": {"power_gpu_soc_mean_watts": 20.707, "power_cpu_cv_mean_watts": 1.659, "power_sys_5v0_mean_watts": 8.559, "gpu_utilization_percent_mean": 72.814, "power_watts_avg": 20.707, "energy_joules_est": 171.1, "duration_seconds": 8.263, "sample_count": 70}, "timestamp": "2026-01-26T09:04:48.160067"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11634.807, "latencies_ms": [11634.807], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of the image, a trio of teddy bears are nestled together, their soft, brown fur contrasting with the dark blue couch they rest upon. The teddy bear on the left, with its black eyes and nose, seems to be the smallest of the group. It's positioned slightly behind the other two bears, as if it's a", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20928.6, "ram_available_mb": 41912.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20946.6, "ram_available_mb": 41894.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.172}, "power_stats": {"power_gpu_soc_mean_watts": 19.179, "power_cpu_cv_mean_watts": 1.89, "power_sys_5v0_mean_watts": 8.573, "gpu_utilization_percent_mean": 69.172, "power_watts_avg": 19.179, "energy_joules_est": 223.16, "duration_seconds": 11.635, "sample_count": 99}, "timestamp": "2026-01-26T09:05:01.831324"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10976.099, "latencies_ms": [10976.099], "images_per_second": 0.091, "prompt_tokens": 39, "response_tokens_est": 71, "n_tiles": 16, "output_text": "- Teddy bear: 3\n- Stuffed animal: 3\n- Stuffed animal: 3\n- Stuffed animal: 3\n- Stuffed animal: 3\n- Stuffed animal: 3\n- Stuffed animal: 3\n- Stuffed animal: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20946.6, "ram_available_mb": 41894.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20950.6, "ram_available_mb": 41890.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.394}, "power_stats": {"power_gpu_soc_mean_watts": 19.648, "power_cpu_cv_mean_watts": 1.828, "power_sys_5v0_mean_watts": 8.571, "gpu_utilization_percent_mean": 71.394, "power_watts_avg": 19.648, "energy_joules_est": 215.67, "duration_seconds": 10.977, "sample_count": 94}, "timestamp": "2026-01-26T09:05:14.853588"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11721.497, "latencies_ms": [11721.497], "images_per_second": 0.085, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the foreground, there is a large brown teddy bear with a smaller one snuggled up against it. The smaller bear is positioned to the left of the larger bear, and they are both facing towards the right side of the image. In the background, there is a blue blanket partially visible, with a darker brown teddy bear resting on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20950.6, "ram_available_mb": 41890.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20932.7, "ram_available_mb": 41908.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.05}, "power_stats": {"power_gpu_soc_mean_watts": 19.347, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 70.05, "power_watts_avg": 19.347, "energy_joules_est": 226.79, "duration_seconds": 11.722, "sample_count": 100}, "timestamp": "2026-01-26T09:05:28.604372"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8057.586, "latencies_ms": [8057.586], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "Three teddy bears are huddled together on a couch, creating a cozy and comforting scene. The bears appear to be snuggled up close, possibly for warmth or companionship.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.7, "ram_available_mb": 41908.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21020.8, "ram_available_mb": 41820.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.055, "power_cpu_cv_mean_watts": 1.578, "power_sys_5v0_mean_watts": 8.532, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 21.055, "energy_joules_est": 169.67, "duration_seconds": 8.058, "sample_count": 68}, "timestamp": "2026-01-26T09:05:38.723586"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7775.55, "latencies_ms": [7775.55], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image features three teddy bears with a warm, golden-brown color palette. The lighting appears to be soft and diffused, highlighting the plush texture of the bears' fur.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20898.8, "ram_available_mb": 41942.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20925.0, "ram_available_mb": 41915.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.338}, "power_stats": {"power_gpu_soc_mean_watts": 21.116, "power_cpu_cv_mean_watts": 1.62, "power_sys_5v0_mean_watts": 8.587, "gpu_utilization_percent_mean": 73.338, "power_watts_avg": 21.116, "energy_joules_est": 164.2, "duration_seconds": 7.776, "sample_count": 65}, "timestamp": "2026-01-26T09:05:48.528928"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11644.295, "latencies_ms": [11644.295], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is skiing down a snowy hill, wearing a red and black ski jacket and black pants. She is holding ski poles and appears to be enjoying her time on the slopes. The woman is the main focus of the scene, with the snowy hill and her skiing equipment surrounding her.\n\nThere are several other people in", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20925.0, "ram_available_mb": 41915.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20893.6, "ram_available_mb": 41947.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.071}, "power_stats": {"power_gpu_soc_mean_watts": 19.322, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 70.071, "power_watts_avg": 19.322, "energy_joules_est": 225.01, "duration_seconds": 11.645, "sample_count": 99}, "timestamp": "2026-01-26T09:06:02.243002"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8930.04, "latencies_ms": [8930.04], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "hill: 1, red jacket: 1, black pants: 1, ski poles: 2, snow: 1, blue boots: 1, white snow: 1, red and white striped hat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20893.6, "ram_available_mb": 41947.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20889.9, "ram_available_mb": 41951.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.675}, "power_stats": {"power_gpu_soc_mean_watts": 20.478, "power_cpu_cv_mean_watts": 1.711, "power_sys_5v0_mean_watts": 8.568, "gpu_utilization_percent_mean": 72.675, "power_watts_avg": 20.478, "energy_joules_est": 182.88, "duration_seconds": 8.931, "sample_count": 77}, "timestamp": "2026-01-26T09:06:13.216769"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10568.574, "latencies_ms": [10568.574], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The skier is positioned in the foreground of the image, actively skiing down the slope. The red poles are held in the skier's hands, aiding in balance and direction. In the background, there are red poles lined up on the snow, possibly indicating a boundary or path for skiers.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20889.9, "ram_available_mb": 41951.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20881.0, "ram_available_mb": 41959.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.044}, "power_stats": {"power_gpu_soc_mean_watts": 19.639, "power_cpu_cv_mean_watts": 1.847, "power_sys_5v0_mean_watts": 8.614, "gpu_utilization_percent_mean": 71.044, "power_watts_avg": 19.639, "energy_joules_est": 207.57, "duration_seconds": 10.569, "sample_count": 90}, "timestamp": "2026-01-26T09:06:25.801645"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9163.697, "latencies_ms": [9163.697], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "A person is skiing on a snowy slope with red poles and wearing a red and black jacket, black pants, and a striped hat. The background shows a snowy mountain with orange poles marking the boundaries of the skiing area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20881.0, "ram_available_mb": 41959.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20877.6, "ram_available_mb": 41963.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.628}, "power_stats": {"power_gpu_soc_mean_watts": 20.282, "power_cpu_cv_mean_watts": 1.71, "power_sys_5v0_mean_watts": 8.573, "gpu_utilization_percent_mean": 71.628, "power_watts_avg": 20.282, "energy_joules_est": 185.87, "duration_seconds": 9.164, "sample_count": 78}, "timestamp": "2026-01-26T09:06:37.021038"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6685.951, "latencies_ms": [6685.951], "images_per_second": 0.15, "prompt_tokens": 36, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The skier is wearing a red and black jacket, black pants, and blue skis. The weather appears to be overcast with snow on the ground.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20877.6, "ram_available_mb": 41963.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 20896.1, "ram_available_mb": 41944.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.911}, "power_stats": {"power_gpu_soc_mean_watts": 21.438, "power_cpu_cv_mean_watts": 1.516, "power_sys_5v0_mean_watts": 8.588, "gpu_utilization_percent_mean": 73.911, "power_watts_avg": 21.438, "energy_joules_est": 143.35, "duration_seconds": 6.687, "sample_count": 56}, "timestamp": "2026-01-26T09:06:45.748271"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11594.248, "latencies_ms": [11594.248], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a small, well-organized kitchen. Dominating the scene is a white refrigerator, standing tall on the right side of the frame. Adjacent to it, a white oven and stove are neatly arranged, their surfaces gleaming under the light. A wooden countertop, rich in its natural hue, stretches across the middle of", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 20896.1, "ram_available_mb": 41944.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 11.5, "ram_used_mb": 20830.4, "ram_available_mb": 42010.5, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.394}, "power_stats": {"power_gpu_soc_mean_watts": 19.187, "power_cpu_cv_mean_watts": 2.403, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 70.394, "power_watts_avg": 19.187, "energy_joules_est": 222.47, "duration_seconds": 11.595, "sample_count": 99}, "timestamp": "2026-01-26T09:06:59.366090"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9077.08, "latencies_ms": [9077.08], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "refrigerator: 1, oven: 1, stove: 1, dishwasher: 1, microwave: 1, ice maker: 1, water dispenser: 1, cabinet: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20830.4, "ram_available_mb": 42010.5, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 11.2, "ram_used_mb": 20930.4, "ram_available_mb": 41910.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.117}, "power_stats": {"power_gpu_soc_mean_watts": 20.483, "power_cpu_cv_mean_watts": 2.58, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 72.117, "power_watts_avg": 20.483, "energy_joules_est": 185.94, "duration_seconds": 9.078, "sample_count": 77}, "timestamp": "2026-01-26T09:07:10.459712"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8940.352, "latencies_ms": [8940.352], "images_per_second": 0.112, "prompt_tokens": 44, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The refrigerator is located in the background, towards the right side of the image, while the oven is in the foreground on the left side. The sink is situated between the oven and the refrigerator, closer to the oven.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20930.4, "ram_available_mb": 41910.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 12.3, "ram_used_mb": 20855.5, "ram_available_mb": 41985.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.714}, "power_stats": {"power_gpu_soc_mean_watts": 20.216, "power_cpu_cv_mean_watts": 2.554, "power_sys_5v0_mean_watts": 8.648, "gpu_utilization_percent_mean": 72.714, "power_watts_avg": 20.216, "energy_joules_est": 180.75, "duration_seconds": 8.941, "sample_count": 77}, "timestamp": "2026-01-26T09:07:21.440456"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9378.934, "latencies_ms": [9378.934], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image shows a small, well-lit kitchen with wooden cabinets and white appliances. There is a white refrigerator on the right side, a white oven and dishwasher on the left, and a wooden drawer cabinet in the center.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20855.5, "ram_available_mb": 41985.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 20838.6, "ram_available_mb": 42002.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.198}, "power_stats": {"power_gpu_soc_mean_watts": 20.39, "power_cpu_cv_mean_watts": 2.304, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 73.198, "power_watts_avg": 20.39, "energy_joules_est": 191.25, "duration_seconds": 9.38, "sample_count": 81}, "timestamp": "2026-01-26T09:07:32.835693"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7642.696, "latencies_ms": [7642.696], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The kitchen has wooden cabinets and white appliances, with a light brownish-beige wall and white tile flooring. The lighting appears to be artificial, coming from ceiling fixtures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20838.6, "ram_available_mb": 42002.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20800.8, "ram_available_mb": 42040.1, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.877}, "power_stats": {"power_gpu_soc_mean_watts": 21.096, "power_cpu_cv_mean_watts": 1.602, "power_sys_5v0_mean_watts": 8.621, "gpu_utilization_percent_mean": 73.877, "power_watts_avg": 21.096, "energy_joules_est": 161.24, "duration_seconds": 7.643, "sample_count": 65}, "timestamp": "2026-01-26T09:07:42.495002"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12445.664, "latencies_ms": [12445.664], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two men playing baseball on a field. One man is running towards the base, while the other man is throwing the ball. The baseball player running is wearing a blue cap and a white shirt, and he is in motion, likely trying to reach the base before the ball is caught. The other player is wearing a green cap and a green shirt,", "error": null, "sys_before": {"cpu_percent": 9.4, "ram_used_mb": 20800.8, "ram_available_mb": 42040.1, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20798.5, "ram_available_mb": 42042.4, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.981}, "power_stats": {"power_gpu_soc_mean_watts": 21.509, "power_cpu_cv_mean_watts": 1.81, "power_sys_5v0_mean_watts": 8.886, "gpu_utilization_percent_mean": 72.981, "power_watts_avg": 21.509, "energy_joules_est": 267.71, "duration_seconds": 12.446, "sample_count": 106}, "timestamp": "2026-01-26T09:07:56.995943"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11351.467, "latencies_ms": [11351.467], "images_per_second": 0.088, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. Baseball players: 2\n2. Baseball glove: 1\n3. Baseball: 1\n4. Baseball field: 1\n5. Trees: 1\n6. Grass: 1\n7. Pole: 1\n8. Dirt: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20798.5, "ram_available_mb": 42042.4, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20834.8, "ram_available_mb": 42006.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.268}, "power_stats": {"power_gpu_soc_mean_watts": 22.086, "power_cpu_cv_mean_watts": 1.664, "power_sys_5v0_mean_watts": 8.756, "gpu_utilization_percent_mean": 75.268, "power_watts_avg": 22.086, "energy_joules_est": 250.73, "duration_seconds": 11.352, "sample_count": 97}, "timestamp": "2026-01-26T09:08:10.404352"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11639.971, "latencies_ms": [11639.971], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there is a baseball player in a white shirt and gray pants running towards the left side of the image, while another player in a green shirt and gray pants is standing on the right side, holding a baseball glove. The background consists of a grassy field with trees and a wooden fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20834.8, "ram_available_mb": 42006.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20824.3, "ram_available_mb": 42016.6, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.71}, "power_stats": {"power_gpu_soc_mean_watts": 21.888, "power_cpu_cv_mean_watts": 1.734, "power_sys_5v0_mean_watts": 8.854, "gpu_utilization_percent_mean": 73.71, "power_watts_avg": 21.888, "energy_joules_est": 254.79, "duration_seconds": 11.641, "sample_count": 100}, "timestamp": "2026-01-26T09:08:24.066438"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7594.725, "latencies_ms": [7594.725], "images_per_second": 0.132, "prompt_tokens": 37, "response_tokens_est": 33, "n_tiles": 16, "output_text": "Two men are playing baseball on a field with trees in the background. One man is running towards the base while the other is preparing to throw the ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20824.3, "ram_available_mb": 42016.6, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 20820.4, "ram_available_mb": 42020.5, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 79.831}, "power_stats": {"power_gpu_soc_mean_watts": 23.983, "power_cpu_cv_mean_watts": 1.312, "power_sys_5v0_mean_watts": 8.693, "gpu_utilization_percent_mean": 79.831, "power_watts_avg": 23.983, "energy_joules_est": 182.16, "duration_seconds": 7.595, "sample_count": 65}, "timestamp": "2026-01-26T09:08:33.677781"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10927.53, "latencies_ms": [10927.53], "images_per_second": 0.092, "prompt_tokens": 36, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image shows two individuals playing baseball on a field with a clear sky in the background, suggesting it is a sunny day. The players are wearing casual attire, with one in a white shirt and the other in a green shirt, and they are both equipped with baseball gloves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20820.4, "ram_available_mb": 42020.5, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20824.1, "ram_available_mb": 42016.8, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.117}, "power_stats": {"power_gpu_soc_mean_watts": 22.113, "power_cpu_cv_mean_watts": 1.696, "power_sys_5v0_mean_watts": 8.863, "gpu_utilization_percent_mean": 74.117, "power_watts_avg": 22.113, "energy_joules_est": 241.66, "duration_seconds": 10.928, "sample_count": 94}, "timestamp": "2026-01-26T09:08:46.623278"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11648.745, "latencies_ms": [11648.745], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a tennis player in action on a tennis court. The player is holding a tennis racket and is in the process of hitting a tennis ball. There are several other people in the background, possibly watching the game or waiting for their turn to play. The tennis player is positioned in the center of the court, and the ball is in motion, indicating an ongoing match.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 20824.1, "ram_available_mb": 42016.8, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20783.9, "ram_available_mb": 42057.0, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.55}, "power_stats": {"power_gpu_soc_mean_watts": 19.195, "power_cpu_cv_mean_watts": 1.898, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 70.55, "power_watts_avg": 19.195, "energy_joules_est": 223.61, "duration_seconds": 11.649, "sample_count": 100}, "timestamp": "2026-01-26T09:09:00.318775"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7908.129, "latencies_ms": [7908.129], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "player: 1, tennis racket: 1, ball: 0, wall: 1, spectators: 10, logo: 2, shoe: 1, hat: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20783.9, "ram_available_mb": 42057.0, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20772.1, "ram_available_mb": 42068.8, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.567}, "power_stats": {"power_gpu_soc_mean_watts": 21.195, "power_cpu_cv_mean_watts": 1.578, "power_sys_5v0_mean_watts": 8.552, "gpu_utilization_percent_mean": 73.567, "power_watts_avg": 21.195, "energy_joules_est": 167.63, "duration_seconds": 7.909, "sample_count": 67}, "timestamp": "2026-01-26T09:09:10.280625"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11605.145, "latencies_ms": [11605.145], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a tennis player is positioned on the court, holding a tennis racket and preparing to hit the ball. The player is near the baseline, which is the line closest to the net. In the background, there is a wall with the J.P. Morgan logo, and a few spectators are seated behind it. To the right of the player,", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20772.1, "ram_available_mb": 42068.8, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20769.8, "ram_available_mb": 42071.1, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.35}, "power_stats": {"power_gpu_soc_mean_watts": 19.292, "power_cpu_cv_mean_watts": 1.902, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 70.35, "power_watts_avg": 19.292, "energy_joules_est": 223.9, "duration_seconds": 11.606, "sample_count": 100}, "timestamp": "2026-01-26T09:09:23.921858"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9960.105, "latencies_ms": [9960.105], "images_per_second": 0.1, "prompt_tokens": 37, "response_tokens_est": 63, "n_tiles": 16, "output_text": "A tennis player is in the midst of a serve on a tennis court, with a J.P. Morgan sign visible in the background. The player is wearing a white cap, a black and white striped shirt, and blue shorts, and is holding a red and white tennis racket.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20769.8, "ram_available_mb": 42071.1, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20770.7, "ram_available_mb": 42070.2, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.824}, "power_stats": {"power_gpu_soc_mean_watts": 19.974, "power_cpu_cv_mean_watts": 1.752, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 70.824, "power_watts_avg": 19.974, "energy_joules_est": 198.96, "duration_seconds": 9.961, "sample_count": 85}, "timestamp": "2026-01-26T09:09:35.942287"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5860.069, "latencies_ms": [5860.069], "images_per_second": 0.171, "prompt_tokens": 36, "response_tokens_est": 29, "n_tiles": 16, "output_text": "The tennis player is wearing a white cap and a white and blue striped shirt. The court is blue with a green surface.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20770.7, "ram_available_mb": 42070.2, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 20768.3, "ram_available_mb": 42072.6, "ram_percent": 33.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.98}, "power_stats": {"power_gpu_soc_mean_watts": 22.258, "power_cpu_cv_mean_watts": 1.369, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 75.98, "power_watts_avg": 22.258, "energy_joules_est": 130.45, "duration_seconds": 5.861, "sample_count": 50}, "timestamp": "2026-01-26T09:09:43.820856"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11602.377, "latencies_ms": [11602.377], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of young children gathered together on a tennis court. They are posing for a picture, with some of them holding tennis rackets. The children are standing in front of a tennis net, which divides the court into two halves. The scene captures a moment of camaraderie and shared interest among the kids as they enjoy their time on the court.", "error": null, "sys_before": {"cpu_percent": 3.6, "ram_used_mb": 20768.3, "ram_available_mb": 42072.6, "ram_percent": 33.0}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20779.1, "ram_available_mb": 42061.8, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.414}, "power_stats": {"power_gpu_soc_mean_watts": 19.321, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 70.414, "power_watts_avg": 19.321, "energy_joules_est": 224.18, "duration_seconds": 11.603, "sample_count": 99}, "timestamp": "2026-01-26T09:09:57.462189"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8535.466, "latencies_ms": [8535.466], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "children: 11\nbags: 2\nrackets: 1\nnets: 2\nsneakers: 5\nhats: 7\ntanks: 1\nshirts: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20779.1, "ram_available_mb": 42061.8, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20772.3, "ram_available_mb": 42068.6, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.458}, "power_stats": {"power_gpu_soc_mean_watts": 20.632, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 8.568, "gpu_utilization_percent_mean": 73.458, "power_watts_avg": 20.632, "energy_joules_est": 176.12, "duration_seconds": 8.536, "sample_count": 72}, "timestamp": "2026-01-26T09:10:08.016598"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11607.789, "latencies_ms": [11607.789], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a group of children is gathered near a tennis court, with some standing closer to the camera and others further away, creating a sense of depth. The children are positioned in front of a green fence that serves as the background, indicating that the tennis court is enclosed. The children are standing on a blue surface, which is likely the tennis court itself, and they", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20772.3, "ram_available_mb": 42068.6, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20773.9, "ram_available_mb": 42067.0, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.061}, "power_stats": {"power_gpu_soc_mean_watts": 19.133, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 70.061, "power_watts_avg": 19.133, "energy_joules_est": 222.1, "duration_seconds": 11.608, "sample_count": 99}, "timestamp": "2026-01-26T09:10:21.644656"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8727.91, "latencies_ms": [8727.91], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A group of children and one adult are gathered on a tennis court, with the adult holding a trophy, suggesting a tennis-related event or competition. The setting appears to be outdoors, with trees and a fence visible in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20773.9, "ram_available_mb": 42067.0, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20775.1, "ram_available_mb": 42065.8, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.878}, "power_stats": {"power_gpu_soc_mean_watts": 20.539, "power_cpu_cv_mean_watts": 1.661, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 72.878, "power_watts_avg": 20.539, "energy_joules_est": 179.28, "duration_seconds": 8.729, "sample_count": 74}, "timestamp": "2026-01-26T09:10:32.409282"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8982.397, "latencies_ms": [8982.397], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a group of children and one adult on a tennis court, with the adult holding a trophy. The court is surrounded by a green fence and there are trees in the background, suggesting an outdoor setting with sunlight casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20775.1, "ram_available_mb": 42065.8, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20771.5, "ram_available_mb": 42069.4, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.333}, "power_stats": {"power_gpu_soc_mean_watts": 19.915, "power_cpu_cv_mean_watts": 1.741, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 71.333, "power_watts_avg": 19.915, "energy_joules_est": 178.9, "duration_seconds": 8.983, "sample_count": 78}, "timestamp": "2026-01-26T09:10:43.414495"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11624.114, "latencies_ms": [11624.114], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people sitting on a sidewalk near a body of water, possibly a river or a lake. There are at least four people visible in the scene, with one person taking a picture of a bird on the ground. The bird appears to be a large white bird, possibly a stork, and it is standing on the grass near the water.\n\nIn addition to", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20771.5, "ram_available_mb": 42069.4, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20771.8, "ram_available_mb": 42069.1, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.758}, "power_stats": {"power_gpu_soc_mean_watts": 19.313, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 70.758, "power_watts_avg": 19.313, "energy_joules_est": 224.51, "duration_seconds": 11.625, "sample_count": 99}, "timestamp": "2026-01-26T09:10:57.074896"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7441.727, "latencies_ms": [7441.727], "images_per_second": 0.134, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "people: 3, birds: 1, camera: 1, backpack: 1, water: 1, sun: 1, buildings: 1, trees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20771.8, "ram_available_mb": 42069.1, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20772.2, "ram_available_mb": 42068.7, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.571}, "power_stats": {"power_gpu_soc_mean_watts": 21.457, "power_cpu_cv_mean_watts": 1.538, "power_sys_5v0_mean_watts": 8.565, "gpu_utilization_percent_mean": 73.571, "power_watts_avg": 21.457, "energy_joules_est": 159.69, "duration_seconds": 7.442, "sample_count": 63}, "timestamp": "2026-01-26T09:11:06.578089"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11268.005, "latencies_ms": [11268.005], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground, there is a woman with a brown bag standing and taking a photo of a pigeon on the ground. To her left, there are two individuals sitting on the edge of a waterfront area, with one of them wearing a red shirt. In the background, there is a body of water with a boat and buildings along the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20772.2, "ram_available_mb": 42068.7, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 20771.9, "ram_available_mb": 42069.0, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.229}, "power_stats": {"power_gpu_soc_mean_watts": 19.235, "power_cpu_cv_mean_watts": 1.873, "power_sys_5v0_mean_watts": 8.629, "gpu_utilization_percent_mean": 70.229, "power_watts_avg": 19.235, "energy_joules_est": 216.75, "duration_seconds": 11.269, "sample_count": 96}, "timestamp": "2026-01-26T09:11:19.901531"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8023.926, "latencies_ms": [8023.926], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image depicts a serene scene by a river with a bridge overhead. People are sitting on the edge of the riverbank, observing a heron in the water, while one person is taking a photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20771.9, "ram_available_mb": 42069.0, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 20779.9, "ram_available_mb": 42061.0, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.897}, "power_stats": {"power_gpu_soc_mean_watts": 20.917, "power_cpu_cv_mean_watts": 1.849, "power_sys_5v0_mean_watts": 8.584, "gpu_utilization_percent_mean": 72.897, "power_watts_avg": 20.917, "energy_joules_est": 167.85, "duration_seconds": 8.025, "sample_count": 68}, "timestamp": "2026-01-26T09:11:29.958556"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11476.068, "latencies_ms": [11476.068], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The image features a serene waterfront scene with a curved metal structure overhead, likely a bridge or an awning, casting shadows on the ground. The lighting suggests it's either early morning or late afternoon, with the sun low in the sky, creating a warm glow and highlighting the calm water and the presence of a single white bird on the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20779.9, "ram_available_mb": 42061.0, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20775.9, "ram_available_mb": 42065.0, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.429}, "power_stats": {"power_gpu_soc_mean_watts": 19.17, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 69.429, "power_watts_avg": 19.17, "energy_joules_est": 220.01, "duration_seconds": 11.477, "sample_count": 98}, "timestamp": "2026-01-26T09:11:43.468247"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11584.153, "latencies_ms": [11584.153], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the image, a woman is standing and holding a cell phone in her hands. She is wearing a white shirt and has a bracelet on her wrist. The woman appears to be looking at the phone screen, possibly reading a message or browsing the internet. The cell phone is positioned in her right hand, and she seems to be focused on the device.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20775.9, "ram_available_mb": 42065.0, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20764.6, "ram_available_mb": 42076.3, "ram_percent": 33.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.96}, "power_stats": {"power_gpu_soc_mean_watts": 19.157, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 69.96, "power_watts_avg": 19.157, "energy_joules_est": 221.93, "duration_seconds": 11.585, "sample_count": 99}, "timestamp": "2026-01-26T09:11:57.095608"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11833.637, "latencies_ms": [11833.637], "images_per_second": 0.085, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "1. Hello Kitty phone case: 1\n2. Woman's face: 1\n3. Woman's hand: 1\n4. Woman's bracelet: 1\n5. Woman's ring: 1\n6. Woman's watch: 1\n7. Woman's ear: 1\n8. Woman's", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20764.6, "ram_available_mb": 42076.3, "ram_percent": 33.0}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20903.5, "ram_available_mb": 41937.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.398}, "power_stats": {"power_gpu_soc_mean_watts": 19.321, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.606, "gpu_utilization_percent_mean": 70.398, "power_watts_avg": 19.321, "energy_joules_est": 228.65, "duration_seconds": 11.834, "sample_count": 103}, "timestamp": "2026-01-26T09:12:10.971674"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11322.854, "latencies_ms": [11322.854], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground, a person is holding a phone with the screen facing the camera, displaying the message \"HELLO KITTY\". The person is wearing a white sleeveless top with a black and white graphic design. In the background, there are other people who are out of focus, suggesting that the main subject is the center of attention.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20903.5, "ram_available_mb": 41937.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20835.7, "ram_available_mb": 42005.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.031}, "power_stats": {"power_gpu_soc_mean_watts": 19.449, "power_cpu_cv_mean_watts": 1.873, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 71.031, "power_watts_avg": 19.449, "energy_joules_est": 220.23, "duration_seconds": 11.323, "sample_count": 96}, "timestamp": "2026-01-26T09:12:24.354517"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8046.795, "latencies_ms": [8046.795], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "A woman is holding a Hello Kitty phone case while looking at her phone. She is wearing a white sleeveless top with a black and white pattern and a green bracelet on her wrist.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20835.7, "ram_available_mb": 42005.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20810.2, "ram_available_mb": 42030.7, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.059}, "power_stats": {"power_gpu_soc_mean_watts": 21.072, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.557, "gpu_utilization_percent_mean": 73.059, "power_watts_avg": 21.072, "energy_joules_est": 169.58, "duration_seconds": 8.048, "sample_count": 68}, "timestamp": "2026-01-26T09:12:34.448969"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8906.135, "latencies_ms": [8906.135], "images_per_second": 0.112, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image features a person holding a phone with a case that has a Hello Kitty design on it. The person is wearing a white sleeveless top with a black and white pattern, and they have a green bracelet on their wrist.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20810.2, "ram_available_mb": 42030.7, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20913.8, "ram_available_mb": 41927.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.908}, "power_stats": {"power_gpu_soc_mean_watts": 20.377, "power_cpu_cv_mean_watts": 1.712, "power_sys_5v0_mean_watts": 8.617, "gpu_utilization_percent_mean": 72.908, "power_watts_avg": 20.377, "energy_joules_est": 181.49, "duration_seconds": 8.907, "sample_count": 76}, "timestamp": "2026-01-26T09:12:45.396547"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11571.462, "latencies_ms": [11571.462], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the image, a group of children is gathered around a red and white striped carousel horse, which is positioned on a wooden floor. The children are sitting on the horse, with some of them hugging each other, creating a sense of camaraderie and excitement. The carousel horse is the central focus of the scene, drawing the attention of the children.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 20913.8, "ram_available_mb": 41927.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20801.7, "ram_available_mb": 42039.2, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.663}, "power_stats": {"power_gpu_soc_mean_watts": 19.112, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.63, "gpu_utilization_percent_mean": 69.663, "power_watts_avg": 19.112, "energy_joules_est": 221.17, "duration_seconds": 11.572, "sample_count": 98}, "timestamp": "2026-01-26T09:12:58.995903"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7841.012, "latencies_ms": [7841.012], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "children: 5, chairs: 1, stage: 1, microphone stand: 1, floor: 1, wall: 1, curtain: 1, cart: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20801.7, "ram_available_mb": 42039.2, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20867.1, "ram_available_mb": 41973.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.636}, "power_stats": {"power_gpu_soc_mean_watts": 21.146, "power_cpu_cv_mean_watts": 1.559, "power_sys_5v0_mean_watts": 8.575, "gpu_utilization_percent_mean": 74.636, "power_watts_avg": 21.146, "energy_joules_est": 165.82, "duration_seconds": 7.842, "sample_count": 66}, "timestamp": "2026-01-26T09:13:08.874587"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11668.619, "latencies_ms": [11668.619], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a red and white striped barrel with a group of children sitting inside, facing towards the right side of the image. The barrel is positioned on a wooden floor, and in the background, there is a large, empty red and gold structure that appears to be a stage or performance area. The children are seated in a row, with some facing", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20867.1, "ram_available_mb": 41973.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20787.5, "ram_available_mb": 42053.4, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.525}, "power_stats": {"power_gpu_soc_mean_watts": 19.299, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 70.525, "power_watts_avg": 19.299, "energy_joules_est": 225.21, "duration_seconds": 11.669, "sample_count": 99}, "timestamp": "2026-01-26T09:13:22.590911"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8735.099, "latencies_ms": [8735.099], "images_per_second": 0.114, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A group of children is seated on a red and white striped carousel ride inside a building with a wooden floor and a brick wall in the background. The children appear to be enjoying the ride, with some looking ahead and others looking around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20787.5, "ram_available_mb": 42053.4, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20870.9, "ram_available_mb": 41970.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.452}, "power_stats": {"power_gpu_soc_mean_watts": 20.624, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 73.452, "power_watts_avg": 20.624, "energy_joules_est": 180.17, "duration_seconds": 8.736, "sample_count": 73}, "timestamp": "2026-01-26T09:13:33.376519"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7410.106, "latencies_ms": [7410.106], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image shows a group of children sitting on a red and white striped carousel ride inside a building with wooden flooring and brick walls. The lighting is dim, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20870.9, "ram_available_mb": 41970.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20929.7, "ram_available_mb": 41911.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.413}, "power_stats": {"power_gpu_soc_mean_watts": 21.301, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 74.413, "power_watts_avg": 21.301, "energy_joules_est": 157.86, "duration_seconds": 7.411, "sample_count": 63}, "timestamp": "2026-01-26T09:13:42.829900"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12418.811, "latencies_ms": [12418.811], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility, featuring a black and white photograph of a sandwich on a white plate. The sandwich, which is the main subject of the image, is composed of two slices of bread, one of which is slightly larger than the other. The larger slice is filled with a dark substance, possibly a type of meat or vegetable, and the", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 20784.4, "ram_available_mb": 42056.5, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20783.7, "ram_available_mb": 42057.2, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.262}, "power_stats": {"power_gpu_soc_mean_watts": 21.507, "power_cpu_cv_mean_watts": 1.811, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 72.262, "power_watts_avg": 21.507, "energy_joules_est": 267.11, "duration_seconds": 12.42, "sample_count": 107}, "timestamp": "2026-01-26T09:13:57.278548"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8941.057, "latencies_ms": [8941.057], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "plate: 1\nbread: 1\nfilling: 1\nknife: 1\nperson: 1\nbackground: 1\nlighting: 1\ntable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20783.7, "ram_available_mb": 42057.2, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20789.6, "ram_available_mb": 42051.3, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.368}, "power_stats": {"power_gpu_soc_mean_watts": 22.94, "power_cpu_cv_mean_watts": 1.459, "power_sys_5v0_mean_watts": 8.73, "gpu_utilization_percent_mean": 77.368, "power_watts_avg": 22.94, "energy_joules_est": 205.12, "duration_seconds": 8.942, "sample_count": 76}, "timestamp": "2026-01-26T09:14:08.260548"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11906.515, "latencies_ms": [11906.515], "images_per_second": 0.084, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The sandwich is positioned in the foreground, occupying the central space of the plate. It is placed near the edge of the plate, with a clear space around it. In the background, there is a blurred object that appears to be a cup or container, suggesting it is farther away from the viewer than the sandwich.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20789.6, "ram_available_mb": 42051.3, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20801.3, "ram_available_mb": 42039.6, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.535}, "power_stats": {"power_gpu_soc_mean_watts": 21.803, "power_cpu_cv_mean_watts": 1.736, "power_sys_5v0_mean_watts": 8.849, "gpu_utilization_percent_mean": 72.535, "power_watts_avg": 21.803, "energy_joules_est": 259.61, "duration_seconds": 11.907, "sample_count": 101}, "timestamp": "2026-01-26T09:14:22.191433"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10415.124, "latencies_ms": [10415.124], "images_per_second": 0.096, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image shows a close-up of a sandwich on a white plate, with a blurred background that suggests a dining setting. The sandwich appears to be a burger with a bun, and there is a small container of sauce on the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20801.3, "ram_available_mb": 42039.6, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20798.2, "ram_available_mb": 42042.7, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.659}, "power_stats": {"power_gpu_soc_mean_watts": 22.419, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.78, "gpu_utilization_percent_mean": 76.659, "power_watts_avg": 22.419, "energy_joules_est": 233.51, "duration_seconds": 10.416, "sample_count": 88}, "timestamp": "2026-01-26T09:14:34.619947"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10955.004, "latencies_ms": [10955.004], "images_per_second": 0.091, "prompt_tokens": 36, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image is a black and white photograph, focusing on a sandwich with a visible filling that appears to be a creamy substance, possibly cheese or a spread. The lighting is soft and diffused, casting gentle shadows and highlighting the textures of the sandwich and the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20798.2, "ram_available_mb": 42042.7, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20809.6, "ram_available_mb": 42031.3, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.065}, "power_stats": {"power_gpu_soc_mean_watts": 22.126, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.85, "gpu_utilization_percent_mean": 74.065, "power_watts_avg": 22.126, "energy_joules_est": 242.4, "duration_seconds": 10.956, "sample_count": 93}, "timestamp": "2026-01-26T09:14:47.593425"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12379.555, "latencies_ms": [12379.555], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility and adventure. In the foreground, a person stands on a paddleboard, dressed in a wetsuit, holding a paddle. The paddleboarder is positioned on a calm body of water, which is the main focus of the image. The water's surface is mostly smooth, with a few small ripples", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 20809.6, "ram_available_mb": 42031.3, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20807.6, "ram_available_mb": 42033.3, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.571}, "power_stats": {"power_gpu_soc_mean_watts": 21.37, "power_cpu_cv_mean_watts": 1.815, "power_sys_5v0_mean_watts": 8.825, "gpu_utilization_percent_mean": 72.571, "power_watts_avg": 21.37, "energy_joules_est": 264.56, "duration_seconds": 12.38, "sample_count": 105}, "timestamp": "2026-01-26T09:15:02.023906"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8505.99, "latencies_ms": [8505.99], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "water: numerous\npaddle: 1\nsurfboard: 1\nperson: 1\nwetsuit: 1\nsand: numerous\ntrees: numerous\nhouses: numerous", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20807.6, "ram_available_mb": 42033.3, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20819.8, "ram_available_mb": 42021.1, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.437}, "power_stats": {"power_gpu_soc_mean_watts": 23.002, "power_cpu_cv_mean_watts": 1.46, "power_sys_5v0_mean_watts": 8.703, "gpu_utilization_percent_mean": 77.437, "power_watts_avg": 23.002, "energy_joules_est": 195.67, "duration_seconds": 8.507, "sample_count": 71}, "timestamp": "2026-01-26T09:15:12.545431"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10332.697, "latencies_ms": [10332.697], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The person is standing on a paddleboard in the foreground, closer to the viewer, while the waves and the horizon line are in the background, indicating they are further away. The paddleboarder is positioned near the center of the image, creating a sense of balance in the composition.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20819.8, "ram_available_mb": 42021.1, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 20826.4, "ram_available_mb": 42014.5, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.318}, "power_stats": {"power_gpu_soc_mean_watts": 21.926, "power_cpu_cv_mean_watts": 1.72, "power_sys_5v0_mean_watts": 8.873, "gpu_utilization_percent_mean": 73.318, "power_watts_avg": 21.926, "energy_joules_est": 226.57, "duration_seconds": 10.333, "sample_count": 88}, "timestamp": "2026-01-26T09:15:24.902389"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9883.44, "latencies_ms": [9883.44], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "A person is paddleboarding on a calm body of water, possibly a lake or a calm sea, with a clear sky above and a distant shoreline with buildings visible. The image is in black and white, adding a timeless quality to the scene.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20826.4, "ram_available_mb": 42014.5, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20828.3, "ram_available_mb": 42012.6, "ram_percent": 33.1}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.578}, "power_stats": {"power_gpu_soc_mean_watts": 22.362, "power_cpu_cv_mean_watts": 1.689, "power_sys_5v0_mean_watts": 8.736, "gpu_utilization_percent_mean": 75.578, "power_watts_avg": 22.362, "energy_joules_est": 221.03, "duration_seconds": 9.884, "sample_count": 83}, "timestamp": "2026-01-26T09:15:36.807757"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7910.315, "latencies_ms": [7910.315], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image is in black and white, featuring a person paddleboarding on a calm body of water. The sky is overcast, and the weather appears to be clear with no signs of rain.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20828.3, "ram_available_mb": 42012.6, "ram_percent": 33.1}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20834.0, "ram_available_mb": 42006.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.627}, "power_stats": {"power_gpu_soc_mean_watts": 22.987, "power_cpu_cv_mean_watts": 1.506, "power_sys_5v0_mean_watts": 8.828, "gpu_utilization_percent_mean": 76.627, "power_watts_avg": 22.987, "energy_joules_est": 181.85, "duration_seconds": 7.911, "sample_count": 67}, "timestamp": "2026-01-26T09:15:46.776366"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11631.722, "latencies_ms": [11631.722], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a workspace, bathed in soft light. Dominating the scene is a white desk, its surface a testament to a busy mind at work. On the left, a laptop sits open, its screen glowing with unseen information. To its right, a desktop computer stands tall, its monitor displaying a white screen. A white keyboard lies", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 20834.0, "ram_available_mb": 42006.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20864.8, "ram_available_mb": 41976.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.808}, "power_stats": {"power_gpu_soc_mean_watts": 19.279, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 70.808, "power_watts_avg": 19.279, "energy_joules_est": 224.26, "duration_seconds": 11.632, "sample_count": 99}, "timestamp": "2026-01-26T09:16:00.431890"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7566.382, "latencies_ms": [7566.382], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "computer: 2, mouse: 1, keyboard: 1, monitor: 1, lamp: 1, figurine: 1, speaker: 2, desk: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20864.8, "ram_available_mb": 41976.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20872.4, "ram_available_mb": 41968.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.406}, "power_stats": {"power_gpu_soc_mean_watts": 21.332, "power_cpu_cv_mean_watts": 1.545, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 73.406, "power_watts_avg": 21.332, "energy_joules_est": 161.42, "duration_seconds": 7.567, "sample_count": 64}, "timestamp": "2026-01-26T09:16:10.013410"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11384.55, "latencies_ms": [11384.55], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a laptop on the left side of the desk, which is positioned near the center of the image. A desktop computer with a monitor is placed slightly behind and to the right of the laptop, creating a sense of depth. The keyboard and mouse are in front of the desktop, suggesting they are the primary input devices for the computer setup.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20872.4, "ram_available_mb": 41968.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20878.1, "ram_available_mb": 41962.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.742}, "power_stats": {"power_gpu_soc_mean_watts": 19.24, "power_cpu_cv_mean_watts": 1.89, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 69.742, "power_watts_avg": 19.24, "energy_joules_est": 219.05, "duration_seconds": 11.385, "sample_count": 97}, "timestamp": "2026-01-26T09:16:23.449396"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8115.85, "latencies_ms": [8115.85], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image depicts a home office setup with a desktop computer, a laptop, and a pair of speakers on a desk. There is a window with blinds partially open, allowing natural light to enter the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20878.1, "ram_available_mb": 41962.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20985.5, "ram_available_mb": 41855.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.594}, "power_stats": {"power_gpu_soc_mean_watts": 21.062, "power_cpu_cv_mean_watts": 1.63, "power_sys_5v0_mean_watts": 8.576, "gpu_utilization_percent_mean": 72.594, "power_watts_avg": 21.062, "energy_joules_est": 170.95, "duration_seconds": 8.116, "sample_count": 69}, "timestamp": "2026-01-26T09:16:33.588343"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8232.926, "latencies_ms": [8232.926], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a workspace with a white color scheme, including a white desk, a white computer monitor, and a white keyboard. The lighting appears to be artificial, coming from a lamp on the right side of the desk.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20985.5, "ram_available_mb": 41855.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20986.2, "ram_available_mb": 41854.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.614}, "power_stats": {"power_gpu_soc_mean_watts": 20.572, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 72.614, "power_watts_avg": 20.572, "energy_joules_est": 169.38, "duration_seconds": 8.234, "sample_count": 70}, "timestamp": "2026-01-26T09:16:43.843902"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11607.037, "latencies_ms": [11607.037], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment on a busy highway, where a green overhead sign is a prominent feature. The sign, mounted on a sturdy metal pole, displays white text indicating the directions to various destinations. The words \"North 101\", \"Hollywood Blvd\", and \"Sunset Blvd\" are clearly visible, guiding drivers towards their respective paths.", "error": null, "sys_before": {"cpu_percent": 13.8, "ram_used_mb": 20847.3, "ram_available_mb": 41993.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20853.8, "ram_available_mb": 41987.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.439}, "power_stats": {"power_gpu_soc_mean_watts": 19.341, "power_cpu_cv_mean_watts": 1.879, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 71.439, "power_watts_avg": 19.341, "energy_joules_est": 224.5, "duration_seconds": 11.608, "sample_count": 98}, "timestamp": "2026-01-26T09:16:57.491501"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9641.762, "latencies_ms": [9641.762], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- Taxi: 1\n\n- Van: 2\n\n- Suv: 3\n\n- Truck: 1\n\n- Car: 2\n\n- Overpass: 1\n\n- Street sign: 2\n\n- Tree: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20853.8, "ram_available_mb": 41987.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20851.7, "ram_available_mb": 41989.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.227, "power_cpu_cv_mean_watts": 1.72, "power_sys_5v0_mean_watts": 8.616, "gpu_utilization_percent_mean": 72.0, "power_watts_avg": 20.227, "energy_joules_est": 195.04, "duration_seconds": 9.642, "sample_count": 81}, "timestamp": "2026-01-26T09:17:09.147877"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11605.159, "latencies_ms": [11605.159], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a black SUV with the license plate LC10055, positioned on the right side of the road. In the background, there is a white van on the left side of the road, and a taxi labeled TAXI 120 is in the far left foreground. The highway signs are mounted on an overpass above", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20851.7, "ram_available_mb": 41989.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20848.2, "ram_available_mb": 41992.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.03}, "power_stats": {"power_gpu_soc_mean_watts": 19.295, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 70.03, "power_watts_avg": 19.295, "energy_joules_est": 223.93, "duration_seconds": 11.606, "sample_count": 99}, "timestamp": "2026-01-26T09:17:22.782094"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10660.416, "latencies_ms": [10660.416], "images_per_second": 0.094, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image shows a busy highway scene with multiple vehicles, including a black SUV and a white van, driving on a multi-lane road. Above the road, there is an overpass with green directional signs indicating the lanes for North 101 towards Ventura, Hollywood Blvd, and Sunset Blvd.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20848.2, "ram_available_mb": 41992.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20857.7, "ram_available_mb": 41983.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.478}, "power_stats": {"power_gpu_soc_mean_watts": 19.729, "power_cpu_cv_mean_watts": 1.793, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 71.478, "power_watts_avg": 19.729, "energy_joules_est": 210.33, "duration_seconds": 10.661, "sample_count": 90}, "timestamp": "2026-01-26T09:17:35.479282"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6904.049, "latencies_ms": [6904.049], "images_per_second": 0.145, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image shows a clear day with bright sunlight casting shadows under the overpass. The overpass is constructed with concrete and metal, and the signs are green with white text.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20857.7, "ram_available_mb": 41983.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20854.3, "ram_available_mb": 41986.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.276}, "power_stats": {"power_gpu_soc_mean_watts": 21.555, "power_cpu_cv_mean_watts": 1.539, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 73.276, "power_watts_avg": 21.555, "energy_joules_est": 148.83, "duration_seconds": 6.905, "sample_count": 58}, "timestamp": "2026-01-26T09:17:44.401315"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12403.78, "latencies_ms": [12403.78], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene on a city street. Dominating the frame is a red double-decker bus, a common sight in many cities around the world. The bus is in motion, driving on the right side of the road, as indicated by the white lines on the road. \n\nThe bus is adorned with a yellow license plate that reads \"AL", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 20854.3, "ram_available_mb": 41986.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20987.0, "ram_available_mb": 41853.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.589}, "power_stats": {"power_gpu_soc_mean_watts": 21.572, "power_cpu_cv_mean_watts": 1.818, "power_sys_5v0_mean_watts": 8.909, "gpu_utilization_percent_mean": 72.589, "power_watts_avg": 21.572, "energy_joules_est": 267.59, "duration_seconds": 12.404, "sample_count": 107}, "timestamp": "2026-01-26T09:17:58.849908"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8880.738, "latencies_ms": [8880.738], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "bus: 1, window: many, license plate: 1, advertisement: 1, pedestrian: 1, building: 1, tree: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20924.3, "ram_available_mb": 41916.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20854.2, "ram_available_mb": 41986.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.961}, "power_stats": {"power_gpu_soc_mean_watts": 23.256, "power_cpu_cv_mean_watts": 1.459, "power_sys_5v0_mean_watts": 8.736, "gpu_utilization_percent_mean": 77.961, "power_watts_avg": 23.256, "energy_joules_est": 206.55, "duration_seconds": 8.881, "sample_count": 76}, "timestamp": "2026-01-26T09:18:09.771310"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10150.639, "latencies_ms": [10150.639], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The red double-decker bus is in the foreground of the image, driving on the road. In the background, there are other vehicles, including a white van on the right side of the image. The bus is closer to the camera than the buildings in the distance.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20854.2, "ram_available_mb": 41986.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20914.6, "ram_available_mb": 41926.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.241}, "power_stats": {"power_gpu_soc_mean_watts": 22.461, "power_cpu_cv_mean_watts": 1.643, "power_sys_5v0_mean_watts": 8.873, "gpu_utilization_percent_mean": 75.241, "power_watts_avg": 22.461, "energy_joules_est": 228.01, "duration_seconds": 10.151, "sample_count": 87}, "timestamp": "2026-01-26T09:18:21.969061"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8896.345, "latencies_ms": [8896.345], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A red double-decker bus is driving on a city street with buildings and trees in the background. The bus has a sign that reads \"St. Paul's Cathedral Fleet Street Aldwych.\"", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20861.3, "ram_available_mb": 41979.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20995.2, "ram_available_mb": 41845.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.68}, "power_stats": {"power_gpu_soc_mean_watts": 23.308, "power_cpu_cv_mean_watts": 1.441, "power_sys_5v0_mean_watts": 8.769, "gpu_utilization_percent_mean": 77.68, "power_watts_avg": 23.308, "energy_joules_est": 207.37, "duration_seconds": 8.897, "sample_count": 75}, "timestamp": "2026-01-26T09:18:32.905490"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9917.491, "latencies_ms": [9917.491], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image features a vibrant red double-decker bus with a clear sky in the background, suggesting a sunny day. The bus is adorned with advertisements and the number 15, indicating it is a public transport vehicle in service.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20995.2, "ram_available_mb": 41845.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20856.7, "ram_available_mb": 41984.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.226}, "power_stats": {"power_gpu_soc_mean_watts": 22.55, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.872, "gpu_utilization_percent_mean": 76.226, "power_watts_avg": 22.55, "energy_joules_est": 223.65, "duration_seconds": 9.918, "sample_count": 84}, "timestamp": "2026-01-26T09:18:44.877425"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11627.424, "latencies_ms": [11627.424], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a black and white cat is the main subject, lying on top of an open laptop. The cat's fur is a mix of black and white, with its eyes looking directly at the camera, giving it a somewhat serious expression. The laptop, which is silver in color, is placed on a white surface. The keyboard of the laptop is visible, and it has a numeric", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 20856.7, "ram_available_mb": 41984.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20861.3, "ram_available_mb": 41979.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.818}, "power_stats": {"power_gpu_soc_mean_watts": 19.315, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 69.818, "power_watts_avg": 19.315, "energy_joules_est": 224.6, "duration_seconds": 11.628, "sample_count": 99}, "timestamp": "2026-01-26T09:18:58.553835"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7583.521, "latencies_ms": [7583.521], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "cat: 1, keyboard: 1, laptop: 1, paper: 1, pen: 1, screwdriver: 1, cloth: 1, box: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20861.3, "ram_available_mb": 41979.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20859.0, "ram_available_mb": 41981.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.766}, "power_stats": {"power_gpu_soc_mean_watts": 21.389, "power_cpu_cv_mean_watts": 1.558, "power_sys_5v0_mean_watts": 8.567, "gpu_utilization_percent_mean": 73.766, "power_watts_avg": 21.389, "energy_joules_est": 162.22, "duration_seconds": 7.584, "sample_count": 64}, "timestamp": "2026-01-26T09:19:08.152078"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9997.629, "latencies_ms": [9997.629], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The cat is positioned in the foreground on the left side of the image, partially obscuring the laptop keyboard which is in the foreground on the right side. The laptop is placed on a surface that appears to be a desk or table, and the background is a plain, light-colored wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20859.0, "ram_available_mb": 41981.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20841.6, "ram_available_mb": 41999.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.141}, "power_stats": {"power_gpu_soc_mean_watts": 19.801, "power_cpu_cv_mean_watts": 1.818, "power_sys_5v0_mean_watts": 8.659, "gpu_utilization_percent_mean": 70.141, "power_watts_avg": 19.801, "energy_joules_est": 197.98, "duration_seconds": 9.998, "sample_count": 85}, "timestamp": "2026-01-26T09:19:20.196675"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8379.965, "latencies_ms": [8379.965], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A black and white cat with striking yellow eyes is lying down on top of an open laptop, partially covering the keyboard. The cat appears to be resting or sleeping, with its body stretched out across the laptop's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20841.6, "ram_available_mb": 41999.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 20853.6, "ram_available_mb": 41987.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.394}, "power_stats": {"power_gpu_soc_mean_watts": 20.908, "power_cpu_cv_mean_watts": 1.68, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 72.394, "power_watts_avg": 20.908, "energy_joules_est": 175.22, "duration_seconds": 8.381, "sample_count": 71}, "timestamp": "2026-01-26T09:19:30.591256"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7787.458, "latencies_ms": [7787.458], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image features a black cat with striking yellow eyes lying on top of a laptop keyboard. The lighting in the image is bright, likely from an indoor source, casting soft shadows on the cat and the laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20853.6, "ram_available_mb": 41987.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 10.3, "ram_used_mb": 20858.4, "ram_available_mb": 41982.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.409}, "power_stats": {"power_gpu_soc_mean_watts": 20.708, "power_cpu_cv_mean_watts": 2.245, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 73.409, "power_watts_avg": 20.708, "energy_joules_est": 161.27, "duration_seconds": 7.788, "sample_count": 66}, "timestamp": "2026-01-26T09:19:40.434781"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11599.213, "latencies_ms": [11599.213], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a breathtaking view of the Sydney Harbour Bridge in Australia. The bridge, a marvel of engineering, stretches across the frame from left to right. Its steel structure is a dark gray color, contrasting beautifully with the lighter gray sky above. \n\nTwo airplanes are captured in mid-flight, soaring above the bridge", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20858.4, "ram_available_mb": 41982.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 11.4, "ram_used_mb": 20994.2, "ram_available_mb": 41846.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.384}, "power_stats": {"power_gpu_soc_mean_watts": 19.049, "power_cpu_cv_mean_watts": 2.581, "power_sys_5v0_mean_watts": 8.695, "gpu_utilization_percent_mean": 70.384, "power_watts_avg": 19.049, "energy_joules_est": 220.97, "duration_seconds": 11.6, "sample_count": 99}, "timestamp": "2026-01-26T09:19:54.057372"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9054.017, "latencies_ms": [9054.017], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Airplanes: 2\n- Clouds: 1\n- Bridge: 1\n- Cars: 1\n- Bus: 1\n- Water: 1\n- Cityscape: 1\n- Trees: 1", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20872.2, "ram_available_mb": 41968.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 10.9, "ram_used_mb": 21013.6, "ram_available_mb": 41827.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.961}, "power_stats": {"power_gpu_soc_mean_watts": 20.698, "power_cpu_cv_mean_watts": 2.444, "power_sys_5v0_mean_watts": 8.66, "gpu_utilization_percent_mean": 72.961, "power_watts_avg": 20.698, "energy_joules_est": 187.41, "duration_seconds": 9.055, "sample_count": 77}, "timestamp": "2026-01-26T09:20:05.137388"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11654.698, "latencies_ms": [11654.698], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large, curved bridge spanning across the image, with two airplanes flying in the background. The airplanes are positioned in the sky, with one flying higher and to the right of the bridge, and the other flying lower and to the left of the bridge. The bridge appears to be in the middle ground of the image, with the", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20925.1, "ram_available_mb": 41915.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 21020.6, "ram_available_mb": 41820.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.788}, "power_stats": {"power_gpu_soc_mean_watts": 19.38, "power_cpu_cv_mean_watts": 2.042, "power_sys_5v0_mean_watts": 8.685, "gpu_utilization_percent_mean": 70.788, "power_watts_avg": 19.38, "energy_joules_est": 225.88, "duration_seconds": 11.655, "sample_count": 99}, "timestamp": "2026-01-26T09:20:18.841978"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8487.442, "latencies_ms": [8487.442], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "Two airplanes are flying in formation over the Sydney Harbour Bridge, with the iconic Sydney Opera House visible in the background. The scene captures the beauty of the city's skyline and the impressive engineering of the bridge.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20860.8, "ram_available_mb": 41980.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20913.3, "ram_available_mb": 41927.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.403}, "power_stats": {"power_gpu_soc_mean_watts": 20.79, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.598, "gpu_utilization_percent_mean": 72.403, "power_watts_avg": 20.79, "energy_joules_est": 176.47, "duration_seconds": 8.488, "sample_count": 72}, "timestamp": "2026-01-26T09:20:29.361495"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9474.478, "latencies_ms": [9474.478], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The sky is overcast with a mix of white and gray clouds, and the lighting is diffused, indicating a cloudy day. Two airplanes with red and white tails are flying in the sky, one above the other, with the Sydney Opera House visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20913.3, "ram_available_mb": 41927.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20997.6, "ram_available_mb": 41843.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.95}, "power_stats": {"power_gpu_soc_mean_watts": 20.148, "power_cpu_cv_mean_watts": 1.746, "power_sys_5v0_mean_watts": 8.63, "gpu_utilization_percent_mean": 71.95, "power_watts_avg": 20.148, "energy_joules_est": 190.91, "duration_seconds": 9.475, "sample_count": 80}, "timestamp": "2026-01-26T09:20:40.859069"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11581.458, "latencies_ms": [11581.458], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in the wild, featuring two zebras in a grassy field. The zebra on the left, with its distinctive black and white stripes, is facing the camera, its head turned to the right. Its mane is a lighter shade of black, contrasting with the darker stripes on its body. The zebra on", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20997.6, "ram_available_mb": 41843.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20858.6, "ram_available_mb": 41982.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.919}, "power_stats": {"power_gpu_soc_mean_watts": 19.331, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 69.919, "power_watts_avg": 19.331, "energy_joules_est": 223.89, "duration_seconds": 11.582, "sample_count": 99}, "timestamp": "2026-01-26T09:20:54.466805"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7430.943, "latencies_ms": [7430.943], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "zebra: 2, grass: numerous, stripes: many, head: 1, ear: 1, eye: 1, nose: 1, mouth: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20858.6, "ram_available_mb": 41982.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20854.1, "ram_available_mb": 41986.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.556}, "power_stats": {"power_gpu_soc_mean_watts": 21.541, "power_cpu_cv_mean_watts": 1.551, "power_sys_5v0_mean_watts": 8.598, "gpu_utilization_percent_mean": 73.556, "power_watts_avg": 21.541, "energy_joules_est": 160.08, "duration_seconds": 7.432, "sample_count": 63}, "timestamp": "2026-01-26T09:21:03.913705"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11596.265, "latencies_ms": [11596.265], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a zebra's head and neck prominently displayed, with its body extending into the background. The zebra's head is positioned near the center of the image, and its body is partially visible to the right, extending towards the edge of the frame. The background consists of a grassy field, which appears to be the environment in", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20854.1, "ram_available_mb": 41986.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20852.4, "ram_available_mb": 41988.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.25}, "power_stats": {"power_gpu_soc_mean_watts": 19.196, "power_cpu_cv_mean_watts": 1.898, "power_sys_5v0_mean_watts": 8.673, "gpu_utilization_percent_mean": 71.25, "power_watts_avg": 19.196, "energy_joules_est": 222.61, "duration_seconds": 11.597, "sample_count": 100}, "timestamp": "2026-01-26T09:21:17.527347"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7239.169, "latencies_ms": [7239.169], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "In the image, two zebras are standing close to each other in a grassy area. One zebra is nuzzling the other, showing affection or social bonding.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20852.4, "ram_available_mb": 41988.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20846.5, "ram_available_mb": 41994.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.574}, "power_stats": {"power_gpu_soc_mean_watts": 21.5, "power_cpu_cv_mean_watts": 1.516, "power_sys_5v0_mean_watts": 8.554, "gpu_utilization_percent_mean": 73.574, "power_watts_avg": 21.5, "energy_joules_est": 155.66, "duration_seconds": 7.24, "sample_count": 61}, "timestamp": "2026-01-26T09:21:26.802243"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7988.834, "latencies_ms": [7988.834], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image is a black and white photograph capturing a moment between two zebras. The lighting is soft and natural, highlighting the intricate patterns of the zebras' stripes and the texture of their fur.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20846.5, "ram_available_mb": 41994.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20846.8, "ram_available_mb": 41994.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.768}, "power_stats": {"power_gpu_soc_mean_watts": 20.794, "power_cpu_cv_mean_watts": 1.677, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 72.768, "power_watts_avg": 20.794, "energy_joules_est": 166.13, "duration_seconds": 7.989, "sample_count": 69}, "timestamp": "2026-01-26T09:21:36.814042"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11569.703, "latencies_ms": [11569.703], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a cozy bedroom scene. Dominating the space is a bed, adorned with a vibrant comforter that boasts a geometric pattern in hues of pink, blue, and green. The bed is positioned against a wall, which is characterized by a window dressed in white blinds, allowing a soft light to filter into the room.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 20846.8, "ram_available_mb": 41994.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20839.9, "ram_available_mb": 42001.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.44}, "power_stats": {"power_gpu_soc_mean_watts": 19.37, "power_cpu_cv_mean_watts": 1.898, "power_sys_5v0_mean_watts": 8.668, "gpu_utilization_percent_mean": 69.44, "power_watts_avg": 19.37, "energy_joules_est": 224.12, "duration_seconds": 11.57, "sample_count": 100}, "timestamp": "2026-01-26T09:21:50.420182"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8358.614, "latencies_ms": [8358.614], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "- bed: 1\n- chair: 1\n- table: 1\n- window: 2\n- wall: 1\n- floor: 1\n- curtain: 1\n- door: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20839.9, "ram_available_mb": 42001.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20853.2, "ram_available_mb": 41987.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.597}, "power_stats": {"power_gpu_soc_mean_watts": 20.839, "power_cpu_cv_mean_watts": 1.646, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 73.597, "power_watts_avg": 20.839, "energy_joules_est": 174.2, "duration_seconds": 8.359, "sample_count": 72}, "timestamp": "2026-01-26T09:22:00.813355"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11557.258, "latencies_ms": [11557.258], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a bed with a colorful patterned cover situated near the right side of the frame. To the left of the bed, there is a small round table with a single black object on top, positioned closer to the viewer than the bed. In the background, there is a window with a view of a building outside, and a chair is", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20853.2, "ram_available_mb": 41987.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20836.4, "ram_available_mb": 42004.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.636}, "power_stats": {"power_gpu_soc_mean_watts": 19.319, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 70.636, "power_watts_avg": 19.319, "energy_joules_est": 223.29, "duration_seconds": 11.558, "sample_count": 99}, "timestamp": "2026-01-26T09:22:14.394628"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9165.34, "latencies_ms": [9165.34], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a small, cozy bedroom with a single bed covered in a colorful, patterned bedspread. There is a round wooden table with a black chair in the corner of the room, and a window with a view of a brick building outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20836.4, "ram_available_mb": 42004.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20839.8, "ram_available_mb": 42001.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.115}, "power_stats": {"power_gpu_soc_mean_watts": 20.469, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 8.581, "gpu_utilization_percent_mean": 71.115, "power_watts_avg": 20.469, "energy_joules_est": 187.62, "duration_seconds": 9.166, "sample_count": 78}, "timestamp": "2026-01-26T09:22:25.603085"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5277.975, "latencies_ms": [5277.975], "images_per_second": 0.189, "prompt_tokens": 36, "response_tokens_est": 24, "n_tiles": 16, "output_text": "The room has a purple wall and a stone wall outside. The bed has a colorful pattern on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20839.8, "ram_available_mb": 42001.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 20850.4, "ram_available_mb": 41990.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.5}, "power_stats": {"power_gpu_soc_mean_watts": 23.279, "power_cpu_cv_mean_watts": 1.301, "power_sys_5v0_mean_watts": 8.592, "gpu_utilization_percent_mean": 76.5, "power_watts_avg": 23.279, "energy_joules_est": 122.88, "duration_seconds": 5.279, "sample_count": 44}, "timestamp": "2026-01-26T09:22:32.916160"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11596.182, "latencies_ms": [11596.182], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a purple bus driving down a street. The bus is a double-decker bus, and it appears to be a public transit vehicle. There are several people visible in the scene, with one person standing near the bus and another person walking on the sidewalk. \n\nIn addition to the bus and pedestrians, there are two traffic lights in the scene", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 20850.4, "ram_available_mb": 41990.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20849.9, "ram_available_mb": 41991.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.03}, "power_stats": {"power_gpu_soc_mean_watts": 19.335, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 70.03, "power_watts_avg": 19.335, "energy_joules_est": 224.22, "duration_seconds": 11.597, "sample_count": 99}, "timestamp": "2026-01-26T09:22:46.548149"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8464.449, "latencies_ms": [8464.449], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "bus: 1, window: 12, wheelchair symbol: 1, traffic light: 1, pedestrian: 1, bus stop: 1, bus route: 1, bus destination: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20849.9, "ram_available_mb": 41991.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20844.2, "ram_available_mb": 41996.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.577}, "power_stats": {"power_gpu_soc_mean_watts": 20.863, "power_cpu_cv_mean_watts": 1.646, "power_sys_5v0_mean_watts": 8.603, "gpu_utilization_percent_mean": 73.577, "power_watts_avg": 20.863, "energy_joules_est": 176.61, "duration_seconds": 8.465, "sample_count": 71}, "timestamp": "2026-01-26T09:22:57.038640"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9086.042, "latencies_ms": [9086.042], "images_per_second": 0.11, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The bus is in the foreground of the image, driving on the road. There is a pedestrian walking on the sidewalk to the left of the bus. In the background, there are trees and a building, indicating that the bus is likely in an urban area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20844.2, "ram_available_mb": 41996.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20836.7, "ram_available_mb": 42004.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.597}, "power_stats": {"power_gpu_soc_mean_watts": 20.214, "power_cpu_cv_mean_watts": 1.736, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 71.597, "power_watts_avg": 20.214, "energy_joules_est": 183.68, "duration_seconds": 9.087, "sample_count": 77}, "timestamp": "2026-01-26T09:23:08.171291"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8110.294, "latencies_ms": [8110.294], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A purple South Tyne Metrocentre bus is driving on the road, with a pedestrian walking on the sidewalk. The bus is marked with the route number 96 and is headed towards Bens.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20836.7, "ram_available_mb": 42004.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20852.2, "ram_available_mb": 41988.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.087}, "power_stats": {"power_gpu_soc_mean_watts": 21.119, "power_cpu_cv_mean_watts": 1.619, "power_sys_5v0_mean_watts": 8.588, "gpu_utilization_percent_mean": 73.087, "power_watts_avg": 21.119, "energy_joules_est": 171.29, "duration_seconds": 8.111, "sample_count": 69}, "timestamp": "2026-01-26T09:23:18.312804"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7763.461, "latencies_ms": [7763.461], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The bus is purple with a colorful design and the words \"Metrocentre via Bens\" on the front. It is a sunny day with clear skies and the bus is driving on a city street.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20852.2, "ram_available_mb": 41988.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20843.6, "ram_available_mb": 41997.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.53}, "power_stats": {"power_gpu_soc_mean_watts": 20.963, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 72.53, "power_watts_avg": 20.963, "energy_joules_est": 162.76, "duration_seconds": 7.764, "sample_count": 66}, "timestamp": "2026-01-26T09:23:28.091514"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11588.207, "latencies_ms": [11588.207], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a close-up view of a white plate holding a group of green apples. There are six apples in total, with one prominently displayed in the foreground and the others slightly blurred in the background. The apples are arranged in a way that they are overlapping each other, creating a sense of depth and dimension. The focus is on the", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20843.6, "ram_available_mb": 41997.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20853.5, "ram_available_mb": 41987.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.242}, "power_stats": {"power_gpu_soc_mean_watts": 19.307, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 70.242, "power_watts_avg": 19.307, "energy_joules_est": 223.75, "duration_seconds": 11.589, "sample_count": 99}, "timestamp": "2026-01-26T09:23:41.713802"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3467.058, "latencies_ms": [3467.058], "images_per_second": 0.288, "prompt_tokens": 39, "response_tokens_est": 6, "n_tiles": 16, "output_text": "apple: 5\n", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20853.5, "ram_available_mb": 41987.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 20843.7, "ram_available_mb": 41997.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 85.552}, "power_stats": {"power_gpu_soc_mean_watts": 26.61, "power_cpu_cv_mean_watts": 0.58, "power_sys_5v0_mean_watts": 8.406, "gpu_utilization_percent_mean": 85.552, "power_watts_avg": 26.61, "energy_joules_est": 92.28, "duration_seconds": 3.468, "sample_count": 29}, "timestamp": "2026-01-26T09:23:47.213544"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11565.479, "latencies_ms": [11565.479], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are several green apples closely packed together, with one prominently in the center. The apples in the background are slightly out of focus, indicating they are further away from the viewer's perspective. The apples in the foreground appear to be near the viewer, while those in the background are nearer to the edge of the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20843.7, "ram_available_mb": 41997.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20840.7, "ram_available_mb": 42000.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.27}, "power_stats": {"power_gpu_soc_mean_watts": 19.299, "power_cpu_cv_mean_watts": 1.898, "power_sys_5v0_mean_watts": 8.666, "gpu_utilization_percent_mean": 70.27, "power_watts_avg": 19.299, "energy_joules_est": 223.21, "duration_seconds": 11.566, "sample_count": 100}, "timestamp": "2026-01-26T09:24:00.820253"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7344.117, "latencies_ms": [7344.117], "images_per_second": 0.136, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image shows a close-up of a group of green apples on a white plate. The apples are fresh and shiny, indicating they are likely ripe and ready to eat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20840.7, "ram_available_mb": 42000.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 7.5, "ram_used_mb": 20852.5, "ram_available_mb": 41988.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.5}, "power_stats": {"power_gpu_soc_mean_watts": 21.45, "power_cpu_cv_mean_watts": 1.621, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 74.5, "power_watts_avg": 21.45, "energy_joules_est": 157.55, "duration_seconds": 7.345, "sample_count": 62}, "timestamp": "2026-01-26T09:24:10.208079"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9052.87, "latencies_ms": [9052.87], "images_per_second": 0.11, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image features a group of green apples with a shiny surface, indicating they might be fresh and possibly wet from washing. The lighting in the image is soft and diffused, casting gentle shadows and highlighting the smooth texture of the apples' skin.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20852.5, "ram_available_mb": 41988.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 11.4, "ram_used_mb": 20851.8, "ram_available_mb": 41989.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.117}, "power_stats": {"power_gpu_soc_mean_watts": 20.073, "power_cpu_cv_mean_watts": 2.512, "power_sys_5v0_mean_watts": 8.692, "gpu_utilization_percent_mean": 71.117, "power_watts_avg": 20.073, "energy_joules_est": 181.73, "duration_seconds": 9.053, "sample_count": 77}, "timestamp": "2026-01-26T09:24:21.311175"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11472.366, "latencies_ms": [11472.366], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a baseball game. The central figures are the batter, the catcher, and the umpire. The batter, dressed in a white uniform with red accents, is in the midst of a powerful swing, his body coiled with the force of the hit. His black bat is caught mid-swing, poised to connect with the incoming ball. ", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 20851.8, "ram_available_mb": 41989.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 11.9, "ram_used_mb": 20866.9, "ram_available_mb": 41974.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.939}, "power_stats": {"power_gpu_soc_mean_watts": 19.607, "power_cpu_cv_mean_watts": 2.86, "power_sys_5v0_mean_watts": 8.789, "gpu_utilization_percent_mean": 70.939, "power_watts_avg": 19.607, "energy_joules_est": 224.95, "duration_seconds": 11.473, "sample_count": 98}, "timestamp": "2026-01-26T09:24:34.830287"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7906.267, "latencies_ms": [7906.267], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "pitcher: 1, catcher: 1, umpire: 1, batter: 1, runner: 1, ball: 1, glove: 1, base: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20866.9, "ram_available_mb": 41974.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 13.1, "ram_used_mb": 20883.6, "ram_available_mb": 41957.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.088}, "power_stats": {"power_gpu_soc_mean_watts": 21.226, "power_cpu_cv_mean_watts": 3.145, "power_sys_5v0_mean_watts": 8.71, "gpu_utilization_percent_mean": 75.088, "power_watts_avg": 21.226, "energy_joules_est": 167.83, "duration_seconds": 7.907, "sample_count": 68}, "timestamp": "2026-01-26T09:24:44.755997"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11586.079, "latencies_ms": [11586.079], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a baseball player is swinging a bat, positioned near the center of the image, with the catcher and umpire behind him, closer to the background. The pitcher, who is further back in the image, has just thrown the ball towards the batter. The batter is standing in the batter's box, which is located on the left side of the", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20883.6, "ram_available_mb": 41957.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20867.8, "ram_available_mb": 41973.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.143}, "power_stats": {"power_gpu_soc_mean_watts": 19.311, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 70.143, "power_watts_avg": 19.311, "energy_joules_est": 223.75, "duration_seconds": 11.587, "sample_count": 98}, "timestamp": "2026-01-26T09:24:58.368143"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10076.532, "latencies_ms": [10076.532], "images_per_second": 0.099, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image captures a moment from a baseball game, with a batter in the midst of a swing, a catcher crouched behind him, and an umpire observing the play. The field is marked with white lines, and the green grass of the outfield is visible through the netting.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20867.8, "ram_available_mb": 41973.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20875.2, "ram_available_mb": 41965.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.814}, "power_stats": {"power_gpu_soc_mean_watts": 20.001, "power_cpu_cv_mean_watts": 1.778, "power_sys_5v0_mean_watts": 8.584, "gpu_utilization_percent_mean": 70.814, "power_watts_avg": 20.001, "energy_joules_est": 201.55, "duration_seconds": 10.077, "sample_count": 86}, "timestamp": "2026-01-26T09:25:10.490863"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8126.159, "latencies_ms": [8126.159], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image captures a baseball game in progress with a clear view of the field and players. The lighting appears to be natural daylight, and the weather seems fair, as there are no signs of rain or overcast skies.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20875.2, "ram_available_mb": 41965.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20865.0, "ram_available_mb": 41975.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.414}, "power_stats": {"power_gpu_soc_mean_watts": 20.692, "power_cpu_cv_mean_watts": 1.698, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 72.414, "power_watts_avg": 20.692, "energy_joules_est": 168.16, "duration_seconds": 8.127, "sample_count": 70}, "timestamp": "2026-01-26T09:25:20.660670"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11599.821, "latencies_ms": [11599.821], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table covered with a red tablecloth, set for a meal. On the table, there is a large white cake topped with red and blue berries, accompanied by a plate of cheese and crackers. A bowl of grapes is also present, adding a touch of freshness to the spread. \n\nIn addition to the", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 20865.0, "ram_available_mb": 41975.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20864.9, "ram_available_mb": 41976.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.899}, "power_stats": {"power_gpu_soc_mean_watts": 19.298, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.679, "gpu_utilization_percent_mean": 69.899, "power_watts_avg": 19.298, "energy_joules_est": 223.87, "duration_seconds": 11.6, "sample_count": 99}, "timestamp": "2026-01-26T09:25:34.331189"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10178.121, "latencies_ms": [10178.121], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "cheese: 10\ngrapes: 1 bunch\nblueberries: 1 cup\nstrawberries: 1 cup\nwine glass: 6\nplates: 10\ncakes: 1\nknife: 2\nglasses: 6", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20864.9, "ram_available_mb": 41976.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20858.2, "ram_available_mb": 41982.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.035}, "power_stats": {"power_gpu_soc_mean_watts": 20.027, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 8.648, "gpu_utilization_percent_mean": 72.035, "power_watts_avg": 20.027, "energy_joules_est": 203.85, "duration_seconds": 10.179, "sample_count": 86}, "timestamp": "2026-01-26T09:25:46.532479"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11604.903, "latencies_ms": [11604.903], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large white cake with red and blue berries on top, placed on the left side of the table. To the right of the cake, there is a plate with various cheeses and crackers, and further to the right, there are stacks of white plates. In the background, there are several wine glasses and a stack of pl", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20858.2, "ram_available_mb": 41982.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20853.1, "ram_available_mb": 41987.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.939}, "power_stats": {"power_gpu_soc_mean_watts": 19.149, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 70.939, "power_watts_avg": 19.149, "energy_joules_est": 222.24, "duration_seconds": 11.606, "sample_count": 99}, "timestamp": "2026-01-26T09:26:00.183238"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9813.762, "latencies_ms": [9813.762], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image depicts a table set for a meal with a variety of foods and drinks. There is a large white cake with red and blue berries on top, a plate of cheese and crackers, a bowl of grapes, and several wine glasses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20853.1, "ram_available_mb": 41987.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20855.4, "ram_available_mb": 41985.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.56}, "power_stats": {"power_gpu_soc_mean_watts": 20.196, "power_cpu_cv_mean_watts": 1.759, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 71.56, "power_watts_avg": 20.196, "energy_joules_est": 198.21, "duration_seconds": 9.814, "sample_count": 84}, "timestamp": "2026-01-26T09:26:12.050987"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9196.806, "latencies_ms": [9196.806], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image features a vibrant red tablecloth that contrasts with the white cheesecake topped with red and blue berries. The table is set with clear glassware and plates, and the lighting appears to be natural, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20855.4, "ram_available_mb": 41985.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20849.7, "ram_available_mb": 41991.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.628}, "power_stats": {"power_gpu_soc_mean_watts": 20.007, "power_cpu_cv_mean_watts": 1.75, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 71.628, "power_watts_avg": 20.007, "energy_joules_est": 184.01, "duration_seconds": 9.197, "sample_count": 78}, "timestamp": "2026-01-26T09:26:23.305664"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11613.32, "latencies_ms": [11613.32], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seen surfing on a wave in the ocean. He is wearing a black wetsuit and is riding a blue surfboard. The wave, which is greenish-blue in color, is curling over to the right side of the image. The man is positioned in the center of the wave, with his left arm raised in the", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 20849.7, "ram_available_mb": 41991.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20852.2, "ram_available_mb": 41988.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.235}, "power_stats": {"power_gpu_soc_mean_watts": 19.315, "power_cpu_cv_mean_watts": 1.879, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 70.235, "power_watts_avg": 19.315, "energy_joules_est": 224.32, "duration_seconds": 11.614, "sample_count": 98}, "timestamp": "2026-01-26T09:26:36.949103"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8610.53, "latencies_ms": [8610.53], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "wave: 1\nsurfboard: 1\nman: 1\nwater: 1\nsurf: 1\nsweat: 1\nsweat stain: 1\nblack shirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20852.2, "ram_available_mb": 41988.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20854.1, "ram_available_mb": 41986.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.37}, "power_stats": {"power_gpu_soc_mean_watts": 20.747, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.592, "gpu_utilization_percent_mean": 72.37, "power_watts_avg": 20.747, "energy_joules_est": 178.66, "duration_seconds": 8.611, "sample_count": 73}, "timestamp": "2026-01-26T09:26:47.586449"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11141.746, "latencies_ms": [11141.746], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is breaking to the right side of the image. The wave originates in the background and extends towards the left, creating a dynamic spatial relationship between the surfer and the wave. The surfer is closer to the viewer than the wave, emphasizing the action of surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20854.1, "ram_available_mb": 41986.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20840.8, "ram_available_mb": 42000.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.99}, "power_stats": {"power_gpu_soc_mean_watts": 19.327, "power_cpu_cv_mean_watts": 1.872, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 69.99, "power_watts_avg": 19.327, "energy_joules_est": 215.35, "duration_seconds": 11.142, "sample_count": 96}, "timestamp": "2026-01-26T09:27:00.749766"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6182.625, "latencies_ms": [6182.625], "images_per_second": 0.162, "prompt_tokens": 37, "response_tokens_est": 30, "n_tiles": 16, "output_text": "A person is surfing a wave in the ocean. The wave is green and the surfer is wearing a black wetsuit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20840.8, "ram_available_mb": 42000.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20841.1, "ram_available_mb": 41999.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.774}, "power_stats": {"power_gpu_soc_mean_watts": 22.449, "power_cpu_cv_mean_watts": 1.383, "power_sys_5v0_mean_watts": 8.537, "gpu_utilization_percent_mean": 75.774, "power_watts_avg": 22.449, "energy_joules_est": 138.81, "duration_seconds": 6.183, "sample_count": 53}, "timestamp": "2026-01-26T09:27:08.957159"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7432.882, "latencies_ms": [7432.882], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The surfer is wearing a black wetsuit and is riding a wave in the ocean. The wave is a vibrant green color and the water is splashing around the surfer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20841.1, "ram_available_mb": 41999.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20844.9, "ram_available_mb": 41996.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.302}, "power_stats": {"power_gpu_soc_mean_watts": 20.952, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 73.302, "power_watts_avg": 20.952, "energy_joules_est": 155.75, "duration_seconds": 7.433, "sample_count": 63}, "timestamp": "2026-01-26T09:27:18.423925"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11568.445, "latencies_ms": [11568.445], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a black and white photograph of a large group of children, likely from a school, posing for a group picture. They are arranged in rows, with some sitting on the ground and others standing. The children are wearing various outfits, including dresses, shirts, and ties. The photograph appears to be from the early 20th century, capt", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20844.9, "ram_available_mb": 41996.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20869.7, "ram_available_mb": 41971.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.53}, "power_stats": {"power_gpu_soc_mean_watts": 19.373, "power_cpu_cv_mean_watts": 1.902, "power_sys_5v0_mean_watts": 8.661, "gpu_utilization_percent_mean": 69.53, "power_watts_avg": 19.373, "energy_joules_est": 224.13, "duration_seconds": 11.569, "sample_count": 100}, "timestamp": "2026-01-26T09:27:32.029371"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7886.286, "latencies_ms": [7886.286], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "children: 30, boys: 12, girls: 18, adults: 2, benches: 2, trees: 1, building: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20869.7, "ram_available_mb": 41971.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20979.7, "ram_available_mb": 41861.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.567}, "power_stats": {"power_gpu_soc_mean_watts": 21.165, "power_cpu_cv_mean_watts": 1.596, "power_sys_5v0_mean_watts": 8.575, "gpu_utilization_percent_mean": 73.567, "power_watts_avg": 21.165, "energy_joules_est": 166.93, "duration_seconds": 7.887, "sample_count": 67}, "timestamp": "2026-01-26T09:27:41.947729"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11561.169, "latencies_ms": [11561.169], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a group of children is seated on the ground, with some sitting closer to the front and others further back, creating a sense of depth. In the background, standing children are spaced out, with some closer to the front of the group and others near the back, providing a layered effect to the composition. The children in the foreground appear to be the main", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20907.9, "ram_available_mb": 41933.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20881.3, "ram_available_mb": 41959.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.56}, "power_stats": {"power_gpu_soc_mean_watts": 19.182, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 8.694, "gpu_utilization_percent_mean": 70.56, "power_watts_avg": 19.182, "energy_joules_est": 221.78, "duration_seconds": 11.562, "sample_count": 100}, "timestamp": "2026-01-26T09:27:55.569482"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7438.36, "latencies_ms": [7438.36], "images_per_second": 0.134, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image depicts a large group of children gathered together, likely for a group photo. They are dressed in a mix of formal and casual attire, suggesting a special occasion or event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20881.3, "ram_available_mb": 41959.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20869.0, "ram_available_mb": 41971.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.254}, "power_stats": {"power_gpu_soc_mean_watts": 21.536, "power_cpu_cv_mean_watts": 1.544, "power_sys_5v0_mean_watts": 8.586, "gpu_utilization_percent_mean": 74.254, "power_watts_avg": 21.536, "energy_joules_est": 160.21, "duration_seconds": 7.439, "sample_count": 63}, "timestamp": "2026-01-26T09:28:05.026831"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8373.969, "latencies_ms": [8373.969], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image is a black and white photograph, suggesting it is old or was taken with a film camera. The lighting is even, with no harsh shadows, indicating it may have been taken indoors or on a cloudy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20869.0, "ram_available_mb": 41971.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20877.1, "ram_available_mb": 41963.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.917}, "power_stats": {"power_gpu_soc_mean_watts": 20.524, "power_cpu_cv_mean_watts": 1.707, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 72.917, "power_watts_avg": 20.524, "energy_joules_est": 171.88, "duration_seconds": 8.375, "sample_count": 72}, "timestamp": "2026-01-26T09:28:15.449581"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11556.3, "latencies_ms": [11556.3], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with a plate of food, including a sandwich and a bowl of soup. The sandwich is placed on the left side of the plate, while the soup is in the center. A wine glass is positioned on the right side of the plate, and a knife is located near the top of the plate. The table is set with a fork and", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20877.1, "ram_available_mb": 41963.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20905.1, "ram_available_mb": 41935.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.99}, "power_stats": {"power_gpu_soc_mean_watts": 19.315, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 69.99, "power_watts_avg": 19.315, "energy_joules_est": 223.22, "duration_seconds": 11.557, "sample_count": 99}, "timestamp": "2026-01-26T09:28:29.058729"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8058.955, "latencies_ms": [8058.955], "images_per_second": 0.124, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "plate: 1\nbread: 2\nknife: 1\nbowl: 1\nplate: 1\nwine glass: 1\nplate: 1\nknife: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20852.1, "ram_available_mb": 41988.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20881.4, "ram_available_mb": 41959.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.529}, "power_stats": {"power_gpu_soc_mean_watts": 20.989, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.564, "gpu_utilization_percent_mean": 73.529, "power_watts_avg": 20.989, "energy_joules_est": 169.16, "duration_seconds": 8.06, "sample_count": 68}, "timestamp": "2026-01-26T09:28:39.165778"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10524.663, "latencies_ms": [10524.663], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a white plate with a piece of bread on it, positioned to the left of a mortar and pestle. The mortar and pestle are placed on the right side of the plate. In the background, there is a glass of red wine and a person sitting at the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20881.4, "ram_available_mb": 41959.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 10.3, "ram_used_mb": 20889.6, "ram_available_mb": 41951.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.033}, "power_stats": {"power_gpu_soc_mean_watts": 19.69, "power_cpu_cv_mean_watts": 2.278, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 71.033, "power_watts_avg": 19.69, "energy_joules_est": 207.24, "duration_seconds": 10.525, "sample_count": 90}, "timestamp": "2026-01-26T09:28:51.751703"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6816.23, "latencies_ms": [6816.23], "images_per_second": 0.147, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The image shows a wooden table with a plate of grilled bread and a glass of red wine. There is also a knife and a napkin holder on the table.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20889.6, "ram_available_mb": 41951.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 11.3, "ram_used_mb": 20904.3, "ram_available_mb": 41936.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.895}, "power_stats": {"power_gpu_soc_mean_watts": 22.468, "power_cpu_cv_mean_watts": 2.649, "power_sys_5v0_mean_watts": 8.703, "gpu_utilization_percent_mean": 75.895, "power_watts_avg": 22.468, "energy_joules_est": 153.16, "duration_seconds": 6.817, "sample_count": 57}, "timestamp": "2026-01-26T09:29:00.579645"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8795.463, "latencies_ms": [8795.463], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image features a wooden table with a warm, natural lighting that highlights the textures of the food and tableware. A glass of red wine and a piece of bread with char marks are visible, suggesting a cozy, indoor dining setting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20904.3, "ram_available_mb": 41936.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 12.4, "ram_used_mb": 20979.1, "ram_available_mb": 41861.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.382}, "power_stats": {"power_gpu_soc_mean_watts": 20.302, "power_cpu_cv_mean_watts": 3.014, "power_sys_5v0_mean_watts": 8.748, "gpu_utilization_percent_mean": 72.382, "power_watts_avg": 20.302, "energy_joules_est": 178.58, "duration_seconds": 8.796, "sample_count": 76}, "timestamp": "2026-01-26T09:29:11.395205"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11597.468, "latencies_ms": [11597.468], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a skier is performing a jump in the air, showcasing their skills. The skier is wearing a colorful outfit and is in the middle of a snowy slope. There are several other people in the scene, including a man standing on the left side of the image and another person on the right side. \n\nIn addition to the skier", "error": null, "sys_before": {"cpu_percent": 11.8, "ram_used_mb": 20979.1, "ram_available_mb": 41861.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 11.4, "ram_used_mb": 20975.3, "ram_available_mb": 41865.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.34}, "power_stats": {"power_gpu_soc_mean_watts": 19.353, "power_cpu_cv_mean_watts": 3.115, "power_sys_5v0_mean_watts": 8.734, "gpu_utilization_percent_mean": 70.34, "power_watts_avg": 19.353, "energy_joules_est": 224.46, "duration_seconds": 11.598, "sample_count": 100}, "timestamp": "2026-01-26T09:29:25.037503"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7797.547, "latencies_ms": [7797.547], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "person: 3, snowboarder: 1, snowboard: 1, trees: 10, snow: 1, mountain: 1, sky: 1, flags: 2", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20864.4, "ram_available_mb": 41976.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20856.7, "ram_available_mb": 41984.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.667}, "power_stats": {"power_gpu_soc_mean_watts": 21.217, "power_cpu_cv_mean_watts": 1.571, "power_sys_5v0_mean_watts": 8.572, "gpu_utilization_percent_mean": 73.667, "power_watts_avg": 21.217, "energy_joules_est": 165.45, "duration_seconds": 7.798, "sample_count": 66}, "timestamp": "2026-01-26T09:29:34.896689"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11476.786, "latencies_ms": [11476.786], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "In the foreground, there is a snowboarder performing a trick in the air, with their snowboard parallel to the ground. In the background, there are two spectators watching the performance, one standing to the left and the other to the right of the snowboarder. The sky is clear and blue, indicating that the event is taking place on a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20856.7, "ram_available_mb": 41984.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20864.5, "ram_available_mb": 41976.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.237}, "power_stats": {"power_gpu_soc_mean_watts": 19.391, "power_cpu_cv_mean_watts": 1.87, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 70.237, "power_watts_avg": 19.391, "energy_joules_est": 222.56, "duration_seconds": 11.477, "sample_count": 97}, "timestamp": "2026-01-26T09:29:48.385655"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8495.815, "latencies_ms": [8495.815], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A skier is performing a mid-air trick on a snowy mountain slope, with a clear blue sky in the background. Spectators, including a child, are watching the skier's impressive jump from the sidelines.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20864.5, "ram_available_mb": 41976.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20868.1, "ram_available_mb": 41972.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.639}, "power_stats": {"power_gpu_soc_mean_watts": 20.813, "power_cpu_cv_mean_watts": 1.64, "power_sys_5v0_mean_watts": 8.575, "gpu_utilization_percent_mean": 72.639, "power_watts_avg": 20.813, "energy_joules_est": 176.84, "duration_seconds": 8.497, "sample_count": 72}, "timestamp": "2026-01-26T09:29:58.942493"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7910.752, "latencies_ms": [7910.752], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The skier is wearing a colorful outfit with a mix of green, red, and white, and is performing a trick in the air. The sky is clear and blue, indicating good weather conditions for skiing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20868.1, "ram_available_mb": 41972.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20857.7, "ram_available_mb": 41983.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.397}, "power_stats": {"power_gpu_soc_mean_watts": 20.618, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 73.397, "power_watts_avg": 20.618, "energy_joules_est": 163.12, "duration_seconds": 7.911, "sample_count": 68}, "timestamp": "2026-01-26T09:30:08.891538"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11569.792, "latencies_ms": [11569.792], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is seen cross-country skiing on a snowy mountain. The skier, dressed in a vibrant green jacket and black pants, is in motion, gliding over the snow-covered terrain. The skier is holding ski poles, aiding in their navigation through the snow. The mountain is blanketed in a thick layer of", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20857.7, "ram_available_mb": 41983.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20886.0, "ram_available_mb": 41954.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.041}, "power_stats": {"power_gpu_soc_mean_watts": 19.317, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.666, "gpu_utilization_percent_mean": 70.041, "power_watts_avg": 19.317, "energy_joules_est": 223.51, "duration_seconds": 11.57, "sample_count": 98}, "timestamp": "2026-01-26T09:30:22.503432"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9099.075, "latencies_ms": [9099.075], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "sky: 1\nclouds: 3\nmountain: 1\nperson: 1\nski poles: 2\ngreen shirt: 1\nblack pants: 1\nsnow: 1\nrocks: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20886.0, "ram_available_mb": 41954.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20882.6, "ram_available_mb": 41958.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.615}, "power_stats": {"power_gpu_soc_mean_watts": 20.315, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 71.615, "power_watts_avg": 20.315, "energy_joules_est": 184.86, "duration_seconds": 9.1, "sample_count": 78}, "timestamp": "2026-01-26T09:30:33.630821"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10395.975, "latencies_ms": [10395.975], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, a person is standing on skis, facing away from the camera, with a clear path leading up to them. The background features a vast expanse of snow-covered mountains under a blue sky with scattered clouds. The person appears to be at a higher elevation, looking down towards the valley below.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20882.6, "ram_available_mb": 41958.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20893.0, "ram_available_mb": 41947.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.91}, "power_stats": {"power_gpu_soc_mean_watts": 19.743, "power_cpu_cv_mean_watts": 1.813, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 70.91, "power_watts_avg": 19.743, "energy_joules_est": 205.26, "duration_seconds": 10.397, "sample_count": 89}, "timestamp": "2026-01-26T09:30:46.085393"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8819.321, "latencies_ms": [8819.321], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "A person is cross-country skiing on a snowy mountain trail with a clear blue sky above and a few clouds scattered across it. The skier is wearing a green top and black pants, and is holding ski poles in their hands.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20893.0, "ram_available_mb": 41947.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20966.2, "ram_available_mb": 41874.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.693}, "power_stats": {"power_gpu_soc_mean_watts": 20.796, "power_cpu_cv_mean_watts": 1.692, "power_sys_5v0_mean_watts": 8.581, "gpu_utilization_percent_mean": 71.693, "power_watts_avg": 20.796, "energy_joules_est": 183.42, "duration_seconds": 8.82, "sample_count": 75}, "timestamp": "2026-01-26T09:30:56.943683"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8478.778, "latencies_ms": [8478.778], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image features a clear blue sky with scattered white clouds, indicating fair weather. The snow-covered landscape is bathed in sunlight, casting shadows and highlighting the white of the snow against the blue of the sky and the earth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20862.7, "ram_available_mb": 41978.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20882.2, "ram_available_mb": 41958.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.521}, "power_stats": {"power_gpu_soc_mean_watts": 20.625, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 72.521, "power_watts_avg": 20.625, "energy_joules_est": 174.89, "duration_seconds": 8.479, "sample_count": 71}, "timestamp": "2026-01-26T09:31:07.445388"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11622.081, "latencies_ms": [11622.081], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a close-up view of a banana and a chocolate-covered donut, both encased in a clear plastic bag. The banana, with its characteristic yellow skin, is positioned to the left of the donut. The donut, with its dark brown color, is on the right. The bag appears to be sealed, as indicated by", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 20882.2, "ram_available_mb": 41958.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20855.9, "ram_available_mb": 41985.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.97}, "power_stats": {"power_gpu_soc_mean_watts": 19.277, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 69.97, "power_watts_avg": 19.277, "energy_joules_est": 224.05, "duration_seconds": 11.623, "sample_count": 100}, "timestamp": "2026-01-26T09:31:21.127345"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4156.355, "latencies_ms": [4156.355], "images_per_second": 0.241, "prompt_tokens": 39, "response_tokens_est": 12, "n_tiles": 16, "output_text": "banana: 1, donut: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20855.9, "ram_available_mb": 41985.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 20867.7, "ram_available_mb": 41973.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 82.114}, "power_stats": {"power_gpu_soc_mean_watts": 25.257, "power_cpu_cv_mean_watts": 0.869, "power_sys_5v0_mean_watts": 8.494, "gpu_utilization_percent_mean": 82.114, "power_watts_avg": 25.257, "energy_joules_est": 104.99, "duration_seconds": 4.157, "sample_count": 35}, "timestamp": "2026-01-26T09:31:27.344026"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10099.654, "latencies_ms": [10099.654], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The banana is positioned in the foreground on the left side of the image, appearing larger and more detailed. The donut is in the background, partially obscured by the banana, and appears smaller due to the perspective. The donut is to the right of the banana when viewing the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20867.7, "ram_available_mb": 41973.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20862.4, "ram_available_mb": 41978.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.035}, "power_stats": {"power_gpu_soc_mean_watts": 19.727, "power_cpu_cv_mean_watts": 1.811, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 72.035, "power_watts_avg": 19.727, "energy_joules_est": 199.25, "duration_seconds": 10.1, "sample_count": 86}, "timestamp": "2026-01-26T09:31:39.473858"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8678.218, "latencies_ms": [8678.218], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a chocolate glazed donut and a banana, both individually wrapped, placed closely together. The setting appears to be a plastic bag, suggesting they might be in a grocery store or a similar retail environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20862.4, "ram_available_mb": 41978.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20856.3, "ram_available_mb": 41984.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.26}, "power_stats": {"power_gpu_soc_mean_watts": 20.783, "power_cpu_cv_mean_watts": 1.672, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 73.26, "power_watts_avg": 20.783, "energy_joules_est": 180.37, "duration_seconds": 8.679, "sample_count": 73}, "timestamp": "2026-01-26T09:31:50.171759"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7679.582, "latencies_ms": [7679.582], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows a banana and a chocolate-covered donut placed closely together. The banana is yellow, and the donut has a glossy, dark brown chocolate coating.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20856.3, "ram_available_mb": 41984.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20865.6, "ram_available_mb": 41975.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.154}, "power_stats": {"power_gpu_soc_mean_watts": 20.993, "power_cpu_cv_mean_watts": 1.626, "power_sys_5v0_mean_watts": 8.602, "gpu_utilization_percent_mean": 73.154, "power_watts_avg": 20.993, "energy_joules_est": 161.23, "duration_seconds": 7.68, "sample_count": 65}, "timestamp": "2026-01-26T09:31:59.899818"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12154.364, "latencies_ms": [12154.364], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The image presents a scene featuring a white mug with a skull and crossbones design, accompanied by a knife with a black handle. The mug is placed on a surface with a striped pattern, and the knife is lying next to it. The arrangement of these objects suggests a playful or humorous theme, possibly related to pirate imagery.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20865.6, "ram_available_mb": 41975.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20851.0, "ram_available_mb": 41989.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.563}, "power_stats": {"power_gpu_soc_mean_watts": 21.343, "power_cpu_cv_mean_watts": 1.792, "power_sys_5v0_mean_watts": 8.804, "gpu_utilization_percent_mean": 72.563, "power_watts_avg": 21.343, "energy_joules_est": 259.42, "duration_seconds": 12.155, "sample_count": 103}, "timestamp": "2026-01-26T09:32:14.092611"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4923.542, "latencies_ms": [4923.542], "images_per_second": 0.203, "prompt_tokens": 39, "response_tokens_est": 12, "n_tiles": 16, "output_text": "mug: 1, knife: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20851.0, "ram_available_mb": 41989.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 20912.9, "ram_available_mb": 41928.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 85.927}, "power_stats": {"power_gpu_soc_mean_watts": 26.418, "power_cpu_cv_mean_watts": 0.781, "power_sys_5v0_mean_watts": 8.576, "gpu_utilization_percent_mean": 85.927, "power_watts_avg": 26.418, "energy_joules_est": 130.09, "duration_seconds": 4.924, "sample_count": 41}, "timestamp": "2026-01-26T09:32:21.051875"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8852.996, "latencies_ms": [8852.996], "images_per_second": 0.113, "prompt_tokens": 44, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The skull and crossbones mug is positioned to the left of the knife, which is lying flat on the surface in the foreground. The background is a textured surface that appears to be a table or countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20912.9, "ram_available_mb": 41928.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20854.8, "ram_available_mb": 41986.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.533}, "power_stats": {"power_gpu_soc_mean_watts": 22.45, "power_cpu_cv_mean_watts": 1.623, "power_sys_5v0_mean_watts": 8.861, "gpu_utilization_percent_mean": 75.533, "power_watts_avg": 22.45, "energy_joules_est": 198.76, "duration_seconds": 8.854, "sample_count": 75}, "timestamp": "2026-01-26T09:32:31.941891"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8949.889, "latencies_ms": [8949.889], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A white mug with a skull and crossbones design is placed on a textured surface, accompanied by a knife with a black handle lying next to it. The setting appears to be a table or countertop.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20854.8, "ram_available_mb": 41986.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20858.6, "ram_available_mb": 41982.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.513}, "power_stats": {"power_gpu_soc_mean_watts": 22.782, "power_cpu_cv_mean_watts": 1.522, "power_sys_5v0_mean_watts": 8.752, "gpu_utilization_percent_mean": 76.513, "power_watts_avg": 22.782, "energy_joules_est": 203.91, "duration_seconds": 8.951, "sample_count": 76}, "timestamp": "2026-01-26T09:32:42.911908"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11343.765, "latencies_ms": [11343.765], "images_per_second": 0.088, "prompt_tokens": 36, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image features a white ceramic mug with a black skull and crossbones symbol, accompanied by a black-handled knife with a silver blade, all placed on a surface with a textured grey and white pattern. The lighting in the image is bright and even, suggesting an indoor setting with artificial lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20858.6, "ram_available_mb": 41982.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20856.6, "ram_available_mb": 41984.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.083}, "power_stats": {"power_gpu_soc_mean_watts": 21.648, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 8.882, "gpu_utilization_percent_mean": 73.083, "power_watts_avg": 21.648, "energy_joules_est": 245.58, "duration_seconds": 11.344, "sample_count": 96}, "timestamp": "2026-01-26T09:32:56.278905"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12416.672, "latencies_ms": [12416.672], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people gathered around a counter in a room. A man is standing at the counter, possibly a bartender, while several other people are standing or sitting nearby. There are at least five people in the scene, with some of them closer to the counter and others standing further back.\n\nThe room has a dining table and a couple of chairs, with", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20856.6, "ram_available_mb": 41984.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20994.3, "ram_available_mb": 41846.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.28}, "power_stats": {"power_gpu_soc_mean_watts": 21.597, "power_cpu_cv_mean_watts": 1.818, "power_sys_5v0_mean_watts": 8.921, "gpu_utilization_percent_mean": 72.28, "power_watts_avg": 21.597, "energy_joules_est": 268.18, "duration_seconds": 12.417, "sample_count": 107}, "timestamp": "2026-01-26T09:33:10.724865"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11296.569, "latencies_ms": [11296.569], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. People: 5\n2. Bottles: 10\n3. Glasses: 4\n4. Barrel: 1\n5. Counter: 1\n6. Window: 1\n7. Chair: 1\n8. Shoe: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20930.6, "ram_available_mb": 41910.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.8, "ram_used_mb": 21021.0, "ram_available_mb": 41819.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.385}, "power_stats": {"power_gpu_soc_mean_watts": 22.256, "power_cpu_cv_mean_watts": 1.789, "power_sys_5v0_mean_watts": 8.823, "gpu_utilization_percent_mean": 75.385, "power_watts_avg": 22.256, "energy_joules_est": 251.43, "duration_seconds": 11.297, "sample_count": 96}, "timestamp": "2026-01-26T09:33:24.080983"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12707.121, "latencies_ms": [12707.121], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is standing in front of a counter with a white apron, while a group of people are gathered around the counter in the background. The person in the foreground is facing away from the camera, and the people in the background are facing towards the person in the foreground. The counter is located in the center of the image, with the people standing around it", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20917.3, "ram_available_mb": 41923.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 11.9, "ram_used_mb": 20959.9, "ram_available_mb": 41881.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.385}, "power_stats": {"power_gpu_soc_mean_watts": 21.683, "power_cpu_cv_mean_watts": 2.803, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 73.385, "power_watts_avg": 21.683, "energy_joules_est": 275.54, "duration_seconds": 12.708, "sample_count": 109}, "timestamp": "2026-01-26T09:33:38.823928"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8355.187, "latencies_ms": [8355.187], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A group of people are gathered around a counter in a restaurant or cafe, with one person standing at the counter and others standing nearby. It appears to be a casual and social atmosphere.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20959.9, "ram_available_mb": 41881.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 11.4, "ram_used_mb": 20935.2, "ram_available_mb": 41905.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.296}, "power_stats": {"power_gpu_soc_mean_watts": 23.675, "power_cpu_cv_mean_watts": 2.572, "power_sys_5v0_mean_watts": 8.883, "gpu_utilization_percent_mean": 78.296, "power_watts_avg": 23.675, "energy_joules_est": 197.82, "duration_seconds": 8.356, "sample_count": 71}, "timestamp": "2026-01-26T09:33:49.213671"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8385.884, "latencies_ms": [8385.884], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image shows a group of people in an indoor setting with warm lighting. The walls are painted in a teal color, and there is a wooden bar counter with a person standing behind it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20935.2, "ram_available_mb": 41905.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 12.1, "ram_used_mb": 20985.0, "ram_available_mb": 41855.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.958}, "power_stats": {"power_gpu_soc_mean_watts": 23.327, "power_cpu_cv_mean_watts": 3.215, "power_sys_5v0_mean_watts": 8.98, "gpu_utilization_percent_mean": 77.958, "power_watts_avg": 23.327, "energy_joules_est": 195.64, "duration_seconds": 8.387, "sample_count": 72}, "timestamp": "2026-01-26T09:33:59.635231"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11598.043, "latencies_ms": [11598.043], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a marshy area with a large body of water in the foreground. Two white birds, possibly egrets, are standing in the grass near the water, adding a touch of life to the scene. In the background, there are several boats docked at a marina, with a few more boats floating on the water. The sky above", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 20879.7, "ram_available_mb": 41961.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20875.7, "ram_available_mb": 41965.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.71}, "power_stats": {"power_gpu_soc_mean_watts": 19.265, "power_cpu_cv_mean_watts": 1.898, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 70.71, "power_watts_avg": 19.265, "energy_joules_est": 223.45, "duration_seconds": 11.599, "sample_count": 100}, "timestamp": "2026-01-26T09:34:13.281157"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7575.976, "latencies_ms": [7575.976], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "sky: 1, clouds: 1, airplane: 1, boats: 5, buildings: 2, poles: 4, grass: 1, birds: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20875.7, "ram_available_mb": 41965.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20870.3, "ram_available_mb": 41970.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.516}, "power_stats": {"power_gpu_soc_mean_watts": 21.16, "power_cpu_cv_mean_watts": 1.57, "power_sys_5v0_mean_watts": 8.586, "gpu_utilization_percent_mean": 74.516, "power_watts_avg": 21.16, "energy_joules_est": 160.32, "duration_seconds": 7.577, "sample_count": 64}, "timestamp": "2026-01-26T09:34:22.911916"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8406.973, "latencies_ms": [8406.973], "images_per_second": 0.119, "prompt_tokens": 44, "response_tokens_est": 51, "n_tiles": 16, "output_text": "In the foreground, there is a grassy area with two birds standing near the center. The background features a marina with several boats and docks extending into the water. The sky above is filled with clouds, suggesting an overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20870.3, "ram_available_mb": 41970.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20873.0, "ram_available_mb": 41967.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.704}, "power_stats": {"power_gpu_soc_mean_watts": 20.507, "power_cpu_cv_mean_watts": 1.703, "power_sys_5v0_mean_watts": 8.619, "gpu_utilization_percent_mean": 71.704, "power_watts_avg": 20.507, "energy_joules_est": 172.41, "duration_seconds": 8.408, "sample_count": 71}, "timestamp": "2026-01-26T09:34:33.348520"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8249.731, "latencies_ms": [8249.731], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image depicts a marshy area with two birds standing in the foreground, and a large industrial area with multiple cranes and buildings in the background. The sky is cloudy, suggesting an overcast day.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20873.0, "ram_available_mb": 41967.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20874.1, "ram_available_mb": 41966.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.855}, "power_stats": {"power_gpu_soc_mean_watts": 21.056, "power_cpu_cv_mean_watts": 1.624, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 73.855, "power_watts_avg": 21.056, "energy_joules_est": 173.72, "duration_seconds": 8.25, "sample_count": 69}, "timestamp": "2026-01-26T09:34:43.626623"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7228.862, "latencies_ms": [7228.862], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The sky is overcast with a mix of blue and gray clouds, suggesting a gloomy or cloudy day. The grass is a vibrant green, indicating it might be spring or summer.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20874.1, "ram_available_mb": 41966.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20860.6, "ram_available_mb": 41980.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.721}, "power_stats": {"power_gpu_soc_mean_watts": 21.274, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 73.721, "power_watts_avg": 21.274, "energy_joules_est": 153.8, "duration_seconds": 7.229, "sample_count": 61}, "timestamp": "2026-01-26T09:34:52.870956"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11560.428, "latencies_ms": [11560.428], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photo, a man is kneeling on the floor in a bathroom, working on a toilet. He is wearing a black shirt and jeans, and has a tool belt around his waist. The bathroom is equipped with a white toilet, a sink, and a mirror. The walls of the bathroom are adorned", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20860.6, "ram_available_mb": 41980.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20896.2, "ram_available_mb": 41944.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.93}, "power_stats": {"power_gpu_soc_mean_watts": 19.301, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 8.694, "gpu_utilization_percent_mean": 69.93, "power_watts_avg": 19.301, "energy_joules_est": 223.14, "duration_seconds": 11.561, "sample_count": 100}, "timestamp": "2026-01-26T09:35:06.472054"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11240.862, "latencies_ms": [11240.862], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 74, "n_tiles": 16, "output_text": "1. Toilet: 1\n2. Glove: 1\n3. Tool belt: 1\n4. Tiles: 1\n5. Tile adhesive: 1\n6. Tile cutter: 1\n7. Hammer: 1\n8. Screwdriver: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20896.2, "ram_available_mb": 41944.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20887.6, "ram_available_mb": 41953.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.453}, "power_stats": {"power_gpu_soc_mean_watts": 19.74, "power_cpu_cv_mean_watts": 1.833, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 71.453, "power_watts_avg": 19.74, "energy_joules_est": 221.91, "duration_seconds": 11.241, "sample_count": 95}, "timestamp": "2026-01-26T09:35:19.728743"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9495.571, "latencies_ms": [9495.571], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The person is seated on a stool to the left of the toilet, which is positioned in the foreground of the image. The background features a wall with a checkered pattern, and there is a shelf with various items on it to the right of the person.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20887.6, "ram_available_mb": 41953.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20885.2, "ram_available_mb": 41955.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.025}, "power_stats": {"power_gpu_soc_mean_watts": 20.1, "power_cpu_cv_mean_watts": 1.764, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 72.025, "power_watts_avg": 20.1, "energy_joules_est": 190.87, "duration_seconds": 9.496, "sample_count": 81}, "timestamp": "2026-01-26T09:35:31.243382"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7924.309, "latencies_ms": [7924.309], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A man is kneeling on the floor in a bathroom, working on a toilet. He is wearing a black shirt and jeans, and there is a tool belt around his waist.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20885.2, "ram_available_mb": 41955.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20897.0, "ram_available_mb": 41943.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.06}, "power_stats": {"power_gpu_soc_mean_watts": 21.151, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.561, "gpu_utilization_percent_mean": 74.06, "power_watts_avg": 21.151, "energy_joules_est": 167.62, "duration_seconds": 7.925, "sample_count": 67}, "timestamp": "2026-01-26T09:35:41.189051"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8806.387, "latencies_ms": [8806.387], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image is in black and white, featuring a person in a dark shirt and jeans working on a toilet. The lighting is bright and even, illuminating the person and the toilet, which is made of porcelain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20897.0, "ram_available_mb": 41943.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20994.4, "ram_available_mb": 41846.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.12}, "power_stats": {"power_gpu_soc_mean_watts": 20.422, "power_cpu_cv_mean_watts": 1.708, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 73.12, "power_watts_avg": 20.422, "energy_joules_est": 179.86, "duration_seconds": 8.807, "sample_count": 75}, "timestamp": "2026-01-26T09:35:52.027643"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12366.424, "latencies_ms": [12366.424], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a group of people is standing on a snow-covered mountain, preparing to ski down the slope. There are at least six people in the group, with some of them holding skis and others holding backpacks. They are positioned near the top of the mountain, possibly taking a break or discussing their route.\n\nThe mountain slope is covered in snow", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20864.1, "ram_available_mb": 41976.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20862.3, "ram_available_mb": 41978.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.752}, "power_stats": {"power_gpu_soc_mean_watts": 21.372, "power_cpu_cv_mean_watts": 1.803, "power_sys_5v0_mean_watts": 8.828, "gpu_utilization_percent_mean": 72.752, "power_watts_avg": 21.372, "energy_joules_est": 264.31, "duration_seconds": 12.367, "sample_count": 105}, "timestamp": "2026-01-26T09:36:06.425040"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6780.212, "latencies_ms": [6780.212], "images_per_second": 0.147, "prompt_tokens": 39, "response_tokens_est": 28, "n_tiles": 16, "output_text": "mountain: 1\nsnow: numerous\nskiers: 5\nskis: 2\ntracks: numerous\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20862.3, "ram_available_mb": 41978.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 20865.0, "ram_available_mb": 41975.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 80.561}, "power_stats": {"power_gpu_soc_mean_watts": 24.164, "power_cpu_cv_mean_watts": 1.201, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 80.561, "power_watts_avg": 24.164, "energy_joules_est": 163.85, "duration_seconds": 6.781, "sample_count": 57}, "timestamp": "2026-01-26T09:36:15.257064"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11856.394, "latencies_ms": [11856.394], "images_per_second": 0.084, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground, there is a group of people standing on skis, positioned near the base of a large snow-covered mountain. The mountain is in the background, with its peak reaching high into the clear blue sky. The ski tracks on the snow create a winding path leading up the mountain, indicating the skiers' route as they ascend.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20865.0, "ram_available_mb": 41975.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20858.5, "ram_available_mb": 41982.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.822}, "power_stats": {"power_gpu_soc_mean_watts": 21.433, "power_cpu_cv_mean_watts": 1.788, "power_sys_5v0_mean_watts": 8.876, "gpu_utilization_percent_mean": 72.822, "power_watts_avg": 21.433, "energy_joules_est": 254.13, "duration_seconds": 11.857, "sample_count": 101}, "timestamp": "2026-01-26T09:36:29.170951"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8164.318, "latencies_ms": [8164.318], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A group of people are standing on a snowy mountain, with a large snow-covered mountain in the background. They appear to be preparing to ski or snowboard down the mountain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20858.5, "ram_available_mb": 41982.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20862.8, "ram_available_mb": 41978.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.899}, "power_stats": {"power_gpu_soc_mean_watts": 23.137, "power_cpu_cv_mean_watts": 1.415, "power_sys_5v0_mean_watts": 8.679, "gpu_utilization_percent_mean": 77.899, "power_watts_avg": 23.137, "energy_joules_est": 188.91, "duration_seconds": 8.165, "sample_count": 69}, "timestamp": "2026-01-26T09:36:39.350612"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8282.219, "latencies_ms": [8282.219], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image features a group of people standing on a snowy mountain with a clear blue sky in the background. The snow on the mountain appears to be fresh and untouched, with some tracks visible in the snow.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20862.8, "ram_available_mb": 41978.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20857.0, "ram_available_mb": 41983.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.058}, "power_stats": {"power_gpu_soc_mean_watts": 22.922, "power_cpu_cv_mean_watts": 1.543, "power_sys_5v0_mean_watts": 8.872, "gpu_utilization_percent_mean": 75.058, "power_watts_avg": 22.922, "energy_joules_est": 189.86, "duration_seconds": 8.283, "sample_count": 69}, "timestamp": "2026-01-26T09:36:49.652954"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12421.73, "latencies_ms": [12421.73], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a well-balanced meal served on a white plate. The plate is divided into three sections. On the left side, there's a serving of white rice, which is the base of the meal. The middle section is filled with a vibrant red and yellow chili, adding a pop of color and likely a spicy kick to the meal. On", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20857.0, "ram_available_mb": 41983.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20861.2, "ram_available_mb": 41979.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.876}, "power_stats": {"power_gpu_soc_mean_watts": 21.535, "power_cpu_cv_mean_watts": 1.815, "power_sys_5v0_mean_watts": 8.912, "gpu_utilization_percent_mean": 72.876, "power_watts_avg": 21.535, "energy_joules_est": 267.52, "duration_seconds": 12.422, "sample_count": 105}, "timestamp": "2026-01-26T09:37:04.103892"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8763.104, "latencies_ms": [8763.104], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "broccoli: 1, rice: lot, beans: lot, tomatoes: lot, onions: lot, bell peppers: lot, garlic: lot, olive oil: lot", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20861.2, "ram_available_mb": 41979.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20857.4, "ram_available_mb": 41983.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.373}, "power_stats": {"power_gpu_soc_mean_watts": 23.287, "power_cpu_cv_mean_watts": 1.452, "power_sys_5v0_mean_watts": 8.757, "gpu_utilization_percent_mean": 78.373, "power_watts_avg": 23.287, "energy_joules_est": 204.08, "duration_seconds": 8.764, "sample_count": 75}, "timestamp": "2026-01-26T09:37:14.885747"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12708.617, "latencies_ms": [12708.617], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the foreground of the image, there is a white bowl containing a meal. To the left of the bowl, there is a piece of broccoli, and to the right, there is a serving of rice. The bowl is placed on a wooden surface, and the broccoli is positioned to the left of the rice, creating a balanced composition.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20857.4, "ram_available_mb": 41983.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20917.5, "ram_available_mb": 41923.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.269}, "power_stats": {"power_gpu_soc_mean_watts": 21.704, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 8.89, "gpu_utilization_percent_mean": 74.269, "power_watts_avg": 21.704, "energy_joules_est": 275.84, "duration_seconds": 12.709, "sample_count": 108}, "timestamp": "2026-01-26T09:37:29.629879"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9823.006, "latencies_ms": [9823.006], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a white bowl filled with a meal consisting of white rice, a serving of stewed vegetables, and a piece of broccoli. The bowl is placed on a dark wooden surface, possibly a table or countertop.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20917.5, "ram_available_mb": 41923.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20937.7, "ram_available_mb": 41903.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.446}, "power_stats": {"power_gpu_soc_mean_watts": 22.783, "power_cpu_cv_mean_watts": 1.534, "power_sys_5v0_mean_watts": 8.784, "gpu_utilization_percent_mean": 76.446, "power_watts_avg": 22.783, "energy_joules_est": 223.81, "duration_seconds": 9.824, "sample_count": 83}, "timestamp": "2026-01-26T09:37:41.496687"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12070.466, "latencies_ms": [12070.466], "images_per_second": 0.083, "prompt_tokens": 36, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image shows a bowl of food with a focus on a white plate containing a serving of white rice, a piece of broccoli, and a chunky tomato-based dish. The lighting in the image is warm and appears to be coming from the upper left side, casting a soft shadow to the right of the objects on the plate.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20937.7, "ram_available_mb": 41903.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20866.9, "ram_available_mb": 41974.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.699}, "power_stats": {"power_gpu_soc_mean_watts": 21.829, "power_cpu_cv_mean_watts": 1.757, "power_sys_5v0_mean_watts": 8.894, "gpu_utilization_percent_mean": 72.699, "power_watts_avg": 21.829, "energy_joules_est": 263.5, "duration_seconds": 12.071, "sample_count": 103}, "timestamp": "2026-01-26T09:37:55.596629"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11584.422, "latencies_ms": [11584.422], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is skillfully skateboarding on a wooden platform. The skateboarder is wearing black and white shoes, and the skateboard itself is black and white. The skateboarder is in the process of performing a trick, with their left foot on the tail of the skateboard and their right foot on the nose. The wooden platform", "error": null, "sys_before": {"cpu_percent": 17.4, "ram_used_mb": 20866.9, "ram_available_mb": 41974.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 11.7, "ram_used_mb": 20874.0, "ram_available_mb": 41966.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.97}, "power_stats": {"power_gpu_soc_mean_watts": 19.395, "power_cpu_cv_mean_watts": 2.6, "power_sys_5v0_mean_watts": 8.707, "gpu_utilization_percent_mean": 69.97, "power_watts_avg": 19.395, "energy_joules_est": 224.69, "duration_seconds": 11.585, "sample_count": 99}, "timestamp": "2026-01-26T09:38:09.241001"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8609.887, "latencies_ms": [8609.887], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "skateboard: 1, wooden platform: 1, grass: multiple patches, person's legs: 2, shoes: 2, wheels: 4, checkered pattern: 1, signature: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20874.0, "ram_available_mb": 41966.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 11.8, "ram_used_mb": 20878.5, "ram_available_mb": 41962.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.392}, "power_stats": {"power_gpu_soc_mean_watts": 20.643, "power_cpu_cv_mean_watts": 2.781, "power_sys_5v0_mean_watts": 8.675, "gpu_utilization_percent_mean": 73.392, "power_watts_avg": 20.643, "energy_joules_est": 177.75, "duration_seconds": 8.611, "sample_count": 74}, "timestamp": "2026-01-26T09:38:19.886727"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8390.17, "latencies_ms": [8390.17], "images_per_second": 0.119, "prompt_tokens": 44, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, performing a trick on a wooden platform. The platform is elevated above the grassy ground, which is in the background. There is another wooden structure partially visible to the right in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20878.5, "ram_available_mb": 41962.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 13.2, "ram_used_mb": 20867.9, "ram_available_mb": 41973.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.375}, "power_stats": {"power_gpu_soc_mean_watts": 20.671, "power_cpu_cv_mean_watts": 3.064, "power_sys_5v0_mean_watts": 8.731, "gpu_utilization_percent_mean": 72.375, "power_watts_avg": 20.671, "energy_joules_est": 173.45, "duration_seconds": 8.391, "sample_count": 72}, "timestamp": "2026-01-26T09:38:30.324964"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5911.27, "latencies_ms": [5911.27], "images_per_second": 0.169, "prompt_tokens": 37, "response_tokens_est": 28, "n_tiles": 16, "output_text": "A person is skateboarding on a wooden platform outdoors, with a blurred background featuring grass and wooden structures.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20868.3, "ram_available_mb": 41972.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 20874.6, "ram_available_mb": 41966.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.54}, "power_stats": {"power_gpu_soc_mean_watts": 23.119, "power_cpu_cv_mean_watts": 2.795, "power_sys_5v0_mean_watts": 8.69, "gpu_utilization_percent_mean": 77.54, "power_watts_avg": 23.119, "energy_joules_est": 136.67, "duration_seconds": 5.912, "sample_count": 50}, "timestamp": "2026-01-26T09:38:38.299711"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8502.62, "latencies_ms": [8502.62], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image captures a moment of a skateboarder in mid-air, with the skateboard positioned on a wooden platform. The lighting is natural and bright, suggesting the photo was taken outdoors on a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20874.6, "ram_available_mb": 41966.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20873.0, "ram_available_mb": 41967.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.833}, "power_stats": {"power_gpu_soc_mean_watts": 20.438, "power_cpu_cv_mean_watts": 1.701, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 71.833, "power_watts_avg": 20.438, "energy_joules_est": 173.79, "duration_seconds": 8.503, "sample_count": 72}, "timestamp": "2026-01-26T09:38:48.828742"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12384.007, "latencies_ms": [12384.007], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a bunch of three ripe bananas with a yellow hue is resting on a wooden desk. The bananas are slightly curved, with one of them appearing to be slightly overripe. The desk, which is a light brown color, is situated in front of a computer monitor that is turned off. The monitor is black and has a", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 20873.0, "ram_available_mb": 41967.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20871.9, "ram_available_mb": 41969.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.724}, "power_stats": {"power_gpu_soc_mean_watts": 21.368, "power_cpu_cv_mean_watts": 1.811, "power_sys_5v0_mean_watts": 8.822, "gpu_utilization_percent_mean": 72.724, "power_watts_avg": 21.368, "energy_joules_est": 264.64, "duration_seconds": 12.385, "sample_count": 105}, "timestamp": "2026-01-26T09:39:03.236131"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4376.48, "latencies_ms": [4376.48], "images_per_second": 0.228, "prompt_tokens": 39, "response_tokens_est": 7, "n_tiles": 16, "output_text": "banana: 5\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20871.9, "ram_available_mb": 41969.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 20870.2, "ram_available_mb": 41970.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 89.583}, "power_stats": {"power_gpu_soc_mean_watts": 27.192, "power_cpu_cv_mean_watts": 0.578, "power_sys_5v0_mean_watts": 8.553, "gpu_utilization_percent_mean": 89.583, "power_watts_avg": 27.192, "energy_joules_est": 119.02, "duration_seconds": 4.377, "sample_count": 36}, "timestamp": "2026-01-26T09:39:09.646306"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9289.297, "latencies_ms": [9289.297], "images_per_second": 0.108, "prompt_tokens": 44, "response_tokens_est": 54, "n_tiles": 16, "output_text": "In the foreground, there is a bunch of bananas with one banana lying on its side, closer to the viewer. In the background, there is a computer monitor and a keyboard, indicating that the bananas are on a desk or table.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20870.2, "ram_available_mb": 41970.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20868.0, "ram_available_mb": 41972.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.088}, "power_stats": {"power_gpu_soc_mean_watts": 22.363, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.877, "gpu_utilization_percent_mean": 74.088, "power_watts_avg": 22.363, "energy_joules_est": 207.76, "duration_seconds": 9.29, "sample_count": 80}, "timestamp": "2026-01-26T09:39:20.960982"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10542.294, "latencies_ms": [10542.294], "images_per_second": 0.095, "prompt_tokens": 37, "response_tokens_est": 61, "n_tiles": 16, "output_text": "A bunch of bananas is precariously balanced on top of another banana, creating an unusual and interesting arrangement. The scene is set on a wooden table, with a computer monitor and keyboard visible in the background, suggesting that this might be taking place in an office or home workspace.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20868.0, "ram_available_mb": 41972.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20874.5, "ram_available_mb": 41966.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.356}, "power_stats": {"power_gpu_soc_mean_watts": 22.078, "power_cpu_cv_mean_watts": 1.668, "power_sys_5v0_mean_watts": 8.777, "gpu_utilization_percent_mean": 74.356, "power_watts_avg": 22.078, "energy_joules_est": 232.77, "duration_seconds": 10.543, "sample_count": 90}, "timestamp": "2026-01-26T09:39:33.532841"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7905.261, "latencies_ms": [7905.261], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The bananas are yellow with some brown spots, indicating ripeness. They are placed on a wooden surface, likely a table, with a blurred background that includes a computer monitor and keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20874.5, "ram_available_mb": 41966.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20869.6, "ram_available_mb": 41971.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.121}, "power_stats": {"power_gpu_soc_mean_watts": 23.076, "power_cpu_cv_mean_watts": 1.504, "power_sys_5v0_mean_watts": 8.856, "gpu_utilization_percent_mean": 76.121, "power_watts_avg": 23.076, "energy_joules_est": 182.44, "duration_seconds": 7.906, "sample_count": 66}, "timestamp": "2026-01-26T09:39:43.454732"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11614.022, "latencies_ms": [11614.022], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a plate of food on a dining table. The plate contains a variety of food items, including rice, broccoli, carrots, and chicken. The broccoli is placed near the center of the plate, while the carrots are scattered around it. The chicken is spread across the plate, with some pieces closer to the edges. The rice is", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20869.6, "ram_available_mb": 41971.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20862.0, "ram_available_mb": 41978.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.626}, "power_stats": {"power_gpu_soc_mean_watts": 19.333, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.681, "gpu_utilization_percent_mean": 70.626, "power_watts_avg": 19.333, "energy_joules_est": 224.55, "duration_seconds": 11.615, "sample_count": 99}, "timestamp": "2026-01-26T09:39:57.105452"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8596.802, "latencies_ms": [8596.802], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "plate: 1\nglass: 1\nwater: 1\nfork: 1\nknife: 1\nchicken: 1\nbroccoli: 1\ncarrot: 1\nrice: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20862.0, "ram_available_mb": 41978.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20870.7, "ram_available_mb": 41970.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.877}, "power_stats": {"power_gpu_soc_mean_watts": 20.741, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.601, "gpu_utilization_percent_mean": 72.877, "power_watts_avg": 20.741, "energy_joules_est": 178.32, "duration_seconds": 8.597, "sample_count": 73}, "timestamp": "2026-01-26T09:40:07.729183"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7244.632, "latencies_ms": [7244.632], "images_per_second": 0.138, "prompt_tokens": 44, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The plate of food is in the foreground, placed on a wooden table. In the background, there is a glass of water and a fork and knife on the right side of the plate.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20870.7, "ram_available_mb": 41970.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20934.3, "ram_available_mb": 41906.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.426}, "power_stats": {"power_gpu_soc_mean_watts": 21.385, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 73.426, "power_watts_avg": 21.385, "energy_joules_est": 154.94, "duration_seconds": 7.245, "sample_count": 61}, "timestamp": "2026-01-26T09:40:17.007889"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6869.847, "latencies_ms": [6869.847], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "A plate of food with rice, vegetables, and meat is on a wooden table. A glass of water and a fork and knife are also present on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20934.3, "ram_available_mb": 41906.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20940.9, "ram_available_mb": 41900.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.655}, "power_stats": {"power_gpu_soc_mean_watts": 21.985, "power_cpu_cv_mean_watts": 1.477, "power_sys_5v0_mean_watts": 8.556, "gpu_utilization_percent_mean": 74.655, "power_watts_avg": 21.985, "energy_joules_est": 151.05, "duration_seconds": 6.871, "sample_count": 58}, "timestamp": "2026-01-26T09:40:25.895276"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7803.053, "latencies_ms": [7803.053], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The plate is colorful with a mix of orange, green, and white hues, and it's placed on a wooden table. There's a glass of water in the background, suggesting a dining setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20870.8, "ram_available_mb": 41970.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20894.8, "ram_available_mb": 41946.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.515}, "power_stats": {"power_gpu_soc_mean_watts": 20.916, "power_cpu_cv_mean_watts": 1.644, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 72.515, "power_watts_avg": 20.916, "energy_joules_est": 163.22, "duration_seconds": 7.804, "sample_count": 66}, "timestamp": "2026-01-26T09:40:35.723790"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11552.13, "latencies_ms": [11552.13], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young girl is standing in a living room, holding a Wii remote and playing a video game. She is the main focus of the scene, with several other people in the room, including a man and a woman standing near the couch. There are also two other people in the background, one closer to the left side of the room and the other near the right side", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20894.8, "ram_available_mb": 41946.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20864.2, "ram_available_mb": 41976.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.673}, "power_stats": {"power_gpu_soc_mean_watts": 19.235, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.653, "gpu_utilization_percent_mean": 69.673, "power_watts_avg": 19.235, "energy_joules_est": 222.22, "duration_seconds": 11.553, "sample_count": 98}, "timestamp": "2026-01-26T09:40:49.317746"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9875.153, "latencies_ms": [9875.153], "images_per_second": 0.101, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "- people: 5\n\n- couch: 1\n\n- sofa: 1\n\n- rug: 1\n\n- wheelbarrow: 1\n\n- remote control: 1\n\n- wine glass: 1\n\n- bottle: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20864.2, "ram_available_mb": 41976.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20864.2, "ram_available_mb": 41976.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.929}, "power_stats": {"power_gpu_soc_mean_watts": 20.111, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 8.616, "gpu_utilization_percent_mean": 71.929, "power_watts_avg": 20.111, "energy_joules_est": 198.61, "duration_seconds": 9.876, "sample_count": 84}, "timestamp": "2026-01-26T09:41:01.211112"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11660.781, "latencies_ms": [11660.781], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a young girl is standing on a white shaggy rug, holding a remote control and appears to be in motion, possibly playing a video game. Behind her, on the right side of the image, there is a couch with red and white patterned pillows. In the background, there are several people standing and interacting with each other, with one person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20864.2, "ram_available_mb": 41976.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20851.1, "ram_available_mb": 41989.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.59}, "power_stats": {"power_gpu_soc_mean_watts": 19.493, "power_cpu_cv_mean_watts": 1.877, "power_sys_5v0_mean_watts": 8.662, "gpu_utilization_percent_mean": 70.59, "power_watts_avg": 19.493, "energy_joules_est": 227.32, "duration_seconds": 11.661, "sample_count": 100}, "timestamp": "2026-01-26T09:41:14.926995"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9199.285, "latencies_ms": [9199.285], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "A young girl is energetically playing with a red ball in a living room, while a group of people watch her from the background. The room is furnished with a couch, a coffee table, and a rug, creating a cozy and lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20851.1, "ram_available_mb": 41989.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20861.9, "ram_available_mb": 41979.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.59}, "power_stats": {"power_gpu_soc_mean_watts": 20.396, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 71.59, "power_watts_avg": 20.396, "energy_joules_est": 187.64, "duration_seconds": 9.2, "sample_count": 78}, "timestamp": "2026-01-26T09:41:26.140070"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8572.799, "latencies_ms": [8572.799], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The room is well-lit with natural light coming from the windows, and the carpet is a soft, plush grey. The couch is adorned with red and white patterned pillows, adding a pop of color to the space.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20861.9, "ram_available_mb": 41979.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20889.0, "ram_available_mb": 41951.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.722}, "power_stats": {"power_gpu_soc_mean_watts": 20.565, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.666, "gpu_utilization_percent_mean": 72.722, "power_watts_avg": 20.565, "energy_joules_est": 176.31, "duration_seconds": 8.573, "sample_count": 72}, "timestamp": "2026-01-26T09:41:36.751117"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11579.119, "latencies_ms": [11579.119], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features two men shaking hands in a room, surrounded by a crowd of people. The men are dressed in business attire, with one wearing a suit and the other wearing a tie. The room appears to be a dining area, as there are multiple dining tables and chairs set up throughout the space.\n\nThere are several other people in the room, some", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 20889.0, "ram_available_mb": 41951.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20892.6, "ram_available_mb": 41948.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.85}, "power_stats": {"power_gpu_soc_mean_watts": 19.349, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 69.85, "power_watts_avg": 19.349, "energy_joules_est": 224.06, "duration_seconds": 11.58, "sample_count": 100}, "timestamp": "2026-01-26T09:41:50.378075"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10504.356, "latencies_ms": [10504.356], "images_per_second": 0.095, "prompt_tokens": 39, "response_tokens_est": 68, "n_tiles": 16, "output_text": "1. People: 12\n2. Tables: 4\n3. Wine glasses: 3\n4. Plates: 2\n5. Napkins: 2\n6. Menus: 1\n7. Glasses: 1\n8. Suits: 2", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20892.6, "ram_available_mb": 41948.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20902.9, "ram_available_mb": 41938.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.101}, "power_stats": {"power_gpu_soc_mean_watts": 19.944, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 71.101, "power_watts_avg": 19.944, "energy_joules_est": 209.51, "duration_seconds": 10.505, "sample_count": 89}, "timestamp": "2026-01-26T09:42:02.895976"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11603.953, "latencies_ms": [11603.953], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, two men are shaking hands, one wearing a patterned shirt and the other in a suit with a yellow tie. They are standing close to each other, indicating a personal interaction. In the background, there are multiple tables set up with white tablecloths, suggesting a banquet or formal event. The tables are arranged in rows, and there are several", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20902.9, "ram_available_mb": 41938.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21003.8, "ram_available_mb": 41837.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.071}, "power_stats": {"power_gpu_soc_mean_watts": 19.301, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 70.071, "power_watts_avg": 19.301, "energy_joules_est": 223.98, "duration_seconds": 11.605, "sample_count": 99}, "timestamp": "2026-01-26T09:42:16.512521"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5285.474, "latencies_ms": [5285.474], "images_per_second": 0.189, "prompt_tokens": 37, "response_tokens_est": 22, "n_tiles": 16, "output_text": "Two men are shaking hands in a formal event setting, with a group of people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21003.8, "ram_available_mb": 41837.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 21004.3, "ram_available_mb": 41836.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.778}, "power_stats": {"power_gpu_soc_mean_watts": 23.517, "power_cpu_cv_mean_watts": 1.183, "power_sys_5v0_mean_watts": 8.502, "gpu_utilization_percent_mean": 77.778, "power_watts_avg": 23.517, "energy_joules_est": 124.31, "duration_seconds": 5.286, "sample_count": 45}, "timestamp": "2026-01-26T09:42:23.825662"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8825.463, "latencies_ms": [8825.463], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows two men shaking hands in an indoor setting with warm lighting. The man on the left is wearing a patterned shirt with a blue collar, while the man on the right is dressed in a dark suit with a yellow tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20858.9, "ram_available_mb": 41981.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 20859.2, "ram_available_mb": 41981.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.72}, "power_stats": {"power_gpu_soc_mean_watts": 20.298, "power_cpu_cv_mean_watts": 1.767, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 71.72, "power_watts_avg": 20.298, "energy_joules_est": 179.15, "duration_seconds": 8.826, "sample_count": 75}, "timestamp": "2026-01-26T09:42:34.681839"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11497.847, "latencies_ms": [11497.847], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is captured in a moment of quiet contemplation. He is dressed in a crisp white shirt, which contrasts with his dark tie. His gaze is directed off to the side, suggesting he is lost in thought. The background is blurred, drawing focus to the man and his introspective state. The lighting is soft, casting a", "error": null, "sys_before": {"cpu_percent": 11.8, "ram_used_mb": 20859.6, "ram_available_mb": 41981.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 12.0, "ram_used_mb": 21006.7, "ram_available_mb": 41834.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.061}, "power_stats": {"power_gpu_soc_mean_watts": 19.382, "power_cpu_cv_mean_watts": 2.794, "power_sys_5v0_mean_watts": 8.741, "gpu_utilization_percent_mean": 70.061, "power_watts_avg": 19.382, "energy_joules_est": 222.86, "duration_seconds": 11.499, "sample_count": 99}, "timestamp": "2026-01-26T09:42:48.231406"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7659.764, "latencies_ms": [7659.764], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "person: 1, shirt: 1, tie: 1, collar: 1, button: 1, pocket: 1, cuff: 1, ear: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20863.3, "ram_available_mb": 41977.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 11.1, "ram_used_mb": 21012.9, "ram_available_mb": 41828.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.246}, "power_stats": {"power_gpu_soc_mean_watts": 21.541, "power_cpu_cv_mean_watts": 2.55, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 74.246, "power_watts_avg": 21.541, "energy_joules_est": 165.01, "duration_seconds": 7.66, "sample_count": 65}, "timestamp": "2026-01-26T09:42:57.949887"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11654.773, "latencies_ms": [11654.773], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The person in the image is wearing a white shirt and a striped tie. The shirt is buttoned up, and the tie is tied in a neat knot. The person's left hand is slightly raised, as if they are gesturing or making a point. The background is blurred, but it appears to be an indoor setting with some objects in the distance", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20941.2, "ram_available_mb": 41899.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 13.5, "ram_used_mb": 20980.2, "ram_available_mb": 41860.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.366}, "power_stats": {"power_gpu_soc_mean_watts": 19.423, "power_cpu_cv_mean_watts": 3.084, "power_sys_5v0_mean_watts": 8.741, "gpu_utilization_percent_mean": 70.366, "power_watts_avg": 19.423, "energy_joules_est": 226.38, "duration_seconds": 11.655, "sample_count": 101}, "timestamp": "2026-01-26T09:43:11.630983"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8583.511, "latencies_ms": [8583.511], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A person is standing in a dimly lit environment, wearing a white shirt and a striped tie. The background is blurred, but it appears to be an indoor setting with some objects that could be furniture or equipment.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20980.2, "ram_available_mb": 41860.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 20937.3, "ram_available_mb": 41903.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.63}, "power_stats": {"power_gpu_soc_mean_watts": 20.823, "power_cpu_cv_mean_watts": 2.232, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 73.63, "power_watts_avg": 20.823, "energy_joules_est": 178.75, "duration_seconds": 8.584, "sample_count": 73}, "timestamp": "2026-01-26T09:43:22.257942"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7643.244, "latencies_ms": [7643.244], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image features a person wearing a white shirt with a yellow collar and a dark striped tie. The lighting appears to be artificial, with a warm tone, possibly from indoor lighting.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20937.3, "ram_available_mb": 41903.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21008.1, "ram_available_mb": 41832.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.646}, "power_stats": {"power_gpu_soc_mean_watts": 21.073, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 73.646, "power_watts_avg": 21.073, "energy_joules_est": 161.08, "duration_seconds": 7.644, "sample_count": 65}, "timestamp": "2026-01-26T09:43:31.924810"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11584.234, "latencies_ms": [11584.234], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a cozy living room scene. Dominating the space is a blue and red plaid sofa, inviting and comfortable. To the left, a wooden chair with a red and white checkered pattern adds a touch of rustic charm. The centerpiece of the room is a wooden entertainment center, housing a television that stands ready for entertainment. Above the", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20946.4, "ram_available_mb": 41894.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21011.2, "ram_available_mb": 41829.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.752}, "power_stats": {"power_gpu_soc_mean_watts": 19.305, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 69.752, "power_watts_avg": 19.305, "energy_joules_est": 223.65, "duration_seconds": 11.585, "sample_count": 101}, "timestamp": "2026-01-26T09:43:45.564026"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7782.235, "latencies_ms": [7782.235], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "Chair: 1, Sofa: 1, Entertainment center: 1, Television: 1, Picture: 1, List: 1, Speaker: 1, Rug: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20906.8, "ram_available_mb": 41934.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20970.6, "ram_available_mb": 41870.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.894}, "power_stats": {"power_gpu_soc_mean_watts": 21.256, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.616, "gpu_utilization_percent_mean": 72.894, "power_watts_avg": 21.256, "energy_joules_est": 165.43, "duration_seconds": 7.783, "sample_count": 66}, "timestamp": "2026-01-26T09:43:55.402853"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11251.103, "latencies_ms": [11251.103], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground of the image, there is a red plaid chair positioned to the left, which is near the center of the image. Behind the chair, slightly to the right, is a wooden entertainment center with a television on top. In the background, towards the right side of the image, there is a blue and red plaid sofa.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20872.1, "ram_available_mb": 41968.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.716}, "power_stats": {"power_gpu_soc_mean_watts": 19.434, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 70.716, "power_watts_avg": 19.434, "energy_joules_est": 218.67, "duration_seconds": 11.252, "sample_count": 95}, "timestamp": "2026-01-26T09:44:08.673496"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9368.813, "latencies_ms": [9368.813], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image depicts a cozy living room setting with a red and blue plaid sofa, a red armchair, and a television on a wooden stand. A whiteboard with writing on it is mounted on the wall, and a framed picture is also visible.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20952.1, "ram_available_mb": 41888.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.937}, "power_stats": {"power_gpu_soc_mean_watts": 20.414, "power_cpu_cv_mean_watts": 1.718, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 71.937, "power_watts_avg": 20.414, "energy_joules_est": 191.27, "duration_seconds": 9.369, "sample_count": 79}, "timestamp": "2026-01-26T09:44:20.070009"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8472.466, "latencies_ms": [8472.466], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The room has a plaid patterned chair and ottoman, with a whiteboard and a black and white photo on the wall. The lighting in the room is dim, and the materials used for the furniture appear to be wood and fabric.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20865.3, "ram_available_mb": 41975.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20867.4, "ram_available_mb": 41973.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.125}, "power_stats": {"power_gpu_soc_mean_watts": 20.495, "power_cpu_cv_mean_watts": 1.707, "power_sys_5v0_mean_watts": 8.648, "gpu_utilization_percent_mean": 72.125, "power_watts_avg": 20.495, "energy_joules_est": 173.66, "duration_seconds": 8.473, "sample_count": 72}, "timestamp": "2026-01-26T09:44:30.567389"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12424.627, "latencies_ms": [12424.627], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a dynamic scene of a surfer riding a wave. The surfer, clad in a vibrant yellow shirt and black shorts, is skillfully maneuvering a white surfboard. The wave, a powerful force of nature, is breaking to the right, creating a spray of white foam that contrasts with the deep blue-green", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 20867.4, "ram_available_mb": 41973.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20868.9, "ram_available_mb": 41972.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.19}, "power_stats": {"power_gpu_soc_mean_watts": 21.565, "power_cpu_cv_mean_watts": 1.811, "power_sys_5v0_mean_watts": 8.915, "gpu_utilization_percent_mean": 72.19, "power_watts_avg": 21.565, "energy_joules_est": 267.95, "duration_seconds": 12.425, "sample_count": 105}, "timestamp": "2026-01-26T09:44:45.043270"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9830.333, "latencies_ms": [9830.333], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "water: 1\nsurfboard: 1\nyellow shirt: 1\nred shorts: 1\nblack gloves: 1\nwhite surfboard: 1\nsurfer: 1\nwave: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20868.9, "ram_available_mb": 41972.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20868.7, "ram_available_mb": 41972.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.458}, "power_stats": {"power_gpu_soc_mean_watts": 22.733, "power_cpu_cv_mean_watts": 1.533, "power_sys_5v0_mean_watts": 8.751, "gpu_utilization_percent_mean": 75.458, "power_watts_avg": 22.733, "energy_joules_est": 223.49, "duration_seconds": 9.831, "sample_count": 83}, "timestamp": "2026-01-26T09:44:56.914358"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12730.273, "latencies_ms": [12730.273], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is breaking to the right of the frame. The wave originates from the background and extends towards the left side of the image, creating a dynamic spatial relationship between the surfer and the water. The surfboard is near the bottom of the image, indicating the surfer is close to the water's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20868.7, "ram_available_mb": 41972.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20913.8, "ram_available_mb": 41927.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.944}, "power_stats": {"power_gpu_soc_mean_watts": 21.558, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.87, "gpu_utilization_percent_mean": 73.944, "power_watts_avg": 21.558, "energy_joules_est": 274.45, "duration_seconds": 12.731, "sample_count": 108}, "timestamp": "2026-01-26T09:45:11.666687"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7151.529, "latencies_ms": [7151.529], "images_per_second": 0.14, "prompt_tokens": 37, "response_tokens_est": 29, "n_tiles": 16, "output_text": "A person is surfing on a wave in the ocean. The surfer is wearing a yellow shirt and red shorts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20913.8, "ram_available_mb": 41927.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 21008.4, "ram_available_mb": 41832.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 80.767}, "power_stats": {"power_gpu_soc_mean_watts": 24.438, "power_cpu_cv_mean_watts": 1.194, "power_sys_5v0_mean_watts": 8.724, "gpu_utilization_percent_mean": 80.767, "power_watts_avg": 24.438, "energy_joules_est": 174.79, "duration_seconds": 7.152, "sample_count": 60}, "timestamp": "2026-01-26T09:45:20.867701"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9441.862, "latencies_ms": [9441.862], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The surfer is wearing a bright yellow shirt and red shorts, which stand out against the white foam of the wave. The lighting is natural and appears to be daylight, casting shadows on the water's surface.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20919.3, "ram_available_mb": 41921.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20860.9, "ram_available_mb": 41980.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.951}, "power_stats": {"power_gpu_soc_mean_watts": 22.589, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 8.876, "gpu_utilization_percent_mean": 75.951, "power_watts_avg": 22.589, "energy_joules_est": 213.3, "duration_seconds": 9.443, "sample_count": 81}, "timestamp": "2026-01-26T09:45:32.324479"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11598.989, "latencies_ms": [11598.989], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a black cat is sitting on a desk, its head tilted to the side as it gazes intently at a computer screen. The screen is displaying a webpage with a white background and black text. The cat's position suggests it is trying to read or understand the content on the screen.\n\nThe desk on which the cat is sitting", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20860.9, "ram_available_mb": 41980.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20865.4, "ram_available_mb": 41975.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.747}, "power_stats": {"power_gpu_soc_mean_watts": 19.348, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.668, "gpu_utilization_percent_mean": 70.747, "power_watts_avg": 19.348, "energy_joules_est": 224.43, "duration_seconds": 11.6, "sample_count": 99}, "timestamp": "2026-01-26T09:45:45.945356"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7425.245, "latencies_ms": [7425.245], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "computer: 2, monitor: 1, keyboard: 1, mouse: 1, cat: 1, calculator: 1, phone: 1, book: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20865.4, "ram_available_mb": 41975.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20854.2, "ram_available_mb": 41986.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.159}, "power_stats": {"power_gpu_soc_mean_watts": 21.475, "power_cpu_cv_mean_watts": 1.557, "power_sys_5v0_mean_watts": 8.613, "gpu_utilization_percent_mean": 74.159, "power_watts_avg": 21.475, "energy_joules_est": 159.47, "duration_seconds": 7.426, "sample_count": 63}, "timestamp": "2026-01-26T09:45:55.430464"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11615.392, "latencies_ms": [11615.392], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a cat sitting in front of a computer monitor, which is positioned slightly to the left of the center of the image. The cat is facing the monitor, and its body is angled towards it, suggesting it is looking at the screen. In the background, there is a keyboard to the left of the monitor and a telephone to the right, both of", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20854.2, "ram_available_mb": 41986.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20854.9, "ram_available_mb": 41986.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.465}, "power_stats": {"power_gpu_soc_mean_watts": 19.261, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.675, "gpu_utilization_percent_mean": 70.465, "power_watts_avg": 19.261, "energy_joules_est": 223.74, "duration_seconds": 11.616, "sample_count": 99}, "timestamp": "2026-01-26T09:46:09.090965"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6999.721, "latencies_ms": [6999.721], "images_per_second": 0.143, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A cat is sitting in front of a computer monitor, looking at the screen. The computer is placed on a desk with a keyboard and a mouse visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20854.9, "ram_available_mb": 41986.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20861.6, "ram_available_mb": 41979.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.339}, "power_stats": {"power_gpu_soc_mean_watts": 21.954, "power_cpu_cv_mean_watts": 1.486, "power_sys_5v0_mean_watts": 8.545, "gpu_utilization_percent_mean": 74.339, "power_watts_avg": 21.954, "energy_joules_est": 153.69, "duration_seconds": 7.0, "sample_count": 59}, "timestamp": "2026-01-26T09:46:18.123152"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7327.389, "latencies_ms": [7327.389], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image shows a cat with a predominantly grey coat, sitting in front of a computer screen. The lighting in the room is bright, illuminating the cat and the computer screen clearly.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20861.6, "ram_available_mb": 41979.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20855.7, "ram_available_mb": 41985.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.919}, "power_stats": {"power_gpu_soc_mean_watts": 21.288, "power_cpu_cv_mean_watts": 1.608, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 72.919, "power_watts_avg": 21.288, "energy_joules_est": 156.0, "duration_seconds": 7.328, "sample_count": 62}, "timestamp": "2026-01-26T09:46:27.476086"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11588.939, "latencies_ms": [11588.939], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people gathered around a little girl who is cutting a red ribbon. The girl is wearing a helmet and is the center of attention as she holds the scissors, ready to cut the ribbon. Several people are standing around her, watching the event unfold.\n\nIn the scene, there are a few bicycles visible, one on the left", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 20855.7, "ram_available_mb": 41985.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20898.6, "ram_available_mb": 41942.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.58}, "power_stats": {"power_gpu_soc_mean_watts": 19.335, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.639, "gpu_utilization_percent_mean": 70.58, "power_watts_avg": 19.335, "energy_joules_est": 224.08, "duration_seconds": 11.59, "sample_count": 100}, "timestamp": "2026-01-26T09:46:41.106077"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10179.17, "latencies_ms": [10179.17], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. People: 15\n2. Balloons: 1\n3. Ribbon: 1\n4. Helmet: 1\n5. Tie: 1\n6. Camera: 1\n7. Chair: 1\n8. Suitcase: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20898.6, "ram_available_mb": 41942.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20984.9, "ram_available_mb": 41856.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.716}, "power_stats": {"power_gpu_soc_mean_watts": 19.971, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 71.716, "power_watts_avg": 19.971, "energy_joules_est": 203.3, "duration_seconds": 10.18, "sample_count": 88}, "timestamp": "2026-01-26T09:46:53.328669"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11591.12, "latencies_ms": [11591.12], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a child wearing a blue helmet is being assisted by an adult to walk through a red ribbon, indicating a ceremonial event. Behind them, a group of people are gathered, with some standing closer to the ribbon and others further back, creating a sense of depth. The balloons are positioned in the middle ground, with one being", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20912.8, "ram_available_mb": 41928.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20909.4, "ram_available_mb": 41931.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.74}, "power_stats": {"power_gpu_soc_mean_watts": 19.313, "power_cpu_cv_mean_watts": 1.902, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 70.74, "power_watts_avg": 19.313, "energy_joules_est": 223.87, "duration_seconds": 11.592, "sample_count": 100}, "timestamp": "2026-01-26T09:47:06.966159"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9717.567, "latencies_ms": [9717.567], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 61, "n_tiles": 16, "output_text": "A group of people, including children, are gathered on a sidewalk in front of a building with a sign that reads \"Johnny's.\" They are participating in a ribbon-cutting ceremony, with some individuals holding a pair of scissors and a blue balloon.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20909.4, "ram_available_mb": 41931.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 9.2, "ram_used_mb": 20923.1, "ram_available_mb": 41917.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.735}, "power_stats": {"power_gpu_soc_mean_watts": 20.089, "power_cpu_cv_mean_watts": 2.055, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 71.735, "power_watts_avg": 20.089, "energy_joules_est": 195.23, "duration_seconds": 9.718, "sample_count": 83}, "timestamp": "2026-01-26T09:47:18.701850"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8034.73, "latencies_ms": [8034.73], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a group of people gathered outdoors on a sunny day, as indicated by the bright lighting and shadows cast on the ground. The weather appears to be clear and pleasant, suitable for an outdoor event.", "error": null, "sys_before": {"cpu_percent": 27.3, "ram_used_mb": 20861.6, "ram_available_mb": 41979.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 10.8, "ram_used_mb": 20992.8, "ram_available_mb": 41848.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.101}, "power_stats": {"power_gpu_soc_mean_watts": 20.77, "power_cpu_cv_mean_watts": 2.512, "power_sys_5v0_mean_watts": 8.758, "gpu_utilization_percent_mean": 73.101, "power_watts_avg": 20.77, "energy_joules_est": 166.9, "duration_seconds": 8.035, "sample_count": 69}, "timestamp": "2026-01-26T09:47:28.781640"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11600.56, "latencies_ms": [11600.56], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large, modern bus parked on the side of a street. The bus is predominantly white, with a pink and blue stripe running along its side. It is a Scania bus, as indicated by the text on the side. The bus is parked in a designated bus stop area, which is marked by a yellow line on the road.\n\nThere", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20872.6, "ram_available_mb": 41968.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 12.2, "ram_used_mb": 20874.6, "ram_available_mb": 41966.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.79}, "power_stats": {"power_gpu_soc_mean_watts": 19.321, "power_cpu_cv_mean_watts": 2.783, "power_sys_5v0_mean_watts": 8.725, "gpu_utilization_percent_mean": 70.79, "power_watts_avg": 19.321, "energy_joules_est": 224.15, "duration_seconds": 11.602, "sample_count": 100}, "timestamp": "2026-01-26T09:47:42.423402"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8150.645, "latencies_ms": [8150.645], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "Bus: 1\nBuilding: 2\nSign: 1\nWi-Fi symbol: 1\nNumber: 1\nLetters: 1\nSymbol: 1\nLetter: 1", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20874.6, "ram_available_mb": 41966.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 12.6, "ram_used_mb": 20887.2, "ram_available_mb": 41953.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.257}, "power_stats": {"power_gpu_soc_mean_watts": 21.028, "power_cpu_cv_mean_watts": 3.135, "power_sys_5v0_mean_watts": 8.716, "gpu_utilization_percent_mean": 73.257, "power_watts_avg": 21.028, "energy_joules_est": 171.4, "duration_seconds": 8.151, "sample_count": 70}, "timestamp": "2026-01-26T09:47:52.590804"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11350.173, "latencies_ms": [11350.173], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The bus is in the foreground of the image, positioned on the left side of the frame, and appears to be moving towards the right. It is parked in a designated bus stop area marked with yellow lines on the road. In the background, there are buildings and a pedestrian walking on the sidewalk, indicating that the bus is in an urban setting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20887.2, "ram_available_mb": 41953.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20888.3, "ram_available_mb": 41952.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.224}, "power_stats": {"power_gpu_soc_mean_watts": 19.38, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 70.224, "power_watts_avg": 19.38, "energy_joules_est": 219.98, "duration_seconds": 11.351, "sample_count": 98}, "timestamp": "2026-01-26T09:48:05.979133"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6400.583, "latencies_ms": [6400.583], "images_per_second": 0.156, "prompt_tokens": 37, "response_tokens_est": 32, "n_tiles": 16, "output_text": "A Scania bus with the sign \"First Group\" is parked on the side of a street. The bus offers free Wi-Fi on board.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20888.3, "ram_available_mb": 41952.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 20886.6, "ram_available_mb": 41954.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.093}, "power_stats": {"power_gpu_soc_mean_watts": 22.432, "power_cpu_cv_mean_watts": 1.416, "power_sys_5v0_mean_watts": 8.562, "gpu_utilization_percent_mean": 75.093, "power_watts_avg": 22.432, "energy_joules_est": 143.6, "duration_seconds": 6.401, "sample_count": 54}, "timestamp": "2026-01-26T09:48:14.420510"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9523.852, "latencies_ms": [9523.852], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The bus is predominantly white with pink and blue accents, and it is parked on a street with a cloudy sky overhead. The bus has a \"First Group\" sign on the front and a \"FREE WI-FI ON BOARD\" sign on the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20886.6, "ram_available_mb": 41954.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20877.9, "ram_available_mb": 41963.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.852}, "power_stats": {"power_gpu_soc_mean_watts": 19.957, "power_cpu_cv_mean_watts": 1.769, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 70.852, "power_watts_avg": 19.957, "energy_joules_est": 190.08, "duration_seconds": 9.525, "sample_count": 81}, "timestamp": "2026-01-26T09:48:25.987943"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11598.208, "latencies_ms": [11598.208], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the image, a man is sitting on the floor in front of a large, ornate mirror. He is wearing a green shirt and is holding a cell phone in his hands. The mirror reflects the room, showing a living room with a couch and a lamp. The man appears to be focused on his phone, possibly taking a photo or browsing the internet.", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 20877.9, "ram_available_mb": 41963.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20903.9, "ram_available_mb": 41937.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.374}, "power_stats": {"power_gpu_soc_mean_watts": 19.128, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 70.374, "power_watts_avg": 19.128, "energy_joules_est": 221.86, "duration_seconds": 11.599, "sample_count": 99}, "timestamp": "2026-01-26T09:48:39.637764"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8962.554, "latencies_ms": [8962.554], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Mirror: 1\n- Sofa: 1\n- Table: 1\n- Lamp: 1\n- Cell phone: 1\n- Chair: 1\n- Rug: 1\n- Floor: 1", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20903.9, "ram_available_mb": 41937.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20916.7, "ram_available_mb": 41924.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.934}, "power_stats": {"power_gpu_soc_mean_watts": 20.668, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 72.934, "power_watts_avg": 20.668, "energy_joules_est": 185.25, "duration_seconds": 8.963, "sample_count": 76}, "timestamp": "2026-01-26T09:48:50.621811"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11675.321, "latencies_ms": [11675.321], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is seated on the floor, positioned in the lower left quadrant of the image, with their reflection visible in the large, ornate mirror that occupies the central foreground. The mirror's frame extends towards the upper right corner of the image, reflecting the interior of the room. In the background, to the right of the mirror, there", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20916.7, "ram_available_mb": 41924.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20908.4, "ram_available_mb": 41932.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.444}, "power_stats": {"power_gpu_soc_mean_watts": 19.395, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.691, "gpu_utilization_percent_mean": 70.444, "power_watts_avg": 19.395, "energy_joules_est": 226.46, "duration_seconds": 11.676, "sample_count": 99}, "timestamp": "2026-01-26T09:49:04.312197"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7904.271, "latencies_ms": [7904.271], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A man is sitting on the floor in front of a large, ornate mirror, holding a smartphone in his hands. The room has a wooden floor, a green wall, and a red wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20908.4, "ram_available_mb": 41932.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20980.1, "ram_available_mb": 41860.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.088}, "power_stats": {"power_gpu_soc_mean_watts": 21.104, "power_cpu_cv_mean_watts": 1.613, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 74.088, "power_watts_avg": 21.104, "energy_joules_est": 166.83, "duration_seconds": 7.905, "sample_count": 68}, "timestamp": "2026-01-26T09:49:14.254637"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7871.736, "latencies_ms": [7871.736], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a person sitting on the floor in front of a large, ornate mirror. The room has wooden flooring and walls painted in a light green color, with a red accent wall visible in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20925.7, "ram_available_mb": 41915.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20965.6, "ram_available_mb": 41875.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.612}, "power_stats": {"power_gpu_soc_mean_watts": 20.988, "power_cpu_cv_mean_watts": 1.625, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 73.612, "power_watts_avg": 20.988, "energy_joules_est": 165.23, "duration_seconds": 7.873, "sample_count": 67}, "timestamp": "2026-01-26T09:49:24.145727"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11618.237, "latencies_ms": [11618.237], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of five men standing together, each holding a surfboard. They are posing for a picture, with one man taking a photo of the group using his cell phone. The surfboards are of various sizes and colors, adding a vibrant touch to the scene.\n\nIn the background, there is a refrigerator, which suggests that the group", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 20868.2, "ram_available_mb": 41972.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20864.8, "ram_available_mb": 41976.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.354}, "power_stats": {"power_gpu_soc_mean_watts": 19.329, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 70.354, "power_watts_avg": 19.329, "energy_joules_est": 224.58, "duration_seconds": 11.619, "sample_count": 99}, "timestamp": "2026-01-26T09:49:37.834872"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10530.552, "latencies_ms": [10530.552], "images_per_second": 0.095, "prompt_tokens": 39, "response_tokens_est": 68, "n_tiles": 16, "output_text": "1. Surfboard: 4\n2. Camera: 1\n3. Cell phone: 1\n4. Flag: 2\n5. Surfboard: 1\n6. Surfboard: 1\n7. Surfboard: 1\n8. Surfboard: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20864.8, "ram_available_mb": 41976.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20860.7, "ram_available_mb": 41980.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.3}, "power_stats": {"power_gpu_soc_mean_watts": 19.87, "power_cpu_cv_mean_watts": 1.797, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 71.3, "power_watts_avg": 19.87, "energy_joules_est": 209.25, "duration_seconds": 10.531, "sample_count": 90}, "timestamp": "2026-01-26T09:49:50.390545"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10430.793, "latencies_ms": [10430.793], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a person holding a yellow surfboard with a red and black design, standing near a group of people. The group of people is gathered around a wooden table, with some holding surfboards and others holding snacks. In the background, there is a wall with various decorations and a window.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20860.7, "ram_available_mb": 41980.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20860.7, "ram_available_mb": 41980.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.393}, "power_stats": {"power_gpu_soc_mean_watts": 19.719, "power_cpu_cv_mean_watts": 1.835, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 71.393, "power_watts_avg": 19.719, "energy_joules_est": 205.7, "duration_seconds": 10.432, "sample_count": 89}, "timestamp": "2026-01-26T09:50:02.874384"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7797.689, "latencies_ms": [7797.689], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A group of young men are gathered in a room, holding surfboards and taking a photo with a camera. The room appears to be a casual setting, possibly a hangout or a surf shop.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20860.7, "ram_available_mb": 41980.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20863.3, "ram_available_mb": 41977.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.591}, "power_stats": {"power_gpu_soc_mean_watts": 21.3, "power_cpu_cv_mean_watts": 1.577, "power_sys_5v0_mean_watts": 8.568, "gpu_utilization_percent_mean": 73.591, "power_watts_avg": 21.3, "energy_joules_est": 166.11, "duration_seconds": 7.798, "sample_count": 66}, "timestamp": "2026-01-26T09:50:12.724267"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8618.184, "latencies_ms": [8618.184], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a group of individuals indoors, with one person taking a photo of the others who are holding surfboards. The lighting appears to be artificial, and the walls are adorned with various surfboard designs and stickers.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20863.3, "ram_available_mb": 41977.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20852.1, "ram_available_mb": 41988.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.378}, "power_stats": {"power_gpu_soc_mean_watts": 20.294, "power_cpu_cv_mean_watts": 1.725, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 72.378, "power_watts_avg": 20.294, "energy_joules_est": 174.91, "duration_seconds": 8.619, "sample_count": 74}, "timestamp": "2026-01-26T09:50:23.379303"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12342.945, "latencies_ms": [12342.945], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large yellow passenger airplane parked on the tarmac at an airport. The airplane is a commercial jet, and it is positioned in the center of the scene, occupying a significant portion of the image. The tarmac appears to be a runway, with the airplane sitting on it.\n\nIn the background, there are several other air", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20852.1, "ram_available_mb": 41988.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20857.2, "ram_available_mb": 41983.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.505}, "power_stats": {"power_gpu_soc_mean_watts": 21.385, "power_cpu_cv_mean_watts": 1.803, "power_sys_5v0_mean_watts": 8.833, "gpu_utilization_percent_mean": 72.505, "power_watts_avg": 21.385, "energy_joules_est": 263.97, "duration_seconds": 12.344, "sample_count": 105}, "timestamp": "2026-01-26T09:50:37.751301"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8483.815, "latencies_ms": [8483.815], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "airplane: 1, cloud: numerous, runway: 1, engine: 1, wing: 1, tail: 1, door: numerous, runway marking: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20857.2, "ram_available_mb": 41983.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20848.6, "ram_available_mb": 41992.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.417}, "power_stats": {"power_gpu_soc_mean_watts": 22.986, "power_cpu_cv_mean_watts": 1.473, "power_sys_5v0_mean_watts": 8.746, "gpu_utilization_percent_mean": 77.417, "power_watts_avg": 22.986, "energy_joules_est": 195.02, "duration_seconds": 8.484, "sample_count": 72}, "timestamp": "2026-01-26T09:50:48.258717"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12172.018, "latencies_ms": [12172.018], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The airplane is positioned in the foreground of the image, appearing large and prominent. It is situated on the tarmac, with its nose pointing towards the left side of the frame. In the background, there are other airplanes and a landscape that appears to be a hilly area with some buildings, indicating that the airport is located in a valley or surrounded by hills.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20848.6, "ram_available_mb": 41992.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20857.4, "ram_available_mb": 41983.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.087}, "power_stats": {"power_gpu_soc_mean_watts": 21.442, "power_cpu_cv_mean_watts": 1.83, "power_sys_5v0_mean_watts": 8.893, "gpu_utilization_percent_mean": 72.087, "power_watts_avg": 21.442, "energy_joules_est": 261.01, "duration_seconds": 12.173, "sample_count": 103}, "timestamp": "2026-01-26T09:51:02.443534"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7238.588, "latencies_ms": [7238.588], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 32, "n_tiles": 16, "output_text": "A LOT Polish Airlines airplane is on the runway, ready for takeoff. The sky is partly cloudy, indicating good weather for flying.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20857.4, "ram_available_mb": 41983.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 20847.8, "ram_available_mb": 41993.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.639}, "power_stats": {"power_gpu_soc_mean_watts": 23.847, "power_cpu_cv_mean_watts": 1.299, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 78.639, "power_watts_avg": 23.847, "energy_joules_est": 172.63, "duration_seconds": 7.239, "sample_count": 61}, "timestamp": "2026-01-26T09:51:11.714883"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9183.312, "latencies_ms": [9183.312], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image features a large, yellow passenger airplane with the logo \"LOT\" on its side, indicating it is operated by Lot Polish Airlines. The aircraft is on a runway with a partly cloudy sky above, suggesting fair weather conditions for flying.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20847.8, "ram_available_mb": 41993.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20854.8, "ram_available_mb": 41986.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.692}, "power_stats": {"power_gpu_soc_mean_watts": 22.336, "power_cpu_cv_mean_watts": 1.637, "power_sys_5v0_mean_watts": 8.855, "gpu_utilization_percent_mean": 74.692, "power_watts_avg": 22.336, "energy_joules_est": 205.13, "duration_seconds": 9.184, "sample_count": 78}, "timestamp": "2026-01-26T09:51:22.950130"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11560.659, "latencies_ms": [11560.659], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a bathroom, bathed in the stark contrast of black and white. Dominating the scene is a toilet, its lid raised in an invitation to use. The seat of the toilet is adorned with a white towel, a common sight in many bathrooms, providing a soft barrier between the user and the porcelain", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 20854.8, "ram_available_mb": 41986.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20995.0, "ram_available_mb": 41845.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.394}, "power_stats": {"power_gpu_soc_mean_watts": 19.311, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 69.394, "power_watts_avg": 19.311, "energy_joules_est": 223.26, "duration_seconds": 11.561, "sample_count": 99}, "timestamp": "2026-01-26T09:51:36.540313"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7901.371, "latencies_ms": [7901.371], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "toilet: 1, flush handle: 1, towel: 1, wall: 1, floor: 1, person: 1, shoes: 1, door: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20855.4, "ram_available_mb": 41985.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20900.3, "ram_available_mb": 41940.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.925}, "power_stats": {"power_gpu_soc_mean_watts": 21.287, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.606, "gpu_utilization_percent_mean": 73.925, "power_watts_avg": 21.287, "energy_joules_est": 168.21, "duration_seconds": 7.902, "sample_count": 67}, "timestamp": "2026-01-26T09:51:46.482090"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10512.942, "latencies_ms": [10512.942], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The toilet is located in the foreground of the image, with its lid open and seat up. In the background, there is a wall-mounted hand dryer and a pair of pants hanging on the wall. The hand dryer is positioned to the right of the toilet when facing the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20900.3, "ram_available_mb": 41940.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 10.1, "ram_used_mb": 21017.9, "ram_available_mb": 41823.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.719}, "power_stats": {"power_gpu_soc_mean_watts": 19.747, "power_cpu_cv_mean_watts": 2.204, "power_sys_5v0_mean_watts": 8.688, "gpu_utilization_percent_mean": 70.719, "power_watts_avg": 19.747, "energy_joules_est": 207.61, "duration_seconds": 10.514, "sample_count": 89}, "timestamp": "2026-01-26T09:51:59.055100"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9449.825, "latencies_ms": [9449.825], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a close-up of a toilet with the seat cover partially up, revealing the toilet bowl. The toilet appears to be in a public restroom, as suggested by the presence of a hand dryer and a sign on the wall.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21017.9, "ram_available_mb": 41823.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 11.0, "ram_used_mb": 21024.6, "ram_available_mb": 41816.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.237}, "power_stats": {"power_gpu_soc_mean_watts": 20.573, "power_cpu_cv_mean_watts": 2.382, "power_sys_5v0_mean_watts": 8.684, "gpu_utilization_percent_mean": 73.237, "power_watts_avg": 20.573, "energy_joules_est": 194.42, "duration_seconds": 9.45, "sample_count": 80}, "timestamp": "2026-01-26T09:52:10.532500"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7555.574, "latencies_ms": [7555.574], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image is in black and white, featuring a toilet with a closed lid and a partially open lid. The lighting appears to be artificial, coming from above, casting shadows on the floor.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21024.8, "ram_available_mb": 41816.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 11.2, "ram_used_mb": 21034.0, "ram_available_mb": 41806.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.016}, "power_stats": {"power_gpu_soc_mean_watts": 21.108, "power_cpu_cv_mean_watts": 2.446, "power_sys_5v0_mean_watts": 8.691, "gpu_utilization_percent_mean": 74.016, "power_watts_avg": 21.108, "energy_joules_est": 159.5, "duration_seconds": 7.556, "sample_count": 63}, "timestamp": "2026-01-26T09:52:20.112110"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11536.731, "latencies_ms": [11536.731], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a skier is captured in the midst of a thrilling descent down a snowy mountain slope. The skier, clad in a blue jacket and black pants, is holding two ski poles, aiding in their navigation through the snow. The skier's helmet, a crucial piece of safety gear, is white and stands out against the", "error": null, "sys_before": {"cpu_percent": 16.0, "ram_used_mb": 21034.0, "ram_available_mb": 41806.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 12.9, "ram_used_mb": 20982.9, "ram_available_mb": 41858.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.62}, "power_stats": {"power_gpu_soc_mean_watts": 19.227, "power_cpu_cv_mean_watts": 3.034, "power_sys_5v0_mean_watts": 8.761, "gpu_utilization_percent_mean": 70.62, "power_watts_avg": 19.227, "energy_joules_est": 221.83, "duration_seconds": 11.537, "sample_count": 100}, "timestamp": "2026-01-26T09:52:33.682998"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8734.327, "latencies_ms": [8734.327], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "tree: multiple, ski: 2, skier: 1, snow: extensive coverage, snowflakes: numerous, ski tracks: visible, helmet: 1, gloves: 1, goggles: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20982.9, "ram_available_mb": 41858.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20914.3, "ram_available_mb": 41926.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.351}, "power_stats": {"power_gpu_soc_mean_watts": 20.783, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 72.351, "power_watts_avg": 20.783, "energy_joules_est": 181.54, "duration_seconds": 8.735, "sample_count": 74}, "timestamp": "2026-01-26T09:52:44.462348"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10055.828, "latencies_ms": [10055.828], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The skier is positioned in the foreground on the left side of the image, moving towards the right. The snow-covered trees form the background, appearing denser and more uniform in the distance. The skier is closer to the camera than the trees, creating a sense of depth in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20914.3, "ram_available_mb": 41926.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21005.7, "ram_available_mb": 41835.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.105}, "power_stats": {"power_gpu_soc_mean_watts": 19.945, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 72.105, "power_watts_avg": 19.945, "energy_joules_est": 200.58, "duration_seconds": 10.056, "sample_count": 86}, "timestamp": "2026-01-26T09:52:56.559271"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8250.075, "latencies_ms": [8250.075], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A person is skiing down a snowy slope with trees covered in snow in the background. The skier is wearing a blue jacket, black pants, and a white helmet, and is holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21005.7, "ram_available_mb": 41835.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20920.7, "ram_available_mb": 41920.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.514}, "power_stats": {"power_gpu_soc_mean_watts": 20.937, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 72.514, "power_watts_avg": 20.937, "energy_joules_est": 172.74, "duration_seconds": 8.251, "sample_count": 70}, "timestamp": "2026-01-26T09:53:06.823170"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7504.603, "latencies_ms": [7504.603], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The skier is wearing a blue jacket and black pants, and the snow is a bright white color. The trees in the background are covered in snow, indicating a cold and snowy weather.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20920.7, "ram_available_mb": 41920.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20909.3, "ram_available_mb": 41931.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.279, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 74.0, "power_watts_avg": 21.279, "energy_joules_est": 159.7, "duration_seconds": 7.505, "sample_count": 63}, "timestamp": "2026-01-26T09:53:16.361045"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11625.024, "latencies_ms": [11625.024], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment from a tennis match. A player in a yellow shirt and black shorts is in the midst of a backhand swing, holding a tennis racket. The court is blue, and there are white lines marking the boundaries. In the background, there are spectators and advertisements for Kia and Garnier. The player's posture suggests", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20909.3, "ram_available_mb": 41931.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20997.5, "ram_available_mb": 41843.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.01}, "power_stats": {"power_gpu_soc_mean_watts": 19.344, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.673, "gpu_utilization_percent_mean": 70.01, "power_watts_avg": 19.344, "energy_joules_est": 224.89, "duration_seconds": 11.626, "sample_count": 99}, "timestamp": "2026-01-26T09:53:30.039745"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7660.808, "latencies_ms": [7660.808], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "player: 1, racket: 1, ball: 0, chair: 2, audience: 1, advertisement: 4, court: 1, logo: 3", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20884.9, "ram_available_mb": 41956.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20942.9, "ram_available_mb": 41898.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.969}, "power_stats": {"power_gpu_soc_mean_watts": 21.415, "power_cpu_cv_mean_watts": 1.551, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 73.969, "power_watts_avg": 21.415, "energy_joules_est": 164.07, "duration_seconds": 7.662, "sample_count": 64}, "timestamp": "2026-01-26T09:53:39.727925"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10796.482, "latencies_ms": [10796.482], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground, a tennis player is positioned near the baseline on the left side of the court, preparing to hit the ball. In the background, there are spectators seated on the right side, watching the game. The court is surrounded by advertisement boards on the walls, indicating the professional nature of the event.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20880.0, "ram_available_mb": 41960.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20934.9, "ram_available_mb": 41906.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.785}, "power_stats": {"power_gpu_soc_mean_watts": 19.399, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 70.785, "power_watts_avg": 19.399, "energy_joules_est": 209.45, "duration_seconds": 10.797, "sample_count": 93}, "timestamp": "2026-01-26T09:53:52.561621"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9874.933, "latencies_ms": [9874.933], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image captures a moment from a tennis match, with a player in a yellow shirt and black shorts in the midst of a backhand swing. The court is surrounded by a crowd of spectators, and there are advertisements for Kia and Garnier visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20934.9, "ram_available_mb": 41906.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20966.8, "ram_available_mb": 41874.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.429}, "power_stats": {"power_gpu_soc_mean_watts": 20.182, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 70.429, "power_watts_avg": 20.182, "energy_joules_est": 199.31, "duration_seconds": 9.876, "sample_count": 84}, "timestamp": "2026-01-26T09:54:04.453695"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8495.338, "latencies_ms": [8495.338], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows an indoor tennis court with a blue surface and white boundary lines. The lighting appears to be artificial, as there are no visible windows or natural light sources, and the shadows on the court are consistent with overhead artificial lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20859.3, "ram_available_mb": 41981.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20850.2, "ram_available_mb": 41990.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.764}, "power_stats": {"power_gpu_soc_mean_watts": 20.348, "power_cpu_cv_mean_watts": 1.712, "power_sys_5v0_mean_watts": 8.619, "gpu_utilization_percent_mean": 71.764, "power_watts_avg": 20.348, "energy_joules_est": 172.87, "duration_seconds": 8.496, "sample_count": 72}, "timestamp": "2026-01-26T09:54:15.002467"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12371.811, "latencies_ms": [12371.811], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with two bowls of food placed on it. One bowl contains a meat-based dish, while the other bowl is filled with sliced peaches. The table is set with a black and white striped tablecloth, and there is a glass of water visible in the background. The arrangement of the food and the table setting create a", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20850.2, "ram_available_mb": 41990.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20852.7, "ram_available_mb": 41988.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.981}, "power_stats": {"power_gpu_soc_mean_watts": 21.376, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.824, "gpu_utilization_percent_mean": 72.981, "power_watts_avg": 21.376, "energy_joules_est": 264.47, "duration_seconds": 12.372, "sample_count": 105}, "timestamp": "2026-01-26T09:54:29.425790"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9077.149, "latencies_ms": [9077.149], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "bowl of food: 1, bowl of fruit: 1, fork: 1, knife: 1, glass: 1, napkin: 1, plate: 1, table: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20852.7, "ram_available_mb": 41988.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20843.9, "ram_available_mb": 41997.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.077}, "power_stats": {"power_gpu_soc_mean_watts": 22.647, "power_cpu_cv_mean_watts": 1.539, "power_sys_5v0_mean_watts": 8.717, "gpu_utilization_percent_mean": 76.077, "power_watts_avg": 22.647, "energy_joules_est": 205.58, "duration_seconds": 9.078, "sample_count": 78}, "timestamp": "2026-01-26T09:54:40.531606"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10918.399, "latencies_ms": [10918.399], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, there is a bowl of sliced peaches placed near the edge of a black plate. Behind it, slightly to the left, is a smaller bowl containing what appears to be a meat-based dish. The larger bowl is closer to the viewer than the smaller bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20843.9, "ram_available_mb": 41997.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20852.7, "ram_available_mb": 41988.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.323}, "power_stats": {"power_gpu_soc_mean_watts": 21.71, "power_cpu_cv_mean_watts": 1.764, "power_sys_5v0_mean_watts": 8.869, "gpu_utilization_percent_mean": 73.323, "power_watts_avg": 21.71, "energy_joules_est": 237.05, "duration_seconds": 10.919, "sample_count": 93}, "timestamp": "2026-01-26T09:54:53.501804"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11272.029, "latencies_ms": [11272.029], "images_per_second": 0.089, "prompt_tokens": 37, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image shows a meal setting with a bowl of sliced peaches and a bowl of what appears to be a meat-based dish, possibly chili, placed on a table. The table has a black surface, and there is a glass of water and a folded napkin on the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20852.7, "ram_available_mb": 41988.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20851.6, "ram_available_mb": 41989.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.825, "power_cpu_cv_mean_watts": 1.708, "power_sys_5v0_mean_watts": 8.779, "gpu_utilization_percent_mean": 74.0, "power_watts_avg": 21.825, "energy_joules_est": 246.03, "duration_seconds": 11.273, "sample_count": 97}, "timestamp": "2026-01-26T09:55:06.807879"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11243.146, "latencies_ms": [11243.146], "images_per_second": 0.089, "prompt_tokens": 36, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The image shows a meal with a dark, richly colored meat dish in a metal bowl and a lighter, pale yellow dessert in a smaller metal bowl. The lighting is warm and artificial, coming from a source not visible in the image, casting a soft glow on the food and the tablecloth beneath.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20851.6, "ram_available_mb": 41989.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20853.9, "ram_available_mb": 41986.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.562}, "power_stats": {"power_gpu_soc_mean_watts": 21.595, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 8.868, "gpu_utilization_percent_mean": 72.562, "power_watts_avg": 21.595, "energy_joules_est": 242.81, "duration_seconds": 11.244, "sample_count": 96}, "timestamp": "2026-01-26T09:55:20.063311"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11605.103, "latencies_ms": [11605.103], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of sheep standing together in a grassy area near a brick building. There are at least five sheep visible in the scene, with some of them standing close to each other, while others are a bit further apart. The sheep appear to be of various sizes, and they are all looking in different directions, possibly observing their surroundings or interacting with each other.", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20853.9, "ram_available_mb": 41986.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20849.8, "ram_available_mb": 41991.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.404}, "power_stats": {"power_gpu_soc_mean_watts": 19.354, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.676, "gpu_utilization_percent_mean": 70.404, "power_watts_avg": 19.354, "energy_joules_est": 224.62, "duration_seconds": 11.606, "sample_count": 99}, "timestamp": "2026-01-26T09:55:33.692845"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10193.913, "latencies_ms": [10193.913], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. Sheep: 5\n2. Grass: 1\n3. Brick wall: 1\n4. Tree: 1\n5. Fence: 1\n6. Barn: 1\n7. Shed: 1\n8. Ground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20849.8, "ram_available_mb": 41991.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20855.9, "ram_available_mb": 41985.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.529}, "power_stats": {"power_gpu_soc_mean_watts": 19.977, "power_cpu_cv_mean_watts": 1.762, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 71.529, "power_watts_avg": 19.977, "energy_joules_est": 203.66, "duration_seconds": 10.195, "sample_count": 87}, "timestamp": "2026-01-26T09:55:45.900249"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10900.272, "latencies_ms": [10900.272], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, there are three sheep standing close together, with one on the left, one in the middle, and one on the right. In the background, there is a brick wall and a fence, with a tree and a shed partially visible. The sheep in the foreground are near the camera, while the background elements are farther away.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 20855.9, "ram_available_mb": 41985.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20853.6, "ram_available_mb": 41987.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.527}, "power_stats": {"power_gpu_soc_mean_watts": 19.537, "power_cpu_cv_mean_watts": 1.846, "power_sys_5v0_mean_watts": 8.684, "gpu_utilization_percent_mean": 70.527, "power_watts_avg": 19.537, "energy_joules_est": 212.97, "duration_seconds": 10.901, "sample_count": 93}, "timestamp": "2026-01-26T09:55:58.858967"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8351.515, "latencies_ms": [8351.515], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A group of sheep are standing in a grassy area near a brick building, possibly a barn or farmhouse. The sheep appear to be grazing or resting, and the setting seems to be a rural or agricultural area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20853.6, "ram_available_mb": 41987.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20859.5, "ram_available_mb": 41981.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.718}, "power_stats": {"power_gpu_soc_mean_watts": 20.888, "power_cpu_cv_mean_watts": 1.64, "power_sys_5v0_mean_watts": 8.609, "gpu_utilization_percent_mean": 72.718, "power_watts_avg": 20.888, "energy_joules_est": 174.46, "duration_seconds": 8.352, "sample_count": 71}, "timestamp": "2026-01-26T09:56:09.228892"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7591.795, "latencies_ms": [7591.795], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image features a group of sheep with thick, curly wool in various shades of brown and beige. The lighting is bright and natural, suggesting the photo was taken on a sunny day.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20859.5, "ram_available_mb": 41981.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20846.8, "ram_available_mb": 41994.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.375}, "power_stats": {"power_gpu_soc_mean_watts": 21.006, "power_cpu_cv_mean_watts": 1.626, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 72.375, "power_watts_avg": 21.006, "energy_joules_est": 159.49, "duration_seconds": 7.592, "sample_count": 64}, "timestamp": "2026-01-26T09:56:18.858306"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10571.856, "latencies_ms": [10571.856], "images_per_second": 0.095, "prompt_tokens": 24, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image features a red apple sitting in the center, surrounded by a bunch of green bananas. The bananas are arranged in a circular pattern around the apple, creating a visually appealing composition. The background consists of a blue and white pattern, which adds a contrasting color to the overall scene.", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 20846.8, "ram_available_mb": 41994.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21000.1, "ram_available_mb": 41840.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.967}, "power_stats": {"power_gpu_soc_mean_watts": 22.028, "power_cpu_cv_mean_watts": 1.716, "power_sys_5v0_mean_watts": 8.913, "gpu_utilization_percent_mean": 73.967, "power_watts_avg": 22.028, "energy_joules_est": 232.9, "duration_seconds": 10.573, "sample_count": 90}, "timestamp": "2026-01-26T09:56:31.467236"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5032.947, "latencies_ms": [5032.947], "images_per_second": 0.199, "prompt_tokens": 39, "response_tokens_est": 11, "n_tiles": 16, "output_text": "banana: 5, apple: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20938.2, "ram_available_mb": 41902.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.0, "ram_used_mb": 21025.1, "ram_available_mb": 41815.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 87.071}, "power_stats": {"power_gpu_soc_mean_watts": 26.722, "power_cpu_cv_mean_watts": 1.267, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 87.071, "power_watts_avg": 26.722, "energy_joules_est": 134.51, "duration_seconds": 5.034, "sample_count": 42}, "timestamp": "2026-01-26T09:56:38.520768"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12658.394, "latencies_ms": [12658.394], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a red apple centrally positioned with a bunch of green bananas leaning against it from the left side. The bananas are closer to the viewer than the apple, creating a sense of depth. The background features a patterned blue and white surface that extends to the edges of the image, providing a contrasting backdrop to the fruits.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20926.3, "ram_available_mb": 41914.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 10.1, "ram_used_mb": 20971.5, "ram_available_mb": 41869.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.204}, "power_stats": {"power_gpu_soc_mean_watts": 21.808, "power_cpu_cv_mean_watts": 2.287, "power_sys_5v0_mean_watts": 8.928, "gpu_utilization_percent_mean": 74.204, "power_watts_avg": 21.808, "energy_joules_est": 276.07, "duration_seconds": 12.659, "sample_count": 108}, "timestamp": "2026-01-26T09:56:53.235841"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10953.427, "latencies_ms": [10953.427], "images_per_second": 0.091, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image shows a group of bananas arranged in a circle around a red apple, all set against a blue textured background. The arrangement creates a visually appealing pattern, with the bananas' yellow color contrasting with the apple's red hue and the background's cool tone.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20971.5, "ram_available_mb": 41869.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 20970.3, "ram_available_mb": 41870.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.87}, "power_stats": {"power_gpu_soc_mean_watts": 22.381, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.826, "gpu_utilization_percent_mean": 74.87, "power_watts_avg": 22.381, "energy_joules_est": 245.16, "duration_seconds": 10.954, "sample_count": 92}, "timestamp": "2026-01-26T09:57:06.202230"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12313.259, "latencies_ms": [12313.259], "images_per_second": 0.081, "prompt_tokens": 36, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The image features a vibrant red apple centrally placed among several green bananas, all set against a textured blue background that resembles a frosted glass surface. The lighting in the image highlights the glossy texture of the apple and the smooth skin of the bananas, creating a contrast with the intricate patterns on the blue background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20970.3, "ram_available_mb": 41870.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.5, "ram_used_mb": 20909.8, "ram_available_mb": 41931.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.66}, "power_stats": {"power_gpu_soc_mean_watts": 21.75, "power_cpu_cv_mean_watts": 1.997, "power_sys_5v0_mean_watts": 8.907, "gpu_utilization_percent_mean": 73.66, "power_watts_avg": 21.75, "energy_joules_est": 267.83, "duration_seconds": 12.314, "sample_count": 106}, "timestamp": "2026-01-26T09:57:20.564565"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12361.291, "latencies_ms": [12361.291], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a blue and white train traveling down the tracks, with several people visible inside the train. The train is moving through a city, and there are multiple cars parked along the side of the road. \n\nIn addition to the train, there are two trucks visible in the scene, one located near the left side of the image and the other further to the left", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 20909.8, "ram_available_mb": 41931.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21003.2, "ram_available_mb": 41837.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.387, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.852, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 21.387, "energy_joules_est": 264.39, "duration_seconds": 12.362, "sample_count": 105}, "timestamp": "2026-01-26T09:57:34.958308"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8755.433, "latencies_ms": [8755.433], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "train: 1\ntruck: 1\nwindow: 12\nseat: 4\npassenger: 10\ndoor: 4\ntrack: 2\nplatform: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20866.2, "ram_available_mb": 41974.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20873.7, "ram_available_mb": 41967.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.662}, "power_stats": {"power_gpu_soc_mean_watts": 22.832, "power_cpu_cv_mean_watts": 1.487, "power_sys_5v0_mean_watts": 8.686, "gpu_utilization_percent_mean": 76.662, "power_watts_avg": 22.832, "energy_joules_est": 199.92, "duration_seconds": 8.756, "sample_count": 74}, "timestamp": "2026-01-26T09:57:45.747755"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9203.796, "latencies_ms": [9203.796], "images_per_second": 0.109, "prompt_tokens": 44, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The train is in the foreground of the image, positioned on the left side, and appears to be moving towards the right. There is a clear sky in the background, and a building can be seen further back on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20873.7, "ram_available_mb": 41967.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20878.1, "ram_available_mb": 41962.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.615}, "power_stats": {"power_gpu_soc_mean_watts": 22.41, "power_cpu_cv_mean_watts": 1.626, "power_sys_5v0_mean_watts": 8.868, "gpu_utilization_percent_mean": 73.615, "power_watts_avg": 22.41, "energy_joules_est": 206.27, "duration_seconds": 9.204, "sample_count": 78}, "timestamp": "2026-01-26T09:57:56.977058"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7595.038, "latencies_ms": [7595.038], "images_per_second": 0.132, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A blue and white train with the number 2 on the front is on a track with trees in the background. There are people visible through the windows of the train.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20878.1, "ram_available_mb": 41962.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 20873.4, "ram_available_mb": 41967.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.797}, "power_stats": {"power_gpu_soc_mean_watts": 23.586, "power_cpu_cv_mean_watts": 1.338, "power_sys_5v0_mean_watts": 8.715, "gpu_utilization_percent_mean": 78.797, "power_watts_avg": 23.586, "energy_joules_est": 179.15, "duration_seconds": 7.596, "sample_count": 64}, "timestamp": "2026-01-26T09:58:06.592558"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6673.914, "latencies_ms": [6673.914], "images_per_second": 0.15, "prompt_tokens": 36, "response_tokens_est": 31, "n_tiles": 16, "output_text": "The train is predominantly blue and white with a red interior. It is a clear day with shadows cast on the ground indicating sunlight.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20873.4, "ram_available_mb": 41967.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20863.6, "ram_available_mb": 41977.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.679}, "power_stats": {"power_gpu_soc_mean_watts": 23.881, "power_cpu_cv_mean_watts": 1.322, "power_sys_5v0_mean_watts": 8.848, "gpu_utilization_percent_mean": 78.679, "power_watts_avg": 23.881, "energy_joules_est": 159.39, "duration_seconds": 6.674, "sample_count": 56}, "timestamp": "2026-01-26T09:58:15.330473"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11618.889, "latencies_ms": [11618.889], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and well-organized bathroom. Dominating the scene is a white bathtub, its interior glowing with a beige hue. A red shower curtain, adorned with a floral pattern, hangs gracefully from the bathtub, adding a touch of warmth to the space. \n\nAdjacent", "error": null, "sys_before": {"cpu_percent": 11.5, "ram_used_mb": 20863.6, "ram_available_mb": 41977.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20862.8, "ram_available_mb": 41978.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.255}, "power_stats": {"power_gpu_soc_mean_watts": 19.344, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.692, "gpu_utilization_percent_mean": 70.255, "power_watts_avg": 19.344, "energy_joules_est": 224.77, "duration_seconds": 11.62, "sample_count": 98}, "timestamp": "2026-01-26T09:58:28.976501"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8010.277, "latencies_ms": [8010.277], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "mirror: 2, sink: 2, shower curtain: 1, towel: 2, rug: 1, cabinet: 2, door: 1, light fixture: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20862.8, "ram_available_mb": 41978.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20859.6, "ram_available_mb": 41981.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.662}, "power_stats": {"power_gpu_soc_mean_watts": 21.104, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.61, "gpu_utilization_percent_mean": 72.662, "power_watts_avg": 21.104, "energy_joules_est": 169.06, "duration_seconds": 8.011, "sample_count": 68}, "timestamp": "2026-01-26T09:58:39.041791"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10573.585, "latencies_ms": [10573.585], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The shower area is located on the left side of the image, adjacent to the bathroom sink which is on the right. The red bath mat is placed in the foreground on the floor, leading the eye towards the shower area. The mirror above the sink is positioned in the background, reflecting the room's interior.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20859.6, "ram_available_mb": 41981.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20864.4, "ram_available_mb": 41976.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.478}, "power_stats": {"power_gpu_soc_mean_watts": 19.582, "power_cpu_cv_mean_watts": 1.85, "power_sys_5v0_mean_watts": 8.685, "gpu_utilization_percent_mean": 70.478, "power_watts_avg": 19.582, "energy_joules_est": 207.06, "duration_seconds": 10.574, "sample_count": 90}, "timestamp": "2026-01-26T09:58:51.634986"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8578.219, "latencies_ms": [8578.219], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image depicts a well-lit, clean bathroom with a large mirror, a bathtub with a shower curtain, and a red rug on the floor. The bathroom has wooden cabinets and a white countertop.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20864.4, "ram_available_mb": 41976.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20869.1, "ram_available_mb": 41971.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.027}, "power_stats": {"power_gpu_soc_mean_watts": 20.782, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 73.027, "power_watts_avg": 20.782, "energy_joules_est": 178.28, "duration_seconds": 8.579, "sample_count": 73}, "timestamp": "2026-01-26T09:59:02.226542"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7009.034, "latencies_ms": [7009.034], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The bathroom features a beige color scheme with a red patterned rug on the floor. There are two light fixtures on the wall, both emitting a warm glow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20869.1, "ram_available_mb": 41971.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20857.7, "ram_available_mb": 41983.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.627}, "power_stats": {"power_gpu_soc_mean_watts": 21.342, "power_cpu_cv_mean_watts": 1.567, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 74.627, "power_watts_avg": 21.342, "energy_joules_est": 149.6, "duration_seconds": 7.01, "sample_count": 59}, "timestamp": "2026-01-26T09:59:11.247669"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11595.262, "latencies_ms": [11595.262], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a dynamic scene of a surfer in action. The surfer, clad in a black wetsuit, is skillfully riding a wave. The wave, a powerful force of nature, is white and frothy, indicating its strength and the speed at which it's moving. The surfer is crouched on the surfboard, arms outstretch", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20857.7, "ram_available_mb": 41983.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20853.9, "ram_available_mb": 41987.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.333}, "power_stats": {"power_gpu_soc_mean_watts": 19.351, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 70.333, "power_watts_avg": 19.351, "energy_joules_est": 224.39, "duration_seconds": 11.596, "sample_count": 99}, "timestamp": "2026-01-26T09:59:24.871954"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7900.616, "latencies_ms": [7900.616], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "water: numerous\nsurfboard: 1\nsurfer: 1\nwave: 1\nspray: numerous\nsea: entire background\nblack and white: entire image\ntext: 3", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 20853.9, "ram_available_mb": 41987.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20861.9, "ram_available_mb": 41979.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.104}, "power_stats": {"power_gpu_soc_mean_watts": 21.227, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 73.104, "power_watts_avg": 21.227, "energy_joules_est": 167.72, "duration_seconds": 7.901, "sample_count": 67}, "timestamp": "2026-01-26T09:59:34.821913"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11595.659, "latencies_ms": [11595.659], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is breaking to the right of the frame. The wave originates from the background and extends towards the left side of the image, creating a dynamic contrast between the surfer's movement and the stationary water. The text 'STB' is placed in the upper right corner, far from the action, suggesting it", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20861.9, "ram_available_mb": 41979.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20861.1, "ram_available_mb": 41979.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.469}, "power_stats": {"power_gpu_soc_mean_watts": 19.348, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.688, "gpu_utilization_percent_mean": 70.469, "power_watts_avg": 19.348, "energy_joules_est": 224.36, "duration_seconds": 11.596, "sample_count": 98}, "timestamp": "2026-01-26T09:59:48.432322"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7345.596, "latencies_ms": [7345.596], "images_per_second": 0.136, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A surfer is captured in mid-air, performing a jump above a wave in the ocean. The image is in black and white, emphasizing the action and the texture of the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20861.1, "ram_available_mb": 41979.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20849.5, "ram_available_mb": 41991.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.661}, "power_stats": {"power_gpu_soc_mean_watts": 21.504, "power_cpu_cv_mean_watts": 1.523, "power_sys_5v0_mean_watts": 8.569, "gpu_utilization_percent_mean": 74.661, "power_watts_avg": 21.504, "energy_joules_est": 157.97, "duration_seconds": 7.346, "sample_count": 62}, "timestamp": "2026-01-26T09:59:57.798976"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7003.616, "latencies_ms": [7003.616], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image is in black and white, featuring a surfer riding a wave. The wave is large and powerful, with water splashing around the surfer as they navigate it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20849.5, "ram_available_mb": 41991.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20855.9, "ram_available_mb": 41985.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.034}, "power_stats": {"power_gpu_soc_mean_watts": 21.477, "power_cpu_cv_mean_watts": 1.533, "power_sys_5v0_mean_watts": 8.606, "gpu_utilization_percent_mean": 73.034, "power_watts_avg": 21.477, "energy_joules_est": 150.43, "duration_seconds": 7.004, "sample_count": 59}, "timestamp": "2026-01-26T10:00:06.859492"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11619.929, "latencies_ms": [11619.929], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a baseball game in progress, with a batter standing at home plate, holding a baseball bat and waiting for the pitch. The catcher is crouched behind the batter, wearing a baseball glove, and the umpire is positioned behind the catcher, observing the game. \n\nThere are several other people in the scene, including players and spect", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20855.9, "ram_available_mb": 41985.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20865.3, "ram_available_mb": 41975.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.566}, "power_stats": {"power_gpu_soc_mean_watts": 19.306, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.669, "gpu_utilization_percent_mean": 70.566, "power_watts_avg": 19.306, "energy_joules_est": 224.35, "duration_seconds": 11.621, "sample_count": 99}, "timestamp": "2026-01-26T10:00:20.521961"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8243.07, "latencies_ms": [8243.07], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "player: 1, catcher: 1, umpire: 1, baseball glove: 1, bat: 1, jersey: 1, bench: 1, audience: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20865.3, "ram_available_mb": 41975.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20846.2, "ram_available_mb": 41994.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.986}, "power_stats": {"power_gpu_soc_mean_watts": 20.946, "power_cpu_cv_mean_watts": 1.641, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 72.986, "power_watts_avg": 20.946, "energy_joules_est": 172.67, "duration_seconds": 8.244, "sample_count": 70}, "timestamp": "2026-01-26T10:00:30.820211"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11588.912, "latencies_ms": [11588.912], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a baseball player in a white uniform with the number 10 is standing at home plate, ready to swing. Behind him, the catcher and umpire are in position, with the catcher crouched behind home plate and the umpire standing to the left of the catcher. In the background, there are spectators seated in the stands", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20846.2, "ram_available_mb": 41994.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20843.2, "ram_available_mb": 41997.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.061}, "power_stats": {"power_gpu_soc_mean_watts": 19.302, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.688, "gpu_utilization_percent_mean": 70.061, "power_watts_avg": 19.302, "energy_joules_est": 223.7, "duration_seconds": 11.59, "sample_count": 99}, "timestamp": "2026-01-26T10:00:44.450922"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8913.08, "latencies_ms": [8913.08], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image captures a moment during a baseball game where a batter is at the plate, ready to swing. The catcher and umpire are in position behind the batter, with the catcher crouched and the umpire standing upright.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20843.2, "ram_available_mb": 41997.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20857.1, "ram_available_mb": 41983.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.066}, "power_stats": {"power_gpu_soc_mean_watts": 20.55, "power_cpu_cv_mean_watts": 1.695, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 73.066, "power_watts_avg": 20.55, "energy_joules_est": 183.18, "duration_seconds": 8.914, "sample_count": 76}, "timestamp": "2026-01-26T10:00:55.378566"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9973.222, "latencies_ms": [9973.222], "images_per_second": 0.1, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows a baseball game in progress with a batter in a white uniform with the number 10, a catcher in a black and red uniform, and an umpire in a blue shirt. The lighting appears to be artificial, likely from stadium lights, and the weather is overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20857.1, "ram_available_mb": 41983.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20848.3, "ram_available_mb": 41992.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.174}, "power_stats": {"power_gpu_soc_mean_watts": 19.781, "power_cpu_cv_mean_watts": 1.819, "power_sys_5v0_mean_watts": 8.687, "gpu_utilization_percent_mean": 71.174, "power_watts_avg": 19.781, "energy_joules_est": 197.29, "duration_seconds": 9.974, "sample_count": 86}, "timestamp": "2026-01-26T10:01:07.405937"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12381.923, "latencies_ms": [12381.923], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a black and white drawing of a variety of fruits and nuts. There are two apples, one on the left and one on the right, with the left apple being larger and the right one smaller. A bunch of grapes is placed between the two apples, with some grapes hanging from the left apple and others attached to the right apple. Additionally", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 20848.3, "ram_available_mb": 41992.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 20859.1, "ram_available_mb": 41981.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.849}, "power_stats": {"power_gpu_soc_mean_watts": 21.604, "power_cpu_cv_mean_watts": 2.084, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 72.849, "power_watts_avg": 21.604, "energy_joules_est": 267.51, "duration_seconds": 12.383, "sample_count": 106}, "timestamp": "2026-01-26T10:01:21.810169"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7352.208, "latencies_ms": [7352.208], "images_per_second": 0.136, "prompt_tokens": 39, "response_tokens_est": 31, "n_tiles": 16, "output_text": "peanuts: 12, grapes: 5, orange: 1, apple: 1, peach: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20859.1, "ram_available_mb": 41981.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 8.3, "ram_used_mb": 20869.6, "ram_available_mb": 41971.3, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 80.21}, "power_stats": {"power_gpu_soc_mean_watts": 24.333, "power_cpu_cv_mean_watts": 1.73, "power_sys_5v0_mean_watts": 8.78, "gpu_utilization_percent_mean": 80.21, "power_watts_avg": 24.333, "energy_joules_est": 178.92, "duration_seconds": 7.353, "sample_count": 62}, "timestamp": "2026-01-26T10:01:31.184862"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12715.655, "latencies_ms": [12715.655], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a pile of peanuts, which appear to be the closest to the viewer. Behind the peanuts, there is a large orange, and further back, there are clusters of grapes, with some hanging from the top left and others resting on the top right. The apple is positioned in the background, slightly to the", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20869.6, "ram_available_mb": 41971.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 9.3, "ram_used_mb": 20877.1, "ram_available_mb": 41963.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.722}, "power_stats": {"power_gpu_soc_mean_watts": 21.716, "power_cpu_cv_mean_watts": 1.916, "power_sys_5v0_mean_watts": 8.903, "gpu_utilization_percent_mean": 73.722, "power_watts_avg": 21.716, "energy_joules_est": 276.15, "duration_seconds": 12.716, "sample_count": 108}, "timestamp": "2026-01-26T10:01:45.945119"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10052.752, "latencies_ms": [10052.752], "images_per_second": 0.099, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a still life arrangement of various fruits and nuts on a white background. There is a large orange, a bunch of grapes, a few peanuts, and a few other fruits that are not clearly identifiable.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20877.1, "ram_available_mb": 41963.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 7.9, "ram_used_mb": 20879.7, "ram_available_mb": 41961.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.035}, "power_stats": {"power_gpu_soc_mean_watts": 22.713, "power_cpu_cv_mean_watts": 2.048, "power_sys_5v0_mean_watts": 8.839, "gpu_utilization_percent_mean": 75.035, "power_watts_avg": 22.713, "energy_joules_est": 228.34, "duration_seconds": 10.053, "sample_count": 85}, "timestamp": "2026-01-26T10:01:58.040466"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8741.562, "latencies_ms": [8741.562], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image is a black and white drawing featuring a variety of fruits. The lighting appears to be coming from the upper left, casting subtle shadows on the right sides of the fruits and nuts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20879.7, "ram_available_mb": 41961.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20875.1, "ram_available_mb": 41965.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.945}, "power_stats": {"power_gpu_soc_mean_watts": 23.203, "power_cpu_cv_mean_watts": 1.491, "power_sys_5v0_mean_watts": 8.857, "gpu_utilization_percent_mean": 77.945, "power_watts_avg": 23.203, "energy_joules_est": 202.84, "duration_seconds": 8.742, "sample_count": 73}, "timestamp": "2026-01-26T10:02:08.800852"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12393.711, "latencies_ms": [12393.711], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a city street with a mix of modern and older buildings. There are several cars parked along the street, and a few cars are driving down the road. A sidewalk runs alongside the street, providing a pedestrian path. A bike lane is also present, indicating that the city promotes alternative modes of transportation.\n\nIn addition to the vehicles", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20875.1, "ram_available_mb": 41965.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20889.7, "ram_available_mb": 41951.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.093}, "power_stats": {"power_gpu_soc_mean_watts": 21.308, "power_cpu_cv_mean_watts": 1.806, "power_sys_5v0_mean_watts": 8.847, "gpu_utilization_percent_mean": 73.093, "power_watts_avg": 21.308, "energy_joules_est": 264.1, "duration_seconds": 12.394, "sample_count": 107}, "timestamp": "2026-01-26T10:02:23.246272"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8980.006, "latencies_ms": [8980.006], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "buildings: 5, cars: 4, street lights: 5, trees: 2, sidewalks: 1, buses: 1, pedestrians: 0, buildings: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20889.7, "ram_available_mb": 41951.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20879.0, "ram_available_mb": 41961.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.143}, "power_stats": {"power_gpu_soc_mean_watts": 22.668, "power_cpu_cv_mean_watts": 1.528, "power_sys_5v0_mean_watts": 8.755, "gpu_utilization_percent_mean": 77.143, "power_watts_avg": 22.668, "energy_joules_est": 203.57, "duration_seconds": 8.981, "sample_count": 77}, "timestamp": "2026-01-26T10:02:34.251304"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11832.983, "latencies_ms": [11832.983], "images_per_second": 0.085, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The image shows a street scene with buildings in the background, a road in the foreground, and a clear sky above. The road has multiple lanes with cars parked on the side, and there is a sidewalk parallel to the road. The buildings appear to be residential or commercial structures, and there is a tree line on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20879.0, "ram_available_mb": 41961.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20874.4, "ram_available_mb": 41966.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.2}, "power_stats": {"power_gpu_soc_mean_watts": 21.5, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.906, "gpu_utilization_percent_mean": 72.2, "power_watts_avg": 21.5, "energy_joules_est": 254.42, "duration_seconds": 11.834, "sample_count": 100}, "timestamp": "2026-01-26T10:02:48.127300"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8503.334, "latencies_ms": [8503.334], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image depicts a street scene with a clear sky and a few clouds. There are multiple buildings on the left side of the street, and a few cars are parked and driving on the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20874.4, "ram_available_mb": 41966.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20875.8, "ram_available_mb": 41965.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.792}, "power_stats": {"power_gpu_soc_mean_watts": 23.002, "power_cpu_cv_mean_watts": 1.456, "power_sys_5v0_mean_watts": 8.725, "gpu_utilization_percent_mean": 76.792, "power_watts_avg": 23.002, "energy_joules_est": 195.61, "duration_seconds": 8.504, "sample_count": 72}, "timestamp": "2026-01-26T10:02:58.645162"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6744.386, "latencies_ms": [6744.386], "images_per_second": 0.148, "prompt_tokens": 36, "response_tokens_est": 32, "n_tiles": 16, "output_text": "The image shows a clear day with a blue sky and some clouds. The buildings are multi-story with a mix of brick and painted exteriors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20875.8, "ram_available_mb": 41965.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20869.9, "ram_available_mb": 41971.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.754}, "power_stats": {"power_gpu_soc_mean_watts": 23.901, "power_cpu_cv_mean_watts": 1.369, "power_sys_5v0_mean_watts": 8.868, "gpu_utilization_percent_mean": 77.754, "power_watts_avg": 23.901, "energy_joules_est": 161.21, "duration_seconds": 6.745, "sample_count": 57}, "timestamp": "2026-01-26T10:03:07.452316"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11562.83, "latencies_ms": [11562.83], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a man and a woman sitting at a table in a restaurant. The man is wearing a blue striped shirt and a black tie with a gold emblem, while the woman is wearing a white tank top. They are both smiling and appear to be happy. The table they are sitting at is brown and there is a plate of food in front of", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20869.9, "ram_available_mb": 41971.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20895.6, "ram_available_mb": 41945.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.46}, "power_stats": {"power_gpu_soc_mean_watts": 19.257, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.696, "gpu_utilization_percent_mean": 70.46, "power_watts_avg": 19.257, "energy_joules_est": 222.68, "duration_seconds": 11.563, "sample_count": 100}, "timestamp": "2026-01-26T10:03:21.040010"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7471.82, "latencies_ms": [7471.82], "images_per_second": 0.134, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "person: 2, table: 1, menu: 1, drink: 1, cell phone: 1, wall: 1, window: 1, television: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20895.6, "ram_available_mb": 41945.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20892.1, "ram_available_mb": 41948.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.81}, "power_stats": {"power_gpu_soc_mean_watts": 21.585, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 8.595, "gpu_utilization_percent_mean": 72.81, "power_watts_avg": 21.585, "energy_joules_est": 161.29, "duration_seconds": 7.472, "sample_count": 63}, "timestamp": "2026-01-26T10:03:30.557987"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11675.186, "latencies_ms": [11675.186], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The man is standing in the foreground on the left side of the image, while the woman is seated in the foreground on the right side. They are positioned close to each other, suggesting a sense of intimacy or companionship. In the background, there are various objects such as a television, a wall with framed pictures, and a menu board, which are at a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20892.1, "ram_available_mb": 41948.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20902.0, "ram_available_mb": 41938.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.788}, "power_stats": {"power_gpu_soc_mean_watts": 19.399, "power_cpu_cv_mean_watts": 1.875, "power_sys_5v0_mean_watts": 8.683, "gpu_utilization_percent_mean": 70.788, "power_watts_avg": 19.399, "energy_joules_est": 226.5, "duration_seconds": 11.676, "sample_count": 99}, "timestamp": "2026-01-26T10:03:44.271539"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10642.459, "latencies_ms": [10642.459], "images_per_second": 0.094, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "A man and a woman are sitting closely together at a table in a dimly lit restaurant, with the man wearing a blue striped shirt and a patterned tie, and the woman in a white sleeveless top. They appear to be enjoying a meal together, with a menu board visible in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20902.0, "ram_available_mb": 41938.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20894.3, "ram_available_mb": 41946.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.67}, "power_stats": {"power_gpu_soc_mean_watts": 19.968, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 70.67, "power_watts_avg": 19.968, "energy_joules_est": 212.52, "duration_seconds": 10.643, "sample_count": 91}, "timestamp": "2026-01-26T10:03:56.970557"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7867.684, "latencies_ms": [7867.684], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows two individuals in an indoor setting with warm lighting. The man is wearing a blue striped shirt with a patterned tie, and the woman is in a white sleeveless top.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20894.3, "ram_available_mb": 41946.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20971.3, "ram_available_mb": 41869.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.493}, "power_stats": {"power_gpu_soc_mean_watts": 21.046, "power_cpu_cv_mean_watts": 1.624, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 73.493, "power_watts_avg": 21.046, "energy_joules_est": 165.6, "duration_seconds": 7.868, "sample_count": 67}, "timestamp": "2026-01-26T10:04:06.860488"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11578.219, "latencies_ms": [11578.219], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing in a crowd of people, talking on her cell phone. She is wearing a black and gold costume, which suggests that she might be dressed up for a special occasion or event. The woman is surrounded by several other people, some of whom are also dressed in costumes. The scene appears to be a lively gathering or celebration, with", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 20867.3, "ram_available_mb": 41973.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20853.9, "ram_available_mb": 41987.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.253}, "power_stats": {"power_gpu_soc_mean_watts": 19.333, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 70.253, "power_watts_avg": 19.333, "energy_joules_est": 223.85, "duration_seconds": 11.579, "sample_count": 99}, "timestamp": "2026-01-26T10:04:20.496013"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9623.904, "latencies_ms": [9623.904], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "1. Woman: 1\n2. Phone: 1\n3. Man: 1\n4. Building: 1\n5. Crowd: 1\n6. Hat: 1\n7. Costume: 1\n8. Accessory: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20853.9, "ram_available_mb": 41987.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20858.3, "ram_available_mb": 41982.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.256}, "power_stats": {"power_gpu_soc_mean_watts": 20.216, "power_cpu_cv_mean_watts": 1.732, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 72.256, "power_watts_avg": 20.216, "energy_joules_est": 194.57, "duration_seconds": 9.625, "sample_count": 82}, "timestamp": "2026-01-26T10:04:32.140757"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11456.796, "latencies_ms": [11456.796], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "In the foreground, a person is holding a phone to their ear, seemingly in the middle of a conversation. Behind them, there are several other individuals, some of whom are partially visible and appear to be engaged in their own activities. The background is out of focus, but it seems to be a crowded outdoor area with buildings that could suggest an urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20858.3, "ram_available_mb": 41982.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20861.9, "ram_available_mb": 41979.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.162}, "power_stats": {"power_gpu_soc_mean_watts": 19.356, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 70.162, "power_watts_avg": 19.356, "energy_joules_est": 221.77, "duration_seconds": 11.457, "sample_count": 99}, "timestamp": "2026-01-26T10:04:45.615268"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9604.5, "latencies_ms": [9604.5], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "A woman dressed in a costume resembling a character from a movie is talking on a phone while standing in a crowd of people. The setting appears to be an outdoor event or gathering, possibly a convention or festival, where people are dressed in various costumes and accessories.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20861.9, "ram_available_mb": 41979.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20858.4, "ram_available_mb": 41982.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.951}, "power_stats": {"power_gpu_soc_mean_watts": 20.269, "power_cpu_cv_mean_watts": 1.744, "power_sys_5v0_mean_watts": 8.616, "gpu_utilization_percent_mean": 71.951, "power_watts_avg": 20.269, "energy_joules_est": 194.69, "duration_seconds": 9.605, "sample_count": 81}, "timestamp": "2026-01-26T10:04:57.240118"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9536.872, "latencies_ms": [9536.872], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image features a person wearing a costume with gold and black colors, and the lighting appears to be natural daylight. The costume seems to be made of a shiny, possibly metallic material, and the person is wearing a headpiece with a gold design.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20858.4, "ram_available_mb": 41982.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20850.0, "ram_available_mb": 41990.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.284}, "power_stats": {"power_gpu_soc_mean_watts": 19.795, "power_cpu_cv_mean_watts": 1.778, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 71.284, "power_watts_avg": 19.795, "energy_joules_est": 188.79, "duration_seconds": 9.537, "sample_count": 81}, "timestamp": "2026-01-26T10:05:08.802387"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11592.854, "latencies_ms": [11592.854], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a bathroom undergoing renovation. Dominating the left side of the frame is a white toilet, its tank and base exposed, revealing the intricate network of pipes and wires that hint at the ongoing work. A green bucket rests on the floor nearby, perhaps holding tools or supplies for the renovation. The walls, covered", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 20850.0, "ram_available_mb": 41990.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20888.2, "ram_available_mb": 41952.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.949}, "power_stats": {"power_gpu_soc_mean_watts": 19.209, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.661, "gpu_utilization_percent_mean": 69.949, "power_watts_avg": 19.209, "energy_joules_est": 222.7, "duration_seconds": 11.593, "sample_count": 98}, "timestamp": "2026-01-26T10:05:22.454711"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7686.139, "latencies_ms": [7686.139], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "showerhead: 1, pipe: 3, tiles: numerous, bucket: 2, drain: 1, toilet: 1, wall: numerous, floor: numerous", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20888.2, "ram_available_mb": 41952.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20876.5, "ram_available_mb": 41964.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.785}, "power_stats": {"power_gpu_soc_mean_watts": 21.466, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.598, "gpu_utilization_percent_mean": 73.785, "power_watts_avg": 21.466, "energy_joules_est": 165.0, "duration_seconds": 7.687, "sample_count": 65}, "timestamp": "2026-01-26T10:05:32.174740"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11769.926, "latencies_ms": [11769.926], "images_per_second": 0.085, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a white pillar with a showerhead attached to its top, positioned near the edge of the image. To the right of the pillar, there is a green bucket placed on the floor, and further back, there is a red bucket. The showerhead is located in the upper left corner of the image, while the green bucket is closer to", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20876.5, "ram_available_mb": 41964.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20887.0, "ram_available_mb": 41953.9, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.87}, "power_stats": {"power_gpu_soc_mean_watts": 19.296, "power_cpu_cv_mean_watts": 1.877, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 70.87, "power_watts_avg": 19.296, "energy_joules_est": 227.13, "duration_seconds": 11.771, "sample_count": 100}, "timestamp": "2026-01-26T10:05:45.979912"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9012.925, "latencies_ms": [9012.925], "images_per_second": 0.111, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a small, cramped bathroom with white tiled walls and floor. There is a white shower head, a white pipe, and a white toilet, with a green bucket and a red bucket placed on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20887.0, "ram_available_mb": 41953.9, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 20889.3, "ram_available_mb": 41951.6, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.355}, "power_stats": {"power_gpu_soc_mean_watts": 20.429, "power_cpu_cv_mean_watts": 1.732, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 72.355, "power_watts_avg": 20.429, "energy_joules_est": 184.14, "duration_seconds": 9.014, "sample_count": 76}, "timestamp": "2026-01-26T10:05:57.010172"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6732.188, "latencies_ms": [6732.188], "images_per_second": 0.149, "prompt_tokens": 36, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The image shows a bathroom with white tiled walls and a white showerhead. There is a white pipe running along the wall and a green bucket on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20889.3, "ram_available_mb": 41951.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 8.2, "ram_used_mb": 20960.7, "ram_available_mb": 41880.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.632}, "power_stats": {"power_gpu_soc_mean_watts": 21.857, "power_cpu_cv_mean_watts": 1.72, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 74.632, "power_watts_avg": 21.857, "energy_joules_est": 147.16, "duration_seconds": 6.733, "sample_count": 57}, "timestamp": "2026-01-26T10:06:05.782240"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11584.385, "latencies_ms": [11584.385], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing next to an elephant, with the elephant's trunk wrapped around the man's neck. The man is wearing glasses and appears to be smiling, possibly enjoying the interaction with the elephant. The scene takes place in a wooded area, with trees visible in the background. The man and the eleph", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20960.7, "ram_available_mb": 41880.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.7, "ram_used_mb": 20996.6, "ram_available_mb": 41844.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.929}, "power_stats": {"power_gpu_soc_mean_watts": 19.348, "power_cpu_cv_mean_watts": 2.193, "power_sys_5v0_mean_watts": 8.701, "gpu_utilization_percent_mean": 69.929, "power_watts_avg": 19.348, "energy_joules_est": 224.15, "duration_seconds": 11.585, "sample_count": 98}, "timestamp": "2026-01-26T10:06:19.415787"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7479.477, "latencies_ms": [7479.477], "images_per_second": 0.134, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "man: 1, elephant: 2, trees: many, sky: 1, mountains: 1, sun: 1, grass: 1, clouds: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20874.4, "ram_available_mb": 41966.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 9.6, "ram_used_mb": 20916.8, "ram_available_mb": 41924.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.937}, "power_stats": {"power_gpu_soc_mean_watts": 21.493, "power_cpu_cv_mean_watts": 1.766, "power_sys_5v0_mean_watts": 8.587, "gpu_utilization_percent_mean": 73.937, "power_watts_avg": 21.493, "energy_joules_est": 160.77, "duration_seconds": 7.48, "sample_count": 63}, "timestamp": "2026-01-26T10:06:28.956816"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11582.341, "latencies_ms": [11582.341], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a man wearing glasses and a light-colored shirt, standing close to an elephant. The elephant is positioned to the left of the man, with its trunk extended towards him. In the background, there is a lush green forest, indicating that the man and the elephant are in a natural, outdoor", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20916.8, "ram_available_mb": 41924.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.0, "ram_used_mb": 20885.1, "ram_available_mb": 41955.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.939}, "power_stats": {"power_gpu_soc_mean_watts": 19.193, "power_cpu_cv_mean_watts": 2.328, "power_sys_5v0_mean_watts": 8.721, "gpu_utilization_percent_mean": 70.939, "power_watts_avg": 19.193, "energy_joules_est": 222.31, "duration_seconds": 11.583, "sample_count": 99}, "timestamp": "2026-01-26T10:06:42.558306"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7430.895, "latencies_ms": [7430.895], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A man is standing in front of an elephant, with the elephant's trunk wrapped around his neck. The background shows a lush green forest and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20885.1, "ram_available_mb": 41955.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20917.9, "ram_available_mb": 41923.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.238}, "power_stats": {"power_gpu_soc_mean_watts": 21.552, "power_cpu_cv_mean_watts": 1.55, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 74.238, "power_watts_avg": 21.552, "energy_joules_est": 160.16, "duration_seconds": 7.432, "sample_count": 63}, "timestamp": "2026-01-26T10:06:52.029136"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7538.853, "latencies_ms": [7538.853], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image features a man wearing glasses and a light-colored t-shirt, standing in front of an elephant. The background shows a lush green forest and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20917.9, "ram_available_mb": 41923.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20897.6, "ram_available_mb": 41943.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.333}, "power_stats": {"power_gpu_soc_mean_watts": 21.182, "power_cpu_cv_mean_watts": 1.651, "power_sys_5v0_mean_watts": 8.659, "gpu_utilization_percent_mean": 73.333, "power_watts_avg": 21.182, "energy_joules_est": 159.7, "duration_seconds": 7.539, "sample_count": 63}, "timestamp": "2026-01-26T10:07:01.594909"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11637.221, "latencies_ms": [11637.221], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a group of five children is sitting on the grass, holding frisbees. They are all wearing different colored shirts, and they seem to be enjoying their time together. The children are sitting in a row, with one child on the far left, and the others are positioned in between. The frisbees they are holding are white and have", "error": null, "sys_before": {"cpu_percent": 6.5, "ram_used_mb": 20897.6, "ram_available_mb": 41943.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20890.1, "ram_available_mb": 41950.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.778}, "power_stats": {"power_gpu_soc_mean_watts": 19.164, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.681, "gpu_utilization_percent_mean": 70.778, "power_watts_avg": 19.164, "energy_joules_est": 223.03, "duration_seconds": 11.638, "sample_count": 99}, "timestamp": "2026-01-26T10:07:15.286453"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8701.13, "latencies_ms": [8701.13], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "children: 5\nfrisbees: 2\ngrass: numerous\ntrees: 4\nsweatshirts: 2\nsocks: 2\nshirts: 3\npants: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20890.1, "ram_available_mb": 41950.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20924.6, "ram_available_mb": 41916.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.676}, "power_stats": {"power_gpu_soc_mean_watts": 20.71, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 72.676, "power_watts_avg": 20.71, "energy_joules_est": 180.21, "duration_seconds": 8.702, "sample_count": 74}, "timestamp": "2026-01-26T10:07:26.011129"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10760.058, "latencies_ms": [10760.058], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground, there are five children sitting on the grass, with one child on the far left and the others spread out towards the right. The children are positioned relatively close to each other, suggesting they are sitting in a group. The background consists of trees and shrubs, indicating that the setting is outdoors in a natural environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20924.6, "ram_available_mb": 41916.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20920.2, "ram_available_mb": 41920.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.25}, "power_stats": {"power_gpu_soc_mean_watts": 19.583, "power_cpu_cv_mean_watts": 1.853, "power_sys_5v0_mean_watts": 8.664, "gpu_utilization_percent_mean": 71.25, "power_watts_avg": 19.583, "energy_joules_est": 210.73, "duration_seconds": 10.761, "sample_count": 92}, "timestamp": "2026-01-26T10:07:38.792317"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6971.731, "latencies_ms": [6971.731], "images_per_second": 0.143, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A group of children is sitting on the grass, each holding a frisbee. They appear to be in a park or a grassy area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20920.2, "ram_available_mb": 41920.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20935.8, "ram_available_mb": 41905.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.814}, "power_stats": {"power_gpu_soc_mean_watts": 21.905, "power_cpu_cv_mean_watts": 1.485, "power_sys_5v0_mean_watts": 8.563, "gpu_utilization_percent_mean": 74.814, "power_watts_avg": 21.905, "energy_joules_est": 152.73, "duration_seconds": 6.972, "sample_count": 59}, "timestamp": "2026-01-26T10:07:47.784080"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8838.54, "latencies_ms": [8838.54], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows a group of children sitting on grass with a clear sky in the background. They are holding frisbees, one of which has the word \"Ultimate\" on it, and the children are wearing a variety of colorful clothing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20935.8, "ram_available_mb": 41905.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20864.4, "ram_available_mb": 41976.5, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.649}, "power_stats": {"power_gpu_soc_mean_watts": 20.338, "power_cpu_cv_mean_watts": 1.72, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 71.649, "power_watts_avg": 20.338, "energy_joules_est": 179.77, "duration_seconds": 8.839, "sample_count": 74}, "timestamp": "2026-01-26T10:07:58.642637"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11583.029, "latencies_ms": [11583.029], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young girl is standing on a sidewalk, holding an umbrella to protect herself from the rain. She is wearing a red coat and white shoes, and she appears to be smiling as she enjoys the rainy weather. The umbrella she is holding is open, providing her with shelter from the rain.\n\nThe scene takes place in a", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20864.4, "ram_available_mb": 41976.5, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20897.3, "ram_available_mb": 41943.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.081}, "power_stats": {"power_gpu_soc_mean_watts": 19.281, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.661, "gpu_utilization_percent_mean": 70.081, "power_watts_avg": 19.281, "energy_joules_est": 223.34, "duration_seconds": 11.584, "sample_count": 99}, "timestamp": "2026-01-26T10:08:12.276536"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9626.266, "latencies_ms": [9626.266], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- umbrella: 1\n- child: 1\n- red coat: 1\n- blue jeans: 1\n- white sneakers: 1\n- green bush: 2\n- puddle: 1\n- houses: 2", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20897.3, "ram_available_mb": 41943.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20884.1, "ram_available_mb": 41956.8, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.841}, "power_stats": {"power_gpu_soc_mean_watts": 20.374, "power_cpu_cv_mean_watts": 1.732, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 71.841, "power_watts_avg": 20.374, "energy_joules_est": 196.14, "duration_seconds": 9.627, "sample_count": 82}, "timestamp": "2026-01-26T10:08:23.921734"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10056.667, "latencies_ms": [10056.667], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "In the foreground, a child is standing on a wet sidewalk, holding an umbrella to protect from the rain. The child is positioned near a large, well-manicured bush. In the background, there are houses and vehicles, indicating that the scene takes place in a residential area.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20884.1, "ram_available_mb": 41956.8, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20893.8, "ram_available_mb": 41947.1, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.588}, "power_stats": {"power_gpu_soc_mean_watts": 19.951, "power_cpu_cv_mean_watts": 1.808, "power_sys_5v0_mean_watts": 8.659, "gpu_utilization_percent_mean": 71.588, "power_watts_avg": 19.951, "energy_joules_est": 200.65, "duration_seconds": 10.057, "sample_count": 85}, "timestamp": "2026-01-26T10:08:36.006546"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6895.28, "latencies_ms": [6895.28], "images_per_second": 0.145, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "A young child is standing on a wet sidewalk, holding an umbrella to shield themselves from the rain. The background shows a residential area with houses and trees.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20893.8, "ram_available_mb": 41947.1, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20954.2, "ram_available_mb": 41886.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.224}, "power_stats": {"power_gpu_soc_mean_watts": 21.952, "power_cpu_cv_mean_watts": 1.483, "power_sys_5v0_mean_watts": 8.584, "gpu_utilization_percent_mean": 74.224, "power_watts_avg": 21.952, "energy_joules_est": 151.38, "duration_seconds": 6.896, "sample_count": 58}, "timestamp": "2026-01-26T10:08:44.951756"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8553.958, "latencies_ms": [8553.958], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A child in a red coat is holding a patterned umbrella, standing on a wet sidewalk with a hedge and houses in the background. The sky is overcast, and the ground is reflective, indicating recent or ongoing rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20857.3, "ram_available_mb": 41983.6, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20946.2, "ram_available_mb": 41894.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.507}, "power_stats": {"power_gpu_soc_mean_watts": 20.577, "power_cpu_cv_mean_watts": 1.716, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 72.507, "power_watts_avg": 20.577, "energy_joules_est": 176.03, "duration_seconds": 8.555, "sample_count": 73}, "timestamp": "2026-01-26T10:08:55.523757"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11599.243, "latencies_ms": [11599.243], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a group of elephants is walking along a dirt road. The main focus is on a baby elephant, which is walking in front of the other elephants. The baby elephant appears to be leading the group, as the other elephants are following behind it. The scene captures the natural behavior of these majestic animals as they move together", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 20868.6, "ram_available_mb": 41972.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20999.0, "ram_available_mb": 41841.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.92}, "power_stats": {"power_gpu_soc_mean_watts": 19.288, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 70.92, "power_watts_avg": 19.288, "energy_joules_est": 223.74, "duration_seconds": 11.6, "sample_count": 100}, "timestamp": "2026-01-26T10:09:09.190886"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7777.42, "latencies_ms": [7777.42], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "elephant: 3, water: 1, trees: 1, ground: 1, sky: 1, clouds: 1, dirt: 1, trunk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20853.6, "ram_available_mb": 41987.3, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20850.2, "ram_available_mb": 41990.7, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.485}, "power_stats": {"power_gpu_soc_mean_watts": 21.237, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 73.485, "power_watts_avg": 21.237, "energy_joules_est": 165.18, "duration_seconds": 7.778, "sample_count": 66}, "timestamp": "2026-01-26T10:09:18.992111"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11594.059, "latencies_ms": [11594.059], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a prominent elephant with its trunk extended towards the camera, appearing to be in the middle of the frame. Behind this elephant, there are two more elephants, one partially visible on the left and another more fully visible on the right, both appearing to be at a distance behind the main subject. The background features a body of water", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20850.2, "ram_available_mb": 41990.7, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20848.7, "ram_available_mb": 41992.2, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.296}, "power_stats": {"power_gpu_soc_mean_watts": 19.339, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 70.296, "power_watts_avg": 19.339, "energy_joules_est": 224.23, "duration_seconds": 11.595, "sample_count": 98}, "timestamp": "2026-01-26T10:09:32.631514"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8359.491, "latencies_ms": [8359.491], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A group of elephants, including a young one, are walking along a dirt path near a body of water. The elephants appear to be in a natural habitat, possibly a savannah or a wildlife reserve.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20848.7, "ram_available_mb": 41992.2, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20849.5, "ram_available_mb": 41991.4, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.186}, "power_stats": {"power_gpu_soc_mean_watts": 20.956, "power_cpu_cv_mean_watts": 1.623, "power_sys_5v0_mean_watts": 8.579, "gpu_utilization_percent_mean": 73.186, "power_watts_avg": 20.956, "energy_joules_est": 175.2, "duration_seconds": 8.36, "sample_count": 70}, "timestamp": "2026-01-26T10:09:43.004708"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10308.133, "latencies_ms": [10308.133], "images_per_second": 0.097, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The elephant in the foreground has a rough, sandy-colored skin, indicative of its natural habitat. The lighting in the image is soft and diffused, suggesting an overcast day, which casts gentle shadows and highlights the textures of the elephants' wrinkled skin.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20849.5, "ram_available_mb": 41991.4, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20850.9, "ram_available_mb": 41990.0, "ram_percent": 33.2}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.091}, "power_stats": {"power_gpu_soc_mean_watts": 19.546, "power_cpu_cv_mean_watts": 1.833, "power_sys_5v0_mean_watts": 8.663, "gpu_utilization_percent_mean": 71.091, "power_watts_avg": 19.546, "energy_joules_est": 201.49, "duration_seconds": 10.309, "sample_count": 88}, "timestamp": "2026-01-26T10:09:55.332138"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12423.377, "latencies_ms": [12423.377], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a thrilling moment of a surfer riding a wave. The surfer, clad in a vibrant red and green wetsuit, is skillfully maneuvering a white surfboard. The wave, a deep green color, is breaking to the right, creating a dynamic and powerful scene. The surfer is positioned on the left side of", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 20850.9, "ram_available_mb": 41990.0, "ram_percent": 33.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20991.1, "ram_available_mb": 41849.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.187}, "power_stats": {"power_gpu_soc_mean_watts": 21.593, "power_cpu_cv_mean_watts": 1.821, "power_sys_5v0_mean_watts": 8.924, "gpu_utilization_percent_mean": 72.187, "power_watts_avg": 21.593, "energy_joules_est": 268.27, "duration_seconds": 12.424, "sample_count": 107}, "timestamp": "2026-01-26T10:10:09.800456"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8877.925, "latencies_ms": [8877.925], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "surfer: 1, wave: multiple, water droplets: numerous, surfboard: 1, air: 1, foam: multiple, ocean: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20909.3, "ram_available_mb": 41931.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21012.1, "ram_available_mb": 41828.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.627}, "power_stats": {"power_gpu_soc_mean_watts": 23.215, "power_cpu_cv_mean_watts": 1.451, "power_sys_5v0_mean_watts": 8.76, "gpu_utilization_percent_mean": 76.627, "power_watts_avg": 23.215, "energy_joules_est": 206.12, "duration_seconds": 8.879, "sample_count": 75}, "timestamp": "2026-01-26T10:10:20.714582"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11878.341, "latencies_ms": [11878.341], "images_per_second": 0.084, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is breaking to the right of the frame. The wave itself is the main object in the middle ground, curving and creating a dynamic background. The spray from the wave is captured in the air, indicating the surfer's movement and the wave's power.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21012.1, "ram_available_mb": 41828.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 20941.7, "ram_available_mb": 41899.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.621}, "power_stats": {"power_gpu_soc_mean_watts": 21.722, "power_cpu_cv_mean_watts": 1.822, "power_sys_5v0_mean_watts": 8.892, "gpu_utilization_percent_mean": 73.621, "power_watts_avg": 21.722, "energy_joules_est": 258.03, "duration_seconds": 11.879, "sample_count": 103}, "timestamp": "2026-01-26T10:10:34.610645"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8733.594, "latencies_ms": [8733.594], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A surfer is captured in mid-air, performing a trick above a large wave. The photo is taken from a distance, showcasing the surfer's skill and the power of the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20941.7, "ram_available_mb": 41899.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 20968.2, "ram_available_mb": 41872.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.162}, "power_stats": {"power_gpu_soc_mean_watts": 23.381, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.813, "gpu_utilization_percent_mean": 78.162, "power_watts_avg": 23.381, "energy_joules_est": 204.22, "duration_seconds": 8.734, "sample_count": 74}, "timestamp": "2026-01-26T10:10:45.362165"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9282.669, "latencies_ms": [9282.669], "images_per_second": 0.108, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The surfer is wearing a bright red and green wetsuit, which stands out against the darker tones of the ocean. The lighting in the image is natural, suggesting it was taken during the day under clear skies.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20968.2, "ram_available_mb": 41872.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.2, "ram_used_mb": 21025.4, "ram_available_mb": 41815.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.57}, "power_stats": {"power_gpu_soc_mean_watts": 22.931, "power_cpu_cv_mean_watts": 1.687, "power_sys_5v0_mean_watts": 8.89, "gpu_utilization_percent_mean": 76.57, "power_watts_avg": 22.931, "energy_joules_est": 212.88, "duration_seconds": 9.283, "sample_count": 79}, "timestamp": "2026-01-26T10:10:56.682063"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12412.388, "latencies_ms": [12412.388], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two people are riding horses on a sandy beach near the ocean. They are both wearing white clothes, and the horses are brown and white. The beach is bustling with activity, as there are several other people visible in the background, enjoying the beach and the ocean.\n\nThere are also a few more horses in the scene, with one horse located", "error": null, "sys_before": {"cpu_percent": 24.1, "ram_used_mb": 20921.7, "ram_available_mb": 41919.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 9.0, "ram_used_mb": 20953.8, "ram_available_mb": 41887.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.636}, "power_stats": {"power_gpu_soc_mean_watts": 21.277, "power_cpu_cv_mean_watts": 2.106, "power_sys_5v0_mean_watts": 8.863, "gpu_utilization_percent_mean": 72.636, "power_watts_avg": 21.277, "energy_joules_est": 264.11, "duration_seconds": 12.413, "sample_count": 107}, "timestamp": "2026-01-26T10:11:11.171029"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 12132.199, "latencies_ms": [12132.199], "images_per_second": 0.082, "prompt_tokens": 39, "response_tokens_est": 75, "n_tiles": 16, "output_text": "1. People: 2\n2. Horses: 2\n3. Sand: extensive coverage\n4. Water: visible in the background\n5. Clouds: scattered across the sky\n6. Beach: visible in the background\n7. Trees: not visible in the image\n8. People: 3 (in the background)", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20953.8, "ram_available_mb": 41887.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.9, "ram_used_mb": 20979.2, "ram_available_mb": 41861.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.6}, "power_stats": {"power_gpu_soc_mean_watts": 21.596, "power_cpu_cv_mean_watts": 2.005, "power_sys_5v0_mean_watts": 8.807, "gpu_utilization_percent_mean": 73.6, "power_watts_avg": 21.596, "energy_joules_est": 262.02, "duration_seconds": 12.133, "sample_count": 105}, "timestamp": "2026-01-26T10:11:25.325510"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11697.373, "latencies_ms": [11697.373], "images_per_second": 0.085, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there are two individuals riding horses, one on the left and one on the right, both moving towards the right side of the image. In the background, there is a beach scene with people in the water and on the shore, and the ocean extends to the horizon. The sky is clear and occupies the upper part of the image.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20979.2, "ram_available_mb": 41861.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21042.6, "ram_available_mb": 41798.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.644}, "power_stats": {"power_gpu_soc_mean_watts": 21.497, "power_cpu_cv_mean_watts": 1.811, "power_sys_5v0_mean_watts": 8.891, "gpu_utilization_percent_mean": 71.644, "power_watts_avg": 21.497, "energy_joules_est": 251.47, "duration_seconds": 11.698, "sample_count": 101}, "timestamp": "2026-01-26T10:11:39.061701"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6722.037, "latencies_ms": [6722.037], "images_per_second": 0.149, "prompt_tokens": 37, "response_tokens_est": 28, "n_tiles": 16, "output_text": "Two people are riding horses on a sandy beach near the ocean, with other people visible in the background enjoying the beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21042.6, "ram_available_mb": 41798.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 21040.6, "ram_available_mb": 41800.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.842}, "power_stats": {"power_gpu_soc_mean_watts": 24.323, "power_cpu_cv_mean_watts": 1.214, "power_sys_5v0_mean_watts": 8.699, "gpu_utilization_percent_mean": 78.842, "power_watts_avg": 24.323, "energy_joules_est": 163.52, "duration_seconds": 6.723, "sample_count": 57}, "timestamp": "2026-01-26T10:11:47.818255"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7796.68, "latencies_ms": [7796.68], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The sky is a clear blue with a few scattered clouds, indicating a sunny day. The sand on the beach is a light beige color, and the water appears a deep blue-green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20936.1, "ram_available_mb": 41904.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20928.2, "ram_available_mb": 41912.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.439}, "power_stats": {"power_gpu_soc_mean_watts": 23.051, "power_cpu_cv_mean_watts": 1.479, "power_sys_5v0_mean_watts": 8.857, "gpu_utilization_percent_mean": 76.439, "power_watts_avg": 23.051, "energy_joules_est": 179.74, "duration_seconds": 7.797, "sample_count": 66}, "timestamp": "2026-01-26T10:11:57.668882"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11606.624, "latencies_ms": [11606.624], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a motorcycle parked in front of a garage. The motorcycle is leaning against a wall, and it appears to be a vintage model. A dog is standing nearby, looking at the motorcycle. The garage is filled with various items, including bicycles and other objects.\n\nIn the background, there is a car parked", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20928.2, "ram_available_mb": 41912.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20923.3, "ram_available_mb": 41917.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.313}, "power_stats": {"power_gpu_soc_mean_watts": 19.334, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 70.313, "power_watts_avg": 19.334, "energy_joules_est": 224.41, "duration_seconds": 11.607, "sample_count": 99}, "timestamp": "2026-01-26T10:12:11.302718"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7684.363, "latencies_ms": [7684.363], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "motorcycle: 1, dog: 1, tire: 2, building: 1, car: 1, trailer: 1, tree: 1, grass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20923.3, "ram_available_mb": 41917.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20918.0, "ram_available_mb": 41922.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.738}, "power_stats": {"power_gpu_soc_mean_watts": 21.318, "power_cpu_cv_mean_watts": 1.57, "power_sys_5v0_mean_watts": 8.603, "gpu_utilization_percent_mean": 73.738, "power_watts_avg": 21.318, "energy_joules_est": 163.83, "duration_seconds": 7.685, "sample_count": 65}, "timestamp": "2026-01-26T10:12:21.008143"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11587.833, "latencies_ms": [11587.833], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a motorcycle with a sidecar parked on the grass. The sidecar is attached to the motorcycle, which is positioned in the middle of the image. In the background, there is a garage with various items inside, and a car covered with a blue tarp is parked further back. A small dog is also visible in the foreground", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20918.0, "ram_available_mb": 41922.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21009.6, "ram_available_mb": 41831.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.838}, "power_stats": {"power_gpu_soc_mean_watts": 19.253, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 69.838, "power_watts_avg": 19.253, "energy_joules_est": 223.11, "duration_seconds": 11.588, "sample_count": 99}, "timestamp": "2026-01-26T10:12:34.653501"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7089.13, "latencies_ms": [7089.13], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A motorcycle is parked in front of a garage with a dog standing nearby. The garage is filled with various items, including bicycles hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21009.6, "ram_available_mb": 41831.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20920.2, "ram_available_mb": 41920.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.383}, "power_stats": {"power_gpu_soc_mean_watts": 21.833, "power_cpu_cv_mean_watts": 1.501, "power_sys_5v0_mean_watts": 8.567, "gpu_utilization_percent_mean": 74.383, "power_watts_avg": 21.833, "energy_joules_est": 154.79, "duration_seconds": 7.09, "sample_count": 60}, "timestamp": "2026-01-26T10:12:43.757308"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5601.769, "latencies_ms": [5601.769], "images_per_second": 0.179, "prompt_tokens": 36, "response_tokens_est": 27, "n_tiles": 16, "output_text": "The motorcycle is primarily silver with a black seat and handlebars. The weather appears to be sunny with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20920.2, "ram_available_mb": 41920.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 20993.6, "ram_available_mb": 41847.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.298}, "power_stats": {"power_gpu_soc_mean_watts": 22.942, "power_cpu_cv_mean_watts": 1.354, "power_sys_5v0_mean_watts": 8.619, "gpu_utilization_percent_mean": 76.298, "power_watts_avg": 22.942, "energy_joules_est": 128.54, "duration_seconds": 5.603, "sample_count": 47}, "timestamp": "2026-01-26T10:12:51.397187"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11596.668, "latencies_ms": [11596.668], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a man flying a kite on a sandy beach. He is standing on the sand, holding onto the kite string, and appears to be enjoying the activity. There are several other people scattered around the beach, some closer to the water and others further away. \n\nIn the background, there are a few boats visible on the water, adding to the beach atmosphere", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20921.5, "ram_available_mb": 41919.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21013.6, "ram_available_mb": 41827.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.576}, "power_stats": {"power_gpu_soc_mean_watts": 19.318, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.679, "gpu_utilization_percent_mean": 70.576, "power_watts_avg": 19.318, "energy_joules_est": 224.04, "duration_seconds": 11.597, "sample_count": 99}, "timestamp": "2026-01-26T10:13:05.047683"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8795.148, "latencies_ms": [8795.148], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "- Kite: 1\n- People: 10\n- Sand: 1\n- Beach: 1\n- Water: 1\n- Trees: 1\n- Buildings: 1\n- Clouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20925.0, "ram_available_mb": 41915.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21014.1, "ram_available_mb": 41826.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.56}, "power_stats": {"power_gpu_soc_mean_watts": 20.68, "power_cpu_cv_mean_watts": 1.681, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 72.56, "power_watts_avg": 20.68, "energy_joules_est": 181.9, "duration_seconds": 8.796, "sample_count": 75}, "timestamp": "2026-01-26T10:13:15.890163"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10758.482, "latencies_ms": [10758.482], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground, a person is flying a kite on a sandy beach. The beach is adjacent to a body of water, with several people visible in the distance near the water's edge. Trees line the background, and there are more people scattered throughout the scene, some closer to the trees and others further away near the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21014.1, "ram_available_mb": 41826.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21014.8, "ram_available_mb": 41826.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.571}, "power_stats": {"power_gpu_soc_mean_watts": 19.587, "power_cpu_cv_mean_watts": 1.856, "power_sys_5v0_mean_watts": 8.659, "gpu_utilization_percent_mean": 70.571, "power_watts_avg": 19.587, "energy_joules_est": 210.74, "duration_seconds": 10.759, "sample_count": 91}, "timestamp": "2026-01-26T10:13:28.695786"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6632.446, "latencies_ms": [6632.446], "images_per_second": 0.151, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "A man is flying a kite on a sandy beach near a body of water. There are several people in the background enjoying the beach and the trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21014.8, "ram_available_mb": 41826.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20918.7, "ram_available_mb": 41922.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.75}, "power_stats": {"power_gpu_soc_mean_watts": 22.162, "power_cpu_cv_mean_watts": 1.443, "power_sys_5v0_mean_watts": 8.577, "gpu_utilization_percent_mean": 75.75, "power_watts_avg": 22.162, "energy_joules_est": 147.0, "duration_seconds": 6.633, "sample_count": 56}, "timestamp": "2026-01-26T10:13:37.346778"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8056.899, "latencies_ms": [8056.899], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image shows a sunny day at the beach with clear blue skies and a few scattered clouds. The sand is a light beige color, and the water appears to be a deep blue, reflecting the bright sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20918.7, "ram_available_mb": 41922.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21003.8, "ram_available_mb": 41837.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.986}, "power_stats": {"power_gpu_soc_mean_watts": 20.518, "power_cpu_cv_mean_watts": 1.664, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 72.986, "power_watts_avg": 20.518, "energy_joules_est": 165.33, "duration_seconds": 8.058, "sample_count": 69}, "timestamp": "2026-01-26T10:13:47.442751"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11604.085, "latencies_ms": [11604.085], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a kitchen with wooden cabinets and a black countertop. The countertop is cluttered with various items, including a green bottle, a white plate, and several bottles. There is a sink in the kitchen, and a refrigerator is visible on the left side of the scene. \n\nIn addition to the main items, there are a few other", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 21003.8, "ram_available_mb": 41837.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21003.8, "ram_available_mb": 41837.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.768}, "power_stats": {"power_gpu_soc_mean_watts": 19.221, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.679, "gpu_utilization_percent_mean": 70.768, "power_watts_avg": 19.221, "energy_joules_est": 223.05, "duration_seconds": 11.605, "sample_count": 99}, "timestamp": "2026-01-26T10:14:01.079859"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8699.068, "latencies_ms": [8699.068], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "refrigerator: 1, sink: 2, dishwasher: 1, microwave: 1, oven: 1, candle: 1, plate: 1, bottle: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20931.1, "ram_available_mb": 41909.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20984.9, "ram_available_mb": 41856.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.365}, "power_stats": {"power_gpu_soc_mean_watts": 20.651, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 8.59, "gpu_utilization_percent_mean": 72.365, "power_watts_avg": 20.651, "energy_joules_est": 179.66, "duration_seconds": 8.7, "sample_count": 74}, "timestamp": "2026-01-26T10:14:11.792635"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11146.827, "latencies_ms": [11146.827], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground of the image, there is a kitchen counter with various items scattered on it, including a green bottle, a white plate with a red object on it, and several bottles. The refrigerator is in the background, to the left of the counter. The sink is located in the foreground, to the left of the counter.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20914.5, "ram_available_mb": 41926.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20923.8, "ram_available_mb": 41917.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.053}, "power_stats": {"power_gpu_soc_mean_watts": 19.365, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.676, "gpu_utilization_percent_mean": 71.053, "power_watts_avg": 19.365, "energy_joules_est": 215.87, "duration_seconds": 11.147, "sample_count": 95}, "timestamp": "2026-01-26T10:14:24.966607"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9260.061, "latencies_ms": [9260.061], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image depicts a kitchen with wooden cabinets and a stainless steel refrigerator. The countertop is cluttered with various items, including a green bottle, a plate with a red bow, and several bottles of different sizes and colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20923.8, "ram_available_mb": 41917.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20940.9, "ram_available_mb": 41899.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.165}, "power_stats": {"power_gpu_soc_mean_watts": 20.43, "power_cpu_cv_mean_watts": 1.717, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 71.165, "power_watts_avg": 20.43, "energy_joules_est": 189.19, "duration_seconds": 9.261, "sample_count": 79}, "timestamp": "2026-01-26T10:14:36.259521"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8141.375, "latencies_ms": [8141.375], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The kitchen is well-lit with natural light coming from the window, and the cabinets are made of wood with a light brown finish. The countertop is black, and there are various cleaning supplies and food items scattered across it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.9, "ram_available_mb": 41899.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20927.7, "ram_available_mb": 41913.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.884}, "power_stats": {"power_gpu_soc_mean_watts": 20.641, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.619, "gpu_utilization_percent_mean": 71.884, "power_watts_avg": 20.641, "energy_joules_est": 168.06, "duration_seconds": 8.142, "sample_count": 69}, "timestamp": "2026-01-26T10:14:46.419758"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11562.183, "latencies_ms": [11562.183], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the vast expanse of the clear blue sky, a vibrant kite soars high. The kite, a striking combination of white and red, is adorned with black and white designs that add a touch of whimsy to its appearance. The kite's tail, a long, flowing streamer, dances in the wind, adding a sense of movement", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 20927.7, "ram_available_mb": 41913.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21027.0, "ram_available_mb": 41813.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.96}, "power_stats": {"power_gpu_soc_mean_watts": 19.325, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.693, "gpu_utilization_percent_mean": 70.96, "power_watts_avg": 19.325, "energy_joules_est": 223.45, "duration_seconds": 11.563, "sample_count": 100}, "timestamp": "2026-01-26T10:15:00.008798"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8535.225, "latencies_ms": [8535.225], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "kite: 1, sky: 1, string: 2, kite shape: 1, kite color: 1, kite design: 1, kite brand: 1, kite material: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20918.3, "ram_available_mb": 41922.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20997.4, "ram_available_mb": 41843.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.5}, "power_stats": {"power_gpu_soc_mean_watts": 20.925, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 73.5, "power_watts_avg": 20.925, "energy_joules_est": 178.61, "duration_seconds": 8.536, "sample_count": 72}, "timestamp": "2026-01-26T10:15:10.587358"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8255.762, "latencies_ms": [8255.762], "images_per_second": 0.121, "prompt_tokens": 44, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The kite is in the foreground and appears to be flying high in the sky. The background is a clear blue sky with no clouds. The kite is positioned in the center of the image, with its tail trailing behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20925.5, "ram_available_mb": 41915.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 21011.2, "ram_available_mb": 41829.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.671}, "power_stats": {"power_gpu_soc_mean_watts": 20.523, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 8.664, "gpu_utilization_percent_mean": 72.671, "power_watts_avg": 20.523, "energy_joules_est": 169.45, "duration_seconds": 8.257, "sample_count": 70}, "timestamp": "2026-01-26T10:15:20.879260"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11745.496, "latencies_ms": [11745.496], "images_per_second": 0.085, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene of a kite soaring high in the clear blue sky. The kite, with its striking design of black and white patterns on a red and white background, is the focal point of the image. It is flying at a considerable height, indicating a strong and steady wind. The kite's curved shape and the way it is t", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21011.2, "ram_available_mb": 41829.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 21015.2, "ram_available_mb": 41825.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.88}, "power_stats": {"power_gpu_soc_mean_watts": 19.533, "power_cpu_cv_mean_watts": 2.005, "power_sys_5v0_mean_watts": 8.66, "gpu_utilization_percent_mean": 69.88, "power_watts_avg": 19.533, "energy_joules_est": 229.44, "duration_seconds": 11.746, "sample_count": 100}, "timestamp": "2026-01-26T10:15:34.689294"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5508.179, "latencies_ms": [5508.179], "images_per_second": 0.182, "prompt_tokens": 36, "response_tokens_est": 26, "n_tiles": 16, "output_text": "The kite is predominantly white with red and black accents. It is flying high in the clear blue sky.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20918.4, "ram_available_mb": 41922.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 20923.4, "ram_available_mb": 41917.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.652}, "power_stats": {"power_gpu_soc_mean_watts": 22.963, "power_cpu_cv_mean_watts": 1.314, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 76.652, "power_watts_avg": 22.963, "energy_joules_est": 126.5, "duration_seconds": 5.509, "sample_count": 46}, "timestamp": "2026-01-26T10:15:42.209708"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11597.786, "latencies_ms": [11597.786], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large, well-lit bedroom with two neatly made beds. The beds are positioned side by side, and the room has a cozy and inviting atmosphere. The room is adorned with a few decorative elements, including a painting on the wall and a couple of lamps. The beds are covered with white bedding, and there are", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20923.4, "ram_available_mb": 41917.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20911.3, "ram_available_mb": 41929.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.384}, "power_stats": {"power_gpu_soc_mean_watts": 19.319, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 70.384, "power_watts_avg": 19.319, "energy_joules_est": 224.07, "duration_seconds": 11.598, "sample_count": 99}, "timestamp": "2026-01-26T10:15:55.839931"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8152.18, "latencies_ms": [8152.18], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "bed: 2\nlamps: 2\npillows: 6\nblankets: 2\nrugs: 2\nartwork: 1\nwindows: 2\ndoors: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20911.3, "ram_available_mb": 41929.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20915.7, "ram_available_mb": 41925.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.145}, "power_stats": {"power_gpu_soc_mean_watts": 20.957, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.568, "gpu_utilization_percent_mean": 73.145, "power_watts_avg": 20.957, "energy_joules_est": 170.86, "duration_seconds": 8.153, "sample_count": 69}, "timestamp": "2026-01-26T10:16:06.004993"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11583.23, "latencies_ms": [11583.23], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a room with two beds positioned parallel to each other, one in the foreground and the other slightly behind it, creating a sense of depth. The beds are the main objects in the foreground, with a clear space between them and the background, which includes a door and a window. The room has a warm and inviting atmosphere with lights placed on either side of", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20915.7, "ram_available_mb": 41925.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20919.6, "ram_available_mb": 41921.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.838}, "power_stats": {"power_gpu_soc_mean_watts": 19.324, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 69.838, "power_watts_avg": 19.324, "energy_joules_est": 223.85, "duration_seconds": 11.584, "sample_count": 99}, "timestamp": "2026-01-26T10:16:19.624147"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8456.091, "latencies_ms": [8456.091], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a well-lit bedroom with two neatly made beds, each adorned with pillows and blankets. The room features a large window with a view of trees outside, and a door leading to another room.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20919.6, "ram_available_mb": 41921.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21003.3, "ram_available_mb": 41837.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.028}, "power_stats": {"power_gpu_soc_mean_watts": 20.904, "power_cpu_cv_mean_watts": 1.64, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 73.028, "power_watts_avg": 20.904, "energy_joules_est": 176.78, "duration_seconds": 8.457, "sample_count": 71}, "timestamp": "2026-01-26T10:16:30.093986"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8973.32, "latencies_ms": [8973.32], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The room is well-lit with warm lighting, featuring a combination of natural light from the window and artificial light from lamps and bedside tables. The walls are painted in a light color, and the wooden flooring adds a touch of warmth to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21003.3, "ram_available_mb": 41837.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21005.5, "ram_available_mb": 41835.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.227}, "power_stats": {"power_gpu_soc_mean_watts": 20.08, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 8.629, "gpu_utilization_percent_mean": 71.227, "power_watts_avg": 20.08, "energy_joules_est": 180.2, "duration_seconds": 8.974, "sample_count": 75}, "timestamp": "2026-01-26T10:16:41.102089"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11570.791, "latencies_ms": [11570.791], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is riding a motorcycle on a road, wearing a white helmet and a white and green racing suit. He is in motion, and the motorcycle is positioned in the center of the scene. There are several people standing on the side of the road, watching the man ride by. Some of them are closer to the motorcycle, while others are", "error": null, "sys_before": {"cpu_percent": 11.5, "ram_used_mb": 21005.5, "ram_available_mb": 41835.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20960.4, "ram_available_mb": 41880.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.465}, "power_stats": {"power_gpu_soc_mean_watts": 19.275, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 69.465, "power_watts_avg": 19.275, "energy_joules_est": 223.04, "duration_seconds": 11.571, "sample_count": 99}, "timestamp": "2026-01-26T10:16:54.724657"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8030.835, "latencies_ms": [8030.835], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "motorcycle: 1, rider: 1, helmet: 1, fence: 4, grass: 1, people: 4, road: 1, motorcycle brand: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20960.4, "ram_available_mb": 41880.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20943.8, "ram_available_mb": 41897.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.203}, "power_stats": {"power_gpu_soc_mean_watts": 20.837, "power_cpu_cv_mean_watts": 1.612, "power_sys_5v0_mean_watts": 8.575, "gpu_utilization_percent_mean": 74.203, "power_watts_avg": 20.837, "energy_joules_est": 167.35, "duration_seconds": 8.031, "sample_count": 69}, "timestamp": "2026-01-26T10:17:04.779190"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7333.51, "latencies_ms": [7333.51], "images_per_second": 0.136, "prompt_tokens": 44, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A motorcyclist is in the foreground, riding on the left side of the road. Spectators are gathered on the right side of the road, watching the rider from a distance.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20943.8, "ram_available_mb": 41897.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20991.4, "ram_available_mb": 41849.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.129}, "power_stats": {"power_gpu_soc_mean_watts": 21.28, "power_cpu_cv_mean_watts": 1.594, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 73.129, "power_watts_avg": 21.28, "energy_joules_est": 156.07, "duration_seconds": 7.334, "sample_count": 62}, "timestamp": "2026-01-26T10:17:14.126991"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7562.971, "latencies_ms": [7562.971], "images_per_second": 0.132, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A motorcyclist is seen riding a white motorcycle with green accents on a paved road. Spectators are gathered on the grassy side of the road, watching the rider.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20991.4, "ram_available_mb": 41849.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20910.9, "ram_available_mb": 41930.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.125}, "power_stats": {"power_gpu_soc_mean_watts": 21.44, "power_cpu_cv_mean_watts": 1.557, "power_sys_5v0_mean_watts": 8.574, "gpu_utilization_percent_mean": 74.125, "power_watts_avg": 21.44, "energy_joules_est": 162.16, "duration_seconds": 7.564, "sample_count": 64}, "timestamp": "2026-01-26T10:17:23.731555"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7321.309, "latencies_ms": [7321.309], "images_per_second": 0.137, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The motorcycle rider is wearing a white helmet and a white and green racing suit. The motorcycle is white with green accents and has the word \"PAJ\" written on it.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20910.9, "ram_available_mb": 41930.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20916.6, "ram_available_mb": 41924.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.5}, "power_stats": {"power_gpu_soc_mean_watts": 21.254, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 72.5, "power_watts_avg": 21.254, "energy_joules_est": 155.62, "duration_seconds": 7.322, "sample_count": 62}, "timestamp": "2026-01-26T10:17:33.070129"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11600.093, "latencies_ms": [11600.093], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene dining scene set in a dimly lit restaurant. The main focus is a table, elegantly dressed with a white tablecloth, which is adorned with a floral centerpiece. The centerpiece is a clear glass vase, filled with white flowers and green leaves, placed in the center of the table. \n\nFlank", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 20916.6, "ram_available_mb": 41924.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20921.5, "ram_available_mb": 41919.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.354}, "power_stats": {"power_gpu_soc_mean_watts": 19.31, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 70.354, "power_watts_avg": 19.31, "energy_joules_est": 224.02, "duration_seconds": 11.601, "sample_count": 99}, "timestamp": "2026-01-26T10:17:46.699980"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7908.416, "latencies_ms": [7908.416], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "table: 1\nglass: 3\nplate: 2\nknife: 2\nfork: 2\nspoon: 1\nvase: 1\nflower: 5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20921.5, "ram_available_mb": 41919.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21006.4, "ram_available_mb": 41834.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.164}, "power_stats": {"power_gpu_soc_mean_watts": 21.131, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 73.164, "power_watts_avg": 21.131, "energy_joules_est": 167.13, "duration_seconds": 7.909, "sample_count": 67}, "timestamp": "2026-01-26T10:17:56.635417"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11554.568, "latencies_ms": [11554.568], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a table set for a meal with a white tablecloth. On the table, there are three wine glasses, two of which are placed closer to the viewer and one further away, creating a sense of depth. The centerpiece is a vase with white flowers, positioned in the middle of the table, drawing the eye as the foc", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20934.5, "ram_available_mb": 41906.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20924.8, "ram_available_mb": 41916.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.42}, "power_stats": {"power_gpu_soc_mean_watts": 19.29, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.698, "gpu_utilization_percent_mean": 70.42, "power_watts_avg": 19.29, "energy_joules_est": 222.9, "duration_seconds": 11.555, "sample_count": 100}, "timestamp": "2026-01-26T10:18:10.230978"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9228.555, "latencies_ms": [9228.555], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image depicts a serene dining setting with a table elegantly set for a meal. The table is adorned with a white tablecloth, and there are three glass vases with white flowers in them, placed centrally on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20924.8, "ram_available_mb": 41916.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20929.1, "ram_available_mb": 41911.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.646}, "power_stats": {"power_gpu_soc_mean_watts": 20.531, "power_cpu_cv_mean_watts": 1.732, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 72.646, "power_watts_avg": 20.531, "energy_joules_est": 189.48, "duration_seconds": 9.229, "sample_count": 79}, "timestamp": "2026-01-26T10:18:21.512842"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10519.59, "latencies_ms": [10519.59], "images_per_second": 0.095, "prompt_tokens": 36, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image features a table setting with a central vase holding white flowers, illuminated by soft, warm lighting that creates a cozy atmosphere. The table is adorned with elegant glassware, including wine glasses and a decanter, and is set with white plates and silverware, suggesting a formal dining experience.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20929.1, "ram_available_mb": 41911.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20987.7, "ram_available_mb": 41853.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.865}, "power_stats": {"power_gpu_soc_mean_watts": 19.47, "power_cpu_cv_mean_watts": 1.844, "power_sys_5v0_mean_watts": 8.661, "gpu_utilization_percent_mean": 70.865, "power_watts_avg": 19.47, "energy_joules_est": 204.83, "duration_seconds": 10.52, "sample_count": 89}, "timestamp": "2026-01-26T10:18:34.046822"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12415.721, "latencies_ms": [12415.721], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment frozen in time, featuring a black and white photograph of a street clock. The clock, standing tall on a pole, is the central focus of the image. It's a hexagonal structure, with each of its six sides adorned with a clock face. Each face is marked with numbers from 1 to 12, indicating the hours.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20916.0, "ram_available_mb": 41924.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20974.9, "ram_available_mb": 41866.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.168}, "power_stats": {"power_gpu_soc_mean_watts": 21.516, "power_cpu_cv_mean_watts": 1.818, "power_sys_5v0_mean_watts": 8.917, "gpu_utilization_percent_mean": 73.168, "power_watts_avg": 21.516, "energy_joules_est": 267.15, "duration_seconds": 12.416, "sample_count": 107}, "timestamp": "2026-01-26T10:18:48.496239"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10145.859, "latencies_ms": [10145.859], "images_per_second": 0.099, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "Clock: 2\nGrass: 1\nPole: 1\nPerson: 1\nWheat: 1\nWheat field: 1\nWheat stalks: 1\nWheat heads: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20912.9, "ram_available_mb": 41928.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20932.7, "ram_available_mb": 41908.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.23}, "power_stats": {"power_gpu_soc_mean_watts": 22.627, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.789, "gpu_utilization_percent_mean": 76.23, "power_watts_avg": 22.627, "energy_joules_est": 229.59, "duration_seconds": 10.147, "sample_count": 87}, "timestamp": "2026-01-26T10:19:00.656412"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11630.934, "latencies_ms": [11630.934], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The clock is positioned in the foreground of the image, appearing larger and more detailed compared to the background. It is situated on the left side of the image, while the background features a blurred field of grass, creating a sense of depth. The clock is near the viewer, making it the main focus of the image.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20932.7, "ram_available_mb": 41908.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20970.7, "ram_available_mb": 41870.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.182}, "power_stats": {"power_gpu_soc_mean_watts": 21.992, "power_cpu_cv_mean_watts": 1.718, "power_sys_5v0_mean_watts": 8.881, "gpu_utilization_percent_mean": 74.182, "power_watts_avg": 21.992, "energy_joules_est": 255.8, "duration_seconds": 11.631, "sample_count": 99}, "timestamp": "2026-01-26T10:19:14.340489"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9001.714, "latencies_ms": [9001.714], "images_per_second": 0.111, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image features a unique, octagonal-shaped clock mounted on a pole, with a grassy field in the background. The clock has three faces, each displaying different times, creating an interesting visual effect.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20970.7, "ram_available_mb": 41870.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20954.8, "ram_available_mb": 41886.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.368}, "power_stats": {"power_gpu_soc_mean_watts": 23.143, "power_cpu_cv_mean_watts": 1.458, "power_sys_5v0_mean_watts": 8.764, "gpu_utilization_percent_mean": 76.368, "power_watts_avg": 23.143, "energy_joules_est": 208.34, "duration_seconds": 9.002, "sample_count": 76}, "timestamp": "2026-01-26T10:19:25.362075"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8619.863, "latencies_ms": [8619.863], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image is a black and white photograph featuring a street clock with three faces, each displaying the time. The clock is mounted on a pole and the background shows a field of crops under a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20954.8, "ram_available_mb": 41886.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.0, "ram_used_mb": 21028.9, "ram_available_mb": 41812.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.521}, "power_stats": {"power_gpu_soc_mean_watts": 23.173, "power_cpu_cv_mean_watts": 1.623, "power_sys_5v0_mean_watts": 8.83, "gpu_utilization_percent_mean": 77.521, "power_watts_avg": 23.173, "energy_joules_est": 199.76, "duration_seconds": 8.62, "sample_count": 73}, "timestamp": "2026-01-26T10:19:36.039611"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11547.618, "latencies_ms": [11547.618], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is skillfully riding a skateboard on a sidewalk. He is wearing a black shirt and a baseball cap, and his feet are firmly planted on the skateboard. The skateboard is positioned in the middle of the sidewalk, and the young man is leaning slightly forward as he rides.\n\nThe", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 21028.9, "ram_available_mb": 41812.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 21031.8, "ram_available_mb": 41809.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.101}, "power_stats": {"power_gpu_soc_mean_watts": 19.295, "power_cpu_cv_mean_watts": 2.018, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 70.101, "power_watts_avg": 19.295, "energy_joules_est": 222.82, "duration_seconds": 11.548, "sample_count": 99}, "timestamp": "2026-01-26T10:19:49.630340"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10659.051, "latencies_ms": [10659.051], "images_per_second": 0.094, "prompt_tokens": 39, "response_tokens_est": 69, "n_tiles": 16, "output_text": "- Skateboard: 1\n\n- Skate park: 1\n\n- Skateboarder: 1\n\n- Pants: 1\n\n- Cap: 1\n\n- T-shirt: 1\n\n- Graffiti: 1\n\n- Trees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20926.3, "ram_available_mb": 41914.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20991.9, "ram_available_mb": 41849.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.121}, "power_stats": {"power_gpu_soc_mean_watts": 19.781, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 71.121, "power_watts_avg": 19.781, "energy_joules_est": 210.86, "duration_seconds": 10.66, "sample_count": 91}, "timestamp": "2026-01-26T10:20:02.325701"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11188.993, "latencies_ms": [11188.993], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, performing a trick on a skateboard. The background features a concrete surface with graffiti and trees, indicating an outdoor skate park setting. The skateboarder is positioned near the center of the image, with ample space around him, suggesting he is the main focus of the scene.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20991.9, "ram_available_mb": 41849.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21021.8, "ram_available_mb": 41819.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.832}, "power_stats": {"power_gpu_soc_mean_watts": 19.534, "power_cpu_cv_mean_watts": 1.858, "power_sys_5v0_mean_watts": 8.666, "gpu_utilization_percent_mean": 70.832, "power_watts_avg": 19.534, "energy_joules_est": 218.58, "duration_seconds": 11.19, "sample_count": 95}, "timestamp": "2026-01-26T10:20:15.539927"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10655.757, "latencies_ms": [10655.757], "images_per_second": 0.094, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "A person is skateboarding on a concrete surface, possibly a skate park, with trees and a graffiti-covered wall in the background. The skateboarder is wearing a black t-shirt, grey pants, and a blue cap, and is performing a trick on the skateboard.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20941.8, "ram_available_mb": 41899.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20925.0, "ram_available_mb": 41915.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.824}, "power_stats": {"power_gpu_soc_mean_watts": 19.82, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.627, "gpu_utilization_percent_mean": 70.824, "power_watts_avg": 19.82, "energy_joules_est": 211.21, "duration_seconds": 10.656, "sample_count": 91}, "timestamp": "2026-01-26T10:20:28.209258"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8110.627, "latencies_ms": [8110.627], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image shows a person skateboarding on a concrete surface with a graffiti-covered wall in the background. The lighting is natural, suggesting it might be daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20925.0, "ram_available_mb": 41915.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21000.6, "ram_available_mb": 41840.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.768}, "power_stats": {"power_gpu_soc_mean_watts": 20.831, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.629, "gpu_utilization_percent_mean": 72.768, "power_watts_avg": 20.831, "energy_joules_est": 168.97, "duration_seconds": 8.111, "sample_count": 69}, "timestamp": "2026-01-26T10:20:38.353267"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12364.51, "latencies_ms": [12364.51], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a white plate filled with a variety of vegetables, including carrots, peas, and beets. The carrots are spread across the plate, with some positioned closer to the center and others towards the edges. The peas are located towards the right side of the plate, while the beets are placed on the far right side. \n\nIn addition", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20929.8, "ram_available_mb": 41911.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21018.2, "ram_available_mb": 41822.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.21}, "power_stats": {"power_gpu_soc_mean_watts": 21.348, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.825, "gpu_utilization_percent_mean": 73.21, "power_watts_avg": 21.348, "energy_joules_est": 263.97, "duration_seconds": 12.365, "sample_count": 105}, "timestamp": "2026-01-26T10:20:52.747856"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9611.004, "latencies_ms": [9611.004], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "carrots: 20, peas: 10, beets: 3, carrot peeler: 1, white bowl: 1, black container: 1, sink: 1, spoon: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20936.2, "ram_available_mb": 41904.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21004.8, "ram_available_mb": 41836.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.805}, "power_stats": {"power_gpu_soc_mean_watts": 22.459, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 8.763, "gpu_utilization_percent_mean": 75.805, "power_watts_avg": 22.459, "energy_joules_est": 215.87, "duration_seconds": 9.612, "sample_count": 82}, "timestamp": "2026-01-26T10:21:04.403676"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11492.041, "latencies_ms": [11492.041], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, there is a white plate filled with orange carrots, which are positioned towards the left side of the plate. To the right of the plate, there is a blue vegetable peeler lying on the countertop. In the background, there is a white container and a white cup, both placed further away from the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20923.5, "ram_available_mb": 41917.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21017.3, "ram_available_mb": 41823.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.351}, "power_stats": {"power_gpu_soc_mean_watts": 21.611, "power_cpu_cv_mean_watts": 1.778, "power_sys_5v0_mean_watts": 8.871, "gpu_utilization_percent_mean": 73.351, "power_watts_avg": 21.611, "energy_joules_est": 248.37, "duration_seconds": 11.493, "sample_count": 97}, "timestamp": "2026-01-26T10:21:17.931764"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9159.535, "latencies_ms": [9159.535], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A white plate filled with orange carrots is placed on a kitchen countertop, with a blue peeler and some green peas nearby. The scene suggests that someone is preparing a meal or snack using fresh vegetables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20920.4, "ram_available_mb": 41920.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21025.0, "ram_available_mb": 41815.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.544}, "power_stats": {"power_gpu_soc_mean_watts": 22.623, "power_cpu_cv_mean_watts": 1.545, "power_sys_5v0_mean_watts": 8.738, "gpu_utilization_percent_mean": 76.544, "power_watts_avg": 22.623, "energy_joules_est": 207.23, "duration_seconds": 9.16, "sample_count": 79}, "timestamp": "2026-01-26T10:21:29.152580"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8147.617, "latencies_ms": [8147.617], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image shows a bowl of orange carrots on a kitchen counter. The lighting in the image is bright and natural, coming from the left side, casting a shadow to the right of the objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20927.1, "ram_available_mb": 41913.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21014.1, "ram_available_mb": 41826.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.725}, "power_stats": {"power_gpu_soc_mean_watts": 22.804, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 8.837, "gpu_utilization_percent_mean": 75.725, "power_watts_avg": 22.804, "energy_joules_est": 185.81, "duration_seconds": 8.148, "sample_count": 69}, "timestamp": "2026-01-26T10:21:39.364506"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11593.369, "latencies_ms": [11593.369], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing on a stage, dressed in a suit and tie, and appears to be giving a speech or presentation. He is holding a microphone in his hand, and there are people in the audience watching him. The audience members are seated in chairs, and there are two bottles on a table in the background. The scene suggests a formal event or conference", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20917.3, "ram_available_mb": 41923.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21009.6, "ram_available_mb": 41831.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.636}, "power_stats": {"power_gpu_soc_mean_watts": 19.323, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 69.636, "power_watts_avg": 19.323, "energy_joules_est": 224.03, "duration_seconds": 11.594, "sample_count": 99}, "timestamp": "2026-01-26T10:21:53.030284"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9705.35, "latencies_ms": [9705.35], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "1. Screen: 1\n2. Man: 1\n3. Microphone: 1\n4. Stage: 1\n5. Audience: 2\n6. Projector: 1\n7. Camera: 1\n8. Screen: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20932.3, "ram_available_mb": 41908.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20983.7, "ram_available_mb": 41857.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.988}, "power_stats": {"power_gpu_soc_mean_watts": 20.228, "power_cpu_cv_mean_watts": 1.737, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 71.988, "power_watts_avg": 20.228, "energy_joules_est": 196.33, "duration_seconds": 9.706, "sample_count": 82}, "timestamp": "2026-01-26T10:22:04.762031"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10679.231, "latencies_ms": [10679.231], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "In the foreground, there is a person with red hair sitting in front of a large screen displaying a man in a suit. The man in the suit is positioned in the center of the screen, appearing to be giving a presentation. The audience member with red hair is seated in the front row, facing the screen and the presenter.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20923.5, "ram_available_mb": 41917.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20976.4, "ram_available_mb": 41864.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.725}, "power_stats": {"power_gpu_soc_mean_watts": 19.468, "power_cpu_cv_mean_watts": 1.856, "power_sys_5v0_mean_watts": 8.653, "gpu_utilization_percent_mean": 70.725, "power_watts_avg": 19.468, "energy_joules_est": 207.92, "duration_seconds": 10.68, "sample_count": 91}, "timestamp": "2026-01-26T10:22:17.465469"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10100.439, "latencies_ms": [10100.439], "images_per_second": 0.099, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image captures a moment from a presentation or speech, where a man in a suit is standing on a stage, addressing an audience. The audience is seated in front of a large screen displaying a colorful geometric pattern, and the man appears to be gesturing with his hand as he speaks.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20976.4, "ram_available_mb": 41864.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20986.2, "ram_available_mb": 41854.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.779}, "power_stats": {"power_gpu_soc_mean_watts": 19.846, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 71.779, "power_watts_avg": 19.846, "energy_joules_est": 200.47, "duration_seconds": 10.101, "sample_count": 86}, "timestamp": "2026-01-26T10:22:29.592082"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7766.843, "latencies_ms": [7766.843], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a person on a stage with a blue background and a colorful geometric design on the right side. The stage lighting casts a warm glow on the speaker, highlighting their suit and gestures.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20924.4, "ram_available_mb": 41916.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21011.7, "ram_available_mb": 41829.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.708}, "power_stats": {"power_gpu_soc_mean_watts": 21.023, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 72.708, "power_watts_avg": 21.023, "energy_joules_est": 163.3, "duration_seconds": 7.768, "sample_count": 65}, "timestamp": "2026-01-26T10:22:39.377116"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11607.869, "latencies_ms": [11607.869], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features two men sitting on a bench in a parking lot, with a motorcycle parked nearby. One man is wearing a blue shirt and the other is wearing a white shirt. They appear to be having a conversation while sitting on the bench. \n\nThere are two motorcycles in the scene, one parked closer to the left side of", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20939.8, "ram_available_mb": 41901.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20997.2, "ram_available_mb": 41843.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.02}, "power_stats": {"power_gpu_soc_mean_watts": 19.295, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.673, "gpu_utilization_percent_mean": 70.02, "power_watts_avg": 19.295, "energy_joules_est": 223.99, "duration_seconds": 11.609, "sample_count": 98}, "timestamp": "2026-01-26T10:22:53.027940"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9941.393, "latencies_ms": [9941.393], "images_per_second": 0.101, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Motorcycle: 2\n2. Scooter: 2\n3. Chair: 1\n4. Pole: 1\n5. Sign: 1\n6. Building: 1\n7. Tree: 1\n8. Trash can: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20935.5, "ram_available_mb": 41905.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20929.7, "ram_available_mb": 41911.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.643}, "power_stats": {"power_gpu_soc_mean_watts": 20.141, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 8.602, "gpu_utilization_percent_mean": 71.643, "power_watts_avg": 20.141, "energy_joules_est": 200.24, "duration_seconds": 9.942, "sample_count": 84}, "timestamp": "2026-01-26T10:23:05.005506"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11579.87, "latencies_ms": [11579.87], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a man sitting on a chair, facing another man who is standing and holding a shovel. The shovel is positioned near the man standing. In the background, there are two motorcycles parked on the left side of the image, and a building with a large advertisement on its facade. The advertisement is located on", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20929.7, "ram_available_mb": 41911.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21002.4, "ram_available_mb": 41838.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.687}, "power_stats": {"power_gpu_soc_mean_watts": 19.31, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.689, "gpu_utilization_percent_mean": 70.687, "power_watts_avg": 19.31, "energy_joules_est": 223.62, "duration_seconds": 11.58, "sample_count": 99}, "timestamp": "2026-01-26T10:23:18.613697"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9928.603, "latencies_ms": [9928.603], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 63, "n_tiles": 16, "output_text": "Two men are sitting on a bench in front of a building with a large advertisement on the wall. One man is wearing a blue uniform and the other is wearing a white shirt and blue jeans. There are two motorcycles parked on the sidewalk next to them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.7, "ram_available_mb": 41900.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21020.8, "ram_available_mb": 41820.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.071}, "power_stats": {"power_gpu_soc_mean_watts": 20.117, "power_cpu_cv_mean_watts": 1.77, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 70.071, "power_watts_avg": 20.117, "energy_joules_est": 199.75, "duration_seconds": 9.929, "sample_count": 85}, "timestamp": "2026-01-26T10:23:30.585338"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8579.889, "latencies_ms": [8579.889], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a street scene with a blue and white sign with a large letter \"L\" on it. There are two men sitting on a bench, one wearing a blue uniform and the other wearing a white shirt and blue jeans.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20915.5, "ram_available_mb": 41925.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20911.3, "ram_available_mb": 41929.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.068}, "power_stats": {"power_gpu_soc_mean_watts": 20.348, "power_cpu_cv_mean_watts": 1.725, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 72.068, "power_watts_avg": 20.348, "energy_joules_est": 174.6, "duration_seconds": 8.581, "sample_count": 74}, "timestamp": "2026-01-26T10:23:41.217627"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10764.281, "latencies_ms": [10764.281], "images_per_second": 0.093, "prompt_tokens": 24, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows a black plastic container filled with a green, chunky vegetable dish, likely a curry, placed on a white paper plate. Beside it, there's a pile of shredded chicken. The plate is on a kitchen counter with a granite-like surface.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 20911.3, "ram_available_mb": 41929.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20951.8, "ram_available_mb": 41889.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.076}, "power_stats": {"power_gpu_soc_mean_watts": 21.981, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 8.935, "gpu_utilization_percent_mean": 73.076, "power_watts_avg": 21.981, "energy_joules_est": 236.63, "duration_seconds": 10.765, "sample_count": 92}, "timestamp": "2026-01-26T10:23:54.041848"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9442.697, "latencies_ms": [9442.697], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "plate: 1, bowl: 1, fork: 1, chicken: 1, shredded chicken: 1, broccoli: 1, sauce: 1, container: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20951.8, "ram_available_mb": 41889.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21039.0, "ram_available_mb": 41801.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.432}, "power_stats": {"power_gpu_soc_mean_watts": 22.944, "power_cpu_cv_mean_watts": 1.517, "power_sys_5v0_mean_watts": 8.764, "gpu_utilization_percent_mean": 77.432, "power_watts_avg": 22.944, "energy_joules_est": 216.67, "duration_seconds": 9.443, "sample_count": 81}, "timestamp": "2026-01-26T10:24:05.507711"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12322.978, "latencies_ms": [12322.978], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground of the image, there is a pile of shredded chicken on the left side, which is closer to the viewer. Behind it, on the right side, there is a black container filled with a green sauce and pieces of broccoli. The container is placed on top of the plate, which is on the table.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20912.2, "ram_available_mb": 41928.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20928.3, "ram_available_mb": 41912.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.349}, "power_stats": {"power_gpu_soc_mean_watts": 21.769, "power_cpu_cv_mean_watts": 1.767, "power_sys_5v0_mean_watts": 8.896, "gpu_utilization_percent_mean": 73.349, "power_watts_avg": 21.769, "energy_joules_est": 268.27, "duration_seconds": 12.324, "sample_count": 106}, "timestamp": "2026-01-26T10:24:19.849662"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9709.402, "latencies_ms": [9709.402], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a meal consisting of shredded chicken and a green vegetable dish, possibly a curry, served on a white paper plate with a fork on the side. The meal is presented on a kitchen countertop.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20928.3, "ram_available_mb": 41912.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21024.4, "ram_available_mb": 41816.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.305}, "power_stats": {"power_gpu_soc_mean_watts": 22.727, "power_cpu_cv_mean_watts": 1.532, "power_sys_5v0_mean_watts": 8.78, "gpu_utilization_percent_mean": 76.305, "power_watts_avg": 22.727, "energy_joules_est": 220.68, "duration_seconds": 9.71, "sample_count": 82}, "timestamp": "2026-01-26T10:24:31.602704"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11523.437, "latencies_ms": [11523.437], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image shows a meal consisting of shredded chicken and a green sauce with chunks of vegetables, possibly in a black takeout container on a white paper plate with a fork on the side. The lighting appears to be artificial, and the surface on which the plate is placed looks like a kitchen countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21024.4, "ram_available_mb": 41816.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20946.5, "ram_available_mb": 41894.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.531}, "power_stats": {"power_gpu_soc_mean_watts": 21.908, "power_cpu_cv_mean_watts": 1.715, "power_sys_5v0_mean_watts": 8.88, "gpu_utilization_percent_mean": 74.531, "power_watts_avg": 21.908, "energy_joules_est": 252.47, "duration_seconds": 11.524, "sample_count": 98}, "timestamp": "2026-01-26T10:24:45.140375"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11597.065, "latencies_ms": [11597.065], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a man who is the main subject. He is wearing a blue shirt and a red tie, and he is also wearing a gray and white plaid hat. The man is smiling and looking directly at the camera. The background of the image is a building with a green roof and a white wall. The man is standing in front of this building.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20946.5, "ram_available_mb": 41894.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20919.1, "ram_available_mb": 41921.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.323}, "power_stats": {"power_gpu_soc_mean_watts": 19.338, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.68, "gpu_utilization_percent_mean": 70.323, "power_watts_avg": 19.338, "energy_joules_est": 224.28, "duration_seconds": 11.598, "sample_count": 99}, "timestamp": "2026-01-26T10:24:58.770581"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11775.703, "latencies_ms": [11775.703], "images_per_second": 0.085, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "1. Building: 1 visible in the background.\n2. Chair: 1 visible in the background.\n3. Hat: 1 visible on the person's head.\n4. Glasses: 1 pair of glasses on the person's face.\n5. Shirt: 1 blue shirt on the person.\n6. Tie:", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20919.1, "ram_available_mb": 41921.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20986.0, "ram_available_mb": 41854.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.317}, "power_stats": {"power_gpu_soc_mean_watts": 19.439, "power_cpu_cv_mean_watts": 1.866, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 70.317, "power_watts_avg": 19.439, "energy_joules_est": 228.92, "duration_seconds": 11.776, "sample_count": 101}, "timestamp": "2026-01-26T10:25:12.585798"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9859.005, "latencies_ms": [9859.005], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The man is positioned in the foreground of the image, appearing to be the main subject. He is standing in front of a building with windows, which serves as the background. The man is wearing a hat and glasses, and he is smiling, indicating a positive and friendly demeanor.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20907.6, "ram_available_mb": 41933.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20996.3, "ram_available_mb": 41844.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.143}, "power_stats": {"power_gpu_soc_mean_watts": 19.846, "power_cpu_cv_mean_watts": 1.791, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 71.143, "power_watts_avg": 19.846, "energy_joules_est": 195.67, "duration_seconds": 9.86, "sample_count": 84}, "timestamp": "2026-01-26T10:25:24.458121"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6304.486, "latencies_ms": [6304.486], "images_per_second": 0.159, "prompt_tokens": 37, "response_tokens_est": 31, "n_tiles": 16, "output_text": "A man is standing outdoors, wearing a blue shirt and a red tie. He is smiling and looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20996.3, "ram_available_mb": 41844.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21003.5, "ram_available_mb": 41837.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.321}, "power_stats": {"power_gpu_soc_mean_watts": 22.301, "power_cpu_cv_mean_watts": 1.374, "power_sys_5v0_mean_watts": 8.541, "gpu_utilization_percent_mean": 76.321, "power_watts_avg": 22.301, "energy_joules_est": 140.61, "duration_seconds": 6.305, "sample_count": 53}, "timestamp": "2026-01-26T10:25:32.827156"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7789.564, "latencies_ms": [7789.564], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image features a person wearing a blue shirt and a red tie, with a plaid hat on their head. The lighting appears to be natural, suggesting the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20906.7, "ram_available_mb": 41934.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20917.9, "ram_available_mb": 41923.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.5}, "power_stats": {"power_gpu_soc_mean_watts": 20.899, "power_cpu_cv_mean_watts": 1.649, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 72.5, "power_watts_avg": 20.899, "energy_joules_est": 162.81, "duration_seconds": 7.79, "sample_count": 66}, "timestamp": "2026-01-26T10:25:42.629110"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12423.083, "latencies_ms": [12423.083], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a collage of six photographs showcasing a pizza with various toppings. The pizza appears to be freshly baked, with a golden-brown crust and a generous amount of melted cheese. The toppings include what looks like tomato sauce, chunks of meat, and possibly some herbs or spices. The p", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20917.9, "ram_available_mb": 41923.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21018.6, "ram_available_mb": 41822.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.198}, "power_stats": {"power_gpu_soc_mean_watts": 21.52, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.931, "gpu_utilization_percent_mean": 73.198, "power_watts_avg": 21.52, "energy_joules_est": 267.36, "duration_seconds": 12.424, "sample_count": 106}, "timestamp": "2026-01-26T10:25:57.095185"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9102.865, "latencies_ms": [9102.865], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "cheese: 6, tomato: 4, crust: 6, plate: 6, fork: 2, bite: 2, sauce: 2, crust: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21018.6, "ram_available_mb": 41822.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20998.1, "ram_available_mb": 41842.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.532}, "power_stats": {"power_gpu_soc_mean_watts": 23.22, "power_cpu_cv_mean_watts": 1.471, "power_sys_5v0_mean_watts": 8.783, "gpu_utilization_percent_mean": 77.532, "power_watts_avg": 23.22, "energy_joules_est": 211.38, "duration_seconds": 9.104, "sample_count": 77}, "timestamp": "2026-01-26T10:26:08.230434"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12705.232, "latencies_ms": [12705.232], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a slice of pizza on the left side of the image, which is positioned closer to the viewer than the other slices. The right side of the image shows a slice of pizza that is further away, with the rest of the pizza visible in the background. The pizza slice in the bottom left corner is the farthest from the", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20917.9, "ram_available_mb": 41923.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20974.5, "ram_available_mb": 41866.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.778}, "power_stats": {"power_gpu_soc_mean_watts": 21.674, "power_cpu_cv_mean_watts": 1.79, "power_sys_5v0_mean_watts": 8.865, "gpu_utilization_percent_mean": 73.778, "power_watts_avg": 21.674, "energy_joules_est": 275.39, "duration_seconds": 12.706, "sample_count": 108}, "timestamp": "2026-01-26T10:26:22.996044"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 12946.582, "latencies_ms": [12946.582], "images_per_second": 0.077, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a collage of six photographs showing a pizza with various toppings being eaten. The pizza appears to be freshly baked with a golden-brown crust, and the toppings include cheese, tomato sauce, and possibly some herbs and vegetables. The photographs are arranged in a 2x3 grid, with each photo showing", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20909.4, "ram_available_mb": 41931.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20988.8, "ram_available_mb": 41852.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.191}, "power_stats": {"power_gpu_soc_mean_watts": 21.768, "power_cpu_cv_mean_watts": 1.746, "power_sys_5v0_mean_watts": 8.842, "gpu_utilization_percent_mean": 74.191, "power_watts_avg": 21.768, "energy_joules_est": 281.84, "duration_seconds": 12.947, "sample_count": 110}, "timestamp": "2026-01-26T10:26:37.967249"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12677.423, "latencies_ms": [12677.423], "images_per_second": 0.079, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image displays a collage of six different photographs of a pizza, each showing varying degrees of consumption. The pizza appears to have a golden-brown crust, topped with melted cheese, tomato sauce, and possibly some herbs and vegetables. The lighting in the images is warm, suggesting an indoor setting with artificial light, and the", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20932.1, "ram_available_mb": 41908.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21019.2, "ram_available_mb": 41821.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.111}, "power_stats": {"power_gpu_soc_mean_watts": 21.652, "power_cpu_cv_mean_watts": 1.782, "power_sys_5v0_mean_watts": 8.875, "gpu_utilization_percent_mean": 74.111, "power_watts_avg": 21.652, "energy_joules_est": 274.51, "duration_seconds": 12.678, "sample_count": 108}, "timestamp": "2026-01-26T10:26:52.666623"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11598.512, "latencies_ms": [11598.512], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two young girls are standing next to each other, petting a black goat. The goat is looking up at the girls, seemingly enjoying the attention. The girls are wearing colorful dresses, adding a vibrant touch to the scene.\n\nThere are two other goats in the background, one closer to the left side of the image and", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 20913.9, "ram_available_mb": 41927.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21007.2, "ram_available_mb": 41833.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.535}, "power_stats": {"power_gpu_soc_mean_watts": 19.358, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.684, "gpu_utilization_percent_mean": 70.535, "power_watts_avg": 19.358, "energy_joules_est": 224.54, "duration_seconds": 11.599, "sample_count": 99}, "timestamp": "2026-01-26T10:27:06.341271"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9938.46, "latencies_ms": [9938.46], "images_per_second": 0.101, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Girl: 2\n2. Goat: 2\n3. Fence: 1\n4. Grass: 1\n5. Dress: 2\n6. Pole: 1\n7. Trees: 1\n8. Sky: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20918.7, "ram_available_mb": 41922.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20936.3, "ram_available_mb": 41904.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.631}, "power_stats": {"power_gpu_soc_mean_watts": 20.136, "power_cpu_cv_mean_watts": 1.767, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 71.631, "power_watts_avg": 20.136, "energy_joules_est": 200.13, "duration_seconds": 9.939, "sample_count": 84}, "timestamp": "2026-01-26T10:27:18.312959"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11605.384, "latencies_ms": [11605.384], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a black goat with its head tilted upwards, seemingly interacting with a person whose hand is gently touching its head. In the background, there are two young girls, one wearing a floral dress and the other in a blue dress with a pink bow, both standing behind a fence and looking at the goat.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20936.3, "ram_available_mb": 41904.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21002.0, "ram_available_mb": 41838.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.541}, "power_stats": {"power_gpu_soc_mean_watts": 19.343, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 70.541, "power_watts_avg": 19.343, "energy_joules_est": 224.5, "duration_seconds": 11.606, "sample_count": 98}, "timestamp": "2026-01-26T10:27:31.937815"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5746.754, "latencies_ms": [5746.754], "images_per_second": 0.174, "prompt_tokens": 37, "response_tokens_est": 26, "n_tiles": 16, "output_text": "Two young girls are petting a black goat in a fenced area, likely at a petting zoo or farm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20914.7, "ram_available_mb": 41926.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21001.7, "ram_available_mb": 41839.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.458}, "power_stats": {"power_gpu_soc_mean_watts": 22.981, "power_cpu_cv_mean_watts": 1.267, "power_sys_5v0_mean_watts": 8.524, "gpu_utilization_percent_mean": 77.458, "power_watts_avg": 22.981, "energy_joules_est": 132.08, "duration_seconds": 5.747, "sample_count": 48}, "timestamp": "2026-01-26T10:27:39.743193"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11581.985, "latencies_ms": [11581.985], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The image features two young girls in colorful dresses, one in pink with a floral pattern and the other in blue with a strawberry pattern, standing in front of a black goat with white horns. The goat is looking up at the girls with curiosity. The background shows a fenced area with trees and a clear blue sky, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20913.3, "ram_available_mb": 41927.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 20940.2, "ram_available_mb": 41900.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.031}, "power_stats": {"power_gpu_soc_mean_watts": 19.279, "power_cpu_cv_mean_watts": 1.976, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 70.031, "power_watts_avg": 19.279, "energy_joules_est": 223.3, "duration_seconds": 11.583, "sample_count": 98}, "timestamp": "2026-01-26T10:27:53.338820"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11596.242, "latencies_ms": [11596.242], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene evening scene at a traffic intersection. The sky, painted in hues of deep blue, serves as a backdrop to the silhouette of a mountain range. The mountains, bathed in the soft glow of the setting sun, add a sense of tranquility to the scene.\n\nIn the foreground, a traffic light stands tall, its", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20940.2, "ram_available_mb": 41900.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 9.5, "ram_used_mb": 21023.8, "ram_available_mb": 41817.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.525}, "power_stats": {"power_gpu_soc_mean_watts": 19.334, "power_cpu_cv_mean_watts": 2.098, "power_sys_5v0_mean_watts": 8.686, "gpu_utilization_percent_mean": 70.525, "power_watts_avg": 19.334, "energy_joules_est": 224.21, "duration_seconds": 11.597, "sample_count": 99}, "timestamp": "2026-01-26T10:28:06.959165"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11773.992, "latencies_ms": [11773.992], "images_per_second": 0.085, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "traffic light: 2, street sign: 1, traffic light: 1, street sign: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light: 1, traffic light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20946.9, "ram_available_mb": 41894.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 10.1, "ram_used_mb": 21006.7, "ram_available_mb": 41834.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.84}, "power_stats": {"power_gpu_soc_mean_watts": 19.449, "power_cpu_cv_mean_watts": 2.133, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 70.84, "power_watts_avg": 19.449, "energy_joules_est": 229.0, "duration_seconds": 11.775, "sample_count": 100}, "timestamp": "2026-01-26T10:28:20.753922"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8134.229, "latencies_ms": [8134.229], "images_per_second": 0.123, "prompt_tokens": 44, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The traffic lights are positioned in the foreground on the right side of the image, while the sun is setting in the background on the left side. The streetlights are closer to the viewer than the mountains in the distance.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20926.5, "ram_available_mb": 41914.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21035.4, "ram_available_mb": 41805.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.464}, "power_stats": {"power_gpu_soc_mean_watts": 20.768, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 72.464, "power_watts_avg": 20.768, "energy_joules_est": 168.94, "duration_seconds": 8.135, "sample_count": 69}, "timestamp": "2026-01-26T10:28:30.922481"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9606.434, "latencies_ms": [9606.434], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image captures a traffic light at dusk with a green light illuminated, indicating that vehicles in the lane it is facing can proceed. The sky is darkening, suggesting that it is either early morning or late evening, and the street is relatively empty with no visible traffic.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20940.9, "ram_available_mb": 41900.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20988.1, "ram_available_mb": 41852.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.086}, "power_stats": {"power_gpu_soc_mean_watts": 20.29, "power_cpu_cv_mean_watts": 1.734, "power_sys_5v0_mean_watts": 8.586, "gpu_utilization_percent_mean": 72.086, "power_watts_avg": 20.29, "energy_joules_est": 194.93, "duration_seconds": 9.607, "sample_count": 81}, "timestamp": "2026-01-26T10:28:42.578894"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9873.49, "latencies_ms": [9873.49], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image captures a night scene with a clear sky transitioning from a deep blue to a lighter shade, indicating it might be dusk. The street lights are on, casting a warm glow on the road and illuminating the traffic lights, which are currently green, allowing vehicles to proceed.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20933.0, "ram_available_mb": 41907.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21006.8, "ram_available_mb": 41834.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.298}, "power_stats": {"power_gpu_soc_mean_watts": 19.657, "power_cpu_cv_mean_watts": 1.791, "power_sys_5v0_mean_watts": 8.629, "gpu_utilization_percent_mean": 70.298, "power_watts_avg": 19.657, "energy_joules_est": 194.1, "duration_seconds": 9.874, "sample_count": 84}, "timestamp": "2026-01-26T10:28:54.511088"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 9303.96, "latencies_ms": [9303.96], "images_per_second": 0.107, "prompt_tokens": 24, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a smiling woman standing behind a table laden with bunches of bananas. She is wearing a patterned dress and is in a building with a yellow wall and a sign that reads 'BANANAS'. The bananas are yellow and appear ripe.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20928.0, "ram_available_mb": 41912.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20929.1, "ram_available_mb": 41911.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.038}, "power_stats": {"power_gpu_soc_mean_watts": 20.227, "power_cpu_cv_mean_watts": 1.753, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 72.038, "power_watts_avg": 20.227, "energy_joules_est": 188.2, "duration_seconds": 9.305, "sample_count": 79}, "timestamp": "2026-01-26T10:29:05.843623"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9365.274, "latencies_ms": [9365.274], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 58, "n_tiles": 16, "output_text": "bananas: 2 bunches, approximately 20-30 bananas each\nwoman: 1\nbuilding: 1\nred sign: 1\nblue sign: 1\nwindow: 1\ndoor: 1\nother people: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20929.1, "ram_available_mb": 41911.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20929.3, "ram_available_mb": 41911.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.848}, "power_stats": {"power_gpu_soc_mean_watts": 20.358, "power_cpu_cv_mean_watts": 1.727, "power_sys_5v0_mean_watts": 8.609, "gpu_utilization_percent_mean": 71.848, "power_watts_avg": 20.358, "energy_joules_est": 190.67, "duration_seconds": 9.366, "sample_count": 79}, "timestamp": "2026-01-26T10:29:17.247534"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11555.958, "latencies_ms": [11555.958], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large bunch of bananas placed on a wooden surface, which is the main focus of the image. The person is standing behind the bananas, slightly to the right from the perspective of the viewer, and appears to be interacting with them. In the background, there is a building with a sign on it, and another individual is partially visible behind the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20929.3, "ram_available_mb": 41911.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20981.9, "ram_available_mb": 41859.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.465}, "power_stats": {"power_gpu_soc_mean_watts": 19.298, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 69.465, "power_watts_avg": 19.298, "energy_joules_est": 223.02, "duration_seconds": 11.557, "sample_count": 99}, "timestamp": "2026-01-26T10:29:30.822843"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9487.723, "latencies_ms": [9487.723], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "A woman is standing behind a table laden with bunches of bananas, likely at a market or a stall. The background shows a building with a sign that reads \"BANANAS,\" suggesting that this is a place where bananas are sold or a themed establishment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20981.9, "ram_available_mb": 41859.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20982.4, "ram_available_mb": 41858.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.049}, "power_stats": {"power_gpu_soc_mean_watts": 20.345, "power_cpu_cv_mean_watts": 1.734, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 72.049, "power_watts_avg": 20.345, "energy_joules_est": 193.04, "duration_seconds": 9.488, "sample_count": 81}, "timestamp": "2026-01-26T10:29:42.334222"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8393.295, "latencies_ms": [8393.295], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image features a bright yellow banana bunch in the foreground, with a woman smiling in the background. The setting appears to be an outdoor market with a building that has a yellow signboard and a red circular sign on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20919.8, "ram_available_mb": 41921.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20975.7, "ram_available_mb": 41865.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.514}, "power_stats": {"power_gpu_soc_mean_watts": 20.281, "power_cpu_cv_mean_watts": 1.706, "power_sys_5v0_mean_watts": 8.596, "gpu_utilization_percent_mean": 71.514, "power_watts_avg": 20.281, "energy_joules_est": 170.24, "duration_seconds": 8.394, "sample_count": 72}, "timestamp": "2026-01-26T10:29:52.769020"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12361.451, "latencies_ms": [12361.451], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene on a city street. Dominating the view is a three-story brick building, its facade a vibrant green. The building is adorned with a fire escape on the second floor, a common sight in urban architecture. The ground floor features a green door, above which hang two garage doors, their surfaces a canvas for graffiti. The", "error": null, "sys_before": {"cpu_percent": 9.4, "ram_used_mb": 20922.0, "ram_available_mb": 41918.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20919.7, "ram_available_mb": 41921.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.657}, "power_stats": {"power_gpu_soc_mean_watts": 21.684, "power_cpu_cv_mean_watts": 1.822, "power_sys_5v0_mean_watts": 8.936, "gpu_utilization_percent_mean": 72.657, "power_watts_avg": 21.684, "energy_joules_est": 268.06, "duration_seconds": 12.362, "sample_count": 105}, "timestamp": "2026-01-26T10:30:07.173442"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11078.272, "latencies_ms": [11078.272], "images_per_second": 0.09, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "- Fire hydrant: 1\n\n- Door: 2\n\n- Graffiti: 5\n\n- Windows: 4\n\n- Bike: 1\n\n- Tree: 1\n\n- Trash can: 1\n\n- Sign: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20919.7, "ram_available_mb": 41921.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20964.2, "ram_available_mb": 41876.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.526}, "power_stats": {"power_gpu_soc_mean_watts": 22.243, "power_cpu_cv_mean_watts": 1.639, "power_sys_5v0_mean_watts": 8.808, "gpu_utilization_percent_mean": 75.526, "power_watts_avg": 22.243, "energy_joules_est": 246.43, "duration_seconds": 11.079, "sample_count": 95}, "timestamp": "2026-01-26T10:30:20.288284"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11993.594, "latencies_ms": [11993.594], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The fire hydrant is located in the foreground on the left side of the image, near the sidewalk. The building with graffiti is in the background, behind the fire hydrant. The green door is on the right side of the building, and the graffiti is spread across the garage doors on both sides of the green door.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20964.2, "ram_available_mb": 41876.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21004.4, "ram_available_mb": 41836.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.49}, "power_stats": {"power_gpu_soc_mean_watts": 21.845, "power_cpu_cv_mean_watts": 1.734, "power_sys_5v0_mean_watts": 8.881, "gpu_utilization_percent_mean": 73.49, "power_watts_avg": 21.845, "energy_joules_est": 262.01, "duration_seconds": 11.994, "sample_count": 102}, "timestamp": "2026-01-26T10:30:34.336236"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8869.917, "latencies_ms": [8869.917], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image depicts a red brick building with a green awning and a fire hydrant on the sidewalk in front of it. The building has multiple windows and a green fire escape on the second floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21004.4, "ram_available_mb": 41836.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 20940.2, "ram_available_mb": 41900.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.293}, "power_stats": {"power_gpu_soc_mean_watts": 23.279, "power_cpu_cv_mean_watts": 1.451, "power_sys_5v0_mean_watts": 8.772, "gpu_utilization_percent_mean": 77.293, "power_watts_avg": 23.279, "energy_joules_est": 206.5, "duration_seconds": 8.87, "sample_count": 75}, "timestamp": "2026-01-26T10:30:45.233117"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8844.526, "latencies_ms": [8844.526], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a red brick building with a green awning and a fire escape on the second floor. The garage doors on the ground floor are covered in various graffiti tags in blue, red, and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.2, "ram_available_mb": 41900.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20971.2, "ram_available_mb": 41869.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.213}, "power_stats": {"power_gpu_soc_mean_watts": 23.078, "power_cpu_cv_mean_watts": 1.521, "power_sys_5v0_mean_watts": 8.865, "gpu_utilization_percent_mean": 77.213, "power_watts_avg": 23.078, "energy_joules_est": 204.13, "duration_seconds": 8.845, "sample_count": 75}, "timestamp": "2026-01-26T10:30:56.130590"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11573.858, "latencies_ms": [11573.858], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man with long, curly hair is the main focus. He is wearing a black shirt and a blue and white striped beanie. The man is holding a bright yellow frisbee in his right hand, preparing to throw it. The background is a simple, unadorned wall. The man's expression is serious, suggesting he is", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20971.2, "ram_available_mb": 41869.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20983.6, "ram_available_mb": 41857.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.939}, "power_stats": {"power_gpu_soc_mean_watts": 19.371, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 70.939, "power_watts_avg": 19.371, "energy_joules_est": 224.21, "duration_seconds": 11.574, "sample_count": 99}, "timestamp": "2026-01-26T10:31:09.756413"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8085.442, "latencies_ms": [8085.442], "images_per_second": 0.124, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "person: 1, camera: 1, yellow frisbee: 1, wall: 1, room: 1, ceiling: 1, light fixture: 1, audience: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20924.9, "ram_available_mb": 41916.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20996.8, "ram_available_mb": 41844.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.059}, "power_stats": {"power_gpu_soc_mean_watts": 21.215, "power_cpu_cv_mean_watts": 1.624, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 73.059, "power_watts_avg": 21.215, "energy_joules_est": 171.55, "duration_seconds": 8.086, "sample_count": 68}, "timestamp": "2026-01-26T10:31:19.868920"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11353.552, "latencies_ms": [11353.552], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The person is in the foreground, holding a yellow frisbee with their right hand, which is extended towards the camera. In the background, there are indistinct figures of people, suggesting they are further away from the camera's perspective. The wall behind the person is plain and occupies the upper part of the image, indicating it is behind the person.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 20996.8, "ram_available_mb": 41844.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20972.7, "ram_available_mb": 41868.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.722}, "power_stats": {"power_gpu_soc_mean_watts": 19.33, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.663, "gpu_utilization_percent_mean": 69.722, "power_watts_avg": 19.33, "energy_joules_est": 219.48, "duration_seconds": 11.354, "sample_count": 97}, "timestamp": "2026-01-26T10:31:33.272933"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8157.868, "latencies_ms": [8157.868], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A person with long curly hair is holding a neon green frisbee in a dimly lit indoor setting, possibly a room or a hallway. The person's face is not visible in the image.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20972.7, "ram_available_mb": 41868.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20972.7, "ram_available_mb": 41868.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.086}, "power_stats": {"power_gpu_soc_mean_watts": 20.728, "power_cpu_cv_mean_watts": 1.641, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 73.086, "power_watts_avg": 20.728, "energy_joules_est": 169.11, "duration_seconds": 8.158, "sample_count": 70}, "timestamp": "2026-01-26T10:31:43.464561"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9142.485, "latencies_ms": [9142.485], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image features a person with long, curly hair wearing a black shirt and a blue and white striped headband. The person is holding a bright yellow frisbee in a dimly lit indoor setting, with a crowd of people visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20911.7, "ram_available_mb": 41929.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20986.8, "ram_available_mb": 41854.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.646}, "power_stats": {"power_gpu_soc_mean_watts": 20.151, "power_cpu_cv_mean_watts": 1.778, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 71.646, "power_watts_avg": 20.151, "energy_joules_est": 184.24, "duration_seconds": 9.143, "sample_count": 79}, "timestamp": "2026-01-26T10:31:54.661878"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11584.052, "latencies_ms": [11584.052], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people sitting around a dining table, each with their own laptop open in front of them. There are at least five people visible, with some sitting closer to the left side of the table and others on the right side. The laptops are placed on the table, and the people seem to be engaged in their work or activities.\n\nIn addition to the", "error": null, "sys_before": {"cpu_percent": 15.0, "ram_used_mb": 20914.9, "ram_available_mb": 41926.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20975.1, "ram_available_mb": 41865.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.959}, "power_stats": {"power_gpu_soc_mean_watts": 19.36, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 70.959, "power_watts_avg": 19.36, "energy_joules_est": 224.28, "duration_seconds": 11.585, "sample_count": 98}, "timestamp": "2026-01-26T10:32:08.272024"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9711.006, "latencies_ms": [9711.006], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Laptop: 3\n\n- Computer mouse: 2\n\n- Keyboard: 2\n\n- Computer monitor: 2\n\n- Laptop screen: 2\n\n- Chair: 2\n\n- Table: 2\n\n- Person: 5", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20975.1, "ram_available_mb": 41865.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20976.3, "ram_available_mb": 41864.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.293}, "power_stats": {"power_gpu_soc_mean_watts": 20.213, "power_cpu_cv_mean_watts": 1.747, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 72.293, "power_watts_avg": 20.213, "energy_joules_est": 196.3, "duration_seconds": 9.712, "sample_count": 82}, "timestamp": "2026-01-26T10:32:19.997731"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10908.447, "latencies_ms": [10908.447], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, there is a laptop with a blurred screen on the left side, and a clear laptop screen in the center. In the background, there are multiple people seated around tables, with some facing the camera and others turned away. The tables are arranged in a way that suggests a communal workspace or a caf\u00e9 setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20922.9, "ram_available_mb": 41918.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.4, "ram_used_mb": 20978.5, "ram_available_mb": 41862.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.29}, "power_stats": {"power_gpu_soc_mean_watts": 19.495, "power_cpu_cv_mean_watts": 1.984, "power_sys_5v0_mean_watts": 8.675, "gpu_utilization_percent_mean": 71.29, "power_watts_avg": 19.495, "energy_joules_est": 212.67, "duration_seconds": 10.909, "sample_count": 93}, "timestamp": "2026-01-26T10:32:32.962178"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9581.306, "latencies_ms": [9581.306], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image depicts a group of people sitting around a table in a casual setting, possibly a cafe or a restaurant, working on their laptops. There are multiple laptops on the table, and some people are focused on their screens while others are engaged in conversation.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20978.5, "ram_available_mb": 41862.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 21004.0, "ram_available_mb": 41836.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.667}, "power_stats": {"power_gpu_soc_mean_watts": 20.394, "power_cpu_cv_mean_watts": 1.996, "power_sys_5v0_mean_watts": 8.627, "gpu_utilization_percent_mean": 71.667, "power_watts_avg": 20.394, "energy_joules_est": 195.41, "duration_seconds": 9.582, "sample_count": 81}, "timestamp": "2026-01-26T10:32:44.603315"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8588.684, "latencies_ms": [8588.684], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image is taken indoors with warm lighting, likely from overhead fixtures, creating a cozy atmosphere. Various objects are scattered around, including laptops, a keyboard, and a mug, suggesting a casual work environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21004.0, "ram_available_mb": 41836.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 21009.5, "ram_available_mb": 41831.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.438}, "power_stats": {"power_gpu_soc_mean_watts": 20.237, "power_cpu_cv_mean_watts": 1.858, "power_sys_5v0_mean_watts": 8.696, "gpu_utilization_percent_mean": 72.438, "power_watts_avg": 20.237, "energy_joules_est": 173.82, "duration_seconds": 8.589, "sample_count": 73}, "timestamp": "2026-01-26T10:32:55.221482"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12410.009, "latencies_ms": [12410.009], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young girl is the main subject. She is standing on a sidewalk, holding a blue umbrella with a wooden handle. The umbrella is open, providing a canopy over her. She is dressed in a pink jacket and blue jeans, adding a pop of color to the scene. Her gaze is directed towards the camera, capturing the", "error": null, "sys_before": {"cpu_percent": 6.5, "ram_used_mb": 20928.4, "ram_available_mb": 41912.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.5, "ram_used_mb": 21016.3, "ram_available_mb": 41824.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.387}, "power_stats": {"power_gpu_soc_mean_watts": 21.671, "power_cpu_cv_mean_watts": 1.975, "power_sys_5v0_mean_watts": 8.914, "gpu_utilization_percent_mean": 73.387, "power_watts_avg": 21.671, "energy_joules_est": 268.95, "duration_seconds": 12.411, "sample_count": 106}, "timestamp": "2026-01-26T10:33:09.665404"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9562.598, "latencies_ms": [9562.598], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "umbrella: 1, girl: 1, pink jacket: 1, blue jeans: 1, brown purse: 1, sandals: 1, ground: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20926.8, "ram_available_mb": 41914.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20971.0, "ram_available_mb": 41869.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.704}, "power_stats": {"power_gpu_soc_mean_watts": 22.998, "power_cpu_cv_mean_watts": 1.517, "power_sys_5v0_mean_watts": 8.754, "gpu_utilization_percent_mean": 76.704, "power_watts_avg": 22.998, "energy_joules_est": 219.94, "duration_seconds": 9.563, "sample_count": 81}, "timestamp": "2026-01-26T10:33:21.285282"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12705.001, "latencies_ms": [12705.001], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The girl is standing in the foreground, holding a blue umbrella with her right hand, which is positioned near the top of the umbrella. The umbrella is open and provides shade over her, indicating it is between her and the sun or rain. The ground appears to be a gravel surface, and there is no other person or object in the immediate vicinity", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20971.0, "ram_available_mb": 41869.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21016.1, "ram_available_mb": 41824.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.574}, "power_stats": {"power_gpu_soc_mean_watts": 21.625, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 8.883, "gpu_utilization_percent_mean": 73.574, "power_watts_avg": 21.625, "energy_joules_est": 274.76, "duration_seconds": 12.706, "sample_count": 108}, "timestamp": "2026-01-26T10:33:36.012876"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9459.852, "latencies_ms": [9459.852], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A young girl is standing under a blue umbrella, likely to protect herself from rain or sun. She is wearing a pink jacket and blue jeans, and appears to be outdoors on a sunny day.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20934.5, "ram_available_mb": 41906.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20933.3, "ram_available_mb": 41907.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.284}, "power_stats": {"power_gpu_soc_mean_watts": 22.943, "power_cpu_cv_mean_watts": 1.512, "power_sys_5v0_mean_watts": 8.779, "gpu_utilization_percent_mean": 77.284, "power_watts_avg": 22.943, "energy_joules_est": 217.05, "duration_seconds": 9.46, "sample_count": 81}, "timestamp": "2026-01-26T10:33:47.490887"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7084.354, "latencies_ms": [7084.354], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 31, "n_tiles": 16, "output_text": "A young child is holding a blue umbrella with a wooden handle. The child is wearing a pink jacket and blue jeans.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 20933.8, "ram_available_mb": 41907.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 20920.7, "ram_available_mb": 41920.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.8}, "power_stats": {"power_gpu_soc_mean_watts": 24.167, "power_cpu_cv_mean_watts": 1.327, "power_sys_5v0_mean_watts": 8.853, "gpu_utilization_percent_mean": 78.8, "power_watts_avg": 24.167, "energy_joules_est": 171.22, "duration_seconds": 7.085, "sample_count": 60}, "timestamp": "2026-01-26T10:33:56.617050"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11606.866, "latencies_ms": [11606.866], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing in front of a desk with a computer and a monitor. The man is wearing a suit and tie, and he is smiling. The desk has a keyboard and a mouse on it, and the computer is turned on. There is a window in the background, and a chair is visible near the desk. The room appears to be an", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20920.7, "ram_available_mb": 41920.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20936.5, "ram_available_mb": 41904.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.212}, "power_stats": {"power_gpu_soc_mean_watts": 19.321, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 70.212, "power_watts_avg": 19.321, "energy_joules_est": 224.27, "duration_seconds": 11.608, "sample_count": 99}, "timestamp": "2026-01-26T10:34:10.268406"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11202.597, "latencies_ms": [11202.597], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 74, "n_tiles": 16, "output_text": "man: 1, suit: 1, tie: 1, shirt: 1, jacket: 1, tie: 1, chair: 2, window: 1, computer: 1, monitor: 1, keyboard: 1, mouse: 1, desk: 2, outlet: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20936.5, "ram_available_mb": 41904.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20940.6, "ram_available_mb": 41900.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.726}, "power_stats": {"power_gpu_soc_mean_watts": 19.63, "power_cpu_cv_mean_watts": 1.832, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 70.726, "power_watts_avg": 19.63, "energy_joules_est": 219.92, "duration_seconds": 11.203, "sample_count": 95}, "timestamp": "2026-01-26T10:34:23.493373"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10789.245, "latencies_ms": [10789.245], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground, there is a man standing in front of a desk with a computer and a monitor. The desk is located near a window with a reflection of a person visible on the glass. The man is standing to the left of the desk, and the computer and monitor are positioned on the right side of the desk.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20940.6, "ram_available_mb": 41900.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20991.8, "ram_available_mb": 41849.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.63}, "power_stats": {"power_gpu_soc_mean_watts": 19.537, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 70.63, "power_watts_avg": 19.537, "energy_joules_est": 210.8, "duration_seconds": 10.79, "sample_count": 92}, "timestamp": "2026-01-26T10:34:36.308775"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8570.559, "latencies_ms": [8570.559], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A man in a suit stands in front of a desk with a computer and a monitor displaying an image of a room with a couch and a chair. The room appears to be an office or a conference room with a window in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20991.8, "ram_available_mb": 41849.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20992.8, "ram_available_mb": 41848.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.676}, "power_stats": {"power_gpu_soc_mean_watts": 20.715, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.586, "gpu_utilization_percent_mean": 73.676, "power_watts_avg": 20.715, "energy_joules_est": 177.55, "duration_seconds": 8.571, "sample_count": 74}, "timestamp": "2026-01-26T10:34:46.895765"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8702.368, "latencies_ms": [8702.368], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The room has a neutral color scheme with white walls and a light-colored carpet. The lighting appears to be artificial, coming from ceiling fixtures, and the room contains a wooden desk with a computer and a monitor displaying an image.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20992.8, "ram_available_mb": 41848.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20983.4, "ram_available_mb": 41857.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.189}, "power_stats": {"power_gpu_soc_mean_watts": 20.261, "power_cpu_cv_mean_watts": 1.714, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 71.189, "power_watts_avg": 20.261, "energy_joules_est": 176.33, "duration_seconds": 8.703, "sample_count": 74}, "timestamp": "2026-01-26T10:34:57.655461"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11591.23, "latencies_ms": [11591.23], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of four men sitting around a wooden dining table, enjoying a meal together. They are all dressed in casual clothing, and each of them has a cup in front of them, possibly containing a beverage. The table is set with various items, including bowls, cups, and bottles, indicating that they are having a meal.\n", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20983.4, "ram_available_mb": 41857.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20930.9, "ram_available_mb": 41910.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.061}, "power_stats": {"power_gpu_soc_mean_watts": 19.3, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 70.061, "power_watts_avg": 19.3, "energy_joules_est": 223.72, "duration_seconds": 11.592, "sample_count": 99}, "timestamp": "2026-01-26T10:35:11.292030"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8933.946, "latencies_ms": [8933.946], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "table: 1\ncups: 4\nbowls: 3\nplates: 5\nsugar packets: 2\nsalt packets: 2\nmugs: 2\nspices: 2\nclock: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20930.9, "ram_available_mb": 41910.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20963.4, "ram_available_mb": 41877.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.065}, "power_stats": {"power_gpu_soc_mean_watts": 20.398, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 8.583, "gpu_utilization_percent_mean": 72.065, "power_watts_avg": 20.398, "energy_joules_est": 182.25, "duration_seconds": 8.935, "sample_count": 77}, "timestamp": "2026-01-26T10:35:22.242456"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11565.6, "latencies_ms": [11565.6], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden table with various items on it, including bowls, cups, and a bottle, indicating a meal is being shared. In the background, there are four individuals seated around the table, with one person on the left side, two in the middle, and one on the right side, all facing towards the center of the table. The", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.4, "ram_available_mb": 41877.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20964.6, "ram_available_mb": 41876.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.758}, "power_stats": {"power_gpu_soc_mean_watts": 19.12, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.659, "gpu_utilization_percent_mean": 69.758, "power_watts_avg": 19.12, "energy_joules_est": 221.15, "duration_seconds": 11.566, "sample_count": 99}, "timestamp": "2026-01-26T10:35:35.834273"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7661.162, "latencies_ms": [7661.162], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A group of people are gathered around a wooden table, enjoying a meal together. The room has a cozy atmosphere with a clock on the wall and a window with curtains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20964.6, "ram_available_mb": 41876.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20965.6, "ram_available_mb": 41875.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.831}, "power_stats": {"power_gpu_soc_mean_watts": 21.292, "power_cpu_cv_mean_watts": 1.57, "power_sys_5v0_mean_watts": 8.565, "gpu_utilization_percent_mean": 73.831, "power_watts_avg": 21.292, "energy_joules_est": 163.13, "duration_seconds": 7.662, "sample_count": 65}, "timestamp": "2026-01-26T10:35:45.515768"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7088.902, "latencies_ms": [7088.902], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The room has a warm and cozy atmosphere with wooden walls and a wooden ceiling. The lighting is soft and natural, coming from the large window on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.6, "ram_available_mb": 41875.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20965.8, "ram_available_mb": 41875.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.475}, "power_stats": {"power_gpu_soc_mean_watts": 21.37, "power_cpu_cv_mean_watts": 1.568, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 73.475, "power_watts_avg": 21.37, "energy_joules_est": 151.51, "duration_seconds": 7.09, "sample_count": 61}, "timestamp": "2026-01-26T10:35:54.620957"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11587.54, "latencies_ms": [11587.54], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a red pickup truck is parked on a snowy street, with a person standing next to it. The person is using a snow shovel to clear snow from the truck. The truck is positioned in front of a house, and there are other houses visible in the background. The scene suggests that the person is preparing the truck for travel", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 20965.8, "ram_available_mb": 41875.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20928.8, "ram_available_mb": 41912.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.173}, "power_stats": {"power_gpu_soc_mean_watts": 19.339, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 70.173, "power_watts_avg": 19.339, "energy_joules_est": 224.1, "duration_seconds": 11.588, "sample_count": 98}, "timestamp": "2026-01-26T10:36:08.254440"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7998.464, "latencies_ms": [7998.464], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "house: 3, truck: 1, snow plow: 1, person: 1, tree: 4, snow: 2, fire hydrant: 1, house number: 1", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20928.8, "ram_available_mb": 41912.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20921.9, "ram_available_mb": 41918.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.676}, "power_stats": {"power_gpu_soc_mean_watts": 21.136, "power_cpu_cv_mean_watts": 1.606, "power_sys_5v0_mean_watts": 8.579, "gpu_utilization_percent_mean": 73.676, "power_watts_avg": 21.136, "energy_joules_est": 169.07, "duration_seconds": 7.999, "sample_count": 68}, "timestamp": "2026-01-26T10:36:18.291267"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9761.116, "latencies_ms": [9761.116], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "A red pickup truck is in the foreground, positioned on the right side of the image, moving towards the left. The truck is closer to the viewer than the houses in the background. A person is standing behind the truck, partially obscured by the snow plow attachment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20921.9, "ram_available_mb": 41918.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20961.4, "ram_available_mb": 41879.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.349}, "power_stats": {"power_gpu_soc_mean_watts": 19.921, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 8.616, "gpu_utilization_percent_mean": 71.349, "power_watts_avg": 19.921, "energy_joules_est": 194.46, "duration_seconds": 9.762, "sample_count": 83}, "timestamp": "2026-01-26T10:36:30.086076"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6975.811, "latencies_ms": [6975.811], "images_per_second": 0.143, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A red pickup truck is being used to clear snow from a residential street. A person is standing next to the truck, assisting in the snow removal process.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.4, "ram_available_mb": 41879.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20962.1, "ram_available_mb": 41878.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.935, "power_cpu_cv_mean_watts": 1.479, "power_sys_5v0_mean_watts": 8.581, "gpu_utilization_percent_mean": 74.0, "power_watts_avg": 21.935, "energy_joules_est": 153.03, "duration_seconds": 6.976, "sample_count": 59}, "timestamp": "2026-01-26T10:36:39.120641"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9154.823, "latencies_ms": [9154.823], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "A red pickup truck is being used to clear snow from a residential street. The truck has a snowplow attached to the front, and the driver is wearing a yellow jacket. The houses in the background are white, and the sky is overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20962.1, "ram_available_mb": 41878.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20963.4, "ram_available_mb": 41877.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.295}, "power_stats": {"power_gpu_soc_mean_watts": 20.069, "power_cpu_cv_mean_watts": 1.749, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 71.295, "power_watts_avg": 20.069, "energy_joules_est": 183.75, "duration_seconds": 9.156, "sample_count": 78}, "timestamp": "2026-01-26T10:36:50.300303"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12402.325, "latencies_ms": [12402.325], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a luxurious bathroom. The room is bathed in soft light, casting a warm glow on the beige walls and marble countertop. A large mirror dominates the wall, reflecting the room's opulence. Above the mirror, a television is mounted, adding a modern touch to the space.\n\nThe countertop", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 20963.4, "ram_available_mb": 41877.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20918.4, "ram_available_mb": 41922.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.113}, "power_stats": {"power_gpu_soc_mean_watts": 21.544, "power_cpu_cv_mean_watts": 1.808, "power_sys_5v0_mean_watts": 8.905, "gpu_utilization_percent_mean": 73.113, "power_watts_avg": 21.544, "energy_joules_est": 267.21, "duration_seconds": 12.403, "sample_count": 106}, "timestamp": "2026-01-26T10:37:04.776413"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9351.053, "latencies_ms": [9351.053], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "mirror: 1\ncamera: 1\nlight fixture: 4\ntowel: 3\nbucket: 1\ntile: 1\nmirror frame: 1\ntile border: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20918.4, "ram_available_mb": 41922.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.8, "ram_used_mb": 20933.8, "ram_available_mb": 41907.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.278}, "power_stats": {"power_gpu_soc_mean_watts": 22.988, "power_cpu_cv_mean_watts": 1.575, "power_sys_5v0_mean_watts": 8.776, "gpu_utilization_percent_mean": 77.278, "power_watts_avg": 22.988, "energy_joules_est": 214.98, "duration_seconds": 9.352, "sample_count": 79}, "timestamp": "2026-01-26T10:37:16.160606"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12674.478, "latencies_ms": [12674.478], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the foreground, there is a large, ornate bathroom vanity with a double sink and a mirror above it. The person taking the photo is standing in front of the mirror, slightly to the left of the vanity. The background features a shower area with a glass door and a bathtub, as well as a television mounted on the wall above the vanity.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20933.8, "ram_available_mb": 41907.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.7, "ram_used_mb": 20957.9, "ram_available_mb": 41883.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.259}, "power_stats": {"power_gpu_soc_mean_watts": 21.741, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.908, "gpu_utilization_percent_mean": 74.259, "power_watts_avg": 21.741, "energy_joules_est": 275.57, "duration_seconds": 12.675, "sample_count": 108}, "timestamp": "2026-01-26T10:37:30.861743"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9332.605, "latencies_ms": [9332.605], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A person is taking a photo of a luxurious bathroom with a large mirror, double sinks, and a bathtub. The bathroom is well-lit with multiple light fixtures and has a modern design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20957.9, "ram_available_mb": 41883.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.4, "ram_used_mb": 20973.8, "ram_available_mb": 41867.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.987}, "power_stats": {"power_gpu_soc_mean_watts": 22.977, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.779, "gpu_utilization_percent_mean": 75.987, "power_watts_avg": 22.977, "energy_joules_est": 214.45, "duration_seconds": 9.333, "sample_count": 79}, "timestamp": "2026-01-26T10:37:42.230721"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9517.958, "latencies_ms": [9517.958], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The bathroom features a large mirror with a dark frame, and the countertop is made of marble with gold faucets. The lighting is warm and ambient, with multiple sconces on the walls and a television mounted above the mirror.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20973.8, "ram_available_mb": 41867.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.9, "ram_used_mb": 21044.5, "ram_available_mb": 41796.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.63}, "power_stats": {"power_gpu_soc_mean_watts": 22.716, "power_cpu_cv_mean_watts": 1.823, "power_sys_5v0_mean_watts": 8.866, "gpu_utilization_percent_mean": 75.63, "power_watts_avg": 22.716, "energy_joules_est": 216.22, "duration_seconds": 9.519, "sample_count": 81}, "timestamp": "2026-01-26T10:37:53.781216"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11612.98, "latencies_ms": [11612.98], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a busy airport scene with several people standing around, some of them loading luggage into a white car. There are two men in the foreground, one of them holding a suitcase while the other is helping him. A few more people can be seen in the background, also handling their luggage.\n\nThere are multiple suitcases scattered around the area", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20940.8, "ram_available_mb": 41900.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 20938.1, "ram_available_mb": 41902.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.333}, "power_stats": {"power_gpu_soc_mean_watts": 19.302, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 70.333, "power_watts_avg": 19.302, "energy_joules_est": 224.17, "duration_seconds": 11.614, "sample_count": 99}, "timestamp": "2026-01-26T10:38:07.419895"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8107.498, "latencies_ms": [8107.498], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "cart: 1\nman: 2\nsuitcase: 3\ncar: 1\nlicense plate: 1\ndenim: 1\nshoes: 1\nbags: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20938.1, "ram_available_mb": 41902.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20928.4, "ram_available_mb": 41912.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.571}, "power_stats": {"power_gpu_soc_mean_watts": 21.036, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 8.587, "gpu_utilization_percent_mean": 73.571, "power_watts_avg": 21.036, "energy_joules_est": 170.56, "duration_seconds": 8.108, "sample_count": 70}, "timestamp": "2026-01-26T10:38:17.555086"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11561.837, "latencies_ms": [11561.837], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a man is standing and appears to be in the process of loading luggage into a white SUV parked on the right side of the image. Another man is in the background, pushing a luggage cart with a suitcase on it. The cart is positioned near the white SUV, suggesting that the man is also preparing to load luggage", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20928.4, "ram_available_mb": 41912.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20990.2, "ram_available_mb": 41850.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.557}, "power_stats": {"power_gpu_soc_mean_watts": 19.416, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 8.668, "gpu_utilization_percent_mean": 70.557, "power_watts_avg": 19.416, "energy_joules_est": 224.5, "duration_seconds": 11.563, "sample_count": 97}, "timestamp": "2026-01-26T10:38:31.129734"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8908.906, "latencies_ms": [8908.906], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "In the image, two men are unloading luggage from a white car in a parking garage. The car is parked in a designated parking spot, and the men are standing next to it, with one of them holding a suitcase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20990.2, "ram_available_mb": 41850.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20990.0, "ram_available_mb": 41850.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.627}, "power_stats": {"power_gpu_soc_mean_watts": 20.595, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 72.627, "power_watts_avg": 20.595, "energy_joules_est": 183.49, "duration_seconds": 8.91, "sample_count": 75}, "timestamp": "2026-01-26T10:38:42.057607"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8133.908, "latencies_ms": [8133.908], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image shows an indoor setting with artificial lighting, predominantly featuring white and grey tones. There are several bags and luggage items scattered on the floor, and a white car is parked in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20990.0, "ram_available_mb": 41850.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20935.1, "ram_available_mb": 41905.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.333}, "power_stats": {"power_gpu_soc_mean_watts": 20.531, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 72.333, "power_watts_avg": 20.531, "energy_joules_est": 167.01, "duration_seconds": 8.135, "sample_count": 69}, "timestamp": "2026-01-26T10:38:52.222005"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11589.644, "latencies_ms": [11589.644], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a plate of food consisting of a sandwich, fries, and a side of ketchup. The sandwich is placed in the center of the plate, with the fries surrounding it. The ketchup is positioned to the left of the sandwich. The plate is white, and the food is arranged in a visually appealing manner. The sandwich appears", "error": null, "sys_before": {"cpu_percent": 10.7, "ram_used_mb": 20935.1, "ram_available_mb": 41905.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20923.6, "ram_available_mb": 41917.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.525}, "power_stats": {"power_gpu_soc_mean_watts": 19.342, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 70.525, "power_watts_avg": 19.342, "energy_joules_est": 224.18, "duration_seconds": 11.59, "sample_count": 99}, "timestamp": "2026-01-26T10:39:05.843733"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9020.467, "latencies_ms": [9020.467], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "Hamburger: 2\nFries: 1\nKetchup: 1\nOnion: 1\nBun: 1\nBun: 1\nBun: 1\nBun: 1\nBun: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20923.6, "ram_available_mb": 41917.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20962.3, "ram_available_mb": 41878.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.641}, "power_stats": {"power_gpu_soc_mean_watts": 20.494, "power_cpu_cv_mean_watts": 1.703, "power_sys_5v0_mean_watts": 8.59, "gpu_utilization_percent_mean": 72.641, "power_watts_avg": 20.494, "energy_joules_est": 184.88, "duration_seconds": 9.021, "sample_count": 78}, "timestamp": "2026-01-26T10:39:16.883214"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11575.884, "latencies_ms": [11575.884], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a plate with two burgers, one of which is cut in half, revealing the inside. Behind the burgers, there is a pile of golden-brown fries. To the left of the plate, there is a small bowl of ketchup and a small bowl of mayonnaise. The fries are positioned in", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20962.3, "ram_available_mb": 41878.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21019.7, "ram_available_mb": 41821.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.354}, "power_stats": {"power_gpu_soc_mean_watts": 19.324, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 68.354, "power_watts_avg": 19.324, "energy_joules_est": 223.71, "duration_seconds": 11.577, "sample_count": 99}, "timestamp": "2026-01-26T10:39:30.480692"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7998.965, "latencies_ms": [7998.965], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a plate of food consisting of a burger, fries, and a small bowl of ketchup. The burger is topped with sesame seeds and the fries are golden brown.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20923.0, "ram_available_mb": 41917.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20988.3, "ram_available_mb": 41852.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.544}, "power_stats": {"power_gpu_soc_mean_watts": 21.125, "power_cpu_cv_mean_watts": 1.606, "power_sys_5v0_mean_watts": 8.575, "gpu_utilization_percent_mean": 72.544, "power_watts_avg": 21.125, "energy_joules_est": 168.99, "duration_seconds": 8.0, "sample_count": 68}, "timestamp": "2026-01-26T10:39:40.525589"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10115.026, "latencies_ms": [10115.026], "images_per_second": 0.099, "prompt_tokens": 36, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image features a meal with a sesame seed bun, golden-brown fries, and a side of ketchup and mayonnaise in blue containers. The lighting in the image is warm and artificial, likely from an indoor source, and the meal is presented on a white plate.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20988.3, "ram_available_mb": 41852.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20942.1, "ram_available_mb": 41898.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.116}, "power_stats": {"power_gpu_soc_mean_watts": 19.578, "power_cpu_cv_mean_watts": 1.805, "power_sys_5v0_mean_watts": 8.614, "gpu_utilization_percent_mean": 71.116, "power_watts_avg": 19.578, "energy_joules_est": 198.04, "duration_seconds": 10.116, "sample_count": 86}, "timestamp": "2026-01-26T10:39:52.662766"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11602.864, "latencies_ms": [11602.864], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and inviting bedroom. Dominating the center of the room is a bed, adorned with a green mosquito net that cascades over the headboard, providing a sense of tranquility. The bed is dressed in a white comforter, and a white pillow rests at its foot, inviting rest and relaxation.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20942.1, "ram_available_mb": 41898.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20947.5, "ram_available_mb": 41893.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.56}, "power_stats": {"power_gpu_soc_mean_watts": 19.263, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 70.56, "power_watts_avg": 19.263, "energy_joules_est": 223.52, "duration_seconds": 11.604, "sample_count": 100}, "timestamp": "2026-01-26T10:40:06.317539"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7663.061, "latencies_ms": [7663.061], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "bed: 1, canopy: 1, window: 4, curtain: 4, painting: 2, table: 1, chair: 1, candle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20947.5, "ram_available_mb": 41893.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20934.3, "ram_available_mb": 41906.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.754}, "power_stats": {"power_gpu_soc_mean_watts": 21.231, "power_cpu_cv_mean_watts": 1.576, "power_sys_5v0_mean_watts": 8.556, "gpu_utilization_percent_mean": 73.754, "power_watts_avg": 21.231, "energy_joules_est": 162.71, "duration_seconds": 7.664, "sample_count": 65}, "timestamp": "2026-01-26T10:40:16.019675"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11576.423, "latencies_ms": [11576.423], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The bed with the mosquito net is positioned in the foreground of the image, occupying a central space in the room. To the right of the bed, there is a small wooden table with a single candle on it, which is near the bed but slightly further back in the room. The windows with curtains are on the left side of the room, framing the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20934.3, "ram_available_mb": 41906.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20965.3, "ram_available_mb": 41875.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.376}, "power_stats": {"power_gpu_soc_mean_watts": 19.17, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 69.376, "power_watts_avg": 19.17, "energy_joules_est": 221.93, "duration_seconds": 11.577, "sample_count": 101}, "timestamp": "2026-01-26T10:40:29.631286"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10411.311, "latencies_ms": [10411.311], "images_per_second": 0.096, "prompt_tokens": 37, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image depicts a cozy bedroom with a large bed covered in a green mosquito net, suggesting a tropical or safari-themed decor. The room has a wooden ceiling, a small wooden table, and two windows with curtains, one of which is open to let in natural light.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20965.3, "ram_available_mb": 41875.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20965.8, "ram_available_mb": 41875.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.989}, "power_stats": {"power_gpu_soc_mean_watts": 19.9, "power_cpu_cv_mean_watts": 1.785, "power_sys_5v0_mean_watts": 8.586, "gpu_utilization_percent_mean": 71.989, "power_watts_avg": 19.9, "energy_joules_est": 207.2, "duration_seconds": 10.412, "sample_count": 89}, "timestamp": "2026-01-26T10:40:42.055684"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7667.673, "latencies_ms": [7667.673], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The room has a warm and inviting atmosphere with wooden furniture and a thatched roof. The walls are painted in a soft orange color, and there are two windows with curtains that let in natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.8, "ram_available_mb": 41875.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20966.5, "ram_available_mb": 41874.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.929, "power_cpu_cv_mean_watts": 1.638, "power_sys_5v0_mean_watts": 8.595, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 20.929, "energy_joules_est": 160.49, "duration_seconds": 7.668, "sample_count": 65}, "timestamp": "2026-01-26T10:40:51.761666"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12343.102, "latencies_ms": [12343.102], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a small gray and white cat is standing on top of a black car in a garage. The cat appears to be curiously looking around, possibly exploring its surroundings. The garage is filled with various items, including a bicycle, a bottle, and a couple of boxes. The presence of these objects suggests that the garage is used for storage", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 20966.5, "ram_available_mb": 41874.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21011.2, "ram_available_mb": 41829.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.486}, "power_stats": {"power_gpu_soc_mean_watts": 21.374, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.799, "gpu_utilization_percent_mean": 72.486, "power_watts_avg": 21.374, "energy_joules_est": 263.84, "duration_seconds": 12.344, "sample_count": 105}, "timestamp": "2026-01-26T10:41:06.170066"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8823.408, "latencies_ms": [8823.408], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "car: 1, cat: 1, lamps: 2, boxes: 1, tires: 2, bicycles: 2, bottles: 2, drawers: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21011.2, "ram_available_mb": 41829.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21012.0, "ram_available_mb": 41828.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.587}, "power_stats": {"power_gpu_soc_mean_watts": 22.755, "power_cpu_cv_mean_watts": 1.478, "power_sys_5v0_mean_watts": 8.709, "gpu_utilization_percent_mean": 76.587, "power_watts_avg": 22.755, "energy_joules_est": 200.79, "duration_seconds": 8.824, "sample_count": 75}, "timestamp": "2026-01-26T10:41:17.057278"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12166.466, "latencies_ms": [12166.466], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a gray tabby cat is standing on the hood of a black car, positioned slightly to the left of the center. The car is in the middle ground of the image, with its hood open and the cat perched on it. In the background, there is a garage setting with various items such as a bicycle, a cardboard box with", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21012.0, "ram_available_mb": 41828.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21012.7, "ram_available_mb": 41828.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.311}, "power_stats": {"power_gpu_soc_mean_watts": 21.42, "power_cpu_cv_mean_watts": 1.81, "power_sys_5v0_mean_watts": 8.856, "gpu_utilization_percent_mean": 72.311, "power_watts_avg": 21.42, "energy_joules_est": 260.62, "duration_seconds": 12.167, "sample_count": 103}, "timestamp": "2026-01-26T10:41:31.262601"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8567.171, "latencies_ms": [8567.171], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A cat is standing on the hood of a black car in a garage. The garage is cluttered with various items, including a bicycle, a lamp, and a cardboard box.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21012.7, "ram_available_mb": 41828.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21013.5, "ram_available_mb": 41827.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.973}, "power_stats": {"power_gpu_soc_mean_watts": 22.95, "power_cpu_cv_mean_watts": 1.485, "power_sys_5v0_mean_watts": 8.7, "gpu_utilization_percent_mean": 75.973, "power_watts_avg": 22.95, "energy_joules_est": 196.63, "duration_seconds": 8.568, "sample_count": 73}, "timestamp": "2026-01-26T10:41:41.857266"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8126.39, "latencies_ms": [8126.39], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image shows an indoor setting with a black car in the foreground. The lighting is artificial, with a primary light source coming from the left side, casting a shadow to the right of the car.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20938.4, "ram_available_mb": 41902.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20975.9, "ram_available_mb": 41865.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.072}, "power_stats": {"power_gpu_soc_mean_watts": 22.855, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 8.824, "gpu_utilization_percent_mean": 76.072, "power_watts_avg": 22.855, "energy_joules_est": 185.74, "duration_seconds": 8.127, "sample_count": 69}, "timestamp": "2026-01-26T10:41:52.003666"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12429.397, "latencies_ms": [12429.397], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a white plate holding a delicious-looking sandwich. The sandwich is made with a toasted bun, filled with a generous amount of meat, and topped with a slice of tomato and a dollop of sauce. The plate is placed on a table, and there's a fork visible on the right side of the plate. The background is bl", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20975.9, "ram_available_mb": 41865.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 21039.7, "ram_available_mb": 41801.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.953}, "power_stats": {"power_gpu_soc_mean_watts": 21.531, "power_cpu_cv_mean_watts": 1.925, "power_sys_5v0_mean_watts": 8.927, "gpu_utilization_percent_mean": 72.953, "power_watts_avg": 21.531, "energy_joules_est": 267.63, "duration_seconds": 12.43, "sample_count": 106}, "timestamp": "2026-01-26T10:42:06.501457"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9560.012, "latencies_ms": [9560.012], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "plate: 1\nmushroom: 1\ngravy: 1\nbread: 1\nmeat: 1\ntomato: 1\nparsley: 1\ngravy: 1\n", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20978.0, "ram_available_mb": 41862.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.2, "ram_used_mb": 20951.1, "ram_available_mb": 41889.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.284}, "power_stats": {"power_gpu_soc_mean_watts": 22.863, "power_cpu_cv_mean_watts": 1.665, "power_sys_5v0_mean_watts": 8.784, "gpu_utilization_percent_mean": 77.284, "power_watts_avg": 22.863, "energy_joules_est": 218.59, "duration_seconds": 9.561, "sample_count": 81}, "timestamp": "2026-01-26T10:42:18.075872"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11543.182, "latencies_ms": [11543.182], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a plate with a sandwich that has a large piece of meat and a slice of tomato on top, garnished with herbs. The plate is placed on a table with a black and white checkered pattern. In the background, there is another plate with what appears to be fries.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20951.1, "ram_available_mb": 41889.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.7, "ram_used_mb": 21056.7, "ram_available_mb": 41784.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.745}, "power_stats": {"power_gpu_soc_mean_watts": 21.909, "power_cpu_cv_mean_watts": 1.842, "power_sys_5v0_mean_watts": 8.866, "gpu_utilization_percent_mean": 72.745, "power_watts_avg": 21.909, "energy_joules_est": 252.91, "duration_seconds": 11.544, "sample_count": 98}, "timestamp": "2026-01-26T10:42:31.637147"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11770.819, "latencies_ms": [11770.819], "images_per_second": 0.085, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image shows a plate of food with a sandwich topped with a slice of tomato and a generous amount of gravy, garnished with chopped herbs. The plate is placed on a table with a black and white checkered tablecloth, and there is a fork on the right side of the plate.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21056.7, "ram_available_mb": 41784.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 21060.9, "ram_available_mb": 41780.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.11}, "power_stats": {"power_gpu_soc_mean_watts": 22.002, "power_cpu_cv_mean_watts": 1.789, "power_sys_5v0_mean_watts": 8.818, "gpu_utilization_percent_mean": 73.11, "power_watts_avg": 22.002, "energy_joules_est": 259.0, "duration_seconds": 11.771, "sample_count": 100}, "timestamp": "2026-01-26T10:42:45.427990"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11385.246, "latencies_ms": [11385.246], "images_per_second": 0.088, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image shows a plate of food with a rich, dark brown gravy and a slice of tomato on top, garnished with green herbs, possibly parsley. The plate is white, and the food is presented on a black and white checkered tablecloth, suggesting an outdoor setting with natural light.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20957.3, "ram_available_mb": 41883.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21044.7, "ram_available_mb": 41796.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.289}, "power_stats": {"power_gpu_soc_mean_watts": 22.03, "power_cpu_cv_mean_watts": 1.728, "power_sys_5v0_mean_watts": 8.88, "gpu_utilization_percent_mean": 75.289, "power_watts_avg": 22.03, "energy_joules_est": 250.83, "duration_seconds": 11.386, "sample_count": 97}, "timestamp": "2026-01-26T10:42:58.856153"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11600.225, "latencies_ms": [11600.225], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of three men sitting on a couch in a living room, enjoying a beer and playing a video game together. They are all focused on the game, with one man holding a remote controller. The room is furnished with a couch, a dining table, and a chair.\n\nVarious items are scattered around the room, including a laptop placed", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20940.8, "ram_available_mb": 41900.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20931.3, "ram_available_mb": 41909.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.88}, "power_stats": {"power_gpu_soc_mean_watts": 19.327, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 70.88, "power_watts_avg": 19.327, "energy_joules_est": 224.21, "duration_seconds": 11.601, "sample_count": 100}, "timestamp": "2026-01-26T10:43:12.491380"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10981.016, "latencies_ms": [10981.016], "images_per_second": 0.091, "prompt_tokens": 39, "response_tokens_est": 72, "n_tiles": 16, "output_text": "- Beer cans: 5\n\n- Beer bottles: 2\n\n- Glasses: 1\n\n- Laptop: 1\n\n- Couch: 1\n\n- Chair: 1\n\n- Beer keg: 1\n\n- Beer keg cap: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20931.3, "ram_available_mb": 41909.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20991.2, "ram_available_mb": 41849.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.926}, "power_stats": {"power_gpu_soc_mean_watts": 19.637, "power_cpu_cv_mean_watts": 1.826, "power_sys_5v0_mean_watts": 8.61, "gpu_utilization_percent_mean": 69.926, "power_watts_avg": 19.637, "energy_joules_est": 215.65, "duration_seconds": 10.982, "sample_count": 94}, "timestamp": "2026-01-26T10:43:25.501608"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11320.071, "latencies_ms": [11320.071], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a red couch with a person sitting on it, and a red table with various items on it. In the background, there is a window with blinds partially open, allowing natural light to enter the room. The person standing on the right side of the image is holding a remote control, and there is a laptop on the couch.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20991.2, "ram_available_mb": 41849.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21007.7, "ram_available_mb": 41833.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.406}, "power_stats": {"power_gpu_soc_mean_watts": 19.462, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 70.406, "power_watts_avg": 19.462, "energy_joules_est": 220.32, "duration_seconds": 11.321, "sample_count": 96}, "timestamp": "2026-01-26T10:43:38.858550"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11090.475, "latencies_ms": [11090.475], "images_per_second": 0.09, "prompt_tokens": 37, "response_tokens_est": 73, "n_tiles": 16, "output_text": "Three men are gathered in a living room, with one man standing and holding a Wii remote, while the other two are sitting on a couch. There is a red couch, a white couch, and a red coffee table in the room, along with various items such as a laptop, a backpack, and cans of beer.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21007.7, "ram_available_mb": 41833.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21009.1, "ram_available_mb": 41831.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.787}, "power_stats": {"power_gpu_soc_mean_watts": 19.695, "power_cpu_cv_mean_watts": 1.822, "power_sys_5v0_mean_watts": 8.618, "gpu_utilization_percent_mean": 70.787, "power_watts_avg": 19.695, "energy_joules_est": 218.44, "duration_seconds": 11.091, "sample_count": 94}, "timestamp": "2026-01-26T10:43:51.971110"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8250.488, "latencies_ms": [8250.488], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The room is dimly lit with a warm yellow light emanating from a lamp on the right side of the image. There is a red couch and a red table with various items on it, including cans and a bottle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21009.1, "ram_available_mb": 41831.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20940.2, "ram_available_mb": 41900.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.1}, "power_stats": {"power_gpu_soc_mean_watts": 20.499, "power_cpu_cv_mean_watts": 1.663, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 72.1, "power_watts_avg": 20.499, "energy_joules_est": 169.14, "duration_seconds": 8.251, "sample_count": 70}, "timestamp": "2026-01-26T10:44:02.259098"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11585.047, "latencies_ms": [11585.047], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a vibrant baseball game, a catcher, clad in a black and white uniform, crouches in anticipation on the dirt infield. His black helmet, a stark contrast against the green of the field, shields his eyes as he prepares for the next play. His black and white uniform, adorned with the number 15", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 20940.2, "ram_available_mb": 41900.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20965.2, "ram_available_mb": 41875.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.465}, "power_stats": {"power_gpu_soc_mean_watts": 19.341, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 70.465, "power_watts_avg": 19.341, "energy_joules_est": 224.08, "duration_seconds": 11.586, "sample_count": 99}, "timestamp": "2026-01-26T10:44:15.880620"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10311.098, "latencies_ms": [10311.098], "images_per_second": 0.097, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "1. Catcher: 1\n2. Home plate: 1\n3. Mound: 1\n4. Baseball glove: 1\n5. Uniform: 1\n6. Helmet: 1\n7. Face mask: 1\n8. Net: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20965.2, "ram_available_mb": 41875.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20965.7, "ram_available_mb": 41875.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.91}, "power_stats": {"power_gpu_soc_mean_watts": 19.691, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 71.91, "power_watts_avg": 19.691, "energy_joules_est": 203.05, "duration_seconds": 10.312, "sample_count": 89}, "timestamp": "2026-01-26T10:44:28.213448"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11344.461, "latencies_ms": [11344.461], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The catcher is positioned in the foreground on the baseball field, crouched behind home plate, which is the focal point of the image. The background features a well-maintained grassy area, likely the outfield, which is part of the baseball diamond. The catcher is near the home plate, indicating readiness to catch a pitch.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20965.7, "ram_available_mb": 41875.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20965.9, "ram_available_mb": 41875.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.299}, "power_stats": {"power_gpu_soc_mean_watts": 19.41, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 68.299, "power_watts_avg": 19.41, "energy_joules_est": 220.21, "duration_seconds": 11.345, "sample_count": 97}, "timestamp": "2026-01-26T10:44:41.578851"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7661.007, "latencies_ms": [7661.007], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A baseball catcher is crouched down on the field, ready to catch the ball. The catcher is wearing a black and white uniform with a helmet and a catcher's mitt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.9, "ram_available_mb": 41875.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20965.9, "ram_available_mb": 41875.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.569}, "power_stats": {"power_gpu_soc_mean_watts": 21.348, "power_cpu_cv_mean_watts": 1.57, "power_sys_5v0_mean_watts": 8.562, "gpu_utilization_percent_mean": 73.569, "power_watts_avg": 21.348, "energy_joules_est": 163.56, "duration_seconds": 7.662, "sample_count": 65}, "timestamp": "2026-01-26T10:44:51.255215"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7972.131, "latencies_ms": [7972.131], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The baseball catcher is wearing a black and white uniform with a black helmet and mask. The field is covered in green grass and the dirt around the catcher's area is brown with white chalk markings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.9, "ram_available_mb": 41875.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20966.9, "ram_available_mb": 41874.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.793, "power_cpu_cv_mean_watts": 1.665, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 72.0, "power_watts_avg": 20.793, "energy_joules_est": 165.78, "duration_seconds": 7.973, "sample_count": 68}, "timestamp": "2026-01-26T10:45:01.257762"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11579.656, "latencies_ms": [11579.656], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a bathroom scene. The dominant color scheme is pink and blue, with the walls painted in these hues. A white toilet is situated in the center of the room, adjacent to a bathtub. The bathtub is white and has a shower curtain hanging from it. A window with a white frame is located above the bathtub,", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 20966.9, "ram_available_mb": 41874.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20967.9, "ram_available_mb": 41873.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.949}, "power_stats": {"power_gpu_soc_mean_watts": 19.316, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.668, "gpu_utilization_percent_mean": 70.949, "power_watts_avg": 19.316, "energy_joules_est": 223.68, "duration_seconds": 11.58, "sample_count": 99}, "timestamp": "2026-01-26T10:45:14.857634"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8141.837, "latencies_ms": [8141.837], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "door: 2, bathtub: 1, toilet: 1, window: 1, shower curtain: 1, sink: 1, cabinet: 1, mirror: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20967.9, "ram_available_mb": 41873.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20968.4, "ram_available_mb": 41872.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.343}, "power_stats": {"power_gpu_soc_mean_watts": 20.816, "power_cpu_cv_mean_watts": 1.623, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 73.343, "power_watts_avg": 20.816, "energy_joules_est": 169.49, "duration_seconds": 8.143, "sample_count": 70}, "timestamp": "2026-01-26T10:45:25.018281"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9388.934, "latencies_ms": [9388.934], "images_per_second": 0.107, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The toilet is located in the foreground of the image, near the left side. The bathtub is in the background, to the left of the toilet. The shower curtain is hanging vertically, closer to the foreground than the bathtub.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20968.4, "ram_available_mb": 41872.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20969.1, "ram_available_mb": 41871.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.763}, "power_stats": {"power_gpu_soc_mean_watts": 20.029, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 70.763, "power_watts_avg": 20.029, "energy_joules_est": 188.06, "duration_seconds": 9.39, "sample_count": 80}, "timestamp": "2026-01-26T10:45:36.447389"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8133.316, "latencies_ms": [8133.316], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a small, well-lit bathroom with light blue walls and a pink tile backsplash. There is a white toilet, a bathtub, and a wooden vanity with a sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20969.1, "ram_available_mb": 41871.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20969.3, "ram_available_mb": 41871.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.072}, "power_stats": {"power_gpu_soc_mean_watts": 20.976, "power_cpu_cv_mean_watts": 1.612, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 73.072, "power_watts_avg": 20.976, "energy_joules_est": 170.62, "duration_seconds": 8.134, "sample_count": 69}, "timestamp": "2026-01-26T10:45:46.594286"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9047.868, "latencies_ms": [9047.868], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image shows a bathroom with a pink and blue color scheme, featuring a white toilet, a bathtub with a shower curtain, and wooden cabinets. The lighting appears to be natural, coming from a window above the bathtub.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20969.3, "ram_available_mb": 41871.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20969.8, "ram_available_mb": 41871.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.104}, "power_stats": {"power_gpu_soc_mean_watts": 20.16, "power_cpu_cv_mean_watts": 1.751, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 71.104, "power_watts_avg": 20.16, "energy_joules_est": 182.42, "duration_seconds": 9.048, "sample_count": 77}, "timestamp": "2026-01-26T10:45:57.676727"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11582.09, "latencies_ms": [11582.09], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and cozy bedroom scene. Dominating the right side of the frame is a bed, dressed in a bedspread of a warm, inviting yellow and white plaid pattern. The bed, with its dark wooden headboard, stands as the centerpiece of the room. \n\nTo the left, a window draped with red cur", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20969.8, "ram_available_mb": 41871.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20987.5, "ram_available_mb": 41853.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.869}, "power_stats": {"power_gpu_soc_mean_watts": 19.337, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 70.869, "power_watts_avg": 19.337, "energy_joules_est": 223.98, "duration_seconds": 11.583, "sample_count": 99}, "timestamp": "2026-01-26T10:46:11.290214"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10058.445, "latencies_ms": [10058.445], "images_per_second": 0.099, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "1. Bed: 1\n2. Pillow: 2\n3. Nightstand: 1\n4. Lamp: 1\n5. Window: 1\n6. Curtains: 2\n7. Drape: 1\n8. Wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20935.0, "ram_available_mb": 41905.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20935.6, "ram_available_mb": 41905.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.588}, "power_stats": {"power_gpu_soc_mean_watts": 20.017, "power_cpu_cv_mean_watts": 1.77, "power_sys_5v0_mean_watts": 8.608, "gpu_utilization_percent_mean": 71.588, "power_watts_avg": 20.017, "energy_joules_est": 201.35, "duration_seconds": 10.059, "sample_count": 85}, "timestamp": "2026-01-26T10:46:23.381179"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9889.892, "latencies_ms": [9889.892], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The bed is located in the foreground of the image, occupying the lower right portion. The window with curtains is in the background, positioned on the left side of the image. The lamp is placed on a small bedside table, situated to the right of the bed and slightly behind it.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20935.6, "ram_available_mb": 41905.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20937.7, "ram_available_mb": 41903.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.607}, "power_stats": {"power_gpu_soc_mean_watts": 19.838, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 71.607, "power_watts_avg": 19.838, "energy_joules_est": 196.21, "duration_seconds": 9.891, "sample_count": 84}, "timestamp": "2026-01-26T10:46:35.286184"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7744.723, "latencies_ms": [7744.723], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image depicts a dimly lit bedroom with a bed placed against the wall. The room has a window with curtains, and a lamp is placed on a side table next to the bed.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20937.7, "ram_available_mb": 41903.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 21000.3, "ram_available_mb": 41840.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.154}, "power_stats": {"power_gpu_soc_mean_watts": 21.43, "power_cpu_cv_mean_watts": 1.699, "power_sys_5v0_mean_watts": 8.627, "gpu_utilization_percent_mean": 74.154, "power_watts_avg": 21.43, "energy_joules_est": 165.98, "duration_seconds": 7.745, "sample_count": 65}, "timestamp": "2026-01-26T10:46:45.044213"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8506.946, "latencies_ms": [8506.946], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The room is dimly lit with natural light coming in through the window, casting a soft glow on the plaid bedspread. The curtains are drawn back, allowing the light to filter into the room and create a serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20945.3, "ram_available_mb": 41895.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21000.3, "ram_available_mb": 41840.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.452}, "power_stats": {"power_gpu_soc_mean_watts": 20.197, "power_cpu_cv_mean_watts": 1.699, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 72.452, "power_watts_avg": 20.197, "energy_joules_est": 171.83, "duration_seconds": 8.508, "sample_count": 73}, "timestamp": "2026-01-26T10:46:55.583100"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12391.568, "latencies_ms": [12391.568], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man and woman are standing close to each other, with the woman placing a white flower on the man's lapel. The man is wearing a black suit and tie, while the woman is dressed in a black dress. They are both smiling, indicating a joyful and celebratory atmosphere. The woman is holding the flower with her right hand, and the", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20995.8, "ram_available_mb": 41845.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.364}, "power_stats": {"power_gpu_soc_mean_watts": 21.483, "power_cpu_cv_mean_watts": 1.821, "power_sys_5v0_mean_watts": 8.901, "gpu_utilization_percent_mean": 72.364, "power_watts_avg": 21.483, "energy_joules_est": 266.22, "duration_seconds": 12.392, "sample_count": 107}, "timestamp": "2026-01-26T10:47:10.013530"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8801.819, "latencies_ms": [8801.819], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "person: 2, suit: 1, tie: 1, flower: 1, hand: 2, dress: 1, bracelet: 1, earring: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.1, "ram_available_mb": 41908.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20998.7, "ram_available_mb": 41842.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.054}, "power_stats": {"power_gpu_soc_mean_watts": 23.046, "power_cpu_cv_mean_watts": 1.433, "power_sys_5v0_mean_watts": 8.719, "gpu_utilization_percent_mean": 77.054, "power_watts_avg": 23.046, "energy_joules_est": 202.86, "duration_seconds": 8.802, "sample_count": 74}, "timestamp": "2026-01-26T10:47:20.840591"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8863.271, "latencies_ms": [8863.271], "images_per_second": 0.113, "prompt_tokens": 44, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The person on the left is standing in the foreground, while the person on the right is slightly behind and to the right of the person on the left. The background is a plain wall with a shadow cast on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20935.3, "ram_available_mb": 41905.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20953.0, "ram_available_mb": 41887.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.554}, "power_stats": {"power_gpu_soc_mean_watts": 23.091, "power_cpu_cv_mean_watts": 1.492, "power_sys_5v0_mean_watts": 8.828, "gpu_utilization_percent_mean": 77.554, "power_watts_avg": 23.091, "energy_joules_est": 204.68, "duration_seconds": 8.864, "sample_count": 74}, "timestamp": "2026-01-26T10:47:31.714292"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10078.688, "latencies_ms": [10078.688], "images_per_second": 0.099, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "In the image, a young man and woman are standing close to each other, with the woman placing a white flower on the man's lapel. They are both dressed in formal attire, suggesting that they are attending a special event or celebration.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20953.0, "ram_available_mb": 41887.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21017.9, "ram_available_mb": 41823.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.576}, "power_stats": {"power_gpu_soc_mean_watts": 22.5, "power_cpu_cv_mean_watts": 1.558, "power_sys_5v0_mean_watts": 8.748, "gpu_utilization_percent_mean": 76.576, "power_watts_avg": 22.5, "energy_joules_est": 226.79, "duration_seconds": 10.08, "sample_count": 85}, "timestamp": "2026-01-26T10:47:43.807837"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10215.851, "latencies_ms": [10215.851], "images_per_second": 0.098, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image features a man in a black suit with a beige tie and a woman in a black and silver sequined dress. The lighting appears to be artificial, likely from indoor lighting, and the overall color scheme is dark with some shimmering silver accents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21017.9, "ram_available_mb": 41823.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20995.9, "ram_available_mb": 41845.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.931}, "power_stats": {"power_gpu_soc_mean_watts": 22.395, "power_cpu_cv_mean_watts": 1.651, "power_sys_5v0_mean_watts": 8.851, "gpu_utilization_percent_mean": 74.931, "power_watts_avg": 22.395, "energy_joules_est": 228.8, "duration_seconds": 10.216, "sample_count": 87}, "timestamp": "2026-01-26T10:47:56.039875"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11615.38, "latencies_ms": [11615.38], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a stop sign mounted on a metal pole, which is positioned behind a chain-link fence. The fence is located in front of a building, and the stop sign is clearly visible through the fence. The scene appears to be set in a residential area, as there are palm trees in the background, adding a touch of greenery to the", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 20925.7, "ram_available_mb": 41915.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20942.2, "ram_available_mb": 41898.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.485}, "power_stats": {"power_gpu_soc_mean_watts": 19.286, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 70.485, "power_watts_avg": 19.286, "energy_joules_est": 224.03, "duration_seconds": 11.616, "sample_count": 99}, "timestamp": "2026-01-26T10:48:09.731149"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7886.516, "latencies_ms": [7886.516], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "fence: 1, stop sign: 1, palm trees: 5, trash: 2, bush: 1, building: 1, car: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20942.2, "ram_available_mb": 41898.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20963.6, "ram_available_mb": 41877.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.881}, "power_stats": {"power_gpu_soc_mean_watts": 21.186, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 73.881, "power_watts_avg": 21.186, "energy_joules_est": 167.1, "duration_seconds": 7.887, "sample_count": 67}, "timestamp": "2026-01-26T10:48:19.645601"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9548.698, "latencies_ms": [9548.698], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The stop sign is positioned in the foreground on the left side of the image, behind the chain-link fence which occupies the middle ground. In the background, there are palm trees and a building, indicating that the fence is likely in an urban or residential area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.6, "ram_available_mb": 41877.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20970.6, "ram_available_mb": 41870.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.036}, "power_stats": {"power_gpu_soc_mean_watts": 19.723, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 72.036, "power_watts_avg": 19.723, "energy_joules_est": 188.34, "duration_seconds": 9.549, "sample_count": 83}, "timestamp": "2026-01-26T10:48:31.224278"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9143.313, "latencies_ms": [9143.313], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "A stop sign is mounted on a chain-link fence, indicating that vehicles must come to a complete stop before proceeding. The fence is located in a grassy area with palm trees and a building in the background, suggesting an urban or residential setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20970.6, "ram_available_mb": 41870.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20920.3, "ram_available_mb": 41920.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.372}, "power_stats": {"power_gpu_soc_mean_watts": 20.47, "power_cpu_cv_mean_watts": 1.713, "power_sys_5v0_mean_watts": 8.58, "gpu_utilization_percent_mean": 70.372, "power_watts_avg": 20.47, "energy_joules_est": 187.18, "duration_seconds": 9.144, "sample_count": 78}, "timestamp": "2026-01-26T10:48:42.389410"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9246.304, "latencies_ms": [9246.304], "images_per_second": 0.108, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image features a red and white stop sign mounted on a metal pole, which is partially obscured by a chain-link fence. The fence is green and appears to be in a state of disrepair, with some debris and trash scattered around the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20920.3, "ram_available_mb": 41920.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21017.4, "ram_available_mb": 41823.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.308}, "power_stats": {"power_gpu_soc_mean_watts": 20.122, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 8.662, "gpu_utilization_percent_mean": 71.308, "power_watts_avg": 20.122, "energy_joules_est": 186.07, "duration_seconds": 9.247, "sample_count": 78}, "timestamp": "2026-01-26T10:48:53.666918"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11577.128, "latencies_ms": [11577.128], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a man standing next to a motorcycle and a bicycle. The motorcycle is parked on the left side of the scene, while the bicycle is positioned on the right side. The man appears to be looking at the bicycle, possibly admiring it or considering whether to ride it.\n\nThere are also two other bicycles", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20920.6, "ram_available_mb": 41920.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20957.8, "ram_available_mb": 41883.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.622}, "power_stats": {"power_gpu_soc_mean_watts": 19.348, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 70.622, "power_watts_avg": 19.348, "energy_joules_est": 224.01, "duration_seconds": 11.578, "sample_count": 98}, "timestamp": "2026-01-26T10:49:07.278301"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7867.185, "latencies_ms": [7867.185], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "motorcycle: 1, bicycle: 3, tree: 2, wall: 1, basket: 1, tire: 3, leaf: 1, sock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20957.8, "ram_available_mb": 41883.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20958.5, "ram_available_mb": 41882.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.242}, "power_stats": {"power_gpu_soc_mean_watts": 21.252, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.576, "gpu_utilization_percent_mean": 74.242, "power_watts_avg": 21.252, "energy_joules_est": 167.21, "duration_seconds": 7.868, "sample_count": 66}, "timestamp": "2026-01-26T10:49:17.164067"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10656.169, "latencies_ms": [10656.169], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "In the foreground, there is a motorcycle on the left side of the image, and a bicycle with a basket in the center. To the right, there is another bicycle leaning against a tree. In the background, there is a person standing near the right side of the image, partially obscured by the trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20958.5, "ram_available_mb": 41882.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20958.0, "ram_available_mb": 41882.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.289}, "power_stats": {"power_gpu_soc_mean_watts": 19.575, "power_cpu_cv_mean_watts": 1.858, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 71.289, "power_watts_avg": 19.575, "energy_joules_est": 208.61, "duration_seconds": 10.657, "sample_count": 90}, "timestamp": "2026-01-26T10:49:29.844279"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8092.495, "latencies_ms": [8092.495], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "In a residential area, a man is standing next to a motorcycle while two bicycles are parked nearby. The scene suggests a casual outdoor setting where people are engaging in different modes of transportation.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20958.0, "ram_available_mb": 41882.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20958.2, "ram_available_mb": 41882.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.029}, "power_stats": {"power_gpu_soc_mean_watts": 21.066, "power_cpu_cv_mean_watts": 1.612, "power_sys_5v0_mean_watts": 8.598, "gpu_utilization_percent_mean": 73.029, "power_watts_avg": 21.066, "energy_joules_est": 170.49, "duration_seconds": 8.093, "sample_count": 69}, "timestamp": "2026-01-26T10:49:39.995996"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8057.863, "latencies_ms": [8057.863], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image shows a sunny day with clear skies, casting shadows on the ground. There are three bicycles, one yellow, one black, and one with a red seat, all parked on a concrete surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20958.2, "ram_available_mb": 41882.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20958.5, "ram_available_mb": 41882.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.319}, "power_stats": {"power_gpu_soc_mean_watts": 20.43, "power_cpu_cv_mean_watts": 1.647, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 71.319, "power_watts_avg": 20.43, "energy_joules_est": 164.64, "duration_seconds": 8.059, "sample_count": 69}, "timestamp": "2026-01-26T10:49:50.082408"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11585.685, "latencies_ms": [11585.685], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a busy city street scene with a man standing on the sidewalk, looking at a street sign. He is wearing a blue shirt and jeans, and there are two other people in the scene, one closer to the left side and another further back. A red car is visible on the street, and a bicycle can be seen parked on the right side", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20958.5, "ram_available_mb": 41882.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20959.4, "ram_available_mb": 41881.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.388}, "power_stats": {"power_gpu_soc_mean_watts": 19.335, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 70.388, "power_watts_avg": 19.335, "energy_joules_est": 224.02, "duration_seconds": 11.586, "sample_count": 98}, "timestamp": "2026-01-26T10:50:03.700799"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9822.255, "latencies_ms": [9822.255], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "- pedestrian: 1\n\n- traffic light: 3\n\n- car: 1\n\n- bicycle: 1\n\n- building: 1\n\n- sign: 2\n\n- trash can: 1\n\n- road: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20959.4, "ram_available_mb": 41881.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20960.2, "ram_available_mb": 41880.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.69}, "power_stats": {"power_gpu_soc_mean_watts": 20.07, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 8.59, "gpu_utilization_percent_mean": 70.69, "power_watts_avg": 20.07, "energy_joules_est": 197.15, "duration_seconds": 9.823, "sample_count": 84}, "timestamp": "2026-01-26T10:50:15.575103"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10195.923, "latencies_ms": [10195.923], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "In the foreground, there is a pedestrian standing on the sidewalk, looking towards the street. Behind the pedestrian, there is a traffic light and a street sign indicating the direction to Procter Street. In the background, there is a red car driving on the road and a building with windows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20960.2, "ram_available_mb": 41880.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20988.9, "ram_available_mb": 41852.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.575}, "power_stats": {"power_gpu_soc_mean_watts": 19.741, "power_cpu_cv_mean_watts": 1.831, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 70.575, "power_watts_avg": 19.741, "energy_joules_est": 201.29, "duration_seconds": 10.196, "sample_count": 87}, "timestamp": "2026-01-26T10:50:27.827272"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9695.099, "latencies_ms": [9695.099], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image depicts a city street scene with a pedestrian standing on the sidewalk, a car parked on the side of the road, and a cyclist riding a bicycle. There are traffic lights and street signs visible, indicating an urban environment with regulated traffic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20935.3, "ram_available_mb": 41905.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20974.2, "ram_available_mb": 41866.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.205}, "power_stats": {"power_gpu_soc_mean_watts": 20.086, "power_cpu_cv_mean_watts": 1.76, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 72.205, "power_watts_avg": 20.086, "energy_joules_est": 194.75, "duration_seconds": 9.696, "sample_count": 83}, "timestamp": "2026-01-26T10:50:39.561051"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9936.539, "latencies_ms": [9936.539], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows a street scene with a pedestrian standing on the sidewalk, a red car in the background, and a building with a brick facade. The weather appears to be overcast, and the lighting is natural but subdued, suggesting it might be an early morning or late afternoon time.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.9, "ram_available_mb": 41867.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 20978.3, "ram_available_mb": 41862.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.718}, "power_stats": {"power_gpu_soc_mean_watts": 19.821, "power_cpu_cv_mean_watts": 2.02, "power_sys_5v0_mean_watts": 8.66, "gpu_utilization_percent_mean": 70.718, "power_watts_avg": 19.821, "energy_joules_est": 196.97, "duration_seconds": 9.937, "sample_count": 85}, "timestamp": "2026-01-26T10:50:51.515549"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11567.727, "latencies_ms": [11567.727], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a bronze statue of two people sitting on a bench, depicting a man and a woman. They appear to be engaged in a conversation, with the man holding a briefcase. The statue is placed on a brick sidewalk, and there are a few people standing nearby, possibly admiring the artwork.\n\nIn the background, there is a car", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20978.3, "ram_available_mb": 41862.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.4, "ram_used_mb": 20996.5, "ram_available_mb": 41844.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.162}, "power_stats": {"power_gpu_soc_mean_watts": 19.359, "power_cpu_cv_mean_watts": 2.163, "power_sys_5v0_mean_watts": 8.66, "gpu_utilization_percent_mean": 70.162, "power_watts_avg": 19.359, "energy_joules_est": 223.95, "duration_seconds": 11.568, "sample_count": 99}, "timestamp": "2026-01-26T10:51:05.149909"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7565.148, "latencies_ms": [7565.148], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "people: 3, statue: 1, bench: 1, suitcase: 1, building: 1, pole: 2, shadow: 1, sun: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20933.6, "ram_available_mb": 41907.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.5, "ram_used_mb": 21006.0, "ram_available_mb": 41834.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.812}, "power_stats": {"power_gpu_soc_mean_watts": 21.314, "power_cpu_cv_mean_watts": 1.751, "power_sys_5v0_mean_watts": 8.579, "gpu_utilization_percent_mean": 73.812, "power_watts_avg": 21.314, "energy_joules_est": 161.26, "duration_seconds": 7.566, "sample_count": 64}, "timestamp": "2026-01-26T10:51:14.743063"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11607.709, "latencies_ms": [11607.709], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a bronze statue of two people sitting closely together, with one person's hand resting on the other's knee, suggesting a sense of intimacy or conversation. Behind the statue, there are two individuals standing, one wearing a bright yellow jacket and the other in a darker outfit, both facing away from the camera, which", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20935.8, "ram_available_mb": 41905.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 9.0, "ram_used_mb": 20939.2, "ram_available_mb": 41901.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.384}, "power_stats": {"power_gpu_soc_mean_watts": 19.213, "power_cpu_cv_mean_watts": 2.058, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 70.384, "power_watts_avg": 19.213, "energy_joules_est": 223.03, "duration_seconds": 11.608, "sample_count": 99}, "timestamp": "2026-01-26T10:51:28.386454"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8696.846, "latencies_ms": [8696.846], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image depicts a bronze statue of two people sitting on a bench, with a third person standing nearby. The statue is located on a paved area, possibly a public square or park, with a closed garage door in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20939.2, "ram_available_mb": 41901.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21023.7, "ram_available_mb": 41817.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.689}, "power_stats": {"power_gpu_soc_mean_watts": 20.711, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.577, "gpu_utilization_percent_mean": 72.689, "power_watts_avg": 20.711, "energy_joules_est": 180.13, "duration_seconds": 8.697, "sample_count": 74}, "timestamp": "2026-01-26T10:51:39.108240"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11311.328, "latencies_ms": [11311.328], "images_per_second": 0.088, "prompt_tokens": 36, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The sculpture depicts two figures seated closely together, with one figure holding a suitcase. The figures are made of a dark material, possibly bronze, and the sculpture is situated on a stone pedestal. The lighting in the image is natural, with shadows cast on the ground, indicating that the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21023.7, "ram_available_mb": 41817.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20985.7, "ram_available_mb": 41855.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.344}, "power_stats": {"power_gpu_soc_mean_watts": 19.36, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 70.344, "power_watts_avg": 19.36, "energy_joules_est": 219.0, "duration_seconds": 11.312, "sample_count": 96}, "timestamp": "2026-01-26T10:51:52.452762"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11548.87, "latencies_ms": [11548.87], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a street sign post, standing tall against the backdrop of a clear blue sky. The post is adorned with a series of signs, each with its own unique color and symbol, guiding the way for different types of vehicles.\n\nAt the top, a blue sign with a white airplane symbol indicates the direction to an airport. Just below", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 20931.3, "ram_available_mb": 41909.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20978.2, "ram_available_mb": 41862.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.673}, "power_stats": {"power_gpu_soc_mean_watts": 19.32, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 70.673, "power_watts_avg": 19.32, "energy_joules_est": 223.14, "duration_seconds": 11.55, "sample_count": 98}, "timestamp": "2026-01-26T10:52:06.041188"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10645.367, "latencies_ms": [10645.367], "images_per_second": 0.094, "prompt_tokens": 39, "response_tokens_est": 69, "n_tiles": 16, "output_text": "- Sign: 5\n\n- Parking symbol: 1\n\n- No parking symbol: 1\n\n- Traffic sign: 1\n\n- Arrow: 4\n\n- Parking area: 1\n\n- Parking symbol: 1\n\n- Traffic sign: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20978.2, "ram_available_mb": 41862.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20978.2, "ram_available_mb": 41862.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.319}, "power_stats": {"power_gpu_soc_mean_watts": 19.894, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 8.586, "gpu_utilization_percent_mean": 71.319, "power_watts_avg": 19.894, "energy_joules_est": 211.79, "duration_seconds": 10.646, "sample_count": 91}, "timestamp": "2026-01-26T10:52:18.723581"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10634.327, "latencies_ms": [10634.327], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The signs are arranged vertically with the 'Severinsbr\u00fccke' sign at the top, followed by 'Koelnmesse' in the middle, and 'Im Sionstal' at the bottom. In the foreground, there is a no parking sign to the left and a no left turn sign to the right.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20978.2, "ram_available_mb": 41862.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20998.2, "ram_available_mb": 41842.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.967}, "power_stats": {"power_gpu_soc_mean_watts": 19.793, "power_cpu_cv_mean_watts": 1.829, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 69.967, "power_watts_avg": 19.793, "energy_joules_est": 210.5, "duration_seconds": 10.635, "sample_count": 91}, "timestamp": "2026-01-26T10:52:31.382557"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11772.989, "latencies_ms": [11772.989], "images_per_second": 0.085, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a collection of traffic signs mounted on a pole, likely at a roadside or intersection. The signs indicate directions to various destinations such as Severinsbr\u00fccke, Koelnmesse, and Im Sionstal, with arrows pointing to the right for each. Additionally, there is a no parking sign and a no left turn sign, indicating traffic regulations at this location", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20998.2, "ram_available_mb": 41842.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20999.7, "ram_available_mb": 41841.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.206}, "power_stats": {"power_gpu_soc_mean_watts": 19.453, "power_cpu_cv_mean_watts": 1.863, "power_sys_5v0_mean_watts": 8.621, "gpu_utilization_percent_mean": 71.206, "power_watts_avg": 19.453, "energy_joules_est": 229.03, "duration_seconds": 11.774, "sample_count": 102}, "timestamp": "2026-01-26T10:52:45.178268"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10251.706, "latencies_ms": [10251.706], "images_per_second": 0.098, "prompt_tokens": 36, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image shows a series of traffic signs mounted on a pole, with the top sign featuring an airplane and a bridge, indicating directions to an airport and a bridge. The signs are in various colors such as blue, yellow, green, and white, with arrows pointing to the right, suggesting the direction to follow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20928.5, "ram_available_mb": 41912.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20967.4, "ram_available_mb": 41873.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.648}, "power_stats": {"power_gpu_soc_mean_watts": 19.781, "power_cpu_cv_mean_watts": 1.819, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 71.648, "power_watts_avg": 19.781, "energy_joules_est": 202.8, "duration_seconds": 10.252, "sample_count": 88}, "timestamp": "2026-01-26T10:52:57.452009"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11546.985, "latencies_ms": [11546.985], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman and a young girl are standing next to each other on a train platform. They are both smiling and posing for the camera, with the woman holding a suitcase. The woman is wearing a red shirt, while the girl is wearing a blue shirt. \n\nThere are other people in the background, with one person standing further back and", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 20967.4, "ram_available_mb": 41873.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20968.6, "ram_available_mb": 41872.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.71}, "power_stats": {"power_gpu_soc_mean_watts": 19.27, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 70.71, "power_watts_avg": 19.27, "energy_joules_est": 222.53, "duration_seconds": 11.548, "sample_count": 100}, "timestamp": "2026-01-26T10:53:11.039893"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8835.7, "latencies_ms": [8835.7], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "- Woman: 2\n- Suitcase: 1\n- Phone: 1\n- Bag: 1\n- Train: 1\n- Floor: 1\n- Tiles: 1\n- Ceiling: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20968.6, "ram_available_mb": 41872.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20969.5, "ram_available_mb": 41871.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.147}, "power_stats": {"power_gpu_soc_mean_watts": 20.525, "power_cpu_cv_mean_watts": 1.665, "power_sys_5v0_mean_watts": 8.584, "gpu_utilization_percent_mean": 73.147, "power_watts_avg": 20.525, "energy_joules_est": 181.37, "duration_seconds": 8.836, "sample_count": 75}, "timestamp": "2026-01-26T10:53:21.933409"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11091.853, "latencies_ms": [11091.853], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "In the foreground, there is a black suitcase with a tag on it, placed on the ground. Behind the suitcase, there are two individuals standing close to each other, with one person wearing a red shirt and the other wearing a blue shirt. In the background, there is a red pillar and a train station platform.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20969.5, "ram_available_mb": 41871.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20970.0, "ram_available_mb": 41870.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.779}, "power_stats": {"power_gpu_soc_mean_watts": 19.495, "power_cpu_cv_mean_watts": 1.853, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 71.779, "power_watts_avg": 19.495, "energy_joules_est": 216.25, "duration_seconds": 11.093, "sample_count": 95}, "timestamp": "2026-01-26T10:53:35.049979"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7529.52, "latencies_ms": [7529.52], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "Two women are standing next to a suitcase on a train platform, possibly waiting for their train or just arrived at their destination. They are both smiling and seem to be in a good mood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20970.0, "ram_available_mb": 41870.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20971.0, "ram_available_mb": 41869.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.953}, "power_stats": {"power_gpu_soc_mean_watts": 21.664, "power_cpu_cv_mean_watts": 1.563, "power_sys_5v0_mean_watts": 8.598, "gpu_utilization_percent_mean": 73.953, "power_watts_avg": 21.664, "energy_joules_est": 163.13, "duration_seconds": 7.53, "sample_count": 64}, "timestamp": "2026-01-26T10:53:44.628212"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10363.545, "latencies_ms": [10363.545], "images_per_second": 0.096, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image shows two individuals standing next to a black suitcase with a tag visible on it, in a well-lit indoor setting with a red and white color scheme. The person on the left is wearing a red hoodie and the person on the right is wearing a blue shirt with a floral pattern.", "error": null, "sys_before": {"cpu_percent": 27.3, "ram_used_mb": 20971.0, "ram_available_mb": 41869.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20971.7, "ram_available_mb": 41869.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.667}, "power_stats": {"power_gpu_soc_mean_watts": 19.756, "power_cpu_cv_mean_watts": 1.823, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 71.667, "power_watts_avg": 19.756, "energy_joules_est": 204.75, "duration_seconds": 10.364, "sample_count": 90}, "timestamp": "2026-01-26T10:53:57.003561"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11580.608, "latencies_ms": [11580.608], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are three zebras standing in a line, facing the camera. They are positioned in front of a tree with purple flowers, possibly a wisteria tree. The zebras are standing on a dirt ground, and their black and white stripes are clearly visible. The scene appears to be set in a natural environment, possibly a wildlife reserve or", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 20971.7, "ram_available_mb": 41869.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20972.4, "ram_available_mb": 41868.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.141}, "power_stats": {"power_gpu_soc_mean_watts": 19.362, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 70.141, "power_watts_avg": 19.362, "energy_joules_est": 224.24, "duration_seconds": 11.581, "sample_count": 99}, "timestamp": "2026-01-26T10:54:10.615852"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7574.575, "latencies_ms": [7574.575], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "zebra: 3, trees: numerous, flowers: numerous, branches: numerous, ground: dirt, rocks: few, grass: dirt, sun: visible, shadows: visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20972.4, "ram_available_mb": 41868.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20972.7, "ram_available_mb": 41868.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.188}, "power_stats": {"power_gpu_soc_mean_watts": 21.334, "power_cpu_cv_mean_watts": 1.551, "power_sys_5v0_mean_watts": 8.548, "gpu_utilization_percent_mean": 74.188, "power_watts_avg": 21.334, "energy_joules_est": 161.61, "duration_seconds": 7.575, "sample_count": 64}, "timestamp": "2026-01-26T10:54:20.248192"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10773.131, "latencies_ms": [10773.131], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "Three zebras are standing in a line, with one on the left, one in the middle, and one on the right. They are all facing the same direction, and there is a tree with purple flowers in the background. The zebras are standing on a dirt ground, and there are some branches and twigs scattered around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20972.7, "ram_available_mb": 41868.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20972.7, "ram_available_mb": 41868.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.846}, "power_stats": {"power_gpu_soc_mean_watts": 19.477, "power_cpu_cv_mean_watts": 1.856, "power_sys_5v0_mean_watts": 8.669, "gpu_utilization_percent_mean": 70.846, "power_watts_avg": 19.477, "energy_joules_est": 209.84, "duration_seconds": 10.774, "sample_count": 91}, "timestamp": "2026-01-26T10:54:33.076676"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7958.787, "latencies_ms": [7958.787], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "Three zebras are standing in a natural setting with trees and purple flowers in the background. The zebras appear to be in a peaceful and serene environment, possibly a wildlife reserve or a zoo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20972.7, "ram_available_mb": 41868.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20973.7, "ram_available_mb": 41867.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.855}, "power_stats": {"power_gpu_soc_mean_watts": 21.184, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.606, "gpu_utilization_percent_mean": 72.855, "power_watts_avg": 21.184, "energy_joules_est": 168.61, "duration_seconds": 7.959, "sample_count": 69}, "timestamp": "2026-01-26T10:54:43.065730"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7196.289, "latencies_ms": [7196.289], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "Three zebras are standing in a natural habitat with trees and purple flowers in the background. The ground is covered in dirt and the zebras have distinct black and white stripes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.7, "ram_available_mb": 41867.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20973.7, "ram_available_mb": 41867.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.492}, "power_stats": {"power_gpu_soc_mean_watts": 21.378, "power_cpu_cv_mean_watts": 1.574, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 73.492, "power_watts_avg": 21.378, "energy_joules_est": 153.86, "duration_seconds": 7.197, "sample_count": 61}, "timestamp": "2026-01-26T10:54:52.281522"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11551.605, "latencies_ms": [11551.605], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a tripod stands firmly on a gray floor, its three legs spread out in a sturdy stance. The tripod is equipped with a camera mounted on top, ready to capture the world around it. The camera, a silent observer, is connected to a laptop via a cable, suggesting a live feed or recording in progress.\n\nTo", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20973.7, "ram_available_mb": 41867.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20992.2, "ram_available_mb": 41848.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.424}, "power_stats": {"power_gpu_soc_mean_watts": 19.201, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 69.424, "power_watts_avg": 19.201, "energy_joules_est": 221.81, "duration_seconds": 11.552, "sample_count": 99}, "timestamp": "2026-01-26T10:55:05.861383"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9923.746, "latencies_ms": [9923.746], "images_per_second": 0.101, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "- tripod: 1\n\n- camera: 1\n\n- laptop: 1\n\n- chair: 1\n\n- cooler: 1\n\n- refrigerator: 1\n\n- water dispenser: 1\n\n- table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20929.5, "ram_available_mb": 41911.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21005.2, "ram_available_mb": 41835.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.857}, "power_stats": {"power_gpu_soc_mean_watts": 19.923, "power_cpu_cv_mean_watts": 1.753, "power_sys_5v0_mean_watts": 8.56, "gpu_utilization_percent_mean": 71.857, "power_watts_avg": 19.923, "energy_joules_est": 197.72, "duration_seconds": 9.924, "sample_count": 84}, "timestamp": "2026-01-26T10:55:17.800949"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11664.188, "latencies_ms": [11664.188], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a tripod with a camera mounted on top, positioned near a laptop that is placed on a chair. The laptop is in the foreground and appears to be the main focus of the image. In the background, there is a vending machine and a table with various items on it, both of which are further away from the camera's viewpoint.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20934.8, "ram_available_mb": 41906.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.4, "ram_used_mb": 20946.8, "ram_available_mb": 41894.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.98}, "power_stats": {"power_gpu_soc_mean_watts": 19.359, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 70.98, "power_watts_avg": 19.359, "energy_joules_est": 225.82, "duration_seconds": 11.665, "sample_count": 99}, "timestamp": "2026-01-26T10:55:31.479964"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8379.129, "latencies_ms": [8379.129], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "In the image, a tripod is set up in a room with a laptop on it, and a vending machine is visible in the background. It appears to be a workspace or a temporary setup for a video or photo shoot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20946.8, "ram_available_mb": 41894.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.5, "ram_used_mb": 21033.4, "ram_available_mb": 41807.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.521}, "power_stats": {"power_gpu_soc_mean_watts": 20.999, "power_cpu_cv_mean_watts": 1.849, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 73.521, "power_watts_avg": 20.999, "energy_joules_est": 175.97, "duration_seconds": 8.38, "sample_count": 71}, "timestamp": "2026-01-26T10:55:41.874498"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9849.591, "latencies_ms": [9849.591], "images_per_second": 0.102, "prompt_tokens": 36, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image features a tripod with a camera mounted on top, positioned in an indoor setting with a softbox lighting setup. The tripod is cream-colored, and the camera is black, contrasting with the white walls and the blue and white vending machine in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21033.4, "ram_available_mb": 41807.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 21003.9, "ram_available_mb": 41837.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.554}, "power_stats": {"power_gpu_soc_mean_watts": 19.945, "power_cpu_cv_mean_watts": 1.938, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 71.554, "power_watts_avg": 19.945, "energy_joules_est": 196.46, "duration_seconds": 9.85, "sample_count": 83}, "timestamp": "2026-01-26T10:55:53.739137"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11392.79, "latencies_ms": [11392.79], "images_per_second": 0.088, "prompt_tokens": 24, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the image, a sheep is standing in a pen, surrounded by a large amount of wool. The sheep is positioned in the center of the pen, with its body facing the camera. The wool is scattered all around the sheep, covering the ground and the sheep itself. The pen appears to be a fenced area, providing a secure environment for the sheep.", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20941.4, "ram_available_mb": 41899.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 9.0, "ram_used_mb": 20947.5, "ram_available_mb": 41893.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.742}, "power_stats": {"power_gpu_soc_mean_watts": 19.34, "power_cpu_cv_mean_watts": 1.943, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 70.742, "power_watts_avg": 19.34, "energy_joules_est": 220.35, "duration_seconds": 11.393, "sample_count": 97}, "timestamp": "2026-01-26T10:56:07.164710"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8391.511, "latencies_ms": [8391.511], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "sheep: 1, wool pile: 1, metal fence: 1, concrete ground: 1, white object: 1, black object: 1, gray object: 1, metal grid: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20947.5, "ram_available_mb": 41893.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.8, "ram_used_mb": 20947.6, "ram_available_mb": 41893.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.169}, "power_stats": {"power_gpu_soc_mean_watts": 21.022, "power_cpu_cv_mean_watts": 1.961, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 73.169, "power_watts_avg": 21.022, "energy_joules_est": 176.42, "duration_seconds": 8.392, "sample_count": 71}, "timestamp": "2026-01-26T10:56:17.612750"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11545.303, "latencies_ms": [11545.303], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a pile of wool that is being sorted by a sheep, which is standing behind the pile. The sheep is positioned in the middle ground of the image, with its body facing towards the left side of the frame. In the background, there is a metal fence that encloses the area where the sheep and the wool are located.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20947.6, "ram_available_mb": 41893.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21045.4, "ram_available_mb": 41795.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.949}, "power_stats": {"power_gpu_soc_mean_watts": 19.409, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 69.949, "power_watts_avg": 19.409, "energy_joules_est": 224.1, "duration_seconds": 11.546, "sample_count": 98}, "timestamp": "2026-01-26T10:56:31.214833"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7218.016, "latencies_ms": [7218.016], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A sheep is standing in a pen with a large amount of wool piled up in front of it. The sheep appears to be eating or nibbling on the wool.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20948.9, "ram_available_mb": 41892.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21036.1, "ram_available_mb": 41804.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.59}, "power_stats": {"power_gpu_soc_mean_watts": 21.641, "power_cpu_cv_mean_watts": 1.515, "power_sys_5v0_mean_watts": 8.564, "gpu_utilization_percent_mean": 73.59, "power_watts_avg": 21.641, "energy_joules_est": 156.22, "duration_seconds": 7.219, "sample_count": 61}, "timestamp": "2026-01-26T10:56:40.455654"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8257.391, "latencies_ms": [8257.391], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a sheep standing inside a metal fence, surrounded by a large amount of wool. The wool appears to be in various shades of grey and white, indicating different stages of shearing or different breeds of sheep.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21036.1, "ram_available_mb": 41804.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20985.6, "ram_available_mb": 41855.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.6}, "power_stats": {"power_gpu_soc_mean_watts": 20.437, "power_cpu_cv_mean_watts": 1.686, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 72.6, "power_watts_avg": 20.437, "energy_joules_est": 168.77, "duration_seconds": 8.258, "sample_count": 70}, "timestamp": "2026-01-26T10:56:50.744350"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11587.35, "latencies_ms": [11587.35], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a tennis match in progress on a blue court. There are two players, one on each side of the net, actively engaged in the game. Both players are holding tennis rackets, and a sports ball can be seen in the middle of the court. \n\nIn addition to the players, there are several other people present in the scene. Some of them are se", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20985.6, "ram_available_mb": 41855.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20986.3, "ram_available_mb": 41854.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.111}, "power_stats": {"power_gpu_soc_mean_watts": 19.36, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 70.111, "power_watts_avg": 19.36, "energy_joules_est": 224.34, "duration_seconds": 11.588, "sample_count": 99}, "timestamp": "2026-01-26T10:57:04.368507"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8773.991, "latencies_ms": [8773.991], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "- Players: 2\n- Ball: 1\n- Net: 1\n- Chair: 1\n- Fan: 1\n- Spectator: 1\n- Banner: 3\n- Sign: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20986.3, "ram_available_mb": 41854.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20986.5, "ram_available_mb": 41854.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.164}, "power_stats": {"power_gpu_soc_mean_watts": 20.407, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 8.561, "gpu_utilization_percent_mean": 73.164, "power_watts_avg": 20.407, "energy_joules_est": 179.06, "duration_seconds": 8.775, "sample_count": 73}, "timestamp": "2026-01-26T10:57:15.167682"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10877.146, "latencies_ms": [10877.146], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, there is a tennis court with two players in the middle of a match, one near the baseline and the other closer to the net. The audience is seated in the background, surrounding the court on all sides. The scoreboard is positioned above the court, near the center, and is at a distance from the players.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20986.5, "ram_available_mb": 41854.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20986.8, "ram_available_mb": 41854.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.688}, "power_stats": {"power_gpu_soc_mean_watts": 19.545, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 70.688, "power_watts_avg": 19.545, "energy_joules_est": 212.61, "duration_seconds": 10.878, "sample_count": 93}, "timestamp": "2026-01-26T10:57:28.086587"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7215.462, "latencies_ms": [7215.462], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image captures a tennis match in progress on a blue court, with players actively engaged in the game. The audience is seated in the stands, watching the match intently.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 20986.8, "ram_available_mb": 41854.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20987.5, "ram_available_mb": 41853.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.983}, "power_stats": {"power_gpu_soc_mean_watts": 21.709, "power_cpu_cv_mean_watts": 1.507, "power_sys_5v0_mean_watts": 8.554, "gpu_utilization_percent_mean": 74.983, "power_watts_avg": 21.709, "energy_joules_est": 156.65, "duration_seconds": 7.216, "sample_count": 60}, "timestamp": "2026-01-26T10:57:37.322503"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7897.03, "latencies_ms": [7897.03], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The tennis court is a vibrant blue color, likely due to the blue paint used on the surface. The lighting appears to be artificial, as it is evenly distributed across the court, suggesting an indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20987.5, "ram_available_mb": 41853.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20988.0, "ram_available_mb": 41852.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.881}, "power_stats": {"power_gpu_soc_mean_watts": 20.772, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 8.603, "gpu_utilization_percent_mean": 71.881, "power_watts_avg": 20.772, "energy_joules_est": 164.05, "duration_seconds": 7.898, "sample_count": 67}, "timestamp": "2026-01-26T10:57:47.243616"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11566.447, "latencies_ms": [11566.447], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is walking through a large, open glass doorway in a building. She is carrying a suitcase and appears to be in a hurry. The woman is wearing a black coat and a hat, and she is walking on a tiled floor. The building has a modern design, with a staircase visible in the background. There are also signs and a", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20988.0, "ram_available_mb": 41852.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20988.9, "ram_available_mb": 41852.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.429}, "power_stats": {"power_gpu_soc_mean_watts": 19.214, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 69.429, "power_watts_avg": 19.214, "energy_joules_est": 222.25, "duration_seconds": 11.567, "sample_count": 98}, "timestamp": "2026-01-26T10:58:00.845293"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10410.229, "latencies_ms": [10410.229], "images_per_second": 0.096, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "- Glass doors: 1\n\n- Suitcase: 1\n\n- Person: 1\n\n- Stairs: 1\n\n- Signage: 1\n\n- Tiles: 1\n\n- Pipes: 1\n\n- Light fixtures: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20988.9, "ram_available_mb": 41852.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20989.2, "ram_available_mb": 41851.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.724}, "power_stats": {"power_gpu_soc_mean_watts": 19.724, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 8.572, "gpu_utilization_percent_mean": 71.724, "power_watts_avg": 19.724, "energy_joules_est": 205.34, "duration_seconds": 10.411, "sample_count": 87}, "timestamp": "2026-01-26T10:58:13.317731"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11660.394, "latencies_ms": [11660.394], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "A person is walking in the foreground, carrying a suitcase, towards the left side of the image. In the background, there are escalators and a signboard with directions, indicating that the person is likely in a transportation hub such as an airport or train station. The person is near the entrance, which is in the foreground, and the escalators are further back", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20989.2, "ram_available_mb": 41851.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20989.2, "ram_available_mb": 41851.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.24}, "power_stats": {"power_gpu_soc_mean_watts": 19.376, "power_cpu_cv_mean_watts": 1.877, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 70.24, "power_watts_avg": 19.376, "energy_joules_est": 225.94, "duration_seconds": 11.661, "sample_count": 100}, "timestamp": "2026-01-26T10:58:26.995986"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7215.434, "latencies_ms": [7215.434], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A person is seen walking through a glass door in a modern building, carrying a suitcase. The interior appears to be a public transportation station with escalators and directional signs.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20989.2, "ram_available_mb": 41851.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20989.4, "ram_available_mb": 41851.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.18}, "power_stats": {"power_gpu_soc_mean_watts": 21.844, "power_cpu_cv_mean_watts": 1.502, "power_sys_5v0_mean_watts": 8.541, "gpu_utilization_percent_mean": 74.18, "power_watts_avg": 21.844, "energy_joules_est": 157.63, "duration_seconds": 7.216, "sample_count": 61}, "timestamp": "2026-01-26T10:58:36.223170"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8792.552, "latencies_ms": [8792.552], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a person in a black coat and hat walking through a modern, well-lit indoor space with a glass door. The floor is made of grey tiles, and there are stainless steel pillars and escalators in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20989.4, "ram_available_mb": 41851.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20992.7, "ram_available_mb": 41848.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.649}, "power_stats": {"power_gpu_soc_mean_watts": 20.447, "power_cpu_cv_mean_watts": 1.698, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 72.649, "power_watts_avg": 20.447, "energy_joules_est": 179.79, "duration_seconds": 8.793, "sample_count": 74}, "timestamp": "2026-01-26T10:58:47.036317"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11577.163, "latencies_ms": [11577.163], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with two pizzas placed on top of it. One pizza is located towards the left side of the table, while the other is positioned more towards the center. There are also two cups on the table, one near the left pizza and the other near the right pizza. \n\nIn addition to the pizzas and cups", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 20992.7, "ram_available_mb": 41848.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20993.0, "ram_available_mb": 41847.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.515}, "power_stats": {"power_gpu_soc_mean_watts": 19.262, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 70.515, "power_watts_avg": 19.262, "energy_joules_est": 223.01, "duration_seconds": 11.578, "sample_count": 99}, "timestamp": "2026-01-26T10:59:00.650897"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8142.534, "latencies_ms": [8142.534], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "pizza: 2\nglass: 3\nbox: 2\nfork: 2\nknife: 2\npizza box: 2\npizza: 2\npizza: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20993.0, "ram_available_mb": 41847.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20992.9, "ram_available_mb": 41847.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.9}, "power_stats": {"power_gpu_soc_mean_watts": 20.876, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.553, "gpu_utilization_percent_mean": 73.9, "power_watts_avg": 20.876, "energy_joules_est": 170.0, "duration_seconds": 8.143, "sample_count": 70}, "timestamp": "2026-01-26T10:59:10.834373"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11633.663, "latencies_ms": [11633.663], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large pizza on a white box with the word 'PIZZA' printed on it, positioned on the left side of the image. In the background, there is a smaller pizza on a similar white box, also with 'PIZZA' printed on it, placed slightly to the right of the larger pizza. The background also", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20992.9, "ram_available_mb": 41847.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20993.4, "ram_available_mb": 41847.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.879}, "power_stats": {"power_gpu_soc_mean_watts": 19.35, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 70.879, "power_watts_avg": 19.35, "energy_joules_est": 225.12, "duration_seconds": 11.634, "sample_count": 99}, "timestamp": "2026-01-26T10:59:24.482489"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9282.512, "latencies_ms": [9282.512], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image shows a dining table with two pizza boxes, one of which is open, and a glass of a dark-colored beverage. There are also utensils and a pizza visible on the table, suggesting a meal is being prepared or enjoyed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20993.4, "ram_available_mb": 41847.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20995.2, "ram_available_mb": 41845.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.253}, "power_stats": {"power_gpu_soc_mean_watts": 20.42, "power_cpu_cv_mean_watts": 1.697, "power_sys_5v0_mean_watts": 8.574, "gpu_utilization_percent_mean": 72.253, "power_watts_avg": 20.42, "energy_joules_est": 189.56, "duration_seconds": 9.283, "sample_count": 79}, "timestamp": "2026-01-26T10:59:35.785820"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10266.904, "latencies_ms": [10266.904], "images_per_second": 0.097, "prompt_tokens": 36, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image shows a cozy indoor setting with a focus on a pizza in a box, suggesting a casual dining atmosphere. The lighting is warm and artificial, likely from an indoor source, and the table is covered with a plaid tablecloth, adding to the homely feel of the scene.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20995.2, "ram_available_mb": 41845.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20996.5, "ram_available_mb": 41844.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.609}, "power_stats": {"power_gpu_soc_mean_watts": 19.797, "power_cpu_cv_mean_watts": 1.808, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 71.609, "power_watts_avg": 19.797, "energy_joules_est": 203.27, "duration_seconds": 10.268, "sample_count": 87}, "timestamp": "2026-01-26T10:59:48.070421"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11557.583, "latencies_ms": [11557.583], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a young boy in a baseball uniform, holding a baseball bat and standing at home plate, ready to swing. He is wearing a helmet and is surrounded by a group of people, including other children and adults, who are watching the game. The boy is in the center of attention, and the spectators are spread out around him, some closer to the field and others", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20996.5, "ram_available_mb": 41844.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20995.7, "ram_available_mb": 41845.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.847}, "power_stats": {"power_gpu_soc_mean_watts": 19.279, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 69.847, "power_watts_avg": 19.279, "energy_joules_est": 222.83, "duration_seconds": 11.558, "sample_count": 98}, "timestamp": "2026-01-26T11:00:01.684195"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9504.259, "latencies_ms": [9504.259], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- children: 10\n- adults: 5\n- baseball bats: 2\n- baseball gloves: 2\n- helmets: 3\n- baseball uniforms: 2\n- benches: 2\n- trees: 4", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20995.7, "ram_available_mb": 41845.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20997.7, "ram_available_mb": 41843.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.79}, "power_stats": {"power_gpu_soc_mean_watts": 20.39, "power_cpu_cv_mean_watts": 1.719, "power_sys_5v0_mean_watts": 8.581, "gpu_utilization_percent_mean": 71.79, "power_watts_avg": 20.39, "energy_joules_est": 193.81, "duration_seconds": 9.505, "sample_count": 81}, "timestamp": "2026-01-26T11:00:13.215090"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11693.373, "latencies_ms": [11693.373], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a young baseball player is standing on a dirt path, holding a baseball bat and wearing a helmet, ready to swing. Behind him, another player is crouched down, wearing a catcher's mitt, positioned to catch the ball. In the background, a group of spectators is seated on the grass, watching the game unfold", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20997.7, "ram_available_mb": 41843.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.5, "ram_used_mb": 21001.1, "ram_available_mb": 41839.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.65}, "power_stats": {"power_gpu_soc_mean_watts": 19.294, "power_cpu_cv_mean_watts": 1.925, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 70.65, "power_watts_avg": 19.294, "energy_joules_est": 225.62, "duration_seconds": 11.694, "sample_count": 100}, "timestamp": "2026-01-26T11:00:26.935566"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11804.865, "latencies_ms": [11804.865], "images_per_second": 0.085, "prompt_tokens": 37, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The image captures a lively scene at a baseball field where a young boy in a white shirt and gray pants is standing at home plate, ready to swing his bat. He is wearing a black helmet and is surrounded by a group of spectators, including a catcher in a red shirt and a baseball glove, who are watching the game intently.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21001.1, "ram_available_mb": 41839.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.6, "ram_used_mb": 21006.0, "ram_available_mb": 41834.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.802}, "power_stats": {"power_gpu_soc_mean_watts": 19.384, "power_cpu_cv_mean_watts": 1.949, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 70.802, "power_watts_avg": 19.384, "energy_joules_est": 228.84, "duration_seconds": 11.806, "sample_count": 101}, "timestamp": "2026-01-26T11:00:40.764654"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8887.049, "latencies_ms": [8887.049], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows a sunny day with clear skies, as evidenced by the bright lighting and shadows cast on the ground. The players are wearing helmets and catcher's gear, indicating a focus on safety in the game.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21006.0, "ram_available_mb": 41834.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.2, "ram_used_mb": 21030.6, "ram_available_mb": 41810.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.667}, "power_stats": {"power_gpu_soc_mean_watts": 20.393, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 71.667, "power_watts_avg": 20.393, "energy_joules_est": 181.25, "duration_seconds": 8.888, "sample_count": 75}, "timestamp": "2026-01-26T11:00:51.712892"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11598.521, "latencies_ms": [11598.521], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a yellow banana is resting on a black corded telephone, which is placed on a white desk. The banana is positioned in such a way that it appears to be leaning against the telephone. The desk is also home to a white computer monitor and a white printer, both of which are situated in the background. The tele", "error": null, "sys_before": {"cpu_percent": 15.8, "ram_used_mb": 20959.5, "ram_available_mb": 41881.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.3, "ram_used_mb": 21036.8, "ram_available_mb": 41804.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.323}, "power_stats": {"power_gpu_soc_mean_watts": 19.376, "power_cpu_cv_mean_watts": 1.956, "power_sys_5v0_mean_watts": 8.653, "gpu_utilization_percent_mean": 70.323, "power_watts_avg": 19.376, "energy_joules_est": 224.75, "duration_seconds": 11.599, "sample_count": 99}, "timestamp": "2026-01-26T11:01:05.383793"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7906.869, "latencies_ms": [7906.869], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "banana: 1, telephone: 1, cord: 1, paper: 1, pen: 1, wall socket: 1, outlet: 1, extension cord: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20964.9, "ram_available_mb": 41876.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20962.4, "ram_available_mb": 41878.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.448}, "power_stats": {"power_gpu_soc_mean_watts": 21.148, "power_cpu_cv_mean_watts": 1.594, "power_sys_5v0_mean_watts": 8.584, "gpu_utilization_percent_mean": 73.448, "power_watts_avg": 21.148, "energy_joules_est": 167.23, "duration_seconds": 7.908, "sample_count": 67}, "timestamp": "2026-01-26T11:01:15.303254"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11214.911, "latencies_ms": [11214.911], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there is a banana with a black cord wrapped around it, positioned near a Philips telephone. The banana is closer to the viewer than the telephone, which is in the background. The telephone is placed on a desk, and there is a piece of paper with text to the right of the telephone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20962.4, "ram_available_mb": 41878.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20954.7, "ram_available_mb": 41886.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.253}, "power_stats": {"power_gpu_soc_mean_watts": 19.255, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.602, "gpu_utilization_percent_mean": 70.253, "power_watts_avg": 19.255, "energy_joules_est": 215.95, "duration_seconds": 11.216, "sample_count": 95}, "timestamp": "2026-01-26T11:01:28.530903"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8562.872, "latencies_ms": [8562.872], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A banana is placed on top of a Philips telephone, which is on a desk with a white surface. The telephone has a corded handset and a small screen, and there is a white computer tower in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20954.7, "ram_available_mb": 41886.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20948.0, "ram_available_mb": 41892.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.973}, "power_stats": {"power_gpu_soc_mean_watts": 20.737, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.577, "gpu_utilization_percent_mean": 72.973, "power_watts_avg": 20.737, "energy_joules_est": 177.58, "duration_seconds": 8.563, "sample_count": 74}, "timestamp": "2026-01-26T11:01:39.117047"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8329.111, "latencies_ms": [8329.111], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A ripe banana is placed on top of a black corded phone, which is situated on a white desk. The desk also holds a white telephone and a small white object that appears to be a piece of paper or card.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20948.0, "ram_available_mb": 41892.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21007.0, "ram_available_mb": 41833.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.38}, "power_stats": {"power_gpu_soc_mean_watts": 20.544, "power_cpu_cv_mean_watts": 1.719, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 72.38, "power_watts_avg": 20.544, "energy_joules_est": 171.13, "duration_seconds": 8.33, "sample_count": 71}, "timestamp": "2026-01-26T11:01:49.467325"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11565.926, "latencies_ms": [11565.926], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a lively scene of a crowd of people gathered in what appears to be a public space, possibly a park or a street. The focus is on a woman in the center holding a large teddy bear, which stands out due to its size and the contrasting color of her outfit. The crowd is diverse, with individuals of various ages and styles, some wearing back", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 21007.0, "ram_available_mb": 41833.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21035.9, "ram_available_mb": 41805.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.061}, "power_stats": {"power_gpu_soc_mean_watts": 19.377, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 70.061, "power_watts_avg": 19.377, "energy_joules_est": 224.13, "duration_seconds": 11.567, "sample_count": 98}, "timestamp": "2026-01-26T11:02:03.073818"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5596.532, "latencies_ms": [5596.532], "images_per_second": 0.179, "prompt_tokens": 39, "response_tokens_est": 25, "n_tiles": 16, "output_text": "people: numerous\nbear: 1\nbackpacks: 3\nscanners: 0\n", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20937.4, "ram_available_mb": 41903.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 20956.5, "ram_available_mb": 41884.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.17}, "power_stats": {"power_gpu_soc_mean_watts": 23.35, "power_cpu_cv_mean_watts": 1.26, "power_sys_5v0_mean_watts": 8.525, "gpu_utilization_percent_mean": 77.17, "power_watts_avg": 23.35, "energy_joules_est": 130.69, "duration_seconds": 5.597, "sample_count": 47}, "timestamp": "2026-01-26T11:02:10.685098"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11578.091, "latencies_ms": [11578.091], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large group of people standing close together, with some individuals facing the camera and others turned away. In the background, there is a bus stop with a sign that reads \"Tomato station,\" and beyond that, there are trees and a clear sky. The people in the foreground appear to be in the midst of a crowd, with no clear space between them", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20956.5, "ram_available_mb": 41884.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20997.9, "ram_available_mb": 41843.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.5}, "power_stats": {"power_gpu_soc_mean_watts": 19.308, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 70.5, "power_watts_avg": 19.308, "energy_joules_est": 223.56, "duration_seconds": 11.579, "sample_count": 98}, "timestamp": "2026-01-26T11:02:24.290789"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10399.836, "latencies_ms": [10399.836], "images_per_second": 0.096, "prompt_tokens": 37, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image depicts a bustling outdoor scene with a large crowd of people, some of whom are carrying backpacks and handbags, suggesting a public event or gathering. In the center of the crowd, a person is holding a large teddy bear, which stands out among the sea of people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20997.9, "ram_available_mb": 41843.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21055.8, "ram_available_mb": 41785.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.307}, "power_stats": {"power_gpu_soc_mean_watts": 19.914, "power_cpu_cv_mean_watts": 1.782, "power_sys_5v0_mean_watts": 8.59, "gpu_utilization_percent_mean": 71.307, "power_watts_avg": 19.914, "energy_joules_est": 207.12, "duration_seconds": 10.401, "sample_count": 88}, "timestamp": "2026-01-26T11:02:36.740040"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9585.692, "latencies_ms": [9585.692], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image depicts a group of people gathered outdoors on a sunny day, with shadows indicating bright sunlight. The crowd is diverse, with individuals wearing various colors of clothing, and the environment appears to be a public space with trees and a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.8, "ram_available_mb": 41900.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20942.1, "ram_available_mb": 41898.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.037}, "power_stats": {"power_gpu_soc_mean_watts": 19.878, "power_cpu_cv_mean_watts": 1.806, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 72.037, "power_watts_avg": 19.878, "energy_joules_est": 190.56, "duration_seconds": 9.586, "sample_count": 82}, "timestamp": "2026-01-26T11:02:48.380873"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11604.855, "latencies_ms": [11604.855], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is holding a baby in his arms while standing next to a brown horse. The man is wearing a red shirt and is smiling as he interacts with the horse, which is also smiling. The baby appears to be enjoying the experience, looking at the horse with curiosity. The scene takes place in a stable or barn, with a stone wall", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20942.1, "ram_available_mb": 41898.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20950.1, "ram_available_mb": 41890.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.949}, "power_stats": {"power_gpu_soc_mean_watts": 19.099, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 69.949, "power_watts_avg": 19.099, "energy_joules_est": 221.65, "duration_seconds": 11.605, "sample_count": 98}, "timestamp": "2026-01-26T11:03:02.040341"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9756.531, "latencies_ms": [9756.531], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "1. Man: 1\n2. Child: 1\n3. Horse: 1\n4. Building: 1\n5. Fence: 1\n6. Door: 1\n7. Mat: 1\n8. Ground: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20950.1, "ram_available_mb": 41890.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21022.2, "ram_available_mb": 41818.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.771}, "power_stats": {"power_gpu_soc_mean_watts": 20.241, "power_cpu_cv_mean_watts": 1.75, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 71.771, "power_watts_avg": 20.241, "energy_joules_est": 197.5, "duration_seconds": 9.757, "sample_count": 83}, "timestamp": "2026-01-26T11:03:13.843493"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9168.873, "latencies_ms": [9168.873], "images_per_second": 0.109, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "In the foreground, a man is holding a baby close to him, and they are both facing a brown horse that is standing near a stone wall in the background. The man and baby are positioned in the middle of the image, with the horse and wall behind them.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21022.2, "ram_available_mb": 41818.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20938.1, "ram_available_mb": 41902.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.462}, "power_stats": {"power_gpu_soc_mean_watts": 20.321, "power_cpu_cv_mean_watts": 1.734, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 72.462, "power_watts_avg": 20.321, "energy_joules_est": 186.33, "duration_seconds": 9.17, "sample_count": 78}, "timestamp": "2026-01-26T11:03:25.037324"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9421.691, "latencies_ms": [9421.691], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "A man is holding a baby while standing next to a brown horse, which is reaching its head towards the man's hand, possibly for a pat or a treat. The setting appears to be a stable or barn, with a stone wall and a wooden structure in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20938.1, "ram_available_mb": 41902.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20934.6, "ram_available_mb": 41906.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.9}, "power_stats": {"power_gpu_soc_mean_watts": 20.3, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 71.9, "power_watts_avg": 20.3, "energy_joules_est": 191.27, "duration_seconds": 9.422, "sample_count": 80}, "timestamp": "2026-01-26T11:03:36.476540"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7762.742, "latencies_ms": [7762.742], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image features a man in a red shirt holding a child, with a brown horse standing close to them. The setting appears to be outdoors, with natural lighting and a stone wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20934.6, "ram_available_mb": 41906.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20938.5, "ram_available_mb": 41902.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.015}, "power_stats": {"power_gpu_soc_mean_watts": 21.048, "power_cpu_cv_mean_watts": 1.63, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 74.015, "power_watts_avg": 21.048, "energy_joules_est": 163.4, "duration_seconds": 7.763, "sample_count": 67}, "timestamp": "2026-01-26T11:03:46.299899"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11531.61, "latencies_ms": [11531.61], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility, featuring a single banana resting on a white plate with a gold rim. The banana, curved and ripe, is positioned on the left side of the plate. On the right side of the plate, there's a small amount of pinkish-brown liquid, possibly a sauce or jam. The plate", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20938.5, "ram_available_mb": 41902.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21007.2, "ram_available_mb": 41833.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.337}, "power_stats": {"power_gpu_soc_mean_watts": 19.336, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 70.337, "power_watts_avg": 19.336, "energy_joules_est": 222.99, "duration_seconds": 11.532, "sample_count": 98}, "timestamp": "2026-01-26T11:03:59.850419"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4506.006, "latencies_ms": [4506.006], "images_per_second": 0.222, "prompt_tokens": 39, "response_tokens_est": 15, "n_tiles": 16, "output_text": "banana: 1, peanut butter: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21007.2, "ram_available_mb": 41833.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 21011.1, "ram_available_mb": 41829.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 81.026}, "power_stats": {"power_gpu_soc_mean_watts": 24.818, "power_cpu_cv_mean_watts": 0.979, "power_sys_5v0_mean_watts": 8.45, "gpu_utilization_percent_mean": 81.026, "power_watts_avg": 24.818, "energy_joules_est": 111.85, "duration_seconds": 4.507, "sample_count": 38}, "timestamp": "2026-01-26T11:04:06.369615"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10071.489, "latencies_ms": [10071.489], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The banana is positioned in the foreground on the left side of the plate, which is placed on a wooden surface. The peanut butter is in the center of the plate, creating a contrast between the smooth texture of the banana and the creamy texture of the peanut butter.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21011.1, "ram_available_mb": 41829.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20991.6, "ram_available_mb": 41849.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.744}, "power_stats": {"power_gpu_soc_mean_watts": 19.822, "power_cpu_cv_mean_watts": 1.81, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 70.744, "power_watts_avg": 19.822, "energy_joules_est": 199.65, "duration_seconds": 10.072, "sample_count": 86}, "timestamp": "2026-01-26T11:04:18.478014"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10663.776, "latencies_ms": [10663.776], "images_per_second": 0.094, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "A single banana is placed on a white plate with a brown rim, and there is a small amount of pink substance, possibly a spread or jam, in the center of the plate. The plate is on a wooden surface, and the setting appears to be casual, possibly for a snack or a light meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20937.8, "ram_available_mb": 41903.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20992.2, "ram_available_mb": 41848.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.319}, "power_stats": {"power_gpu_soc_mean_watts": 19.799, "power_cpu_cv_mean_watts": 1.803, "power_sys_5v0_mean_watts": 8.592, "gpu_utilization_percent_mean": 71.319, "power_watts_avg": 19.799, "energy_joules_est": 211.14, "duration_seconds": 10.664, "sample_count": 91}, "timestamp": "2026-01-26T11:04:31.159713"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11666.134, "latencies_ms": [11666.134], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a banana placed on a plate with a pink substance in the center. The plate has a white base with a brown rim, and the banana is positioned in such a way that it appears to be peeling or sliced. The lighting in the image is warm, suggesting it may have been taken indoors with artificial lighting. The material of", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20937.1, "ram_available_mb": 41903.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20994.0, "ram_available_mb": 41846.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.485}, "power_stats": {"power_gpu_soc_mean_watts": 19.323, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 70.485, "power_watts_avg": 19.323, "energy_joules_est": 225.44, "duration_seconds": 11.667, "sample_count": 99}, "timestamp": "2026-01-26T11:04:44.843772"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11556.01, "latencies_ms": [11556.01], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is kneeling down and working on a motorcycle tire. He is using a wrench to loosen the lug nuts on the tire, which is a crucial step in changing a tire. The man appears to be focused on his task, ensuring that the lug nuts are properly removed.\n\nThe motorcycle is parked on a", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20994.0, "ram_available_mb": 41846.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21003.2, "ram_available_mb": 41837.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.58}, "power_stats": {"power_gpu_soc_mean_watts": 19.342, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.662, "gpu_utilization_percent_mean": 69.58, "power_watts_avg": 19.342, "energy_joules_est": 223.53, "duration_seconds": 11.557, "sample_count": 100}, "timestamp": "2026-01-26T11:04:58.450376"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9840.13, "latencies_ms": [9840.13], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "- Man: 1\n\n- Bike: 2\n\n- Wheel: 2\n\n- Tool: 1\n\n- Ground: 1\n\n- Basket: 1\n\n- Bicycle: 1\n\n- Basket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20925.5, "ram_available_mb": 41915.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.2, "ram_used_mb": 20940.4, "ram_available_mb": 41900.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.867}, "power_stats": {"power_gpu_soc_mean_watts": 20.145, "power_cpu_cv_mean_watts": 1.924, "power_sys_5v0_mean_watts": 8.614, "gpu_utilization_percent_mean": 71.867, "power_watts_avg": 20.145, "energy_joules_est": 198.24, "duration_seconds": 9.841, "sample_count": 83}, "timestamp": "2026-01-26T11:05:10.315393"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11583.432, "latencies_ms": [11583.432], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is kneeling on the ground, working on a motorcycle wheel. The motorcycle is positioned to the left of the image, with its front wheel visible and the rear wheel partially obscured by another object. In the background, there is another motorcycle and a red plastic container, both of which are further away from the viewer than the person and", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.4, "ram_available_mb": 41900.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 20998.9, "ram_available_mb": 41842.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.424}, "power_stats": {"power_gpu_soc_mean_watts": 19.295, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 70.424, "power_watts_avg": 19.295, "energy_joules_est": 223.52, "duration_seconds": 11.584, "sample_count": 99}, "timestamp": "2026-01-26T11:05:23.941257"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7642.809, "latencies_ms": [7642.809], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A man is repairing a motorcycle tire in an outdoor setting, possibly a garage or workshop. He is kneeling on the ground and using a tool to work on the tire.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20945.4, "ram_available_mb": 41895.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20996.2, "ram_available_mb": 41844.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.923}, "power_stats": {"power_gpu_soc_mean_watts": 21.405, "power_cpu_cv_mean_watts": 1.576, "power_sys_5v0_mean_watts": 8.571, "gpu_utilization_percent_mean": 71.923, "power_watts_avg": 21.405, "energy_joules_est": 163.61, "duration_seconds": 7.643, "sample_count": 65}, "timestamp": "2026-01-26T11:05:33.619864"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9485.997, "latencies_ms": [9485.997], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "A man is repairing a motorcycle tire in a workshop. The tire is black with a silver rim, and the man is wearing a green shirt and black pants. The workshop has a rustic feel with a wooden floor and a motorcycle in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20996.2, "ram_available_mb": 41844.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21016.7, "ram_available_mb": 41824.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.075}, "power_stats": {"power_gpu_soc_mean_watts": 19.949, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 8.63, "gpu_utilization_percent_mean": 71.075, "power_watts_avg": 19.949, "energy_joules_est": 189.26, "duration_seconds": 9.487, "sample_count": 80}, "timestamp": "2026-01-26T11:05:45.154007"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11564.184, "latencies_ms": [11564.184], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a skateboarder is captured in the midst of performing a trick on a ramp. The skateboarder, dressed in a black t-shirt and black pants, is balancing on one foot on the skateboard while the other foot is lifted off the board. The skateboard, which is brown with a white logo on it, is position", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21016.7, "ram_available_mb": 41824.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21017.7, "ram_available_mb": 41823.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.622}, "power_stats": {"power_gpu_soc_mean_watts": 19.247, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 70.622, "power_watts_avg": 19.247, "energy_joules_est": 222.59, "duration_seconds": 11.565, "sample_count": 98}, "timestamp": "2026-01-26T11:05:58.765021"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7892.209, "latencies_ms": [7892.209], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "skateboard: 1\nskateboarder: 1\nfence: multiple\ntrees: multiple\ngrass: multiple\nbuilding: multiple\nsky: 1\nwristband: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20938.2, "ram_available_mb": 41902.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20986.0, "ram_available_mb": 41854.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.94}, "power_stats": {"power_gpu_soc_mean_watts": 21.329, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.567, "gpu_utilization_percent_mean": 73.94, "power_watts_avg": 21.329, "energy_joules_est": 168.35, "duration_seconds": 7.893, "sample_count": 67}, "timestamp": "2026-01-26T11:06:08.670819"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11067.822, "latencies_ms": [11067.822], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, performing a trick on a ramp. In the background, there is a fence and some greenery, indicating the skate park is likely outdoors. The skateboarder is near the edge of the ramp, suggesting they are in the process of grinding or sliding down it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20986.0, "ram_available_mb": 41854.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21001.7, "ram_available_mb": 41839.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.511}, "power_stats": {"power_gpu_soc_mean_watts": 19.594, "power_cpu_cv_mean_watts": 1.852, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 70.511, "power_watts_avg": 19.594, "energy_joules_est": 216.88, "duration_seconds": 11.068, "sample_count": 94}, "timestamp": "2026-01-26T11:06:21.789127"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6559.246, "latencies_ms": [6559.246], "images_per_second": 0.152, "prompt_tokens": 37, "response_tokens_est": 33, "n_tiles": 16, "output_text": "A skateboarder with long hair is performing a trick on a ramp in a park-like setting with trees and a fence in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21001.7, "ram_available_mb": 41839.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 21031.0, "ram_available_mb": 41809.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.309}, "power_stats": {"power_gpu_soc_mean_watts": 22.271, "power_cpu_cv_mean_watts": 1.397, "power_sys_5v0_mean_watts": 8.528, "gpu_utilization_percent_mean": 75.309, "power_watts_avg": 22.271, "energy_joules_est": 146.1, "duration_seconds": 6.56, "sample_count": 55}, "timestamp": "2026-01-26T11:06:30.378446"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9579.044, "latencies_ms": [9579.044], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The skateboarder is wearing a black t-shirt and black pants, and the skateboard is a mix of colors with a prominent red and white design. The lighting is natural, suggesting it is daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20933.5, "ram_available_mb": 41907.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.679}, "power_stats": {"power_gpu_soc_mean_watts": 20.234, "power_cpu_cv_mean_watts": 1.769, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 71.679, "power_watts_avg": 20.234, "energy_joules_est": 193.84, "duration_seconds": 9.58, "sample_count": 81}, "timestamp": "2026-01-26T11:06:41.976716"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11620.659, "latencies_ms": [11620.659], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of four men standing in a grassy field, each holding a Frisbee. They are posing for a picture, with the Frisbees in their hands, showcasing their shared interest in the sport. The men are standing close to each other, creating a sense of camaraderie and enjoyment. The field appears to be a perfect spot", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20933.5, "ram_available_mb": 41907.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20939.3, "ram_available_mb": 41901.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.253}, "power_stats": {"power_gpu_soc_mean_watts": 19.295, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 70.253, "power_watts_avg": 19.295, "energy_joules_est": 224.23, "duration_seconds": 11.621, "sample_count": 99}, "timestamp": "2026-01-26T11:06:55.633271"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10975.395, "latencies_ms": [10975.395], "images_per_second": 0.091, "prompt_tokens": 39, "response_tokens_est": 72, "n_tiles": 16, "output_text": "1. Frisbee: 4\n2. People: 4\n3. Grass: 1 (field)\n4. Sky: 1\n5. Trees: 1 (in the background)\n6. Clouds: 0\n7. Sun: 0\n8. Clouds: 0", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20939.3, "ram_available_mb": 41901.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21000.0, "ram_available_mb": 41840.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.957}, "power_stats": {"power_gpu_soc_mean_watts": 19.64, "power_cpu_cv_mean_watts": 1.831, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 69.957, "power_watts_avg": 19.64, "energy_joules_est": 215.57, "duration_seconds": 10.976, "sample_count": 94}, "timestamp": "2026-01-26T11:07:08.628561"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11113.373, "latencies_ms": [11113.373], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there are four individuals standing on a grassy field, each holding a frisbee. The person on the far left is standing with their back to the camera, while the others are facing the camera. In the background, there are two tall poles, one on the right side and one on the far right side of the image.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21000.0, "ram_available_mb": 41840.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21032.4, "ram_available_mb": 41808.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.453}, "power_stats": {"power_gpu_soc_mean_watts": 19.429, "power_cpu_cv_mean_watts": 1.883, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 71.453, "power_watts_avg": 19.429, "energy_joules_est": 215.93, "duration_seconds": 11.114, "sample_count": 95}, "timestamp": "2026-01-26T11:07:21.762422"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7637.827, "latencies_ms": [7637.827], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "Four people are standing in a grassy field holding frisbees, likely preparing to play a game of frisbee. The sky is clear and it appears to be a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21032.4, "ram_available_mb": 41808.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21032.4, "ram_available_mb": 41808.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.031}, "power_stats": {"power_gpu_soc_mean_watts": 21.405, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.581, "gpu_utilization_percent_mean": 74.031, "power_watts_avg": 21.405, "energy_joules_est": 163.5, "duration_seconds": 7.638, "sample_count": 65}, "timestamp": "2026-01-26T11:07:31.442795"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9373.342, "latencies_ms": [9373.342], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image shows a group of people in a field during what appears to be either sunrise or sunset, given the warm lighting and long shadows. They are wearing casual clothing and are holding frisbees, suggesting they are enjoying a recreational activity.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20929.0, "ram_available_mb": 41911.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20990.3, "ram_available_mb": 41850.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.2}, "power_stats": {"power_gpu_soc_mean_watts": 20.063, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 71.2, "power_watts_avg": 20.063, "energy_joules_est": 188.07, "duration_seconds": 9.374, "sample_count": 80}, "timestamp": "2026-01-26T11:07:42.844469"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11628.351, "latencies_ms": [11628.351], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large white airplane with a red tail parked on the tarmac at an airport. The airplane is surrounded by several people, who are likely airport staff or passengers. There are also a few trucks and a car visible in the scene, indicating that the airplane is being serviced or prepared for departure.\n\nIn addition to the airplane", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 20990.3, "ram_available_mb": 41850.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21008.1, "ram_available_mb": 41832.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.222}, "power_stats": {"power_gpu_soc_mean_watts": 19.189, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 69.222, "power_watts_avg": 19.189, "energy_joules_est": 223.15, "duration_seconds": 11.629, "sample_count": 99}, "timestamp": "2026-01-26T11:07:56.503690"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10047.303, "latencies_ms": [10047.303], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "1. Airplane: 1\n2. Windows: 10\n3. Doors: 2\n4. Tires: 4\n5. People: 4\n6. Trucks: 2\n7. Cones: 1\n8. Trees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21008.1, "ram_available_mb": 41832.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21008.8, "ram_available_mb": 41832.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.224}, "power_stats": {"power_gpu_soc_mean_watts": 20.042, "power_cpu_cv_mean_watts": 1.784, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 72.224, "power_watts_avg": 20.042, "energy_joules_est": 201.38, "duration_seconds": 10.048, "sample_count": 85}, "timestamp": "2026-01-26T11:08:08.591207"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11480.884, "latencies_ms": [11480.884], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The airplane is parked on the tarmac in the foreground of the image, with its nose facing towards the right side of the frame. In the background, there are several people and vehicles scattered around the airport, as well as a building and palm trees. The sky is visible above the airplane and the ground, with a few clouds scattered across it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20936.9, "ram_available_mb": 41904.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20988.8, "ram_available_mb": 41852.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.867}, "power_stats": {"power_gpu_soc_mean_watts": 19.185, "power_cpu_cv_mean_watts": 1.874, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 70.867, "power_watts_avg": 19.185, "energy_joules_est": 220.27, "duration_seconds": 11.481, "sample_count": 98}, "timestamp": "2026-01-26T11:08:22.105804"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8458.51, "latencies_ms": [8458.51], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A JAL airplane is parked on the tarmac at an airport, with ground crew members attending to it. The sky is blue with some clouds, and there are other airplanes and buildings visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20926.1, "ram_available_mb": 41914.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20986.7, "ram_available_mb": 41854.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.125}, "power_stats": {"power_gpu_soc_mean_watts": 20.82, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.61, "gpu_utilization_percent_mean": 73.125, "power_watts_avg": 20.82, "energy_joules_est": 176.12, "duration_seconds": 8.459, "sample_count": 72}, "timestamp": "2026-01-26T11:08:32.605805"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7113.939, "latencies_ms": [7113.939], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image features a large commercial airplane with a white body and a red tail, parked on the tarmac. The sky is blue with scattered white clouds, indicating fair weather conditions.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20931.6, "ram_available_mb": 41909.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20926.5, "ram_available_mb": 41914.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.867}, "power_stats": {"power_gpu_soc_mean_watts": 21.094, "power_cpu_cv_mean_watts": 1.554, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 72.867, "power_watts_avg": 21.094, "energy_joules_est": 150.07, "duration_seconds": 7.115, "sample_count": 60}, "timestamp": "2026-01-26T11:08:41.771266"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11585.234, "latencies_ms": [11585.234], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is skillfully riding a skateboard on a cement ledge. He is performing a trick, possibly a grind, as he balances on the edge of the ledge. The skateboarder is wearing a yellow shirt and is in the process of executing the trick.\n\nThe scene takes place in a park, with a", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20926.5, "ram_available_mb": 41914.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20987.5, "ram_available_mb": 41853.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.97}, "power_stats": {"power_gpu_soc_mean_watts": 19.319, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 69.97, "power_watts_avg": 19.319, "energy_joules_est": 223.83, "duration_seconds": 11.586, "sample_count": 99}, "timestamp": "2026-01-26T11:08:55.384278"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9614.355, "latencies_ms": [9614.355], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- Bench: 2\n- Trash can: 1\n- Skateboard: 1\n- Skateboarder: 1\n- Graffiti: 1\n- Park: 1\n- Trees: 1\n- Fence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20934.0, "ram_available_mb": 41906.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20931.9, "ram_available_mb": 41909.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.361}, "power_stats": {"power_gpu_soc_mean_watts": 20.076, "power_cpu_cv_mean_watts": 1.75, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 72.361, "power_watts_avg": 20.076, "energy_joules_est": 193.03, "duration_seconds": 9.615, "sample_count": 83}, "timestamp": "2026-01-26T11:09:07.042094"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11344.745, "latencies_ms": [11344.745], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, a skateboarder is performing a trick on a concrete ledge. The ledge is located near the center of the image and is surrounded by a grassy area with a trash can and a bench in the background. There is a person standing near the bench, and a car is visible in the distance behind the grassy area.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20931.9, "ram_available_mb": 41909.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.9, "ram_used_mb": 20990.2, "ram_available_mb": 41850.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.875}, "power_stats": {"power_gpu_soc_mean_watts": 19.397, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 69.875, "power_watts_avg": 19.397, "energy_joules_est": 220.07, "duration_seconds": 11.345, "sample_count": 96}, "timestamp": "2026-01-26T11:09:20.400343"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7075.571, "latencies_ms": [7075.571], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A person is skateboarding on a concrete ledge in a park. The ledge has graffiti on it and there are benches and a trash can nearby.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20927.0, "ram_available_mb": 41913.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.6, "ram_used_mb": 20982.6, "ram_available_mb": 41858.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.136}, "power_stats": {"power_gpu_soc_mean_watts": 21.831, "power_cpu_cv_mean_watts": 1.641, "power_sys_5v0_mean_watts": 8.564, "gpu_utilization_percent_mean": 75.136, "power_watts_avg": 21.831, "energy_joules_est": 154.48, "duration_seconds": 7.076, "sample_count": 59}, "timestamp": "2026-01-26T11:09:29.487621"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6747.279, "latencies_ms": [6747.279], "images_per_second": 0.148, "prompt_tokens": 36, "response_tokens_est": 37, "n_tiles": 16, "output_text": "The image shows a person skateboarding on a concrete ledge with graffiti on it. The weather appears to be sunny with shadows cast on the ground.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20982.6, "ram_available_mb": 41858.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.9, "ram_used_mb": 21010.1, "ram_available_mb": 41830.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.333}, "power_stats": {"power_gpu_soc_mean_watts": 21.586, "power_cpu_cv_mean_watts": 1.713, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 73.333, "power_watts_avg": 21.586, "energy_joules_est": 145.66, "duration_seconds": 6.748, "sample_count": 57}, "timestamp": "2026-01-26T11:09:38.272412"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11585.312, "latencies_ms": [11585.312], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a delightful moment of indulgence, featuring a single slice of chocolate tart resting on a white plate adorned with gold floral patterns. The tart, with its rich, dark chocolate filling, is generously topped with a glossy chocolate glaze that adds an extra layer of decadence. The glaze is artistically", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 20930.8, "ram_available_mb": 41910.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 9.2, "ram_used_mb": 20999.7, "ram_available_mb": 41841.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.592}, "power_stats": {"power_gpu_soc_mean_watts": 19.319, "power_cpu_cv_mean_watts": 2.107, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 69.592, "power_watts_avg": 19.319, "energy_joules_est": 223.83, "duration_seconds": 11.586, "sample_count": 98}, "timestamp": "2026-01-26T11:09:51.891625"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8679.907, "latencies_ms": [8679.907], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "slice: 1\nplate: 1\nchocolate: 1\ncaramel: 1\nnuts: 1\nchocolate drizzle: 1\nplate pattern: 1\nplate border: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20946.2, "ram_available_mb": 41894.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.3, "ram_used_mb": 21007.7, "ram_available_mb": 41833.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.108}, "power_stats": {"power_gpu_soc_mean_watts": 20.689, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 73.108, "power_watts_avg": 20.689, "energy_joules_est": 179.59, "duration_seconds": 8.681, "sample_count": 74}, "timestamp": "2026-01-26T11:10:02.611344"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11587.702, "latencies_ms": [11587.702], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The slice of chocolate tart is positioned in the foreground, appearing to be the main object of focus. It is placed on the left side of the plate, which is in the center of the image. The caramel sauce is drizzled around the base of the tart, with some of it reaching the right side of the plate, creating a sense of depth and adding", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21007.7, "ram_available_mb": 41833.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 21001.0, "ram_available_mb": 41839.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.889}, "power_stats": {"power_gpu_soc_mean_watts": 19.302, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 70.889, "power_watts_avg": 19.302, "energy_joules_est": 223.68, "duration_seconds": 11.588, "sample_count": 99}, "timestamp": "2026-01-26T11:10:16.237057"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6985.921, "latencies_ms": [6985.921], "images_per_second": 0.143, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A slice of chocolate tart with a drizzle of caramel sauce is placed on a white plate with gold floral patterns, set on a wooden table.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20940.6, "ram_available_mb": 41900.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21002.5, "ram_available_mb": 41838.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.305}, "power_stats": {"power_gpu_soc_mean_watts": 21.907, "power_cpu_cv_mean_watts": 1.479, "power_sys_5v0_mean_watts": 8.589, "gpu_utilization_percent_mean": 74.305, "power_watts_avg": 21.907, "energy_joules_est": 153.05, "duration_seconds": 6.987, "sample_count": 59}, "timestamp": "2026-01-26T11:10:25.285133"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10206.532, "latencies_ms": [10206.532], "images_per_second": 0.098, "prompt_tokens": 36, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image features a slice of chocolate tart on a white plate with a gold floral pattern around the edge. The tart has a glossy chocolate glaze on top with a decorative drizzle pattern, and there is a caramel sauce drizzled around the base of the slice.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20947.5, "ram_available_mb": 41893.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20936.0, "ram_available_mb": 41904.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.241}, "power_stats": {"power_gpu_soc_mean_watts": 19.551, "power_cpu_cv_mean_watts": 1.817, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 70.241, "power_watts_avg": 19.551, "energy_joules_est": 199.56, "duration_seconds": 10.207, "sample_count": 87}, "timestamp": "2026-01-26T11:10:37.546017"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11595.608, "latencies_ms": [11595.608], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a busy office environment with several people working on their laptops. There are at least four people visible, with one man sitting at a desk in the foreground, and others working at their desks in the background. The office is filled with desks and chairs, and there are multiple laptops on the tables.\n\nIn addition to the la", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20936.0, "ram_available_mb": 41904.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20930.4, "ram_available_mb": 41910.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.525}, "power_stats": {"power_gpu_soc_mean_watts": 19.322, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 70.525, "power_watts_avg": 19.322, "energy_joules_est": 224.06, "duration_seconds": 11.596, "sample_count": 99}, "timestamp": "2026-01-26T11:10:51.190234"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11141.617, "latencies_ms": [11141.617], "images_per_second": 0.09, "prompt_tokens": 39, "response_tokens_est": 73, "n_tiles": 16, "output_text": "- Laptops: 3\n\n- Chairs: 5\n\n- Desks: 3\n\n- Papers: numerous, exact count unspecified\n\n- Boxes: 1\n\n- Cables: numerous, exact count unspecified\n\n- Computers: 2\n\n- People: 4", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20930.4, "ram_available_mb": 41910.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20996.3, "ram_available_mb": 41844.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.042}, "power_stats": {"power_gpu_soc_mean_watts": 19.447, "power_cpu_cv_mean_watts": 1.826, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 70.042, "power_watts_avg": 19.447, "energy_joules_est": 216.68, "duration_seconds": 11.142, "sample_count": 96}, "timestamp": "2026-01-26T11:11:04.364507"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11550.561, "latencies_ms": [11550.561], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a man sitting at a desk with a laptop in front of him, and another laptop is on the desk to his left. In the background, there are several other people working at desks with computers, some of which are facing the camera while others are turned away. The room appears to be a busy workspace with multiple workstations and people engaged", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20933.7, "ram_available_mb": 41907.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20985.6, "ram_available_mb": 41855.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.85}, "power_stats": {"power_gpu_soc_mean_watts": 19.313, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.648, "gpu_utilization_percent_mean": 69.85, "power_watts_avg": 19.313, "energy_joules_est": 223.09, "duration_seconds": 11.551, "sample_count": 100}, "timestamp": "2026-01-26T11:11:17.937101"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8102.229, "latencies_ms": [8102.229], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image depicts a busy office environment with multiple individuals working on laptops and other electronic devices. There are several desks and chairs scattered throughout the room, with people sitting and standing, engaged in various tasks.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20985.6, "ram_available_mb": 41855.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20926.1, "ram_available_mb": 41914.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.486}, "power_stats": {"power_gpu_soc_mean_watts": 21.043, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 73.486, "power_watts_avg": 21.043, "energy_joules_est": 170.51, "duration_seconds": 8.103, "sample_count": 70}, "timestamp": "2026-01-26T11:11:28.080208"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7859.842, "latencies_ms": [7859.842], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows an indoor setting with a yellowish light, possibly from fluorescent lights, illuminating the room. Various materials such as cardboard boxes, electronic devices, and papers are scattered across the tables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20926.1, "ram_available_mb": 41914.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20990.8, "ram_available_mb": 41850.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.394}, "power_stats": {"power_gpu_soc_mean_watts": 21.028, "power_cpu_cv_mean_watts": 1.673, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 73.394, "power_watts_avg": 21.028, "energy_joules_est": 165.29, "duration_seconds": 7.86, "sample_count": 66}, "timestamp": "2026-01-26T11:11:37.970767"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11614.182, "latencies_ms": [11614.182], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people gathered in a living room, playing a video game using Wii controllers. There are four people in the scene, with two men and two women. One of the men is holding a Wii remote, while the other is holding a Wii nunchuck. The group appears to be enjoying their time together, engaging in a fun and interactive activity.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 20990.8, "ram_available_mb": 41850.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21003.5, "ram_available_mb": 41837.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.071}, "power_stats": {"power_gpu_soc_mean_watts": 19.287, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 70.071, "power_watts_avg": 19.287, "energy_joules_est": 224.01, "duration_seconds": 11.615, "sample_count": 99}, "timestamp": "2026-01-26T11:11:51.632371"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8253.908, "latencies_ms": [8253.908], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "person: 4\nwii controller: 2\ncouch: 1\nbasket: 1\nbeer bottle: 2\npicture frame: 2\nframe: 2\ntable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20925.9, "ram_available_mb": 41915.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20989.5, "ram_available_mb": 41851.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.314}, "power_stats": {"power_gpu_soc_mean_watts": 20.879, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.556, "gpu_utilization_percent_mean": 73.314, "power_watts_avg": 20.879, "energy_joules_est": 172.35, "duration_seconds": 8.255, "sample_count": 70}, "timestamp": "2026-01-26T11:12:01.949912"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11595.796, "latencies_ms": [11595.796], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a woman in a white dress is standing and facing a man in a striped shirt who is also standing. In the background, two more men are standing near a couch, and a wooden table with bottles and a picture frame is to the right. The room appears to be a living space with a carpeted floor and a door leading to another room.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20926.8, "ram_available_mb": 41914.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20983.8, "ram_available_mb": 41857.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.778}, "power_stats": {"power_gpu_soc_mean_watts": 19.17, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.66, "gpu_utilization_percent_mean": 70.778, "power_watts_avg": 19.17, "energy_joules_est": 222.31, "duration_seconds": 11.597, "sample_count": 99}, "timestamp": "2026-01-26T11:12:15.570845"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7660.668, "latencies_ms": [7660.668], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "In the image, a group of people are gathered in a living room, playing a video game together. They are holding Wii controllers and standing in front of a TV, which is mounted on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20928.1, "ram_available_mb": 41912.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20980.0, "ram_available_mb": 41860.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.953}, "power_stats": {"power_gpu_soc_mean_watts": 21.434, "power_cpu_cv_mean_watts": 1.557, "power_sys_5v0_mean_watts": 8.554, "gpu_utilization_percent_mean": 73.953, "power_watts_avg": 21.434, "energy_joules_est": 164.21, "duration_seconds": 7.661, "sample_count": 64}, "timestamp": "2026-01-26T11:12:25.263014"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7241.967, "latencies_ms": [7241.967], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The room is dimly lit with natural light coming from the window with blinds. The walls are painted in a light color, and there is a wooden cabinet and a wooden table in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20980.0, "ram_available_mb": 41860.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20999.7, "ram_available_mb": 41841.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.574}, "power_stats": {"power_gpu_soc_mean_watts": 21.081, "power_cpu_cv_mean_watts": 1.562, "power_sys_5v0_mean_watts": 8.601, "gpu_utilization_percent_mean": 73.574, "power_watts_avg": 21.081, "energy_joules_est": 152.68, "duration_seconds": 7.243, "sample_count": 61}, "timestamp": "2026-01-26T11:12:34.560267"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11587.007, "latencies_ms": [11587.007], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene beach scene at sunset. A person is standing on the wet sand, facing the ocean. The sun is setting in the background, casting a warm glow over the scene. The sky is a gradient of blue and orange, indicating the time of day. The ocean is calm with small waves lapping at the shore. The person appears to be looking out towards", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 20999.7, "ram_available_mb": 41841.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21000.2, "ram_available_mb": 41840.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.082}, "power_stats": {"power_gpu_soc_mean_watts": 19.364, "power_cpu_cv_mean_watts": 1.871, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 71.082, "power_watts_avg": 19.364, "energy_joules_est": 224.38, "duration_seconds": 11.588, "sample_count": 98}, "timestamp": "2026-01-26T11:12:48.179401"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7450.829, "latencies_ms": [7450.829], "images_per_second": 0.134, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "person: 1, sun: 1, frisbee: 1, horizon: 1, water: multiple, sand: multiple, reflection: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20931.1, "ram_available_mb": 41909.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20986.1, "ram_available_mb": 41854.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.27}, "power_stats": {"power_gpu_soc_mean_watts": 21.466, "power_cpu_cv_mean_watts": 1.525, "power_sys_5v0_mean_watts": 8.553, "gpu_utilization_percent_mean": 74.27, "power_watts_avg": 21.466, "energy_joules_est": 159.95, "duration_seconds": 7.451, "sample_count": 63}, "timestamp": "2026-01-26T11:12:57.679658"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9512.089, "latencies_ms": [9512.089], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "In the foreground, there is a person standing on the beach, facing the ocean. The ocean extends towards the horizon, where the sun is setting, creating a reflection on the wet sand. The person appears to be at a distance from the sun, closer to the viewer than the horizon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20986.1, "ram_available_mb": 41854.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20938.2, "ram_available_mb": 41902.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.309}, "power_stats": {"power_gpu_soc_mean_watts": 20.028, "power_cpu_cv_mean_watts": 1.764, "power_sys_5v0_mean_watts": 8.617, "gpu_utilization_percent_mean": 71.309, "power_watts_avg": 20.028, "energy_joules_est": 190.52, "duration_seconds": 9.513, "sample_count": 81}, "timestamp": "2026-01-26T11:13:09.208399"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6412.172, "latencies_ms": [6412.172], "images_per_second": 0.156, "prompt_tokens": 37, "response_tokens_est": 32, "n_tiles": 16, "output_text": "A person is standing on a beach, looking at the sunset. The sun is setting over the horizon, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20938.2, "ram_available_mb": 41902.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20936.3, "ram_available_mb": 41904.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.148}, "power_stats": {"power_gpu_soc_mean_watts": 22.395, "power_cpu_cv_mean_watts": 1.401, "power_sys_5v0_mean_watts": 8.524, "gpu_utilization_percent_mean": 75.148, "power_watts_avg": 22.395, "energy_joules_est": 143.61, "duration_seconds": 6.413, "sample_count": 54}, "timestamp": "2026-01-26T11:13:17.633887"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10053.784, "latencies_ms": [10053.784], "images_per_second": 0.099, "prompt_tokens": 36, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image captures a serene beach scene at sunset with the sun casting a warm orange glow across the sky and reflecting off the wet sand. A person is silhouetted against the bright horizon, standing on the beach with a frisbee in hand, suggesting a leisurely activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20936.3, "ram_available_mb": 41904.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20928.7, "ram_available_mb": 41912.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.058}, "power_stats": {"power_gpu_soc_mean_watts": 19.716, "power_cpu_cv_mean_watts": 1.82, "power_sys_5v0_mean_watts": 8.627, "gpu_utilization_percent_mean": 71.058, "power_watts_avg": 19.716, "energy_joules_est": 198.23, "duration_seconds": 10.054, "sample_count": 86}, "timestamp": "2026-01-26T11:13:29.718390"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11582.275, "latencies_ms": [11582.275], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a spacious living room with a variety of furniture and decorations. There is a white couch positioned against the wall, accompanied by a dining table with four chairs around it. The chairs are arranged in a semi-circle, creating a cozy and inviting atmosphere. \n\nIn addition to the main furniture, there are two potted", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20928.7, "ram_available_mb": 41912.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20932.4, "ram_available_mb": 41908.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.768}, "power_stats": {"power_gpu_soc_mean_watts": 19.321, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 70.768, "power_watts_avg": 19.321, "energy_joules_est": 223.79, "duration_seconds": 11.583, "sample_count": 99}, "timestamp": "2026-01-26T11:13:43.328694"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8574.517, "latencies_ms": [8574.517], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "- Chair: 2\n- Table: 1\n- Sofa: 1\n- Rug: 1\n- Plant: 2\n- Television: 1\n- Screen: 1\n- Couch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.4, "ram_available_mb": 41908.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20991.4, "ram_available_mb": 41849.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.945}, "power_stats": {"power_gpu_soc_mean_watts": 20.733, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.572, "gpu_utilization_percent_mean": 72.945, "power_watts_avg": 20.733, "energy_joules_est": 177.79, "duration_seconds": 8.575, "sample_count": 73}, "timestamp": "2026-01-26T11:13:53.935227"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11590.984, "latencies_ms": [11590.984], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a white round table with a vase of flowers on it, positioned near the center of the image. Behind the table, there is a red chair to the left and a white couch to the right, both facing the table. In the background, there is a television set on a wooden stand, a window with a view of greenery outside,", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20991.4, "ram_available_mb": 41849.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.8, "ram_used_mb": 20932.1, "ram_available_mb": 41908.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.848}, "power_stats": {"power_gpu_soc_mean_watts": 19.331, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 69.848, "power_watts_avg": 19.331, "energy_joules_est": 224.08, "duration_seconds": 11.592, "sample_count": 99}, "timestamp": "2026-01-26T11:14:07.553780"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8909.51, "latencies_ms": [8909.51], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a white sofa, a red chair, and a white coffee table. There is a flat-screen TV mounted on the wall, and the room is decorated with various artworks and decorative items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.1, "ram_available_mb": 41908.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20995.4, "ram_available_mb": 41845.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.474}, "power_stats": {"power_gpu_soc_mean_watts": 20.516, "power_cpu_cv_mean_watts": 1.717, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 73.474, "power_watts_avg": 20.516, "energy_joules_est": 182.8, "duration_seconds": 8.91, "sample_count": 76}, "timestamp": "2026-01-26T11:14:18.486837"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6848.311, "latencies_ms": [6848.311], "images_per_second": 0.146, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The living room is bright and airy with natural light coming in from the large windows. The furniture is modern and colorful, with red chairs and a white coffee table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20995.4, "ram_available_mb": 41845.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21034.9, "ram_available_mb": 41806.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.569}, "power_stats": {"power_gpu_soc_mean_watts": 21.674, "power_cpu_cv_mean_watts": 1.525, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 74.569, "power_watts_avg": 21.674, "energy_joules_est": 148.45, "duration_seconds": 6.849, "sample_count": 58}, "timestamp": "2026-01-26T11:14:27.372455"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11590.072, "latencies_ms": [11590.072], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a cozy kitchen, a curious cat has found an unusual perch. The feline, adorned with a vibrant blue collar, is comfortably seated atop a blue refrigerator. The refrigerator, a beacon of coolness in the room, stands tall against the warm tones of the kitchen. \n\nTo", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21034.9, "ram_available_mb": 41806.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20974.0, "ram_available_mb": 41866.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.653}, "power_stats": {"power_gpu_soc_mean_watts": 19.345, "power_cpu_cv_mean_watts": 1.875, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 70.653, "power_watts_avg": 19.345, "energy_joules_est": 224.22, "duration_seconds": 11.591, "sample_count": 98}, "timestamp": "2026-01-26T11:14:40.983552"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8356.594, "latencies_ms": [8356.594], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "refrigerator: 2, cat: 1, light fixture: 1, cabinet: 1, drawer: 1, bracelet: 1, keychain: 1, key: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20974.0, "ram_available_mb": 41866.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20992.2, "ram_available_mb": 41848.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.493}, "power_stats": {"power_gpu_soc_mean_watts": 20.846, "power_cpu_cv_mean_watts": 1.646, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 73.493, "power_watts_avg": 20.846, "energy_joules_est": 174.21, "duration_seconds": 8.357, "sample_count": 71}, "timestamp": "2026-01-26T11:14:51.380941"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11566.391, "latencies_ms": [11566.391], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a blue refrigerator with a cat sitting on top of it. The refrigerator is positioned in the right half of the image. In the background, there is a white cabinet with a glass door, partially visible on the left side of the image. The cat is closer to the camera than the cabinet, making it the main focus of the", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20992.2, "ram_available_mb": 41848.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21008.1, "ram_available_mb": 41832.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.39}, "power_stats": {"power_gpu_soc_mean_watts": 19.328, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 70.39, "power_watts_avg": 19.328, "energy_joules_est": 223.57, "duration_seconds": 11.567, "sample_count": 100}, "timestamp": "2026-01-26T11:15:05.000192"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7868.713, "latencies_ms": [7868.713], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A cat is sitting on top of a blue refrigerator, looking curiously to the side. The refrigerator is in a kitchen with a white cabinet and a light fixture on the ceiling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21008.1, "ram_available_mb": 41832.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20937.2, "ram_available_mb": 41903.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.433}, "power_stats": {"power_gpu_soc_mean_watts": 21.212, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.57, "gpu_utilization_percent_mean": 73.433, "power_watts_avg": 21.212, "energy_joules_est": 166.92, "duration_seconds": 7.869, "sample_count": 67}, "timestamp": "2026-01-26T11:15:14.896776"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7554.459, "latencies_ms": [7554.459], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A cat with a striped pattern is perched on top of a blue refrigerator. The refrigerator is in a kitchen with a white cabinet and a light fixture on the ceiling.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20937.2, "ram_available_mb": 41903.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20991.9, "ram_available_mb": 41849.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.328}, "power_stats": {"power_gpu_soc_mean_watts": 20.895, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 73.328, "power_watts_avg": 20.895, "energy_joules_est": 157.86, "duration_seconds": 7.555, "sample_count": 64}, "timestamp": "2026-01-26T11:15:24.493435"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11573.468, "latencies_ms": [11573.468], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a small, cluttered living room with a wooden floor. The room is filled with various items, including a refrigerator on the left side, a TV, a couch, and a dining table. There are multiple books scattered throughout the room, some on the dining table and others on the floor. \n\nSeveral balloons are present", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 20936.9, "ram_available_mb": 41904.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20929.7, "ram_available_mb": 41911.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.39}, "power_stats": {"power_gpu_soc_mean_watts": 19.314, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.661, "gpu_utilization_percent_mean": 69.39, "power_watts_avg": 19.314, "energy_joules_est": 223.54, "duration_seconds": 11.574, "sample_count": 100}, "timestamp": "2026-01-26T11:15:38.105184"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9128.521, "latencies_ms": [9128.521], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Balloons: 4\n- Furniture: 3\n- Electronics: 2\n- Books: 1\n- Plant: 1\n- Lamp: 1\n- Curtains: 1\n- Bed: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20929.7, "ram_available_mb": 41911.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20993.4, "ram_available_mb": 41847.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.506}, "power_stats": {"power_gpu_soc_mean_watts": 20.469, "power_cpu_cv_mean_watts": 1.705, "power_sys_5v0_mean_watts": 8.59, "gpu_utilization_percent_mean": 72.506, "power_watts_avg": 20.469, "energy_joules_est": 186.87, "duration_seconds": 9.129, "sample_count": 77}, "timestamp": "2026-01-26T11:15:49.279835"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11593.96, "latencies_ms": [11593.96], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden entertainment center with a TV and various electronic devices, positioned near the center of the room. To the left of the entertainment center, there is a refrigerator with a smiley face balloon on top. In the background, there is a bed with a gray blanket and a window with curtains. The room is", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20993.4, "ram_available_mb": 41847.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20932.6, "ram_available_mb": 41908.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.01}, "power_stats": {"power_gpu_soc_mean_watts": 19.26, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 70.01, "power_watts_avg": 19.26, "energy_joules_est": 223.31, "duration_seconds": 11.595, "sample_count": 99}, "timestamp": "2026-01-26T11:16:02.905448"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8121.206, "latencies_ms": [8121.206], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a cozy and cluttered living room with a variety of objects scattered throughout the space. There are balloons, books, and other items on the shelves, and a bed in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.6, "ram_available_mb": 41908.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20924.5, "ram_available_mb": 41916.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.157}, "power_stats": {"power_gpu_soc_mean_watts": 20.996, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.581, "gpu_utilization_percent_mean": 73.157, "power_watts_avg": 20.996, "energy_joules_est": 170.57, "duration_seconds": 8.124, "sample_count": 70}, "timestamp": "2026-01-26T11:16:13.069861"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7531.969, "latencies_ms": [7531.969], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The room is decorated with yellow and blue balloons, and has a wooden floor. There is a bookshelf with various items on it, and a bed with a gray blanket in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20924.5, "ram_available_mb": 41916.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20932.0, "ram_available_mb": 41908.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.719}, "power_stats": {"power_gpu_soc_mean_watts": 21.022, "power_cpu_cv_mean_watts": 1.644, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 72.719, "power_watts_avg": 21.022, "energy_joules_est": 158.35, "duration_seconds": 7.533, "sample_count": 64}, "timestamp": "2026-01-26T11:16:22.650346"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11547.239, "latencies_ms": [11547.239], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is sitting on a train, engrossed in his laptop. He is wearing a green jacket and has a headset on, suggesting he might be working or studying. The laptop, which is silver and has a distinctive Apple logo, is placed on a gray table in front of him. The train's interior is visible in the background, with a", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20932.0, "ram_available_mb": 41908.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20968.7, "ram_available_mb": 41872.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_gpu_soc_mean_watts": 19.48, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.66, "gpu_utilization_percent_mean": 70.0, "power_watts_avg": 19.48, "energy_joules_est": 224.95, "duration_seconds": 11.548, "sample_count": 98}, "timestamp": "2026-01-26T11:16:36.220003"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8943.7, "latencies_ms": [8943.7], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Man: 1\n- Computer: 1\n- Train window: 1\n- Seat: 1\n- Headphones: 1\n- Train tracks: 2\n- Train: 1\n- Train seat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20968.7, "ram_available_mb": 41872.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20977.2, "ram_available_mb": 41863.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.921}, "power_stats": {"power_gpu_soc_mean_watts": 20.37, "power_cpu_cv_mean_watts": 1.69, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 72.921, "power_watts_avg": 20.37, "energy_joules_est": 182.2, "duration_seconds": 8.944, "sample_count": 76}, "timestamp": "2026-01-26T11:16:47.190714"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11569.861, "latencies_ms": [11569.861], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The laptop is positioned in the foreground on a table, with the person's hands on it, indicating they are using it. The background shows a window with a view of train tracks, suggesting the setting is inside a train. The person is seated to the right of the laptop, with their back to the camera, and there is a bag on the floor to the left of the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20977.2, "ram_available_mb": 41863.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20940.7, "ram_available_mb": 41900.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.404}, "power_stats": {"power_gpu_soc_mean_watts": 19.295, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 69.404, "power_watts_avg": 19.295, "energy_joules_est": 223.25, "duration_seconds": 11.57, "sample_count": 99}, "timestamp": "2026-01-26T11:17:00.795624"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7748.72, "latencies_ms": [7748.72], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A person is sitting on a train seat, using a laptop placed on a table in front of them. The train appears to be in motion, as the window shows a blurred view of the tracks outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.7, "ram_available_mb": 41900.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20952.4, "ram_available_mb": 41888.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.182}, "power_stats": {"power_gpu_soc_mean_watts": 21.313, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.576, "gpu_utilization_percent_mean": 74.182, "power_watts_avg": 21.313, "energy_joules_est": 165.16, "duration_seconds": 7.749, "sample_count": 66}, "timestamp": "2026-01-26T11:17:10.582662"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6781.927, "latencies_ms": [6781.927], "images_per_second": 0.147, "prompt_tokens": 36, "response_tokens_est": 37, "n_tiles": 16, "output_text": "The laptop is silver and has an Apple logo on the lid. The man is wearing a green shirt and is seated by a window with a view of train tracks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20952.4, "ram_available_mb": 41888.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20986.3, "ram_available_mb": 41854.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.439}, "power_stats": {"power_gpu_soc_mean_watts": 21.56, "power_cpu_cv_mean_watts": 1.538, "power_sys_5v0_mean_watts": 8.579, "gpu_utilization_percent_mean": 73.439, "power_watts_avg": 21.56, "energy_joules_est": 146.23, "duration_seconds": 6.783, "sample_count": 57}, "timestamp": "2026-01-26T11:17:19.389163"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11641.83, "latencies_ms": [11641.83], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene day at a train station. A silver train, gleaming under the clear blue sky, is stationed on the tracks, ready to embark on its journey. The train is positioned on the right side of the image, its sleek form contrasting with the vibrant green of the trees in the background. \n\nA white bridge, a", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20923.9, "ram_available_mb": 41917.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20983.2, "ram_available_mb": 41857.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.6}, "power_stats": {"power_gpu_soc_mean_watts": 19.11, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 70.6, "power_watts_avg": 19.11, "energy_joules_est": 222.49, "duration_seconds": 11.643, "sample_count": 100}, "timestamp": "2026-01-26T11:17:33.087763"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9752.634, "latencies_ms": [9752.634], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Train: 1\n\n- Train tracks: 4\n\n- Cars: 1\n\n- Bridge: 1\n\n- Town: 1\n\n- Clouds: 10\n\n- Trees: 1\n\n- Buildings: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20928.9, "ram_available_mb": 41912.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20984.8, "ram_available_mb": 41856.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.892}, "power_stats": {"power_gpu_soc_mean_watts": 20.078, "power_cpu_cv_mean_watts": 1.736, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 70.892, "power_watts_avg": 20.078, "energy_joules_est": 195.83, "duration_seconds": 9.753, "sample_count": 83}, "timestamp": "2026-01-26T11:17:44.883889"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10205.893, "latencies_ms": [10205.893], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The train is in the foreground, moving along the tracks, while the bridge is in the background, spanning across the image. The road is in the foreground on the left side of the image, with the bridge crossing over it in the background. The town is in the far background, nestled among the hills.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20984.8, "ram_available_mb": 41856.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20999.0, "ram_available_mb": 41841.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.54}, "power_stats": {"power_gpu_soc_mean_watts": 19.661, "power_cpu_cv_mean_watts": 1.831, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 70.54, "power_watts_avg": 19.661, "energy_joules_est": 200.68, "duration_seconds": 10.207, "sample_count": 87}, "timestamp": "2026-01-26T11:17:57.135333"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7999.759, "latencies_ms": [7999.759], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "A modern white bridge spans over a railway track, with a train traveling on the track and a car driving on the bridge. The sky is blue with some clouds, and there are buildings and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20918.9, "ram_available_mb": 41922.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20971.3, "ram_available_mb": 41869.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.075}, "power_stats": {"power_gpu_soc_mean_watts": 21.143, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.563, "gpu_utilization_percent_mean": 74.075, "power_watts_avg": 21.143, "energy_joules_est": 169.15, "duration_seconds": 8.0, "sample_count": 67}, "timestamp": "2026-01-26T11:18:07.162434"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7692.664, "latencies_ms": [7692.664], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The bridge is a striking white color, contrasting with the blue sky and the greenery in the background. The train is silver and appears to be in motion, with a red car following it on the road below.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20971.3, "ram_available_mb": 41869.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.4, "ram_used_mb": 20994.1, "ram_available_mb": 41846.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.621}, "power_stats": {"power_gpu_soc_mean_watts": 20.816, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 72.621, "power_watts_avg": 20.816, "energy_joules_est": 160.14, "duration_seconds": 7.693, "sample_count": 66}, "timestamp": "2026-01-26T11:18:16.900133"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11625.236, "latencies_ms": [11625.236], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people gathered in a park, enjoying a sunny day. A woman is flying a large kite in the grassy field, while several other people are scattered around the area, some standing and others sitting. The kite is soaring high in the sky, capturing the attention of the onlookers.\n\nThere are multiple backpacks and a", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 20924.1, "ram_available_mb": 41916.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.4, "ram_used_mb": 20990.5, "ram_available_mb": 41850.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.92}, "power_stats": {"power_gpu_soc_mean_watts": 19.254, "power_cpu_cv_mean_watts": 1.977, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 68.92, "power_watts_avg": 19.254, "energy_joules_est": 223.85, "duration_seconds": 11.626, "sample_count": 100}, "timestamp": "2026-01-26T11:18:30.580740"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10070.057, "latencies_ms": [10070.057], "images_per_second": 0.099, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "1. Kite: 1\n2. People: 10\n3. Grass: 1\n4. Chair: 1\n5. Trees: 1\n6. Field: 1\n7. Ball: 0\n8. Frisbee: 0", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20936.3, "ram_available_mb": 41904.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.7, "ram_used_mb": 20932.5, "ram_available_mb": 41908.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.116}, "power_stats": {"power_gpu_soc_mean_watts": 20.029, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.61, "gpu_utilization_percent_mean": 71.116, "power_watts_avg": 20.029, "energy_joules_est": 201.71, "duration_seconds": 10.071, "sample_count": 86}, "timestamp": "2026-01-26T11:18:42.685273"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11565.95, "latencies_ms": [11565.95], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the foreground, there is a vibrant kite with a butterfly design flying in the air. In the background, there are people on a grassy field, some of whom are flying kites as well. The kite in the foreground is closer to the camera, while the people in the background are further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.5, "ram_available_mb": 41908.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 20998.3, "ram_available_mb": 41842.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.612}, "power_stats": {"power_gpu_soc_mean_watts": 19.341, "power_cpu_cv_mean_watts": 1.993, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 70.612, "power_watts_avg": 19.341, "energy_joules_est": 223.71, "duration_seconds": 11.567, "sample_count": 98}, "timestamp": "2026-01-26T11:18:56.282143"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8178.374, "latencies_ms": [8178.374], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A group of people are gathered in a park, flying kites on a sunny day. The kites are colorful and soaring high in the sky, while some people are sitting on the grass, watching the fun.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20944.9, "ram_available_mb": 41896.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.0, "ram_used_mb": 21007.8, "ram_available_mb": 41833.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.681}, "power_stats": {"power_gpu_soc_mean_watts": 20.769, "power_cpu_cv_mean_watts": 1.717, "power_sys_5v0_mean_watts": 8.553, "gpu_utilization_percent_mean": 73.681, "power_watts_avg": 20.769, "energy_joules_est": 169.87, "duration_seconds": 8.179, "sample_count": 69}, "timestamp": "2026-01-26T11:19:06.520826"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9619.069, "latencies_ms": [9619.069], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image features a vibrant kite with a combination of blue, purple, yellow, and red colors flying in the sky. The kite appears to be made of a lightweight fabric, and the weather seems to be sunny and windy, making it ideal for kite flying.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21007.8, "ram_available_mb": 41833.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21006.3, "ram_available_mb": 41834.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.244}, "power_stats": {"power_gpu_soc_mean_watts": 19.751, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 71.244, "power_watts_avg": 19.751, "energy_joules_est": 190.0, "duration_seconds": 9.62, "sample_count": 82}, "timestamp": "2026-01-26T11:19:18.203660"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11601.014, "latencies_ms": [11601.014], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a miniature model train set, meticulously crafted to resemble a real-life scenario. Dominating the scene is a vibrant red Virgin brand train, its sleek design accentuated by a white stripe running along its side. The train is in motion, traveling from the left to the right of the frame, as indicated by the bl", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 20953.0, "ram_available_mb": 41887.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20936.2, "ram_available_mb": 41904.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.061}, "power_stats": {"power_gpu_soc_mean_watts": 19.321, "power_cpu_cv_mean_watts": 1.883, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 70.061, "power_watts_avg": 19.321, "energy_joules_est": 224.16, "duration_seconds": 11.602, "sample_count": 98}, "timestamp": "2026-01-26T11:19:31.850734"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9603.662, "latencies_ms": [9603.662], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- Virgin: 1\n\n- Train: 1\n\n- Workers: 5\n\n- Train tracks: 4\n\n- Fence: 1\n\n- Bushes: 1\n\n- Hill: 1\n\n- Cable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20936.2, "ram_available_mb": 41904.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21000.3, "ram_available_mb": 41840.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.889}, "power_stats": {"power_gpu_soc_mean_watts": 20.229, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.592, "gpu_utilization_percent_mean": 71.889, "power_watts_avg": 20.229, "energy_joules_est": 194.29, "duration_seconds": 9.605, "sample_count": 81}, "timestamp": "2026-01-26T11:19:43.475606"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11603.405, "latencies_ms": [11603.405], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, there is a model train on a track with miniature workers in orange uniforms standing beside it. The train is positioned on the left side of the image, moving towards the right. In the background, there is a blurred landscape with greenery and a fence, suggesting that the train is on a track that is elevated above the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20945.4, "ram_available_mb": 41895.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20996.1, "ram_available_mb": 41844.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.707}, "power_stats": {"power_gpu_soc_mean_watts": 19.249, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.619, "gpu_utilization_percent_mean": 68.707, "power_watts_avg": 19.249, "energy_joules_est": 223.37, "duration_seconds": 11.604, "sample_count": 99}, "timestamp": "2026-01-26T11:19:57.106274"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8678.922, "latencies_ms": [8678.922], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A model train set depicts a Virgin brand train traveling on tracks with miniature workers in orange uniforms standing on the tracks beside it. The scene is set in a miniature landscape with grassy hills and a fence in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20996.1, "ram_available_mb": 41844.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21012.6, "ram_available_mb": 41828.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.014}, "power_stats": {"power_gpu_soc_mean_watts": 20.684, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 72.014, "power_watts_avg": 20.684, "energy_joules_est": 179.53, "duration_seconds": 8.68, "sample_count": 74}, "timestamp": "2026-01-26T11:20:07.813646"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7444.114, "latencies_ms": [7444.114], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A model train set is displayed on a track with miniature figures of workers in orange uniforms. The train is predominantly red and black with a yellow front and the Virgin branding on the side.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21012.6, "ram_available_mb": 41828.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20950.1, "ram_available_mb": 41890.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.714}, "power_stats": {"power_gpu_soc_mean_watts": 21.18, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 73.714, "power_watts_avg": 21.18, "energy_joules_est": 157.68, "duration_seconds": 7.445, "sample_count": 63}, "timestamp": "2026-01-26T11:20:17.270672"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11548.038, "latencies_ms": [11548.038], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a close-up view of a cat's fur, which is predominantly white with a distinct brown patch on its back. The fur appears soft and well-groomed, with a natural sheen that suggests it is healthy and well-cared for. The background is blurred, but it seems to be a textured surface, possibly a piece", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 20950.1, "ram_available_mb": 41890.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20926.4, "ram_available_mb": 41914.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.48}, "power_stats": {"power_gpu_soc_mean_watts": 19.313, "power_cpu_cv_mean_watts": 1.883, "power_sys_5v0_mean_watts": 8.627, "gpu_utilization_percent_mean": 69.48, "power_watts_avg": 19.313, "energy_joules_est": 223.04, "duration_seconds": 11.549, "sample_count": 98}, "timestamp": "2026-01-26T11:20:30.865132"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7363.657, "latencies_ms": [7363.657], "images_per_second": 0.136, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "animal: 1, fur: numerous, stripes: 2, pattern: 1, background: 1, texture: 1, color: 2, direction: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20926.4, "ram_available_mb": 41914.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20935.6, "ram_available_mb": 41905.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.065}, "power_stats": {"power_gpu_soc_mean_watts": 21.47, "power_cpu_cv_mean_watts": 1.51, "power_sys_5v0_mean_watts": 8.542, "gpu_utilization_percent_mean": 74.065, "power_watts_avg": 21.47, "energy_joules_est": 158.11, "duration_seconds": 7.364, "sample_count": 62}, "timestamp": "2026-01-26T11:20:40.284838"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11651.27, "latencies_ms": [11651.27], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a close-up of an animal's fur in the foreground, with a soft, textured background that appears to be a patterned fabric, possibly a blanket or a piece of clothing. The fur is in sharp focus, while the background is out of focus, creating a sense of depth. The fur's texture and color contrast with the background, making the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20935.6, "ram_available_mb": 41905.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20934.3, "ram_available_mb": 41906.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.182}, "power_stats": {"power_gpu_soc_mean_watts": 19.37, "power_cpu_cv_mean_watts": 1.868, "power_sys_5v0_mean_watts": 8.653, "gpu_utilization_percent_mean": 71.182, "power_watts_avg": 19.37, "energy_joules_est": 225.7, "duration_seconds": 11.652, "sample_count": 99}, "timestamp": "2026-01-26T11:20:53.978624"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10137.435, "latencies_ms": [10137.435], "images_per_second": 0.099, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image shows a close-up of a cat's fur, with a blurred background that suggests the cat is lying on a textured surface, possibly a bed or a couch. The fur appears to be well-groomed and healthy, indicating the cat is well taken care of.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20934.3, "ram_available_mb": 41906.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.953}, "power_stats": {"power_gpu_soc_mean_watts": 19.791, "power_cpu_cv_mean_watts": 1.746, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 70.953, "power_watts_avg": 19.791, "energy_joules_est": 200.64, "duration_seconds": 10.138, "sample_count": 86}, "timestamp": "2026-01-26T11:21:06.143278"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8776.859, "latencies_ms": [8776.859], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a close-up of a cat's fur, which is predominantly white with dark brown patches. The fur appears soft and well-groomed, and the lighting is soft, suggesting an indoor setting with natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20985.5, "ram_available_mb": 41855.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.813}, "power_stats": {"power_gpu_soc_mean_watts": 20.401, "power_cpu_cv_mean_watts": 1.713, "power_sys_5v0_mean_watts": 8.618, "gpu_utilization_percent_mean": 72.813, "power_watts_avg": 20.401, "energy_joules_est": 179.07, "duration_seconds": 8.778, "sample_count": 75}, "timestamp": "2026-01-26T11:21:16.952162"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11577.381, "latencies_ms": [11577.381], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a black and white cow is standing in a barn, with its head lowered towards the ground. The cow is positioned next to a metal gate, which is slightly ajar. The cow's head is close to a metal pipe, possibly a ventilation pipe, that is located on the ground. The cow's head is also close to a metal grate", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 20985.5, "ram_available_mb": 41855.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20930.4, "ram_available_mb": 41910.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.697}, "power_stats": {"power_gpu_soc_mean_watts": 19.341, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 70.697, "power_watts_avg": 19.341, "energy_joules_est": 223.93, "duration_seconds": 11.578, "sample_count": 99}, "timestamp": "2026-01-26T11:21:30.575851"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9943.478, "latencies_ms": [9943.478], "images_per_second": 0.101, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Cow: 1\n2. Feeder: 1\n3. Hay: 1\n4. Straw: 1\n5. Fence: 1\n6. Bucket: 1\n7. Stick: 1\n8. Straw: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20930.4, "ram_available_mb": 41910.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20995.8, "ram_available_mb": 41845.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.753}, "power_stats": {"power_gpu_soc_mean_watts": 20.03, "power_cpu_cv_mean_watts": 1.766, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 71.753, "power_watts_avg": 20.03, "energy_joules_est": 199.18, "duration_seconds": 9.944, "sample_count": 85}, "timestamp": "2026-01-26T11:21:42.534706"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11571.23, "latencies_ms": [11571.23], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two red and white objects that appear to be milking cups attached to the udder of a cow, which is positioned near the center of the image. The cow's hind legs are visible in the background, and there is a yellow tag on the cow's ear. The floor is covered with straw and hay, indicating the cow is", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20934.0, "ram_available_mb": 41906.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20990.0, "ram_available_mb": 41850.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.283}, "power_stats": {"power_gpu_soc_mean_watts": 19.318, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 70.283, "power_watts_avg": 19.318, "energy_joules_est": 223.55, "duration_seconds": 11.572, "sample_count": 99}, "timestamp": "2026-01-26T11:21:56.150847"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8856.99, "latencies_ms": [8856.99], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "A cow is standing in a barn with its hind legs visible, and there are three red and white cylindrical objects attached to its legs. The cow is standing on a black rubber mat, and there is a yellow label on the mat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20936.6, "ram_available_mb": 41904.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20993.8, "ram_available_mb": 41847.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.293}, "power_stats": {"power_gpu_soc_mean_watts": 20.456, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.552, "gpu_utilization_percent_mean": 72.293, "power_watts_avg": 20.456, "energy_joules_est": 181.19, "duration_seconds": 8.858, "sample_count": 75}, "timestamp": "2026-01-26T11:22:07.061702"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8808.573, "latencies_ms": [8808.573], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows a close-up of a cow's udder with two red and white milking machines attached. The cow's fur is predominantly black with some white patches, and the environment appears to be indoors with artificial lighting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20993.8, "ram_available_mb": 41847.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21008.7, "ram_available_mb": 41832.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.693}, "power_stats": {"power_gpu_soc_mean_watts": 20.343, "power_cpu_cv_mean_watts": 1.719, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 71.693, "power_watts_avg": 20.343, "energy_joules_est": 179.2, "duration_seconds": 8.809, "sample_count": 75}, "timestamp": "2026-01-26T11:22:17.904887"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11630.069, "latencies_ms": [11630.069], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a sandwich with a slice taken out of it rests on a white plate with a floral pattern. The sandwich is made with two slices of white bread, and the filling is a vibrant red, suggesting it might be made of beetroot or a similar ingredient. The plate is placed on a green tablecloth, which", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21008.7, "ram_available_mb": 41832.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20930.2, "ram_available_mb": 41910.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.287}, "power_stats": {"power_gpu_soc_mean_watts": 19.204, "power_cpu_cv_mean_watts": 1.894, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 70.287, "power_watts_avg": 19.204, "energy_joules_est": 223.36, "duration_seconds": 11.631, "sample_count": 101}, "timestamp": "2026-01-26T11:22:31.596878"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8552.848, "latencies_ms": [8552.848], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "sandwich: 3\nbutter knife: 1\nplate: 1\ntablecloth: 1\ndark background: 1\nlight source: 1\nplate design: 1\nknife handle: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20930.2, "ram_available_mb": 41910.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20991.7, "ram_available_mb": 41849.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.757}, "power_stats": {"power_gpu_soc_mean_watts": 20.701, "power_cpu_cv_mean_watts": 1.677, "power_sys_5v0_mean_watts": 8.586, "gpu_utilization_percent_mean": 72.757, "power_watts_avg": 20.701, "energy_joules_est": 177.07, "duration_seconds": 8.554, "sample_count": 74}, "timestamp": "2026-01-26T11:22:42.197588"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8254.343, "latencies_ms": [8254.343], "images_per_second": 0.121, "prompt_tokens": 44, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The sandwich is placed on the left side of the plate, which is in the foreground of the image. The knife is positioned on the right side of the plate, near the edge, indicating it is ready to be used.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20928.5, "ram_available_mb": 41912.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20978.2, "ram_available_mb": 41862.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.314}, "power_stats": {"power_gpu_soc_mean_watts": 20.679, "power_cpu_cv_mean_watts": 1.687, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 71.314, "power_watts_avg": 20.679, "energy_joules_est": 170.71, "duration_seconds": 8.255, "sample_count": 70}, "timestamp": "2026-01-26T11:22:52.484382"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9511.929, "latencies_ms": [9511.929], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "A sandwich with a bite taken out of it is placed on a decorative plate with a knife beside it, suggesting that someone is about to enjoy a meal. The plate is on a table with a green tablecloth, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20978.2, "ram_available_mb": 41862.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20980.7, "ram_available_mb": 41860.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.902}, "power_stats": {"power_gpu_soc_mean_watts": 20.074, "power_cpu_cv_mean_watts": 1.742, "power_sys_5v0_mean_watts": 8.577, "gpu_utilization_percent_mean": 70.902, "power_watts_avg": 20.074, "energy_joules_est": 190.96, "duration_seconds": 9.513, "sample_count": 82}, "timestamp": "2026-01-26T11:23:04.021290"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11540.6, "latencies_ms": [11540.6], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a sandwich with a visible filling of red berries, possibly raspberries or strawberries, nestled within a lightly toasted bread. The sandwich is placed on a plate with a delicate floral pattern, accompanied by a knife with a dark handle, all set against a dark background that contrasts with the warm tones of the food.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20980.7, "ram_available_mb": 41860.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21007.4, "ram_available_mb": 41833.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.72}, "power_stats": {"power_gpu_soc_mean_watts": 19.248, "power_cpu_cv_mean_watts": 1.921, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 70.72, "power_watts_avg": 19.248, "energy_joules_est": 222.14, "duration_seconds": 11.541, "sample_count": 100}, "timestamp": "2026-01-26T11:23:17.618644"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11567.421, "latencies_ms": [11567.421], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with a tray containing a variety of food items. There are four different bowls on the table, each filled with different types of food. One bowl contains a salad, another has carrots, the third one has grapes, and the fourth one has pasta with meat and cheese. The arrangement of the bowls and the variety", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20928.4, "ram_available_mb": 41912.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20931.6, "ram_available_mb": 41909.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.733}, "power_stats": {"power_gpu_soc_mean_watts": 19.264, "power_cpu_cv_mean_watts": 1.902, "power_sys_5v0_mean_watts": 8.66, "gpu_utilization_percent_mean": 69.733, "power_watts_avg": 19.264, "energy_joules_est": 222.85, "duration_seconds": 11.568, "sample_count": 101}, "timestamp": "2026-01-26T11:23:31.221352"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8795.293, "latencies_ms": [8795.293], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "salad: 1, carrots: 4, grapes: 6, pasta: 1, cheese: 1, tomato sauce: 1, meat: 1, zucchini: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20931.6, "ram_available_mb": 41909.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20929.1, "ram_available_mb": 41911.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.467}, "power_stats": {"power_gpu_soc_mean_watts": 20.63, "power_cpu_cv_mean_watts": 1.686, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 72.467, "power_watts_avg": 20.63, "energy_joules_est": 181.46, "duration_seconds": 8.796, "sample_count": 75}, "timestamp": "2026-01-26T11:23:42.058463"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11562.17, "latencies_ms": [11562.17], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a purple tray with four compartments, each containing different food items. The leftmost compartment has a salad with various vegetables, the second compartment contains sliced carrots, the third compartment has a pasta dish with cheese on top, and the fourth compartment contains green grapes. The background is dark", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20929.1, "ram_available_mb": 41911.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20985.7, "ram_available_mb": 41855.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.95}, "power_stats": {"power_gpu_soc_mean_watts": 19.276, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 69.95, "power_watts_avg": 19.276, "energy_joules_est": 222.88, "duration_seconds": 11.563, "sample_count": 100}, "timestamp": "2026-01-26T11:23:55.631367"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7079.402, "latencies_ms": [7079.402], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image shows a meal prep container with four different sections, each containing a different type of food. The container is placed on a dark surface, possibly a table or countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20985.7, "ram_available_mb": 41855.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20996.7, "ram_available_mb": 41844.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.18}, "power_stats": {"power_gpu_soc_mean_watts": 21.746, "power_cpu_cv_mean_watts": 1.509, "power_sys_5v0_mean_watts": 8.564, "gpu_utilization_percent_mean": 75.18, "power_watts_avg": 21.746, "energy_joules_est": 153.96, "duration_seconds": 7.08, "sample_count": 61}, "timestamp": "2026-01-26T11:24:04.741528"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11549.778, "latencies_ms": [11549.778], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a meal prep container with four different sections, each with a distinct color. The colors are vibrant, with the salad in a pink container, the carrots in a green container, the pasta with meat sauce in an orange container, and the grapes in a blue container. The lighting in the image is bright, highlighting the fresh", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20996.7, "ram_available_mb": 41844.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21033.7, "ram_available_mb": 41807.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.49}, "power_stats": {"power_gpu_soc_mean_watts": 19.269, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 70.49, "power_watts_avg": 19.269, "energy_joules_est": 222.56, "duration_seconds": 11.55, "sample_count": 98}, "timestamp": "2026-01-26T11:24:18.338503"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11562.016, "latencies_ms": [11562.016], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a cherry blossom tree in full bloom, its branches adorned with delicate pink flowers. The tree stands tall against a backdrop of a clear blue sky, its branches reaching out in all directions. The perspective of the image is from below, looking up at the tree, giving a sense of its grandeur and majesty", "error": null, "sys_before": {"cpu_percent": 9.4, "ram_used_mb": 20946.7, "ram_available_mb": 41894.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21002.6, "ram_available_mb": 41838.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.242}, "power_stats": {"power_gpu_soc_mean_watts": 19.264, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 70.242, "power_watts_avg": 19.264, "energy_joules_est": 222.74, "duration_seconds": 11.563, "sample_count": 99}, "timestamp": "2026-01-26T11:24:31.962548"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6673.214, "latencies_ms": [6673.214], "images_per_second": 0.15, "prompt_tokens": 39, "response_tokens_est": 34, "n_tiles": 16, "output_text": "- Trees: numerous\n- Flowers: numerous\n- Traffic lights: 2\n- Red light: 1\n- Black background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21002.6, "ram_available_mb": 41838.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 21003.1, "ram_available_mb": 41837.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.393}, "power_stats": {"power_gpu_soc_mean_watts": 22.279, "power_cpu_cv_mean_watts": 1.415, "power_sys_5v0_mean_watts": 8.557, "gpu_utilization_percent_mean": 75.393, "power_watts_avg": 22.279, "energy_joules_est": 148.69, "duration_seconds": 6.674, "sample_count": 56}, "timestamp": "2026-01-26T11:24:40.657977"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9826.609, "latencies_ms": [9826.609], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The traffic lights are positioned in the foreground on the left side of the image, while the cherry blossom trees cover the background and extend to the right side of the image. The trees appear to be in full bloom, creating a beautiful and lush canopy over the traffic lights.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21003.1, "ram_available_mb": 41837.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21004.3, "ram_available_mb": 41836.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.578}, "power_stats": {"power_gpu_soc_mean_watts": 20.017, "power_cpu_cv_mean_watts": 1.784, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 71.578, "power_watts_avg": 20.017, "energy_joules_est": 196.71, "duration_seconds": 9.827, "sample_count": 83}, "timestamp": "2026-01-26T11:24:52.503044"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10661.239, "latencies_ms": [10661.239], "images_per_second": 0.094, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image captures a vibrant scene of a traffic light with a red signal, set against a backdrop of cherry blossom trees in full bloom. The trees, adorned with delicate pink flowers, create a picturesque and serene setting, enhancing the visual appeal of the traffic light.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21004.3, "ram_available_mb": 41836.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21022.8, "ram_available_mb": 41818.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.967}, "power_stats": {"power_gpu_soc_mean_watts": 19.924, "power_cpu_cv_mean_watts": 1.808, "power_sys_5v0_mean_watts": 8.596, "gpu_utilization_percent_mean": 70.967, "power_watts_avg": 19.924, "energy_joules_est": 212.43, "duration_seconds": 10.662, "sample_count": 91}, "timestamp": "2026-01-26T11:25:05.198529"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9642.751, "latencies_ms": [9642.751], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image features a traffic light with a red signal illuminated, set against a backdrop of cherry blossoms in full bloom. The blossoms are predominantly white with hints of pink, and the trees are densely packed, creating a canopy of flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20927.5, "ram_available_mb": 41913.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20928.6, "ram_available_mb": 41912.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.951}, "power_stats": {"power_gpu_soc_mean_watts": 20.269, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 71.951, "power_watts_avg": 20.269, "energy_joules_est": 195.46, "duration_seconds": 9.643, "sample_count": 82}, "timestamp": "2026-01-26T11:25:16.857728"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11576.398, "latencies_ms": [11576.398], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a white plate filled with a delicious meal consisting of broccoli and meat. The plate is placed on a dining table, and the food is arranged in a visually appealing manner. The broccoli is spread across the plate, with some pieces located closer to the edges and others near the center. The meat, possibly chicken, is placed on top of", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20928.6, "ram_available_mb": 41912.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20937.5, "ram_available_mb": 41903.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.455}, "power_stats": {"power_gpu_soc_mean_watts": 19.346, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 70.455, "power_watts_avg": 19.346, "energy_joules_est": 223.97, "duration_seconds": 11.577, "sample_count": 99}, "timestamp": "2026-01-26T11:25:30.456493"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11451.311, "latencies_ms": [11451.311], "images_per_second": 0.087, "prompt_tokens": 39, "response_tokens_est": 76, "n_tiles": 16, "output_text": "1. Broccoli florets: 12\n2. Onions: 5\n3. Bacon pieces: 8\n4. Peppercorns: 10\n5. Garlic cloves: 3\n6. Olive oil: 1\n7. Salt: 1\n8. Pepper: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20937.5, "ram_available_mb": 41903.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20984.2, "ram_available_mb": 41856.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.031}, "power_stats": {"power_gpu_soc_mean_watts": 19.543, "power_cpu_cv_mean_watts": 1.85, "power_sys_5v0_mean_watts": 8.609, "gpu_utilization_percent_mean": 69.031, "power_watts_avg": 19.543, "energy_joules_est": 223.81, "duration_seconds": 11.452, "sample_count": 98}, "timestamp": "2026-01-26T11:25:43.952197"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11645.281, "latencies_ms": [11645.281], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a close-up view of a plate of food, with the main objects being pieces of broccoli and a grilled salmon fillet. The broccoli is in the near foreground, occupying the majority of the plate, while the salmon is in the background, slightly to the right of the center of the plate. The", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20984.2, "ram_available_mb": 41856.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21020.4, "ram_available_mb": 41820.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.889}, "power_stats": {"power_gpu_soc_mean_watts": 19.106, "power_cpu_cv_mean_watts": 1.872, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 69.889, "power_watts_avg": 19.106, "energy_joules_est": 222.51, "duration_seconds": 11.646, "sample_count": 99}, "timestamp": "2026-01-26T11:25:57.658969"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8337.896, "latencies_ms": [8337.896], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A plate of food is shown with a piece of grilled salmon and a mix of cooked vegetables, including broccoli and cauliflower. The dish appears to be a healthy and balanced meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20931.9, "ram_available_mb": 41909.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20944.0, "ram_available_mb": 41896.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.437}, "power_stats": {"power_gpu_soc_mean_watts": 20.894, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 8.569, "gpu_utilization_percent_mean": 73.437, "power_watts_avg": 20.894, "energy_joules_est": 174.23, "duration_seconds": 8.339, "sample_count": 71}, "timestamp": "2026-01-26T11:26:08.014259"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9850.305, "latencies_ms": [9850.305], "images_per_second": 0.102, "prompt_tokens": 36, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image shows a plate of food with a piece of grilled salmon and a mix of cooked vegetables, including broccoli and cauliflower. The colors are vibrant, with the green of the broccoli and the orange of the salmon standing out against the white plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20944.0, "ram_available_mb": 41896.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20984.5, "ram_available_mb": 41856.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.655}, "power_stats": {"power_gpu_soc_mean_watts": 19.859, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 70.655, "power_watts_avg": 19.859, "energy_joules_est": 195.63, "duration_seconds": 9.851, "sample_count": 84}, "timestamp": "2026-01-26T11:26:19.900245"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11582.413, "latencies_ms": [11582.413], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a dimly lit restaurant, where three individuals are seated at a table. The person on the left, clad in a black shirt, is engrossed in their phone, perhaps browsing or texting. The middle person, wearing a red shirt, is captured mid-bite, savoring a piece of food. The person on", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 20930.0, "ram_available_mb": 41910.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20989.6, "ram_available_mb": 41851.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.765}, "power_stats": {"power_gpu_soc_mean_watts": 19.392, "power_cpu_cv_mean_watts": 1.875, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 69.765, "power_watts_avg": 19.392, "energy_joules_est": 224.62, "duration_seconds": 11.583, "sample_count": 98}, "timestamp": "2026-01-26T11:26:33.520306"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9836.402, "latencies_ms": [9836.402], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "1. Person: 3\n2. Table: 1\n3. Chair: 1\n4. Cell phone: 1\n5. Napkin: 1\n6. Glass: 1\n7. Plate: 1\n8. Silverware: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20927.8, "ram_available_mb": 41913.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20992.6, "ram_available_mb": 41848.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.831}, "power_stats": {"power_gpu_soc_mean_watts": 20.134, "power_cpu_cv_mean_watts": 1.746, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 71.831, "power_watts_avg": 20.134, "energy_joules_est": 198.06, "duration_seconds": 9.837, "sample_count": 83}, "timestamp": "2026-01-26T11:26:45.370139"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11602.446, "latencies_ms": [11602.446], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person on the left side of the image, partially obscured by a metal headboard with a decorative design. In the background, there are two other individuals seated across from each other, with one person slightly closer to the camera than the other. The person on the right appears to be in the farthest part of the image, with a glass", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20939.2, "ram_available_mb": 41901.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20982.5, "ram_available_mb": 41858.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.192}, "power_stats": {"power_gpu_soc_mean_watts": 19.288, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 70.192, "power_watts_avg": 19.288, "energy_joules_est": 223.8, "duration_seconds": 11.603, "sample_count": 99}, "timestamp": "2026-01-26T11:26:59.012729"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8107.063, "latencies_ms": [8107.063], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image depicts a group of people sitting at a table in a dimly lit restaurant or bar. The atmosphere appears to be casual and relaxed, with the individuals engaged in conversation and enjoying their time together.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20929.1, "ram_available_mb": 41911.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 20987.8, "ram_available_mb": 41853.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.391}, "power_stats": {"power_gpu_soc_mean_watts": 21.028, "power_cpu_cv_mean_watts": 1.619, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 73.391, "power_watts_avg": 21.028, "energy_joules_est": 170.49, "duration_seconds": 8.108, "sample_count": 69}, "timestamp": "2026-01-26T11:27:09.157280"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8003.317, "latencies_ms": [8003.317], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image features a warm, dimly lit interior with a dominant orange hue, likely from artificial lighting. The decor includes a metal railing with a leaf-like design, suggesting a cozy, possibly indoor setting.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20987.8, "ram_available_mb": 41853.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.3, "ram_used_mb": 21005.7, "ram_available_mb": 41835.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.132}, "power_stats": {"power_gpu_soc_mean_watts": 20.703, "power_cpu_cv_mean_watts": 1.86, "power_sys_5v0_mean_watts": 8.618, "gpu_utilization_percent_mean": 71.132, "power_watts_avg": 20.703, "energy_joules_est": 165.71, "duration_seconds": 8.004, "sample_count": 68}, "timestamp": "2026-01-26T11:27:19.177774"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11578.697, "latencies_ms": [11578.697], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a city street with a yellow and white bus driving down the road. The bus is positioned in the middle of the street, surrounded by various vehicles, including cars and trucks. There are multiple cars parked or driving along the street, with some closer to the bus and others further away.\n\nThe street is lined with trees, providing a pleasant atmosphere to the", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 20933.1, "ram_available_mb": 41907.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 20941.0, "ram_available_mb": 41899.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.98}, "power_stats": {"power_gpu_soc_mean_watts": 19.369, "power_cpu_cv_mean_watts": 2.01, "power_sys_5v0_mean_watts": 8.661, "gpu_utilization_percent_mean": 69.98, "power_watts_avg": 19.369, "energy_joules_est": 224.28, "duration_seconds": 11.579, "sample_count": 98}, "timestamp": "2026-01-26T11:27:32.812889"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7768.192, "latencies_ms": [7768.192], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "building: 1\nbus: 2\nvan: 2\ntree: 4\nbench: 1\ntrash can: 1\nsidewalk: 1\nroad: 1", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20941.0, "ram_available_mb": 41899.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 9.0, "ram_used_mb": 20976.7, "ram_available_mb": 41864.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.394}, "power_stats": {"power_gpu_soc_mean_watts": 21.264, "power_cpu_cv_mean_watts": 1.996, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 73.394, "power_watts_avg": 21.264, "energy_joules_est": 165.2, "duration_seconds": 7.769, "sample_count": 66}, "timestamp": "2026-01-26T11:27:42.597534"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11134.757, "latencies_ms": [11134.757], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there is a yellow bus on the right side of the image, near a bus stop with a bench. In the background, there is a large building with many windows, and a few cars and vans parked in front of it. The bus is on the right side of the image, while the building is on the left side.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20976.7, "ram_available_mb": 41864.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.8, "ram_used_mb": 20970.8, "ram_available_mb": 41870.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.729}, "power_stats": {"power_gpu_soc_mean_watts": 19.261, "power_cpu_cv_mean_watts": 2.235, "power_sys_5v0_mean_watts": 8.691, "gpu_utilization_percent_mean": 70.729, "power_watts_avg": 19.261, "energy_joules_est": 214.48, "duration_seconds": 11.135, "sample_count": 96}, "timestamp": "2026-01-26T11:27:55.789771"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11550.601, "latencies_ms": [11550.601], "images_per_second": 0.087, "prompt_tokens": 37, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The image depicts a city street with a yellow bus parked on the side of the road. There are several buildings in the background, including a large multi-story building with a curved facade. The street is lined with trees and there are a few cars parked along the curb. The sky is clear and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20970.8, "ram_available_mb": 41870.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 21023.2, "ram_available_mb": 41817.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.04}, "power_stats": {"power_gpu_soc_mean_watts": 19.514, "power_cpu_cv_mean_watts": 1.92, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 71.04, "power_watts_avg": 19.514, "energy_joules_est": 225.41, "duration_seconds": 11.551, "sample_count": 99}, "timestamp": "2026-01-26T11:28:09.361773"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8822.883, "latencies_ms": [8822.883], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows a clear day with sunlight casting shadows on the ground, which is paved with rectangular bricks. The bus is yellow and white, and it is driving on the road in front of a large, modern building with a lot of windows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20952.3, "ram_available_mb": 41888.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21010.6, "ram_available_mb": 41830.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.333}, "power_stats": {"power_gpu_soc_mean_watts": 20.255, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 71.333, "power_watts_avg": 20.255, "energy_joules_est": 178.72, "duration_seconds": 8.823, "sample_count": 75}, "timestamp": "2026-01-26T11:28:20.214626"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11562.75, "latencies_ms": [11562.75], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene on a city street. Dominating the foreground is a red stop sign, its octagonal shape and bold white letters standing out against the urban backdrop. The sign is affixed to a sturdy metal pole, which is slightly tilted to the left, adding a dynamic element to the composition.\n\nThe pole is not alone", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20950.5, "ram_available_mb": 41890.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20976.9, "ram_available_mb": 41864.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.969}, "power_stats": {"power_gpu_soc_mean_watts": 19.253, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 69.969, "power_watts_avg": 19.253, "energy_joules_est": 222.63, "duration_seconds": 11.563, "sample_count": 98}, "timestamp": "2026-01-26T11:28:33.827027"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8929.298, "latencies_ms": [8929.298], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Stop sign: 1\n- Pole: 1\n- Street: 1\n- Sun: 1\n- Buildings: 1\n- Trees: 1\n- Bushes: 1\n- Fence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20976.9, "ram_available_mb": 41864.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20945.8, "ram_available_mb": 41895.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.882}, "power_stats": {"power_gpu_soc_mean_watts": 20.569, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 8.574, "gpu_utilization_percent_mean": 72.882, "power_watts_avg": 20.569, "energy_joules_est": 183.68, "duration_seconds": 8.93, "sample_count": 76}, "timestamp": "2026-01-26T11:28:44.814619"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11682.753, "latencies_ms": [11682.753], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The stop sign is positioned in the foreground on the right side of the image, while the buildings and vehicles are in the background, indicating that the sign is closer to the viewer than the buildings and vehicles. The sun is shining from the left side, casting a shadow of the stop sign on the ground, which is in the foreground on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20945.8, "ram_available_mb": 41895.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20981.2, "ram_available_mb": 41859.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.39}, "power_stats": {"power_gpu_soc_mean_watts": 19.312, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 70.39, "power_watts_avg": 19.312, "energy_joules_est": 225.63, "duration_seconds": 11.683, "sample_count": 100}, "timestamp": "2026-01-26T11:28:58.538874"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7326.095, "latencies_ms": [7326.095], "images_per_second": 0.136, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image shows a red stop sign mounted on a metal pole at an intersection. The sign is positioned in front of a metal fence, with buildings and a street visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20980.9, "ram_available_mb": 41860.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20947.0, "ram_available_mb": 41893.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.129}, "power_stats": {"power_gpu_soc_mean_watts": 21.649, "power_cpu_cv_mean_watts": 1.543, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 74.129, "power_watts_avg": 21.649, "energy_joules_est": 158.62, "duration_seconds": 7.327, "sample_count": 62}, "timestamp": "2026-01-26T11:29:07.918638"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8774.708, "latencies_ms": [8774.708], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image features a red stop sign with white lettering, mounted on a metal pole. The sign is positioned on the side of a road with buildings in the background, and there is a sun flare effect visible in the top left corner of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20947.0, "ram_available_mb": 41893.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20941.1, "ram_available_mb": 41899.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.824}, "power_stats": {"power_gpu_soc_mean_watts": 20.528, "power_cpu_cv_mean_watts": 1.698, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 72.824, "power_watts_avg": 20.528, "energy_joules_est": 180.14, "duration_seconds": 8.775, "sample_count": 74}, "timestamp": "2026-01-26T11:29:18.705946"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11613.441, "latencies_ms": [11613.441], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of this image, a white and brown cat is the main subject. The cat is lying on its side, its body stretched out in a relaxed manner. Its head is tilted slightly to the left, and its eyes are looking directly at the camera, giving an impression of curiosity or alertness. The cat's fur is a mix of white and brown, with", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 20941.1, "ram_available_mb": 41899.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20986.8, "ram_available_mb": 41854.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.429}, "power_stats": {"power_gpu_soc_mean_watts": 19.305, "power_cpu_cv_mean_watts": 1.883, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 69.429, "power_watts_avg": 19.305, "energy_joules_est": 224.21, "duration_seconds": 11.614, "sample_count": 98}, "timestamp": "2026-01-26T11:29:32.381258"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7776.369, "latencies_ms": [7776.369], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "cat: 1, mouse: 1, computer mouse: 1, cord: 1, black surface: 1, white surface: 1, paw: 1, ear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20925.2, "ram_available_mb": 41915.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20947.1, "ram_available_mb": 41893.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.833}, "power_stats": {"power_gpu_soc_mean_watts": 21.186, "power_cpu_cv_mean_watts": 1.571, "power_sys_5v0_mean_watts": 8.569, "gpu_utilization_percent_mean": 73.833, "power_watts_avg": 21.186, "energy_joules_est": 164.76, "duration_seconds": 7.777, "sample_count": 66}, "timestamp": "2026-01-26T11:29:42.207988"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9639.29, "latencies_ms": [9639.29], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The cat is lying on its side with its body stretched out towards the left side of the image, while its head is turned towards the right side. The computer mouse is positioned in the foreground on the left side, and the cords are trailing off to the right side of the image.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20947.1, "ram_available_mb": 41893.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20937.3, "ram_available_mb": 41903.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.568}, "power_stats": {"power_gpu_soc_mean_watts": 20.011, "power_cpu_cv_mean_watts": 1.789, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 71.568, "power_watts_avg": 20.011, "energy_joules_est": 192.9, "duration_seconds": 9.64, "sample_count": 81}, "timestamp": "2026-01-26T11:29:53.882330"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9585.074, "latencies_ms": [9585.074], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "A cat with a white and brown coat is lying on a black surface, possibly a couch or a bed, with its paws stretched out in front of it. Next to the cat is a computer mouse, suggesting that the cat may be a pet of someone who uses the computer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20937.3, "ram_available_mb": 41903.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20995.9, "ram_available_mb": 41845.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.302, "power_cpu_cv_mean_watts": 1.744, "power_sys_5v0_mean_watts": 8.596, "gpu_utilization_percent_mean": 72.0, "power_watts_avg": 20.302, "energy_joules_est": 194.61, "duration_seconds": 9.586, "sample_count": 81}, "timestamp": "2026-01-26T11:30:05.496388"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8250.557, "latencies_ms": [8250.557], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image features a cat with a mix of white and dark fur, possibly black or brown, lying on a dark surface. The lighting is dim, creating a moody atmosphere, and the cat's eyes are a striking green color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20933.5, "ram_available_mb": 41907.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20943.3, "ram_available_mb": 41897.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.914}, "power_stats": {"power_gpu_soc_mean_watts": 20.588, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 71.914, "power_watts_avg": 20.588, "energy_joules_est": 169.88, "duration_seconds": 8.251, "sample_count": 70}, "timestamp": "2026-01-26T11:30:15.768670"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11599.018, "latencies_ms": [11599.018], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a city street with a large number of buses parked in a parking lot. There are at least 13 buses visible, with some parked closer to the foreground and others further back in the scene. The buses are of various sizes and colors, indicating a diverse fleet.\n\nIn addition to the buses, there are several cars parked along", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20943.3, "ram_available_mb": 41897.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20976.5, "ram_available_mb": 41864.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.455}, "power_stats": {"power_gpu_soc_mean_watts": 19.277, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 70.455, "power_watts_avg": 19.277, "energy_joules_est": 223.61, "duration_seconds": 11.6, "sample_count": 99}, "timestamp": "2026-01-26T11:30:29.402761"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8446.603, "latencies_ms": [8446.603], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "bus: 10, building: 15, tree: 5, lamppost: 2, flag: 3, bus stop: 2, bus lane: 1, bus stop sign: 1", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20976.5, "ram_available_mb": 41864.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20977.7, "ram_available_mb": 41863.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.901}, "power_stats": {"power_gpu_soc_mean_watts": 20.955, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.609, "gpu_utilization_percent_mean": 72.901, "power_watts_avg": 20.955, "energy_joules_est": 177.02, "duration_seconds": 8.447, "sample_count": 71}, "timestamp": "2026-01-26T11:30:39.870852"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11573.015, "latencies_ms": [11573.015], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are several buses parked under a covered area, with one bus prominently in the center. The buses are positioned near a road that curves to the left in the background. Further back, there are multiple high-rise buildings, with the tallest one located on the right side of the image. The sky is visible in the upper part of", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20977.7, "ram_available_mb": 41863.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20988.1, "ram_available_mb": 41852.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.667}, "power_stats": {"power_gpu_soc_mean_watts": 19.3, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 70.667, "power_watts_avg": 19.3, "energy_joules_est": 223.37, "duration_seconds": 11.574, "sample_count": 99}, "timestamp": "2026-01-26T11:30:53.474410"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7180.343, "latencies_ms": [7180.343], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image depicts a bustling city scene with multiple buses parked at a bus station. The station is surrounded by tall buildings, and the sky is partly cloudy.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20924.8, "ram_available_mb": 41916.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20931.2, "ram_available_mb": 41909.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.532}, "power_stats": {"power_gpu_soc_mean_watts": 21.714, "power_cpu_cv_mean_watts": 1.517, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 74.532, "power_watts_avg": 21.714, "energy_joules_est": 155.93, "duration_seconds": 7.181, "sample_count": 62}, "timestamp": "2026-01-26T11:31:02.685297"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9261.033, "latencies_ms": [9261.033], "images_per_second": 0.108, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a clear day with a few clouds in the sky, and the lighting suggests it is daytime. The buses are predominantly white with some having green and blue accents, and they are parked in a large, open area with a concrete structure overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20931.2, "ram_available_mb": 41909.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20963.4, "ram_available_mb": 41877.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.962}, "power_stats": {"power_gpu_soc_mean_watts": 20.108, "power_cpu_cv_mean_watts": 1.75, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 70.962, "power_watts_avg": 20.108, "energy_joules_est": 186.23, "duration_seconds": 9.262, "sample_count": 78}, "timestamp": "2026-01-26T11:31:13.978576"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11562.337, "latencies_ms": [11562.337], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is skillfully riding a skateboard down the side of a ramp. He is wearing a cowboy hat and appears to be enjoying the activity. The skateboarder is positioned in the center of the scene, with the ramp extending from the left to the right side of the image.\n\nThere are several chairs placed around", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20963.4, "ram_available_mb": 41877.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20949.4, "ram_available_mb": 41891.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.465}, "power_stats": {"power_gpu_soc_mean_watts": 19.268, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 69.465, "power_watts_avg": 19.268, "energy_joules_est": 222.8, "duration_seconds": 11.563, "sample_count": 99}, "timestamp": "2026-01-26T11:31:27.575133"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8676.318, "latencies_ms": [8676.318], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "skateboard: 1\nskateboarder: 1\ntent: 3\nsandals: 1\nshade: 1\ntable: 1\nchairs: 1\numbrella: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20949.4, "ram_available_mb": 41891.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20989.1, "ram_available_mb": 41851.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.459}, "power_stats": {"power_gpu_soc_mean_watts": 20.903, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.601, "gpu_utilization_percent_mean": 73.459, "power_watts_avg": 20.903, "energy_joules_est": 181.37, "duration_seconds": 8.677, "sample_count": 74}, "timestamp": "2026-01-26T11:31:38.271736"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9608.979, "latencies_ms": [9608.979], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "In the foreground, there is a skateboard ramp with a person skateboarding on it. The skateboarder is in the middle ground, and there are large green tents in the background. There are also some chairs and tables set up near the tents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20989.1, "ram_available_mb": 41851.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20926.5, "ram_available_mb": 41914.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.171}, "power_stats": {"power_gpu_soc_mean_watts": 20.058, "power_cpu_cv_mean_watts": 1.762, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 71.171, "power_watts_avg": 20.058, "energy_joules_est": 192.75, "duration_seconds": 9.61, "sample_count": 82}, "timestamp": "2026-01-26T11:31:49.917380"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7709.065, "latencies_ms": [7709.065], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A person is skateboarding on a ramp in an outdoor setting with large green tents in the background. The skateboarder is wearing a cowboy hat and black shorts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20926.5, "ram_available_mb": 41914.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 20987.2, "ram_available_mb": 41853.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.462}, "power_stats": {"power_gpu_soc_mean_watts": 21.104, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.55, "gpu_utilization_percent_mean": 74.462, "power_watts_avg": 21.104, "energy_joules_est": 162.7, "duration_seconds": 7.71, "sample_count": 65}, "timestamp": "2026-01-26T11:31:59.665239"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11629.234, "latencies_ms": [11629.234], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a person skateboarding on a ramp with a clear blue sky in the background. The skateboarder is wearing a black tank top and shorts, and the ramp is made of wood. There are several large green tents set up in the background, and a few people can be seen sitting on chairs under the tents. The lighting is", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20987.2, "ram_available_mb": 41853.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 21013.0, "ram_available_mb": 41827.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.838}, "power_stats": {"power_gpu_soc_mean_watts": 19.344, "power_cpu_cv_mean_watts": 2.058, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 70.838, "power_watts_avg": 19.344, "energy_joules_est": 224.97, "duration_seconds": 11.63, "sample_count": 99}, "timestamp": "2026-01-26T11:32:13.320723"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12344.245, "latencies_ms": [12344.245], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a dynamic scene of a person windsurfing on a sunny day. The windsurfer, clad in a black wetsuit, is skillfully maneuvering a white sailboat with a blue and white design. The sailboat is tethered to a blue and white kite, which is soaring high in the sky, harness", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20931.1, "ram_available_mb": 41909.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 21039.3, "ram_available_mb": 41801.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.867}, "power_stats": {"power_gpu_soc_mean_watts": 21.406, "power_cpu_cv_mean_watts": 1.959, "power_sys_5v0_mean_watts": 8.832, "gpu_utilization_percent_mean": 72.867, "power_watts_avg": 21.406, "energy_joules_est": 264.26, "duration_seconds": 12.345, "sample_count": 105}, "timestamp": "2026-01-26T11:32:27.712736"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8485.53, "latencies_ms": [8485.53], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "person: 1, surfboard: 1, kite: 4, wave: multiple, ocean: multiple, sky: multiple, wind: multiple, kite surfing: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20942.4, "ram_available_mb": 41898.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 21058.7, "ram_available_mb": 41782.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.194}, "power_stats": {"power_gpu_soc_mean_watts": 22.945, "power_cpu_cv_mean_watts": 1.517, "power_sys_5v0_mean_watts": 8.706, "gpu_utilization_percent_mean": 77.194, "power_watts_avg": 22.945, "energy_joules_est": 194.71, "duration_seconds": 8.486, "sample_count": 72}, "timestamp": "2026-01-26T11:32:38.215458"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10795.719, "latencies_ms": [10795.719], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "In the foreground, there is a person standing on a surfboard with a sail, positioned near the water's edge. The waves are closer to the viewer, creating a sense of depth. In the background, there are multiple kites flying high in the sky, which appear smaller due to the distance.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 20955.0, "ram_available_mb": 41885.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 20936.9, "ram_available_mb": 41904.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.613}, "power_stats": {"power_gpu_soc_mean_watts": 21.739, "power_cpu_cv_mean_watts": 1.872, "power_sys_5v0_mean_watts": 8.853, "gpu_utilization_percent_mean": 73.613, "power_watts_avg": 21.739, "energy_joules_est": 234.7, "duration_seconds": 10.796, "sample_count": 93}, "timestamp": "2026-01-26T11:32:51.062394"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7338.301, "latencies_ms": [7338.301], "images_per_second": 0.136, "prompt_tokens": 37, "response_tokens_est": 33, "n_tiles": 16, "output_text": "A person is windsurfing on a sunny day with several kites flying in the sky. The ocean is rough with waves suitable for the sport.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20936.9, "ram_available_mb": 41904.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21028.6, "ram_available_mb": 41812.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.758}, "power_stats": {"power_gpu_soc_mean_watts": 23.719, "power_cpu_cv_mean_watts": 1.31, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 78.758, "power_watts_avg": 23.719, "energy_joules_est": 174.08, "duration_seconds": 7.339, "sample_count": 62}, "timestamp": "2026-01-26T11:33:00.419303"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8600.758, "latencies_ms": [8600.758], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image captures a bright and sunny day at the beach with clear blue skies and a few clouds. The ocean is a deep blue-green color, and the waves are white-capped, indicating strong winds.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20940.8, "ram_available_mb": 41900.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21037.8, "ram_available_mb": 41803.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.918}, "power_stats": {"power_gpu_soc_mean_watts": 22.602, "power_cpu_cv_mean_watts": 1.584, "power_sys_5v0_mean_watts": 8.817, "gpu_utilization_percent_mean": 74.918, "power_watts_avg": 22.602, "energy_joules_est": 194.41, "duration_seconds": 8.601, "sample_count": 73}, "timestamp": "2026-01-26T11:33:11.059914"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11573.339, "latencies_ms": [11573.339], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a lush garden, a vibrant red fire hydrant stands as a beacon of safety. Its black cap and nozzles gleam under the sunlight, contrasting sharply with its red body. The hydrant is positioned on a patch of green grass, surrounded by an array of flowers and plants that add a touch of nature's beauty to the", "error": null, "sys_before": {"cpu_percent": 3.6, "ram_used_mb": 20958.0, "ram_available_mb": 41882.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21019.4, "ram_available_mb": 41821.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.273}, "power_stats": {"power_gpu_soc_mean_watts": 19.282, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.664, "gpu_utilization_percent_mean": 70.273, "power_watts_avg": 19.282, "energy_joules_est": 223.17, "duration_seconds": 11.574, "sample_count": 99}, "timestamp": "2026-01-26T11:33:24.675325"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8615.673, "latencies_ms": [8615.673], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "fire hydrant: 1, grass: numerous, dandelions: 8, house: 1, tree: 1, window: 1, flowers: 1, roof: 1, wisteria: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.8, "ram_available_mb": 41900.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.877}, "power_stats": {"power_gpu_soc_mean_watts": 20.848, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.596, "gpu_utilization_percent_mean": 72.877, "power_watts_avg": 20.848, "energy_joules_est": 179.63, "duration_seconds": 8.616, "sample_count": 73}, "timestamp": "2026-01-26T11:33:35.337679"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9846.743, "latencies_ms": [9846.743], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The fire hydrant is located in the foreground of the image, standing on a grassy area. In the background, there is a white house with a thatched roof and a tree with purple flowers. The fire hydrant is positioned closer to the viewer than the house and the tree.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20995.4, "ram_available_mb": 41845.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.917}, "power_stats": {"power_gpu_soc_mean_watts": 19.99, "power_cpu_cv_mean_watts": 1.772, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 70.917, "power_watts_avg": 19.99, "energy_joules_est": 196.85, "duration_seconds": 9.847, "sample_count": 84}, "timestamp": "2026-01-26T11:33:47.218941"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7778.177, "latencies_ms": [7778.177], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A red fire hydrant is situated in a grassy area with a white house and trees in the background. The hydrant appears to be old and weathered, with some rust and dirt on its surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20995.4, "ram_available_mb": 41845.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20947.5, "ram_available_mb": 41893.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.864}, "power_stats": {"power_gpu_soc_mean_watts": 21.318, "power_cpu_cv_mean_watts": 1.576, "power_sys_5v0_mean_watts": 8.565, "gpu_utilization_percent_mean": 72.864, "power_watts_avg": 21.318, "energy_joules_est": 165.83, "duration_seconds": 7.779, "sample_count": 66}, "timestamp": "2026-01-26T11:33:57.054519"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7100.258, "latencies_ms": [7100.258], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The fire hydrant in the image is bright red and appears to be made of metal. It is situated in a grassy area with some purple flowers and green trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20947.5, "ram_available_mb": 41893.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20942.4, "ram_available_mb": 41898.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.18}, "power_stats": {"power_gpu_soc_mean_watts": 21.254, "power_cpu_cv_mean_watts": 1.555, "power_sys_5v0_mean_watts": 8.579, "gpu_utilization_percent_mean": 74.18, "power_watts_avg": 21.254, "energy_joules_est": 150.92, "duration_seconds": 7.101, "sample_count": 61}, "timestamp": "2026-01-26T11:34:06.197446"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11587.719, "latencies_ms": [11587.719], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of a bird in flight, with its wings spread wide and its head turned towards the camera. The bird is perched on a wooden surface, which is painted in a light blue color. The surface is composed of multiple wooden planks, each with a slightly different shade of blue, creating a subtle pattern. The bird's feathers are a mix of", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20942.4, "ram_available_mb": 41898.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20939.6, "ram_available_mb": 41901.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_gpu_soc_mean_watts": 19.36, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 71.0, "power_watts_avg": 19.36, "energy_joules_est": 224.35, "duration_seconds": 11.588, "sample_count": 99}, "timestamp": "2026-01-26T11:34:19.828348"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8795.149, "latencies_ms": [8795.149], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "bird: 2, wooden plank: 5, paint peeling: 3, color blue: 1, color green: 1, color brown: 1, bird's wing: 1, bird's head: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20939.6, "ram_available_mb": 41901.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20972.1, "ram_available_mb": 41868.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.467}, "power_stats": {"power_gpu_soc_mean_watts": 20.593, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.577, "gpu_utilization_percent_mean": 72.467, "power_watts_avg": 20.593, "energy_joules_est": 181.13, "duration_seconds": 8.796, "sample_count": 75}, "timestamp": "2026-01-26T11:34:30.655327"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11590.41, "latencies_ms": [11590.41], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a bird with its wings spread, positioned near the center of the image, appearing to be in mid-flight. In the background, there are two more birds, one partially visible on the left and another on the right, both situated on what appears to be a wooden surface with horizontal planks. The bird in the foreground is closer to the vie", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20972.1, "ram_available_mb": 41868.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20933.6, "ram_available_mb": 41907.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.869}, "power_stats": {"power_gpu_soc_mean_watts": 19.29, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 69.869, "power_watts_avg": 19.29, "energy_joules_est": 223.59, "duration_seconds": 11.591, "sample_count": 99}, "timestamp": "2026-01-26T11:34:44.282507"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8697.259, "latencies_ms": [8697.259], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A small bird with a long, pointed beak is perched on a wooden surface, possibly a roof or a deck, with its wings spread out. The wood has a weathered look with peeling paint, indicating it may be an older structure.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20933.6, "ram_available_mb": 41907.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20999.0, "ram_available_mb": 41841.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.635}, "power_stats": {"power_gpu_soc_mean_watts": 20.686, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 72.635, "power_watts_avg": 20.686, "energy_joules_est": 179.93, "duration_seconds": 8.698, "sample_count": 74}, "timestamp": "2026-01-26T11:34:55.007042"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10069.438, "latencies_ms": [10069.438], "images_per_second": 0.099, "prompt_tokens": 36, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image features a close-up of a bird with a dark body and a lighter underbelly, perched on a weathered wooden surface with peeling blue paint. The lighting is soft and diffused, casting gentle shadows and highlighting the texture of the wood and the bird's feathers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20999.0, "ram_available_mb": 41841.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20962.8, "ram_available_mb": 41878.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.224}, "power_stats": {"power_gpu_soc_mean_watts": 19.803, "power_cpu_cv_mean_watts": 1.813, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 71.224, "power_watts_avg": 19.803, "energy_joules_est": 199.42, "duration_seconds": 10.07, "sample_count": 85}, "timestamp": "2026-01-26T11:35:07.109242"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11590.469, "latencies_ms": [11590.469], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing in a barn, watching a brown horse with a red halter walking around. The horse is positioned in the center of the scene, and the woman is standing to the right of it. The barn has a wooden floor, and there are several buckets placed around the room. \n\nIn addition to the woman and the horse, there", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 20962.8, "ram_available_mb": 41878.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20992.3, "ram_available_mb": 41848.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.929}, "power_stats": {"power_gpu_soc_mean_watts": 19.362, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.639, "gpu_utilization_percent_mean": 70.929, "power_watts_avg": 19.362, "energy_joules_est": 224.43, "duration_seconds": 11.591, "sample_count": 99}, "timestamp": "2026-01-26T11:35:20.752235"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7446.058, "latencies_ms": [7446.058], "images_per_second": 0.134, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "door: 1, horse: 1, bucket: 1, wall: 1, window: 1, floor: 1, person: 1, desk: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20938.8, "ram_available_mb": 41902.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20940.5, "ram_available_mb": 41900.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.381}, "power_stats": {"power_gpu_soc_mean_watts": 21.476, "power_cpu_cv_mean_watts": 1.544, "power_sys_5v0_mean_watts": 8.562, "gpu_utilization_percent_mean": 74.381, "power_watts_avg": 21.476, "energy_joules_est": 159.93, "duration_seconds": 7.447, "sample_count": 63}, "timestamp": "2026-01-26T11:35:30.252270"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11562.763, "latencies_ms": [11562.763], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a brown horse with a red halter, positioned near the center of the image, walking towards the left side. A person, wearing blue jeans and a grey shirt, is standing to the right of the horse, closer to the background. The background features a wooden wall with various items hanging on it, a window, and a red door", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20940.5, "ram_available_mb": 41900.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20985.0, "ram_available_mb": 41855.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.388}, "power_stats": {"power_gpu_soc_mean_watts": 19.34, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 70.388, "power_watts_avg": 19.34, "energy_joules_est": 223.64, "duration_seconds": 11.563, "sample_count": 98}, "timestamp": "2026-01-26T11:35:43.838763"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8108.733, "latencies_ms": [8108.733], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A woman is standing in a barn with a horse that is walking towards her. The barn has wooden walls and a red door, and there are various items scattered around the room, including a desk and a window.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20931.5, "ram_available_mb": 41909.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20923.1, "ram_available_mb": 41917.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.574}, "power_stats": {"power_gpu_soc_mean_watts": 21.073, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.575, "gpu_utilization_percent_mean": 73.574, "power_watts_avg": 21.073, "energy_joules_est": 170.89, "duration_seconds": 8.109, "sample_count": 68}, "timestamp": "2026-01-26T11:35:53.976128"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6981.847, "latencies_ms": [6981.847], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image depicts an indoor setting with wooden walls and a concrete floor. The lighting appears to be natural daylight coming from a window, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20923.1, "ram_available_mb": 41917.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20987.2, "ram_available_mb": 41853.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.356}, "power_stats": {"power_gpu_soc_mean_watts": 21.512, "power_cpu_cv_mean_watts": 1.547, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 73.356, "power_watts_avg": 21.512, "energy_joules_est": 150.21, "duration_seconds": 6.982, "sample_count": 59}, "timestamp": "2026-01-26T11:36:02.978005"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11591.104, "latencies_ms": [11591.104], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a lush green field with a variety of animals grazing and roaming around. There are several zebras and cows scattered throughout the field, with some standing closer to the foreground and others further in the background. The animals appear to be enjoying the grassy area, which is surrounded by trees.\n\nIn addition to the zebras and cows,", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20933.8, "ram_available_mb": 41907.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20972.2, "ram_available_mb": 41868.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.889}, "power_stats": {"power_gpu_soc_mean_watts": 19.346, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 70.889, "power_watts_avg": 19.346, "energy_joules_est": 224.25, "duration_seconds": 11.592, "sample_count": 99}, "timestamp": "2026-01-26T11:36:16.621871"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9623.708, "latencies_ms": [9623.708], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "grass: 1\n\ntree: 10\n\nwild boar: 2\n\nzebra: 2\n\ngiraffe: 1\n\nantelope: 2\n\nrhino: 2\n\nelephant: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20972.2, "ram_available_mb": 41868.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20972.2, "ram_available_mb": 41868.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.549}, "power_stats": {"power_gpu_soc_mean_watts": 20.138, "power_cpu_cv_mean_watts": 1.738, "power_sys_5v0_mean_watts": 8.584, "gpu_utilization_percent_mean": 71.549, "power_watts_avg": 20.138, "energy_joules_est": 193.81, "duration_seconds": 9.624, "sample_count": 82}, "timestamp": "2026-01-26T11:36:28.269988"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11595.431, "latencies_ms": [11595.431], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a grassy field with a zebra grazing on the left side and a few other animals scattered around. In the background, there are trees and a pond, with a large tree trunk on the right side of the image. The animals are positioned at various distances from the viewer, with some closer to the foreground and others further away", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20972.2, "ram_available_mb": 41868.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 20973.4, "ram_available_mb": 41867.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.255}, "power_stats": {"power_gpu_soc_mean_watts": 19.147, "power_cpu_cv_mean_watts": 1.883, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 70.255, "power_watts_avg": 19.147, "energy_joules_est": 222.03, "duration_seconds": 11.596, "sample_count": 98}, "timestamp": "2026-01-26T11:36:41.918107"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8102.743, "latencies_ms": [8102.743], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image depicts a lush green field with a variety of animals grazing and roaming around. There are zebras, wildebeests, and other animals peacefully coexisting in this natural environment.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20973.4, "ram_available_mb": 41867.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 21008.2, "ram_available_mb": 41832.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.059}, "power_stats": {"power_gpu_soc_mean_watts": 21.081, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 8.584, "gpu_utilization_percent_mean": 73.059, "power_watts_avg": 21.081, "energy_joules_est": 170.83, "duration_seconds": 8.103, "sample_count": 68}, "timestamp": "2026-01-26T11:36:52.049759"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8815.661, "latencies_ms": [8815.661], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image depicts a sunny day with clear skies, as evidenced by the bright lighting and shadows cast on the ground. The landscape is a mix of green grass and brown rocks, with trees in the background providing a natural backdrop.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20938.0, "ram_available_mb": 41902.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20973.2, "ram_available_mb": 41867.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.947}, "power_stats": {"power_gpu_soc_mean_watts": 20.132, "power_cpu_cv_mean_watts": 1.743, "power_sys_5v0_mean_watts": 8.617, "gpu_utilization_percent_mean": 71.947, "power_watts_avg": 20.132, "energy_joules_est": 177.49, "duration_seconds": 8.816, "sample_count": 76}, "timestamp": "2026-01-26T11:37:02.925659"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11623.614, "latencies_ms": [11623.614], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a bustling theme park, where a white horse-drawn carriage, adorned with a green and gold canopy, is the centerpiece. The carriage, labeled \"Disneyland\", is being pulled by a majestic white horse, its hooves treading on a gray pavement. The horse's harness is a", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20973.2, "ram_available_mb": 41867.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20973.2, "ram_available_mb": 41867.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.75}, "power_stats": {"power_gpu_soc_mean_watts": 19.285, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 70.75, "power_watts_avg": 19.285, "energy_joules_est": 224.17, "duration_seconds": 11.624, "sample_count": 100}, "timestamp": "2026-01-26T11:37:16.574290"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8114.51, "latencies_ms": [8114.51], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "people: 10, disneyland sign: 1, horse: 1, carriage: 1, bench: 1, trees: 5, sky: 1, umbrella: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.2, "ram_available_mb": 41867.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20995.6, "ram_available_mb": 41845.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.353}, "power_stats": {"power_gpu_soc_mean_watts": 21.045, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.558, "gpu_utilization_percent_mean": 73.353, "power_watts_avg": 21.045, "energy_joules_est": 170.78, "duration_seconds": 8.115, "sample_count": 68}, "timestamp": "2026-01-26T11:37:26.707184"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10074.186, "latencies_ms": [10074.186], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 66, "n_tiles": 16, "output_text": "In the foreground, there is a white horse pulling a green and yellow trolley with the word \"Disneyland\" on it. The trolley is on a street with people walking on the sidewalks and sitting on benches. In the background, there are trees and more people walking or standing.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20934.8, "ram_available_mb": 41906.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21028.6, "ram_available_mb": 41812.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.988}, "power_stats": {"power_gpu_soc_mean_watts": 19.852, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 70.988, "power_watts_avg": 19.852, "energy_joules_est": 200.0, "duration_seconds": 10.075, "sample_count": 85}, "timestamp": "2026-01-26T11:37:38.802861"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10151.139, "latencies_ms": [10151.139], "images_per_second": 0.099, "prompt_tokens": 37, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image captures a vintage scene at Disneyland, featuring a horse-drawn carriage with a sign that reads \"Disneyland\" and \"Main Street, U.S.A.\" The carriage is being pulled by a white horse, and there are several people on board, enjoying the ride.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21028.6, "ram_available_mb": 41812.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21034.6, "ram_available_mb": 41806.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.126}, "power_stats": {"power_gpu_soc_mean_watts": 20.016, "power_cpu_cv_mean_watts": 1.79, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 71.126, "power_watts_avg": 20.016, "energy_joules_est": 203.2, "duration_seconds": 10.152, "sample_count": 87}, "timestamp": "2026-01-26T11:37:50.979317"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10287.148, "latencies_ms": [10287.148], "images_per_second": 0.097, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image features a vibrant scene with a horse-drawn carriage in the center, painted in green and gold with a red roof, indicating a festive or touristic setting. The lighting suggests it's a sunny day, casting shadows on the ground, and the weather appears to be clear and pleasant.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20921.0, "ram_available_mb": 41919.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20993.9, "ram_available_mb": 41847.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.402}, "power_stats": {"power_gpu_soc_mean_watts": 19.722, "power_cpu_cv_mean_watts": 1.831, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 70.402, "power_watts_avg": 19.722, "energy_joules_est": 202.9, "duration_seconds": 10.288, "sample_count": 87}, "timestamp": "2026-01-26T11:38:03.286818"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11601.998, "latencies_ms": [11601.998], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of older men sitting on benches in a public area, likely a park or a waiting area. There are at least five men visible, with some sitting on the benches and others standing nearby. They are all engaged in various activities, such as reading newspapers, looking at their cell phones, or simply enjoying the outdoors.\n\nThere are multiple", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 20923.8, "ram_available_mb": 41917.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20943.2, "ram_available_mb": 41897.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.808}, "power_stats": {"power_gpu_soc_mean_watts": 19.257, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 69.808, "power_watts_avg": 19.257, "energy_joules_est": 223.43, "duration_seconds": 11.603, "sample_count": 99}, "timestamp": "2026-01-26T11:38:16.918930"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8021.98, "latencies_ms": [8021.98], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "bench: 5\nman: 5\nnewspaper: 1\ntrash can: 1\nwindow: 1\ndoor: 1\nplant: 1\nposter: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20943.2, "ram_available_mb": 41897.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21016.4, "ram_available_mb": 41824.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.388}, "power_stats": {"power_gpu_soc_mean_watts": 21.097, "power_cpu_cv_mean_watts": 1.577, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 73.388, "power_watts_avg": 21.097, "energy_joules_est": 169.25, "duration_seconds": 8.023, "sample_count": 67}, "timestamp": "2026-01-26T11:38:26.954689"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11252.634, "latencies_ms": [11252.634], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground, there is a man sitting on a green bench reading a newspaper. Behind him, there are several other benches with people sitting on them, and a building with a sign that reads \"SOCIETY\". The man on the far right is wearing a red shirt and appears to be the furthest away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20926.1, "ram_available_mb": 41914.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20963.3, "ram_available_mb": 41877.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.469}, "power_stats": {"power_gpu_soc_mean_watts": 19.355, "power_cpu_cv_mean_watts": 1.868, "power_sys_5v0_mean_watts": 8.618, "gpu_utilization_percent_mean": 70.469, "power_watts_avg": 19.355, "energy_joules_est": 217.81, "duration_seconds": 11.254, "sample_count": 96}, "timestamp": "2026-01-26T11:38:40.254380"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8356.747, "latencies_ms": [8356.747], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image depicts a group of older men sitting on benches in a public area, with one man reading a newspaper. The setting appears to be a park or a public square with multiple benches and a building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.3, "ram_available_mb": 41877.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20964.6, "ram_available_mb": 41876.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.479}, "power_stats": {"power_gpu_soc_mean_watts": 20.701, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 8.556, "gpu_utilization_percent_mean": 72.479, "power_watts_avg": 20.701, "energy_joules_est": 173.01, "duration_seconds": 8.357, "sample_count": 71}, "timestamp": "2026-01-26T11:38:50.623930"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6071.209, "latencies_ms": [6071.209], "images_per_second": 0.165, "prompt_tokens": 36, "response_tokens_est": 31, "n_tiles": 16, "output_text": "The image shows a sunny day with clear skies, casting shadows on the ground. The men are seated on green metal benches.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20964.6, "ram_available_mb": 41876.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20964.6, "ram_available_mb": 41876.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.788}, "power_stats": {"power_gpu_soc_mean_watts": 22.186, "power_cpu_cv_mean_watts": 1.409, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 75.788, "power_watts_avg": 22.186, "energy_joules_est": 134.71, "duration_seconds": 6.072, "sample_count": 52}, "timestamp": "2026-01-26T11:38:58.730395"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11591.767, "latencies_ms": [11591.767], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a desk with a laptop computer sitting on top of it. The laptop is open, and the screen is turned on, displaying a desktop wallpaper. Beside the laptop, there is a glass of orange juice placed on the desk. The desk is well-organized, with a lamp positioned to the right of the laptop, providing light for the workspace", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20964.6, "ram_available_mb": 41876.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20928.8, "ram_available_mb": 41912.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.131}, "power_stats": {"power_gpu_soc_mean_watts": 19.343, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 71.131, "power_watts_avg": 19.343, "energy_joules_est": 224.24, "duration_seconds": 11.593, "sample_count": 99}, "timestamp": "2026-01-26T11:39:12.354131"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9046.318, "latencies_ms": [9046.318], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Desk: 1\n- Laptop: 1\n- Phone: 1\n- Glass: 1\n- Lamp: 1\n- Candle holder: 1\n- Picture: 1\n- Drawer: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20928.8, "ram_available_mb": 41912.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20965.3, "ram_available_mb": 41875.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.26}, "power_stats": {"power_gpu_soc_mean_watts": 20.475, "power_cpu_cv_mean_watts": 1.695, "power_sys_5v0_mean_watts": 8.575, "gpu_utilization_percent_mean": 72.26, "power_watts_avg": 20.475, "energy_joules_est": 185.24, "duration_seconds": 9.047, "sample_count": 77}, "timestamp": "2026-01-26T11:39:23.421381"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10425.297, "latencies_ms": [10425.297], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The laptop is positioned on the left side of the desk, closer to the foreground, while the lamp is on the right side, further back. The phone is placed to the left of the laptop, and the glass of orange juice is on the right side of the laptop, closer to the edge of the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.3, "ram_available_mb": 41875.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20966.3, "ram_available_mb": 41874.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.898}, "power_stats": {"power_gpu_soc_mean_watts": 19.694, "power_cpu_cv_mean_watts": 1.815, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 70.898, "power_watts_avg": 19.694, "energy_joules_est": 205.33, "duration_seconds": 10.426, "sample_count": 88}, "timestamp": "2026-01-26T11:39:35.871485"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8571.899, "latencies_ms": [8571.899], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image depicts a well-lit workspace with a laptop open on a wooden desk, displaying a desktop screen. A lamp with a black shade is placed to the right of the laptop, providing additional lighting to the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20966.3, "ram_available_mb": 41874.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20967.3, "ram_available_mb": 41873.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.25}, "power_stats": {"power_gpu_soc_mean_watts": 20.828, "power_cpu_cv_mean_watts": 1.646, "power_sys_5v0_mean_watts": 8.572, "gpu_utilization_percent_mean": 73.25, "power_watts_avg": 20.828, "energy_joules_est": 178.55, "duration_seconds": 8.573, "sample_count": 72}, "timestamp": "2026-01-26T11:39:46.468975"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7339.709, "latencies_ms": [7339.709], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image features a desk with a laptop, a lamp, and a glass of orange juice. The lamp is turned on, casting a warm glow on the desk and the laptop screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20967.3, "ram_available_mb": 41873.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20997.7, "ram_available_mb": 41843.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.161}, "power_stats": {"power_gpu_soc_mean_watts": 21.093, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.589, "gpu_utilization_percent_mean": 73.161, "power_watts_avg": 21.093, "energy_joules_est": 154.83, "duration_seconds": 7.34, "sample_count": 62}, "timestamp": "2026-01-26T11:39:55.870129"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12428.263, "latencies_ms": [12428.263], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene beach scene. In the foreground, there's a blue and white striped towel neatly spread out on the sand. A blue surfboard with a white stripe is placed next to the towel, ready for a day of fun in the water. A red and white striped bag is also visible, perhaps containing personal belongings or beach", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20942.8, "ram_available_mb": 41898.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20964.4, "ram_available_mb": 41876.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.733}, "power_stats": {"power_gpu_soc_mean_watts": 21.502, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.883, "gpu_utilization_percent_mean": 71.733, "power_watts_avg": 21.502, "energy_joules_est": 267.25, "duration_seconds": 12.429, "sample_count": 105}, "timestamp": "2026-01-26T11:40:10.349629"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9837.811, "latencies_ms": [9837.811], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "beach chair: 2, umbrella: 1, surfboard: 2, towel: 1, cooler: 1, flip flops: 1, handbag: 1, backpack: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20964.4, "ram_available_mb": 41876.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20934.4, "ram_available_mb": 41906.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.145}, "power_stats": {"power_gpu_soc_mean_watts": 22.717, "power_cpu_cv_mean_watts": 1.543, "power_sys_5v0_mean_watts": 8.738, "gpu_utilization_percent_mean": 76.145, "power_watts_avg": 22.717, "energy_joules_est": 223.5, "duration_seconds": 9.838, "sample_count": 83}, "timestamp": "2026-01-26T11:40:22.208543"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12721.644, "latencies_ms": [12721.644], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a blue and white striped towel laid out on the sand, with a pink and white surfboard placed next to it. In the background, there is a beach chair with an umbrella, and a person is visible in the water further back. The surfboard and towel are near the edge of the frame, while the chair and", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20934.4, "ram_available_mb": 41906.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20926.1, "ram_available_mb": 41914.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.769}, "power_stats": {"power_gpu_soc_mean_watts": 21.588, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 8.857, "gpu_utilization_percent_mean": 71.769, "power_watts_avg": 21.588, "energy_joules_est": 274.65, "duration_seconds": 12.722, "sample_count": 108}, "timestamp": "2026-01-26T11:40:36.967749"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11818.126, "latencies_ms": [11818.126], "images_per_second": 0.085, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image depicts a beach scene with a blue and white striped towel laid out on the sand, a pink and white surfboard, and a blue surfboard. There is a green umbrella and a chair set up for relaxation, and a person can be seen in the water in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20926.1, "ram_available_mb": 41914.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 21017.4, "ram_available_mb": 41823.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.569}, "power_stats": {"power_gpu_soc_mean_watts": 21.827, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.787, "gpu_utilization_percent_mean": 74.569, "power_watts_avg": 21.827, "energy_joules_est": 257.97, "duration_seconds": 11.819, "sample_count": 102}, "timestamp": "2026-01-26T11:40:50.830388"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11744.6, "latencies_ms": [11744.6], "images_per_second": 0.085, "prompt_tokens": 36, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The beach scene features a bright blue sky and calm ocean waves, with a clear day that suggests good weather for outdoor activities. The sand is a light beige color, and there are various items scattered on it, including a blue and white striped towel, a pink and white surfboard, and a brown leather bag.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20930.3, "ram_available_mb": 41910.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.1, "ram_used_mb": 21028.8, "ram_available_mb": 41812.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.48}, "power_stats": {"power_gpu_soc_mean_watts": 21.867, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.876, "gpu_utilization_percent_mean": 73.48, "power_watts_avg": 21.867, "energy_joules_est": 256.83, "duration_seconds": 11.745, "sample_count": 100}, "timestamp": "2026-01-26T11:41:04.611977"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11600.868, "latencies_ms": [11600.868], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a solitary sheep stands majestically on a rocky outcrop. The sheep, with its white wool, is facing towards the left side of the image, as if gazing into the distance. The rocky outcrop on which the sheep stands is gray and rugged, with patches of green moss adding a touch of color to the otherwise mon", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20946.8, "ram_available_mb": 41894.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 20985.4, "ram_available_mb": 41855.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.224}, "power_stats": {"power_gpu_soc_mean_watts": 19.101, "power_cpu_cv_mean_watts": 1.993, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 70.224, "power_watts_avg": 19.101, "energy_joules_est": 221.6, "duration_seconds": 11.602, "sample_count": 98}, "timestamp": "2026-01-26T11:41:18.238773"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7911.835, "latencies_ms": [7911.835], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "rock: 10\nsheep: 1\ncloud: 10\nblue: 1\nwhite: 1\ngrass: 1\nmountain: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20985.4, "ram_available_mb": 41855.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.0, "ram_used_mb": 20959.0, "ram_available_mb": 41881.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.433}, "power_stats": {"power_gpu_soc_mean_watts": 21.233, "power_cpu_cv_mean_watts": 1.745, "power_sys_5v0_mean_watts": 8.563, "gpu_utilization_percent_mean": 73.433, "power_watts_avg": 21.233, "energy_joules_est": 168.01, "duration_seconds": 7.912, "sample_count": 67}, "timestamp": "2026-01-26T11:41:28.195704"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8805.735, "latencies_ms": [8805.735], "images_per_second": 0.114, "prompt_tokens": 44, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The sheep is positioned in the foreground on the left side of the image, standing on a rocky outcrop. The sky occupies the background, with clouds scattered across it, suggesting the sheep is on a higher elevation compared to the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20959.0, "ram_available_mb": 41881.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.1, "ram_used_mb": 21007.9, "ram_available_mb": 41833.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.122}, "power_stats": {"power_gpu_soc_mean_watts": 20.465, "power_cpu_cv_mean_watts": 1.975, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 72.122, "power_watts_avg": 20.465, "energy_joules_est": 180.22, "duration_seconds": 8.806, "sample_count": 74}, "timestamp": "2026-01-26T11:41:39.030254"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7092.654, "latencies_ms": [7092.654], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A sheep stands alone on a rocky outcrop under a clear blue sky with scattered clouds. The terrain is rugged and the sheep appears to be gazing into the distance.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20954.6, "ram_available_mb": 41886.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 21004.9, "ram_available_mb": 41836.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.283}, "power_stats": {"power_gpu_soc_mean_watts": 21.768, "power_cpu_cv_mean_watts": 1.594, "power_sys_5v0_mean_watts": 8.591, "gpu_utilization_percent_mean": 75.283, "power_watts_avg": 21.768, "energy_joules_est": 154.41, "duration_seconds": 7.093, "sample_count": 60}, "timestamp": "2026-01-26T11:41:48.164709"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7742.312, "latencies_ms": [7742.312], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The sheep is standing on a rocky outcrop with a clear blue sky and fluffy white clouds in the background. The rocks are dark and jagged, and the sheep is a light tan color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21004.9, "ram_available_mb": 41836.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21030.7, "ram_available_mb": 41810.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.409}, "power_stats": {"power_gpu_soc_mean_watts": 20.978, "power_cpu_cv_mean_watts": 1.632, "power_sys_5v0_mean_watts": 8.617, "gpu_utilization_percent_mean": 73.409, "power_watts_avg": 20.978, "energy_joules_est": 162.43, "duration_seconds": 7.743, "sample_count": 66}, "timestamp": "2026-01-26T11:41:57.931632"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11535.842, "latencies_ms": [11535.842], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person with striking blue hair is captured in a moment of self-reflection. They are wearing a blue shirt and a black tie, adding a touch of professionalism to their appearance. The person is holding a phone in their right hand, which is displaying a vibrant image of a red flower. The background is a simple beige wall, providing a neutral", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21030.7, "ram_available_mb": 41810.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21036.3, "ram_available_mb": 41804.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.7}, "power_stats": {"power_gpu_soc_mean_watts": 19.241, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.673, "gpu_utilization_percent_mean": 70.7, "power_watts_avg": 19.241, "energy_joules_est": 221.97, "duration_seconds": 11.536, "sample_count": 100}, "timestamp": "2026-01-26T11:42:11.539066"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7572.082, "latencies_ms": [7572.082], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "person: 1, hair: 1, phone: 1, tie: 1, shirt: 1, button: 1, pocket: 1, earring: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.0, "ram_available_mb": 41875.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21018.7, "ram_available_mb": 41822.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.154}, "power_stats": {"power_gpu_soc_mean_watts": 21.418, "power_cpu_cv_mean_watts": 1.546, "power_sys_5v0_mean_watts": 8.564, "gpu_utilization_percent_mean": 74.154, "power_watts_avg": 21.418, "energy_joules_est": 162.19, "duration_seconds": 7.573, "sample_count": 65}, "timestamp": "2026-01-26T11:42:21.149537"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11297.545, "latencies_ms": [11297.545], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The person with blue hair is in the foreground, taking a selfie with a smartphone held in their right hand. The phone is positioned in the right hand, slightly in front of the person's face, capturing the image. The background is a plain wall with a door handle visible on the left side, indicating the photo was taken indoors.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21018.7, "ram_available_mb": 41822.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20958.8, "ram_available_mb": 41882.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.802}, "power_stats": {"power_gpu_soc_mean_watts": 19.473, "power_cpu_cv_mean_watts": 1.868, "power_sys_5v0_mean_watts": 8.63, "gpu_utilization_percent_mean": 70.802, "power_watts_avg": 19.473, "energy_joules_est": 220.01, "duration_seconds": 11.298, "sample_count": 96}, "timestamp": "2026-01-26T11:42:34.502418"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4730.844, "latencies_ms": [4730.844], "images_per_second": 0.211, "prompt_tokens": 37, "response_tokens_est": 17, "n_tiles": 16, "output_text": "A person with blue hair is taking a selfie in a bathroom mirror.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20958.8, "ram_available_mb": 41882.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 21023.7, "ram_available_mb": 41817.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 80.325}, "power_stats": {"power_gpu_soc_mean_watts": 24.37, "power_cpu_cv_mean_watts": 1.041, "power_sys_5v0_mean_watts": 8.472, "gpu_utilization_percent_mean": 80.325, "power_watts_avg": 24.37, "energy_joules_est": 115.31, "duration_seconds": 4.732, "sample_count": 40}, "timestamp": "2026-01-26T11:42:41.269257"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9137.004, "latencies_ms": [9137.004], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The person in the image has vibrant blue hair and is wearing a dark blue shirt with a black tie. They are holding a smartphone in their right hand, taking a selfie in an indoor setting with a neutral-colored wall in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21023.7, "ram_available_mb": 41817.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21034.9, "ram_available_mb": 41806.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.734}, "power_stats": {"power_gpu_soc_mean_watts": 20.112, "power_cpu_cv_mean_watts": 1.743, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 71.734, "power_watts_avg": 20.112, "energy_joules_est": 183.78, "duration_seconds": 9.138, "sample_count": 79}, "timestamp": "2026-01-26T11:42:52.464793"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11601.636, "latencies_ms": [11601.636], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a room that exudes a sense of history and elegance. Dominating the scene is a fireplace, its mantel adorned with a black and gold frame, housing a painting of a woman. The fireplace, a symbol of warmth and comfort, is flanked by two lamps, their presence adding a touch of light to the room.\n\n", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 21034.9, "ram_available_mb": 41806.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20968.6, "ram_available_mb": 41872.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.24}, "power_stats": {"power_gpu_soc_mean_watts": 19.233, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 70.24, "power_watts_avg": 19.233, "energy_joules_est": 223.15, "duration_seconds": 11.602, "sample_count": 100}, "timestamp": "2026-01-26T11:43:06.096527"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7345.94, "latencies_ms": [7345.94], "images_per_second": 0.136, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20968.6, "ram_available_mb": 41872.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20944.0, "ram_available_mb": 41896.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.937}, "power_stats": {"power_gpu_soc_mean_watts": 21.273, "power_cpu_cv_mean_watts": 1.544, "power_sys_5v0_mean_watts": 8.555, "gpu_utilization_percent_mean": 74.937, "power_watts_avg": 21.273, "energy_joules_est": 156.28, "duration_seconds": 7.347, "sample_count": 63}, "timestamp": "2026-01-26T11:43:15.458695"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10552.155, "latencies_ms": [10552.155], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground of the image, there is a fireplace with a white mantel and a fire burning inside. To the right of the fireplace, there is a wooden cabinet with a red curtain hanging in front of it. In the background, there are two paintings hanging on the wall, one above the other.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20944.0, "ram_available_mb": 41896.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20943.3, "ram_available_mb": 41897.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.407}, "power_stats": {"power_gpu_soc_mean_watts": 19.621, "power_cpu_cv_mean_watts": 1.839, "power_sys_5v0_mean_watts": 8.616, "gpu_utilization_percent_mean": 70.407, "power_watts_avg": 19.621, "energy_joules_est": 207.06, "duration_seconds": 10.553, "sample_count": 91}, "timestamp": "2026-01-26T11:43:28.029785"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9103.088, "latencies_ms": [9103.088], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image depicts a cozy, vintage-style room with a fireplace, a wooden cabinet, and two chairs. The room has a classic and elegant atmosphere, with a fire burning in the fireplace and a book resting on the cabinet.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20943.3, "ram_available_mb": 41897.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20986.0, "ram_available_mb": 41854.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.753}, "power_stats": {"power_gpu_soc_mean_watts": 20.562, "power_cpu_cv_mean_watts": 1.721, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 72.753, "power_watts_avg": 20.562, "energy_joules_est": 187.19, "duration_seconds": 9.104, "sample_count": 77}, "timestamp": "2026-01-26T11:43:39.174646"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8348.082, "latencies_ms": [8348.082], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The room is dimly lit with natural light coming from the window, which has red curtains. The furniture, including a wooden cabinet and a chair with a black leather seat, adds a classic and antique feel to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20986.0, "ram_available_mb": 41854.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20986.7, "ram_available_mb": 41854.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.141}, "power_stats": {"power_gpu_soc_mean_watts": 20.562, "power_cpu_cv_mean_watts": 1.697, "power_sys_5v0_mean_watts": 8.609, "gpu_utilization_percent_mean": 72.141, "power_watts_avg": 20.562, "energy_joules_est": 171.67, "duration_seconds": 8.349, "sample_count": 71}, "timestamp": "2026-01-26T11:43:49.539409"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12449.676, "latencies_ms": [12449.676], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a brown dog is captured in mid-air, leaping towards a red frisbee. The dog's body is stretched out, and its front paws are extended forward, ready to catch the frisbee. The frisbee is positioned above the dog's head, slightly to the right. The dog is surrounded by a lush", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 20986.9, "ram_available_mb": 41854.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20966.1, "ram_available_mb": 41874.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.99}, "power_stats": {"power_gpu_soc_mean_watts": 21.516, "power_cpu_cv_mean_watts": 1.815, "power_sys_5v0_mean_watts": 8.9, "gpu_utilization_percent_mean": 70.99, "power_watts_avg": 21.516, "energy_joules_est": 267.88, "duration_seconds": 12.45, "sample_count": 105}, "timestamp": "2026-01-26T11:44:04.062436"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8774.425, "latencies_ms": [8774.425], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "tree: 1, frisbee: 1, dog: 1, car: 1, grass: 1, bush: 1, path: 1, leaf: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20966.1, "ram_available_mb": 41874.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21001.5, "ram_available_mb": 41839.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_gpu_soc_mean_watts": 23.206, "power_cpu_cv_mean_watts": 1.455, "power_sys_5v0_mean_watts": 8.738, "gpu_utilization_percent_mean": 78.0, "power_watts_avg": 23.206, "energy_joules_est": 203.64, "duration_seconds": 8.775, "sample_count": 74}, "timestamp": "2026-01-26T11:44:14.898666"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12729.138, "latencies_ms": [12729.138], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a brown dog is captured in mid-air, leaping towards a red frisbee that is positioned near the top right corner of the image, suggesting the dog is attempting to catch it. The frisbee is closer to the camera than the dog, creating a sense of depth. In the background, there is a black car parked on the left", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20939.6, "ram_available_mb": 41901.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20963.4, "ram_available_mb": 41877.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.064}, "power_stats": {"power_gpu_soc_mean_watts": 21.591, "power_cpu_cv_mean_watts": 1.798, "power_sys_5v0_mean_watts": 8.865, "gpu_utilization_percent_mean": 73.064, "power_watts_avg": 21.591, "energy_joules_est": 274.85, "duration_seconds": 12.73, "sample_count": 110}, "timestamp": "2026-01-26T11:44:29.664549"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9777.914, "latencies_ms": [9777.914], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A brown dog is in mid-air, leaping towards a red frisbee, with a black car parked in the background. The setting appears to be a residential area with a well-maintained lawn and a tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.4, "ram_available_mb": 41877.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20940.7, "ram_available_mb": 41900.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.373}, "power_stats": {"power_gpu_soc_mean_watts": 22.79, "power_cpu_cv_mean_watts": 1.538, "power_sys_5v0_mean_watts": 8.772, "gpu_utilization_percent_mean": 76.373, "power_watts_avg": 22.79, "energy_joules_est": 222.85, "duration_seconds": 9.779, "sample_count": 83}, "timestamp": "2026-01-26T11:44:41.486673"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12680.213, "latencies_ms": [12680.213], "images_per_second": 0.079, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a brown dog in mid-air, reaching for a red frisbee. The dog is standing on its hind legs, with its front paws extended forward. The background features a black car parked on a driveway, with a tree and some bushes in the foreground. The grass is green and well-maintained, and the weather appears to be", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.7, "ram_available_mb": 41900.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20932.4, "ram_available_mb": 41908.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.899}, "power_stats": {"power_gpu_soc_mean_watts": 21.555, "power_cpu_cv_mean_watts": 1.789, "power_sys_5v0_mean_watts": 8.859, "gpu_utilization_percent_mean": 72.899, "power_watts_avg": 21.555, "energy_joules_est": 273.34, "duration_seconds": 12.681, "sample_count": 109}, "timestamp": "2026-01-26T11:44:56.211876"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11599.685, "latencies_ms": [11599.685], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a verdant landscape, a giraffe stands tall and majestic. Its long neck, a marvel of nature's design, stretches upwards towards the sky, while its large ears, adorned with black tips, are perked up in alertness. The giraffe's coat, a beautiful mosaic of brown and white spots", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.4, "ram_available_mb": 41908.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20979.1, "ram_available_mb": 41861.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.939}, "power_stats": {"power_gpu_soc_mean_watts": 19.294, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 69.939, "power_watts_avg": 19.294, "energy_joules_est": 223.82, "duration_seconds": 11.6, "sample_count": 99}, "timestamp": "2026-01-26T11:45:09.850905"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7549.901, "latencies_ms": [7549.901], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "ear: 2\neye: 2\nnose: 1\nmouth: 1\nfur: 1\near: 1\near: 1\near: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20979.1, "ram_available_mb": 41861.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20947.2, "ram_available_mb": 41893.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.844}, "power_stats": {"power_gpu_soc_mean_watts": 21.362, "power_cpu_cv_mean_watts": 1.545, "power_sys_5v0_mean_watts": 8.581, "gpu_utilization_percent_mean": 73.844, "power_watts_avg": 21.362, "energy_joules_est": 161.29, "duration_seconds": 7.551, "sample_count": 64}, "timestamp": "2026-01-26T11:45:19.454277"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10008.78, "latencies_ms": [10008.78], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "In the foreground, there is a giraffe with its head turned to the left, facing the camera. The giraffe's ears are large and pointed, and it has a long neck. In the background, there are trees and foliage, providing a natural habitat for the giraffe.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20947.2, "ram_available_mb": 41893.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20989.6, "ram_available_mb": 41851.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.337}, "power_stats": {"power_gpu_soc_mean_watts": 19.686, "power_cpu_cv_mean_watts": 1.806, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 71.337, "power_watts_avg": 19.686, "energy_joules_est": 197.05, "duration_seconds": 10.009, "sample_count": 86}, "timestamp": "2026-01-26T11:45:31.499447"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9654.489, "latencies_ms": [9654.489], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "In the image, a giraffe is standing in a lush green forest, surrounded by trees and foliage. The giraffe is looking directly at the camera, its head slightly tilted to the left, showcasing its long neck and distinctive spotted pattern.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20936.3, "ram_available_mb": 41904.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 21033.5, "ram_available_mb": 41807.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.537}, "power_stats": {"power_gpu_soc_mean_watts": 20.011, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 72.537, "power_watts_avg": 20.011, "energy_joules_est": 193.21, "duration_seconds": 9.655, "sample_count": 82}, "timestamp": "2026-01-26T11:45:43.169646"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6760.494, "latencies_ms": [6760.494], "images_per_second": 0.148, "prompt_tokens": 36, "response_tokens_est": 37, "n_tiles": 16, "output_text": "The giraffe has a light brown coat with darker brown spots. The background is a mix of green and yellow hues, indicating a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20944.0, "ram_available_mb": 41896.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 20984.5, "ram_available_mb": 41856.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.86}, "power_stats": {"power_gpu_soc_mean_watts": 21.635, "power_cpu_cv_mean_watts": 1.721, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 74.86, "power_watts_avg": 21.635, "energy_joules_est": 146.28, "duration_seconds": 6.761, "sample_count": 57}, "timestamp": "2026-01-26T11:45:51.991290"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11558.679, "latencies_ms": [11558.679], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two zebras standing side by side in a grassy area. They are facing away from the camera, and their backs are visible. The zebras have black and white stripes, and their tails are also black and white. The background features a chain-link fence, and there are some rocks scattered around the area. The zebras", "error": null, "sys_before": {"cpu_percent": 9.7, "ram_used_mb": 20984.5, "ram_available_mb": 41856.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.4, "ram_used_mb": 20958.2, "ram_available_mb": 41882.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.052}, "power_stats": {"power_gpu_soc_mean_watts": 19.283, "power_cpu_cv_mean_watts": 2.134, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 70.052, "power_watts_avg": 19.283, "energy_joules_est": 222.9, "duration_seconds": 11.559, "sample_count": 97}, "timestamp": "2026-01-26T11:46:05.585183"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7466.794, "latencies_ms": [7466.794], "images_per_second": 0.134, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "zebra: 2, fence: 1, leaves: many, ground: dry, rocks: 1, sunlight: bright, trees: 1, sky: not visible", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20958.2, "ram_available_mb": 41882.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.2, "ram_used_mb": 20948.9, "ram_available_mb": 41892.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.857}, "power_stats": {"power_gpu_soc_mean_watts": 21.585, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 8.581, "gpu_utilization_percent_mean": 73.857, "power_watts_avg": 21.585, "energy_joules_est": 161.18, "duration_seconds": 7.467, "sample_count": 63}, "timestamp": "2026-01-26T11:46:15.068912"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11678.343, "latencies_ms": [11678.343], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The two zebras are standing close to each other, with one slightly in front of the other, creating a sense of depth in the image. They are positioned in the foreground, while the chain-link fence and the background of trees and foliage provide a sense of the environment they are in. The zebras are near the camera, making them the main focus of", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20948.9, "ram_available_mb": 41892.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 9.6, "ram_used_mb": 20948.6, "ram_available_mb": 41892.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.222}, "power_stats": {"power_gpu_soc_mean_watts": 19.371, "power_cpu_cv_mean_watts": 2.091, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 70.222, "power_watts_avg": 19.371, "energy_joules_est": 226.23, "duration_seconds": 11.679, "sample_count": 99}, "timestamp": "2026-01-26T11:46:28.768215"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8115.196, "latencies_ms": [8115.196], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "Two zebras are standing in a grassy area with a chain-link fence in the background. The zebras are facing away from the camera, and their distinctive black and white stripes are clearly visible.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20948.6, "ram_available_mb": 41892.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 21010.3, "ram_available_mb": 41830.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.265}, "power_stats": {"power_gpu_soc_mean_watts": 21.14, "power_cpu_cv_mean_watts": 1.737, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 73.265, "power_watts_avg": 21.14, "energy_joules_est": 171.57, "duration_seconds": 8.116, "sample_count": 68}, "timestamp": "2026-01-26T11:46:38.920106"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8659.275, "latencies_ms": [8659.275], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The zebras have distinct black and white stripes, and the image is taken in natural daylight with shadows cast on the ground. The background shows a wire fence, indicating the zebras are likely in a zoo or wildlife park.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21010.3, "ram_available_mb": 41830.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21017.3, "ram_available_mb": 41823.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.973}, "power_stats": {"power_gpu_soc_mean_watts": 20.475, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 8.616, "gpu_utilization_percent_mean": 72.973, "power_watts_avg": 20.475, "energy_joules_est": 177.31, "duration_seconds": 8.66, "sample_count": 74}, "timestamp": "2026-01-26T11:46:49.637141"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11570.549, "latencies_ms": [11570.549], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a group of horses is walking down a road, with some of them standing near the side of the road. There are at least four horses visible, with one horse standing closer to the left side of the road and the other three horses positioned more towards the center. \n\nA car is parked on the right side of the road, occupying a significant portion of", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 20939.4, "ram_available_mb": 41901.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21011.4, "ram_available_mb": 41829.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.505}, "power_stats": {"power_gpu_soc_mean_watts": 19.35, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 69.505, "power_watts_avg": 19.35, "energy_joules_est": 223.9, "duration_seconds": 11.571, "sample_count": 99}, "timestamp": "2026-01-26T11:47:03.258369"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9949.89, "latencies_ms": [9949.89], "images_per_second": 0.101, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Car: 1\n2. Horse: 3\n3. Fence: 1\n4. Trees: 1\n5. Road: 1\n6. Bushes: 1\n7. Dirt: 1\n8. Grass: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20947.2, "ram_available_mb": 41893.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20994.1, "ram_available_mb": 41846.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.826}, "power_stats": {"power_gpu_soc_mean_watts": 19.906, "power_cpu_cv_mean_watts": 1.773, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 71.826, "power_watts_avg": 19.906, "energy_joules_est": 198.08, "duration_seconds": 9.951, "sample_count": 86}, "timestamp": "2026-01-26T11:47:15.247896"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10474.147, "latencies_ms": [10474.147], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a silver car parked on the right side of the road. In the background, there are three horses walking on the road, with one horse closer to the camera and two others further away. The car is positioned near the middle of the road, while the horses are walking parallel to the car.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.7, "ram_available_mb": 41900.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20996.8, "ram_available_mb": 41844.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.789}, "power_stats": {"power_gpu_soc_mean_watts": 19.426, "power_cpu_cv_mean_watts": 1.846, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 70.789, "power_watts_avg": 19.426, "energy_joules_est": 203.48, "duration_seconds": 10.475, "sample_count": 90}, "timestamp": "2026-01-26T11:47:27.739944"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7850.006, "latencies_ms": [7850.006], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A group of horses are standing on the side of a road, with a silver car parked on the right side of the road. The road is lined with trees and a wooden fence on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.0, "ram_available_mb": 41908.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20983.1, "ram_available_mb": 41857.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.5}, "power_stats": {"power_gpu_soc_mean_watts": 21.209, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 74.5, "power_watts_avg": 21.209, "energy_joules_est": 166.51, "duration_seconds": 7.851, "sample_count": 66}, "timestamp": "2026-01-26T11:47:37.611598"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8915.27, "latencies_ms": [8915.27], "images_per_second": 0.112, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a sunny day with clear skies, casting natural light on the scene. A silver car is parked on the side of a road, and there are three horses standing on the road, with one brown horse leading the way and two others following behind.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20983.1, "ram_available_mb": 41857.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20983.6, "ram_available_mb": 41857.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.208}, "power_stats": {"power_gpu_soc_mean_watts": 20.198, "power_cpu_cv_mean_watts": 1.747, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 72.208, "power_watts_avg": 20.198, "energy_joules_est": 180.08, "duration_seconds": 8.916, "sample_count": 77}, "timestamp": "2026-01-26T11:47:48.568679"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11621.747, "latencies_ms": [11621.747], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The image features a wooden desk with a chair positioned in front of it. On the desk, there is a stack of books, including a blue book and a red apple. The chair is placed in front of a blackboard, which is located behind the desk. The blackboard is empty, and the scene appears to be set up for a classroom or study environment.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20983.6, "ram_available_mb": 41857.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20943.4, "ram_available_mb": 41897.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.119}, "power_stats": {"power_gpu_soc_mean_watts": 19.266, "power_cpu_cv_mean_watts": 1.898, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 70.119, "power_watts_avg": 19.266, "energy_joules_est": 223.92, "duration_seconds": 11.622, "sample_count": 101}, "timestamp": "2026-01-26T11:48:02.218076"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8573.945, "latencies_ms": [8573.945], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "- Chair: 1\n- Desk: 1\n- Book: 3\n- Apple: 1\n- Desk lamp: 1\n- Chair: 1\n- Chair: 1\n- Chair: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20943.4, "ram_available_mb": 41897.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20970.4, "ram_available_mb": 41870.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.324}, "power_stats": {"power_gpu_soc_mean_watts": 20.6, "power_cpu_cv_mean_watts": 1.677, "power_sys_5v0_mean_watts": 8.592, "gpu_utilization_percent_mean": 73.324, "power_watts_avg": 20.6, "energy_joules_est": 176.64, "duration_seconds": 8.575, "sample_count": 74}, "timestamp": "2026-01-26T11:48:12.834572"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11574.685, "latencies_ms": [11574.685], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden desk with a stack of books leaning against it, and a red apple resting on the desk. To the left of the desk, there is a wooden chair. In the background, there is a large blackboard mounted on the wall. The chair is positioned in front of the desk, and the blackboard is behind the", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 20970.4, "ram_available_mb": 41870.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20955.5, "ram_available_mb": 41885.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.74}, "power_stats": {"power_gpu_soc_mean_watts": 19.124, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.639, "gpu_utilization_percent_mean": 69.74, "power_watts_avg": 19.124, "energy_joules_est": 221.37, "duration_seconds": 11.575, "sample_count": 100}, "timestamp": "2026-01-26T11:48:26.432725"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8954.294, "latencies_ms": [8954.294], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a classroom setting with a wooden desk in the foreground. On the desk, there is a stack of books, an apple, and a quill pen. The background features a chalkboard with some writing on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20955.5, "ram_available_mb": 41885.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21014.1, "ram_available_mb": 41826.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.75}, "power_stats": {"power_gpu_soc_mean_watts": 20.385, "power_cpu_cv_mean_watts": 1.691, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 72.75, "power_watts_avg": 20.385, "energy_joules_est": 182.54, "duration_seconds": 8.955, "sample_count": 76}, "timestamp": "2026-01-26T11:48:37.400887"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9379.293, "latencies_ms": [9379.293], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image features a blackboard with a chalkboard texture, and the lighting appears to be coming from the left side, casting shadows to the right. The desk is made of wood with a dark finish, and there is a blue book with gold trim resting on it.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 20954.3, "ram_available_mb": 41886.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20959.4, "ram_available_mb": 41881.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.704}, "power_stats": {"power_gpu_soc_mean_watts": 20.031, "power_cpu_cv_mean_watts": 1.769, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 71.704, "power_watts_avg": 20.031, "energy_joules_est": 187.89, "duration_seconds": 9.38, "sample_count": 81}, "timestamp": "2026-01-26T11:48:48.822950"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11612.563, "latencies_ms": [11612.563], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment on a bustling street in India. Dominating the scene is a yellow and white bus, adorned with a red and white stripe running along its side. The bus is in motion, driving on the right side of the road, as is customary in India. The number 475 is prominently displayed on the back of the bus", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20959.4, "ram_available_mb": 41881.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20944.9, "ram_available_mb": 41896.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.89}, "power_stats": {"power_gpu_soc_mean_watts": 19.256, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 69.89, "power_watts_avg": 19.256, "energy_joules_est": 223.63, "duration_seconds": 11.613, "sample_count": 100}, "timestamp": "2026-01-26T11:49:02.489245"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7770.98, "latencies_ms": [7770.98], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "bus: 1, van: 1, motorcycle: 1, car: 1, license plate: 1, rear light: 2, side mirror: 1, window: 2", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20944.9, "ram_available_mb": 41896.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20995.8, "ram_available_mb": 41845.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.606}, "power_stats": {"power_gpu_soc_mean_watts": 21.294, "power_cpu_cv_mean_watts": 1.577, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 73.606, "power_watts_avg": 21.294, "energy_joules_est": 165.49, "duration_seconds": 7.772, "sample_count": 66}, "timestamp": "2026-01-26T11:49:12.286657"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9277.229, "latencies_ms": [9277.229], "images_per_second": 0.108, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The bus is in the foreground on the left side of the image, moving towards the right. There is a white van on the right side of the image, further back in the scene. The background is less distinct but appears to be an urban street with buildings and possibly other vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20995.8, "ram_available_mb": 41845.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20992.8, "ram_available_mb": 41848.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.747}, "power_stats": {"power_gpu_soc_mean_watts": 20.173, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 71.747, "power_watts_avg": 20.173, "energy_joules_est": 187.16, "duration_seconds": 9.278, "sample_count": 79}, "timestamp": "2026-01-26T11:49:23.597580"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7226.54, "latencies_ms": [7226.54], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A yellow and white bus with the number 475 and some text in Hindi is driving on a busy street. There are other vehicles and a motorcycle visible in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20992.8, "ram_available_mb": 41848.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20941.5, "ram_available_mb": 41899.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.133}, "power_stats": {"power_gpu_soc_mean_watts": 21.689, "power_cpu_cv_mean_watts": 1.528, "power_sys_5v0_mean_watts": 8.552, "gpu_utilization_percent_mean": 76.133, "power_watts_avg": 21.689, "energy_joules_est": 156.75, "duration_seconds": 7.227, "sample_count": 60}, "timestamp": "2026-01-26T11:49:32.861210"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7232.03, "latencies_ms": [7232.03], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The bus in the image has a yellow and white color scheme with red and black text. It is a cloudy day, and the lighting is overcast, with no direct sunlight visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20941.5, "ram_available_mb": 41899.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21002.4, "ram_available_mb": 41838.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.098}, "power_stats": {"power_gpu_soc_mean_watts": 21.254, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 8.606, "gpu_utilization_percent_mean": 73.098, "power_watts_avg": 21.254, "energy_joules_est": 153.72, "duration_seconds": 7.233, "sample_count": 61}, "timestamp": "2026-01-26T11:49:42.148669"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11624.25, "latencies_ms": [11624.25], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene from a bathroom, where a television is mounted on the wall above a sink. The television is turned on, displaying a sports game, possibly a football match, with players in action. The bathroom features a sink with a mirror above it, reflecting the television screen. The walls are adorned with tiles, and there is a toilet visible", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 20939.8, "ram_available_mb": 41901.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20932.2, "ram_available_mb": 41908.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.194}, "power_stats": {"power_gpu_soc_mean_watts": 19.101, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 70.194, "power_watts_avg": 19.101, "energy_joules_est": 222.05, "duration_seconds": 11.625, "sample_count": 98}, "timestamp": "2026-01-26T11:49:55.799637"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11140.036, "latencies_ms": [11140.036], "images_per_second": 0.09, "prompt_tokens": 39, "response_tokens_est": 73, "n_tiles": 16, "output_text": "- Television: 1\n\n- Mirror: 1\n\n- Sink: 1\n\n- Faucet: 1\n\n- Tiles: Multiple (exact count not determinable)\n\n- Towel dispenser: 1\n\n- Trash can: 1\n\n- Door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.2, "ram_available_mb": 41908.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20926.9, "ram_available_mb": 41914.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.432}, "power_stats": {"power_gpu_soc_mean_watts": 19.714, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 71.432, "power_watts_avg": 19.714, "energy_joules_est": 219.63, "duration_seconds": 11.141, "sample_count": 95}, "timestamp": "2026-01-26T11:50:08.975153"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10971.249, "latencies_ms": [10971.249], "images_per_second": 0.091, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground of the image, there is a bathroom sink with a white basin and a silver faucet. The television is mounted on the wall in the background, above the sink. The toilet paper holder is located to the right of the television, and there is a trash can in the bottom left corner of the image.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20926.9, "ram_available_mb": 41914.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20991.9, "ram_available_mb": 41849.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.234}, "power_stats": {"power_gpu_soc_mean_watts": 19.663, "power_cpu_cv_mean_watts": 1.844, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 70.234, "power_watts_avg": 19.663, "energy_joules_est": 215.74, "duration_seconds": 10.972, "sample_count": 94}, "timestamp": "2026-01-26T11:50:22.000467"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7240.82, "latencies_ms": [7240.82], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image shows a bathroom with a large mirror reflecting a television screen displaying a football game. The bathroom has beige tiles on the walls and a granite countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20991.9, "ram_available_mb": 41849.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20948.5, "ram_available_mb": 41892.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.554, "power_cpu_cv_mean_watts": 1.504, "power_sys_5v0_mean_watts": 8.558, "gpu_utilization_percent_mean": 75.0, "power_watts_avg": 21.554, "energy_joules_est": 156.08, "duration_seconds": 7.241, "sample_count": 62}, "timestamp": "2026-01-26T11:50:31.277124"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7287.925, "latencies_ms": [7287.925], "images_per_second": 0.137, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image shows a bathroom with beige tiled walls and a dark brown door. A television mounted on the wall displays a blue-toned image of what appears to be a football game.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20948.5, "ram_available_mb": 41892.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.8, "ram_used_mb": 20989.4, "ram_available_mb": 41851.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.393}, "power_stats": {"power_gpu_soc_mean_watts": 21.383, "power_cpu_cv_mean_watts": 1.706, "power_sys_5v0_mean_watts": 8.627, "gpu_utilization_percent_mean": 74.393, "power_watts_avg": 21.383, "energy_joules_est": 155.85, "duration_seconds": 7.289, "sample_count": 61}, "timestamp": "2026-01-26T11:50:40.609360"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12394.121, "latencies_ms": [12394.121], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photograph, a solitary figure is captured in a moment of quiet contemplation. The person, dressed in a white shirt and dark pants, is seated on a wooden bench. Their gaze is directed downwards, suggesting deep thought or perhaps a moment of introspection.\n\nThe bench is situated in a park, surrounded by lush green", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20944.3, "ram_available_mb": 41896.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.6, "ram_used_mb": 20942.1, "ram_available_mb": 41898.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.887}, "power_stats": {"power_gpu_soc_mean_watts": 21.531, "power_cpu_cv_mean_watts": 1.956, "power_sys_5v0_mean_watts": 8.932, "gpu_utilization_percent_mean": 72.887, "power_watts_avg": 21.531, "energy_joules_est": 266.87, "duration_seconds": 12.395, "sample_count": 106}, "timestamp": "2026-01-26T11:50:55.058610"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10626.395, "latencies_ms": [10626.395], "images_per_second": 0.094, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "1. Bench: 1\n2. Man: 1\n3. Tree: 1\n4. Street lamp: 1\n5. Bush: 1\n6. Bush: 1\n7. Bush: 1\n8. Bush: 1", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 20942.1, "ram_available_mb": 41898.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.2, "ram_used_mb": 21019.0, "ram_available_mb": 41821.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.056}, "power_stats": {"power_gpu_soc_mean_watts": 22.449, "power_cpu_cv_mean_watts": 1.699, "power_sys_5v0_mean_watts": 8.794, "gpu_utilization_percent_mean": 76.056, "power_watts_avg": 22.449, "energy_joules_est": 238.57, "duration_seconds": 10.627, "sample_count": 90}, "timestamp": "2026-01-26T11:51:07.749878"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12578.549, "latencies_ms": [12578.549], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "In the foreground, there is a person sitting on a bench, positioned on the left side of the image. The bench is located in a park-like setting with trees and bushes around it. In the background, there is a tall clock tower with a steeple, situated behind the trees and bushes, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 20948.9, "ram_available_mb": 41892.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.4, "ram_used_mb": 20953.4, "ram_available_mb": 41887.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.757}, "power_stats": {"power_gpu_soc_mean_watts": 21.642, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.877, "gpu_utilization_percent_mean": 73.757, "power_watts_avg": 21.642, "energy_joules_est": 272.24, "duration_seconds": 12.579, "sample_count": 107}, "timestamp": "2026-01-26T11:51:22.381772"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8518.511, "latencies_ms": [8518.511], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A man is sitting on a bench in a park-like setting with a tall clock tower in the background. The scene is in black and white, giving it a timeless and classic feel.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20953.4, "ram_available_mb": 41887.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 20983.4, "ram_available_mb": 41857.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.458}, "power_stats": {"power_gpu_soc_mean_watts": 23.457, "power_cpu_cv_mean_watts": 1.418, "power_sys_5v0_mean_watts": 8.753, "gpu_utilization_percent_mean": 78.458, "power_watts_avg": 23.457, "energy_joules_est": 199.83, "duration_seconds": 8.519, "sample_count": 72}, "timestamp": "2026-01-26T11:51:32.948549"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12673.327, "latencies_ms": [12673.327], "images_per_second": 0.079, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a black and white photograph, giving it a timeless and classic feel. The lighting is soft and natural, with the sunlight filtering through the trees and casting shadows on the ground. The weather appears to be overcast, with a cloudy sky and no visible sun. The photograph is taken in a park-like setting, with a man sitting on a bench in", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 20983.4, "ram_available_mb": 41857.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21041.8, "ram_available_mb": 41799.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.269}, "power_stats": {"power_gpu_soc_mean_watts": 21.649, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 8.871, "gpu_utilization_percent_mean": 73.269, "power_watts_avg": 21.649, "energy_joules_est": 274.38, "duration_seconds": 12.674, "sample_count": 108}, "timestamp": "2026-01-26T11:51:47.642469"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11615.838, "latencies_ms": [11615.838], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a bustling street scene under a clear blue sky. A row of cars, each with its own unique color and model, are parked neatly along the side of the road. The cars are of various sizes and colors, including white, black, silver, and blue. \n\nA few people can be seen walking on the sidewalk, going about their day", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20953.0, "ram_available_mb": 41887.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21014.7, "ram_available_mb": 41826.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.091}, "power_stats": {"power_gpu_soc_mean_watts": 19.281, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 70.091, "power_watts_avg": 19.281, "energy_joules_est": 223.98, "duration_seconds": 11.617, "sample_count": 99}, "timestamp": "2026-01-26T11:52:01.285806"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7661.079, "latencies_ms": [7661.079], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "car: 8, truck: 2, person: 3, building: 1, sign: 2, tree: 1, street light: 1, bus stop: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20948.7, "ram_available_mb": 41892.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21009.7, "ram_available_mb": 41831.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.769}, "power_stats": {"power_gpu_soc_mean_watts": 21.315, "power_cpu_cv_mean_watts": 1.57, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 73.769, "power_watts_avg": 21.315, "energy_joules_est": 163.31, "duration_seconds": 7.662, "sample_count": 65}, "timestamp": "2026-01-26T11:52:10.972684"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11569.666, "latencies_ms": [11569.666], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a busy street scene with multiple cars and a bus, indicating a high-traffic area. The cars are parked and in motion, with some near the bus stop and others further down the road. In the background, there is a large stone wall, which appears to be part of a historical or significant building, adding to the urban environment. The sky is", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20955.5, "ram_available_mb": 41885.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20987.0, "ram_available_mb": 41853.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.102}, "power_stats": {"power_gpu_soc_mean_watts": 19.369, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.664, "gpu_utilization_percent_mean": 70.102, "power_watts_avg": 19.369, "energy_joules_est": 224.1, "duration_seconds": 11.57, "sample_count": 98}, "timestamp": "2026-01-26T11:52:24.586340"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7650.402, "latencies_ms": [7650.402], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image depicts a busy street scene with multiple cars and a bus stopped at a bus stop. There are several people walking on the sidewalk, and a stone wall can be seen in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20987.0, "ram_available_mb": 41853.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20987.0, "ram_available_mb": 41853.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.908}, "power_stats": {"power_gpu_soc_mean_watts": 21.401, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.579, "gpu_utilization_percent_mean": 73.908, "power_watts_avg": 21.401, "energy_joules_est": 163.74, "duration_seconds": 7.651, "sample_count": 65}, "timestamp": "2026-01-26T11:52:34.252005"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8695.542, "latencies_ms": [8695.542], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a clear blue sky with sunlight casting shadows on the road, indicating it is a sunny day. The road is paved with asphalt, and there are various vehicles, including cars and a bus, indicating a busy traffic scenario.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20987.0, "ram_available_mb": 41853.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20987.0, "ram_available_mb": 41853.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.187}, "power_stats": {"power_gpu_soc_mean_watts": 20.363, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 8.617, "gpu_utilization_percent_mean": 72.187, "power_watts_avg": 20.363, "energy_joules_est": 177.08, "duration_seconds": 8.696, "sample_count": 75}, "timestamp": "2026-01-26T11:52:44.977920"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11589.909, "latencies_ms": [11589.909], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a tranquil scene of a meal, possibly breakfast, set on a wooden table. The table, with its rich brown hue, contrasts beautifully with the white tiles of the floor. On the table, there's a white plate holding a slice of pancakes, topped with sliced bananas and strawberries. The p", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 20987.0, "ram_available_mb": 41853.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21011.9, "ram_available_mb": 41829.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.616}, "power_stats": {"power_gpu_soc_mean_watts": 19.316, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.653, "gpu_utilization_percent_mean": 70.616, "power_watts_avg": 19.316, "energy_joules_est": 223.88, "duration_seconds": 11.591, "sample_count": 99}, "timestamp": "2026-01-26T11:52:58.595457"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8139.901, "latencies_ms": [8139.901], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "plate: 2, cup: 2, spoon: 1, fork: 2, knife: 1, bowl: 1, watermelon: 1, banana: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20959.8, "ram_available_mb": 41881.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21015.3, "ram_available_mb": 41825.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.681}, "power_stats": {"power_gpu_soc_mean_watts": 21.013, "power_cpu_cv_mean_watts": 1.613, "power_sys_5v0_mean_watts": 8.553, "gpu_utilization_percent_mean": 72.681, "power_watts_avg": 21.013, "energy_joules_est": 171.06, "duration_seconds": 8.141, "sample_count": 69}, "timestamp": "2026-01-26T11:53:08.778338"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11614.769, "latencies_ms": [11614.769], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The main objects are arranged on a wooden table. In the foreground, there is a white plate with a partially eaten omelette and a bowl of fruit salad to the right. In the background, there is a white cup with a red liquid and a small bowl with a spoon. The table is positioned on a tiled floor, and the shadows of the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20952.7, "ram_available_mb": 41888.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20954.2, "ram_available_mb": 41886.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.71}, "power_stats": {"power_gpu_soc_mean_watts": 19.268, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 70.71, "power_watts_avg": 19.268, "energy_joules_est": 223.81, "duration_seconds": 11.615, "sample_count": 100}, "timestamp": "2026-01-26T11:53:22.434075"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7658.565, "latencies_ms": [7658.565], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image shows a wooden table with a plate of food and a cup of tea on it. The table is placed on a tiled floor, and there is a shadow of a person visible on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20954.2, "ram_available_mb": 41886.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21022.3, "ram_available_mb": 41818.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.106}, "power_stats": {"power_gpu_soc_mean_watts": 21.306, "power_cpu_cv_mean_watts": 1.577, "power_sys_5v0_mean_watts": 8.575, "gpu_utilization_percent_mean": 74.106, "power_watts_avg": 21.306, "energy_joules_est": 163.19, "duration_seconds": 7.659, "sample_count": 66}, "timestamp": "2026-01-26T11:53:32.151747"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7034.443, "latencies_ms": [7034.443], "images_per_second": 0.142, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image shows a wooden table with a plate of food and a cup of tea on it. The lighting is natural, casting a shadow of the table on the white tiled floor.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21022.3, "ram_available_mb": 41818.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21023.3, "ram_available_mb": 41817.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.05}, "power_stats": {"power_gpu_soc_mean_watts": 21.126, "power_cpu_cv_mean_watts": 1.541, "power_sys_5v0_mean_watts": 8.577, "gpu_utilization_percent_mean": 73.05, "power_watts_avg": 21.126, "energy_joules_est": 148.62, "duration_seconds": 7.035, "sample_count": 60}, "timestamp": "2026-01-26T11:53:41.214546"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11577.808, "latencies_ms": [11577.808], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the image, an elderly woman is standing at a dining table, cutting dough with a knife. The table is filled with various food items, including rolls, cookies, and a pizza. There are also cups and a bowl on the table. The woman is wearing an apron, and the scene appears to be a kitchen or dining area.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 20945.6, "ram_available_mb": 41895.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20998.0, "ram_available_mb": 41842.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.242}, "power_stats": {"power_gpu_soc_mean_watts": 19.244, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 70.242, "power_watts_avg": 19.244, "energy_joules_est": 222.82, "duration_seconds": 11.578, "sample_count": 99}, "timestamp": "2026-01-26T11:53:54.844876"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11236.994, "latencies_ms": [11236.994], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 74, "n_tiles": 16, "output_text": "- Bread: 12\n\n- Cookie cutter: 1\n\n- Cookie dough: 1\n\n- Baking sheet: 1\n\n- Baking pan: 1\n\n- Rolling pin: 1\n\n- Cutting board: 1\n\n- Frosting: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20936.3, "ram_available_mb": 41904.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21000.1, "ram_available_mb": 41840.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.802}, "power_stats": {"power_gpu_soc_mean_watts": 19.55, "power_cpu_cv_mean_watts": 1.831, "power_sys_5v0_mean_watts": 8.613, "gpu_utilization_percent_mean": 70.802, "power_watts_avg": 19.55, "energy_joules_est": 219.7, "duration_seconds": 11.238, "sample_count": 96}, "timestamp": "2026-01-26T11:54:08.110788"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11644.494, "latencies_ms": [11644.494], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden table with various baked goods and items scattered across its surface. To the left, a person is standing behind the table, leaning over with their hand on the table, suggesting they are in the process of preparing or serving food. In the background, there is a couch and a door, indicating that the table is located in a living room", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21000.1, "ram_available_mb": 41840.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20949.0, "ram_available_mb": 41891.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.081}, "power_stats": {"power_gpu_soc_mean_watts": 19.395, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.63, "gpu_utilization_percent_mean": 71.081, "power_watts_avg": 19.395, "energy_joules_est": 225.86, "duration_seconds": 11.645, "sample_count": 99}, "timestamp": "2026-01-26T11:54:21.781317"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8393.488, "latencies_ms": [8393.488], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "An elderly woman is standing at a kitchen table, cutting dough with a knife. The table is covered with various baked goods, including rolls and cookies, and there are cups and a bowl on the table.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20949.0, "ram_available_mb": 41891.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20993.5, "ram_available_mb": 41847.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.042}, "power_stats": {"power_gpu_soc_mean_watts": 20.828, "power_cpu_cv_mean_watts": 1.624, "power_sys_5v0_mean_watts": 8.577, "gpu_utilization_percent_mean": 73.042, "power_watts_avg": 20.828, "energy_joules_est": 174.83, "duration_seconds": 8.394, "sample_count": 71}, "timestamp": "2026-01-26T11:54:32.226739"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9728.799, "latencies_ms": [9728.799], "images_per_second": 0.103, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image shows an indoor setting with warm lighting, likely from artificial sources, as there are no visible windows or natural light. The table is covered with a patterned tablecloth, and there are various baked goods, including rolls and cookies, suggesting a cozy, domestic environment.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20941.6, "ram_available_mb": 41899.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20998.2, "ram_available_mb": 41842.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.768}, "power_stats": {"power_gpu_soc_mean_watts": 20.01, "power_cpu_cv_mean_watts": 1.782, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 71.768, "power_watts_avg": 20.01, "energy_joules_est": 194.69, "duration_seconds": 9.729, "sample_count": 82}, "timestamp": "2026-01-26T11:54:43.981247"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11574.379, "latencies_ms": [11574.379], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing in front of a traffic light, which is currently displaying a red light. The man is wearing a white shirt and shorts, and he appears to be smiling. The traffic light is positioned on the right side of the image.\n\nThe scene also features a variety of plants, including a tree and a bush, which are located on", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20944.8, "ram_available_mb": 41896.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20938.4, "ram_available_mb": 41902.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.714}, "power_stats": {"power_gpu_soc_mean_watts": 19.238, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 69.714, "power_watts_avg": 19.238, "energy_joules_est": 222.68, "duration_seconds": 11.575, "sample_count": 98}, "timestamp": "2026-01-26T11:54:57.581959"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9071.845, "latencies_ms": [9071.845], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Man: 1\n- Traffic light: 1\n- Bushes: 1\n- Flowers: 1\n- Sign: 1\n- Gravel: 1\n- Sandals: 1\n- Pole: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20938.4, "ram_available_mb": 41902.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20994.6, "ram_available_mb": 41846.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.541, "power_cpu_cv_mean_watts": 1.7, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 72.0, "power_watts_avg": 20.541, "energy_joules_est": 186.36, "duration_seconds": 9.072, "sample_count": 77}, "timestamp": "2026-01-26T11:55:08.687498"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10404.787, "latencies_ms": [10404.787], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "A man is standing in the foreground on the left side of the image, near a traffic light that is positioned slightly to the right and in the background. The traffic light is in front of a sign that reads \"AUSTRALIA TRAFFIC LIGHT\" and is surrounded by lush green foliage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20994.6, "ram_available_mb": 41846.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 20995.5, "ram_available_mb": 41845.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.261}, "power_stats": {"power_gpu_soc_mean_watts": 19.76, "power_cpu_cv_mean_watts": 1.81, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 71.261, "power_watts_avg": 19.76, "energy_joules_est": 205.61, "duration_seconds": 10.405, "sample_count": 88}, "timestamp": "2026-01-26T11:55:21.137317"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7103.326, "latencies_ms": [7103.326], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A man is standing in front of a traffic light that is showing a red signal. The traffic light is located in a tropical setting with lush green foliage in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20995.5, "ram_available_mb": 41845.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.9, "ram_used_mb": 20999.9, "ram_available_mb": 41841.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.728, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 8.565, "gpu_utilization_percent_mean": 75.0, "power_watts_avg": 21.728, "energy_joules_est": 154.35, "duration_seconds": 7.104, "sample_count": 60}, "timestamp": "2026-01-26T11:55:30.303569"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6712.23, "latencies_ms": [6712.23], "images_per_second": 0.149, "prompt_tokens": 36, "response_tokens_est": 36, "n_tiles": 16, "output_text": "A man wearing a white t-shirt and khaki shorts stands in front of a traffic light. The traffic light is red, yellow, and green.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20999.9, "ram_available_mb": 41841.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 20971.5, "ram_available_mb": 41869.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.857}, "power_stats": {"power_gpu_soc_mean_watts": 21.894, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 75.857, "power_watts_avg": 21.894, "energy_joules_est": 146.97, "duration_seconds": 6.713, "sample_count": 56}, "timestamp": "2026-01-26T11:55:39.033383"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11638.963, "latencies_ms": [11638.963], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large group of people gathered in a park, flying various kites. There are numerous kites of different shapes and sizes, with some resembling fish, and others being more abstract or traditional designs. The kites are soaring high in the sky, creating a lively and colorful scene.\n\nThe people in the park are spread out, with some standing closer", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 20971.5, "ram_available_mb": 41869.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20945.6, "ram_available_mb": 41895.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.76}, "power_stats": {"power_gpu_soc_mean_watts": 19.266, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 70.76, "power_watts_avg": 19.266, "energy_joules_est": 224.25, "duration_seconds": 11.64, "sample_count": 100}, "timestamp": "2026-01-26T11:55:52.715077"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9845.973, "latencies_ms": [9845.973], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "Kites: 10\nFish-shaped kites: 5\nFlagpoles: 10\nGrassy field: 1\nBuilding in background: 1\nPeople: 10\nBags: 1\nCooler: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20945.6, "ram_available_mb": 41895.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20942.3, "ram_available_mb": 41898.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.5}, "power_stats": {"power_gpu_soc_mean_watts": 20.126, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 8.598, "gpu_utilization_percent_mean": 72.5, "power_watts_avg": 20.126, "energy_joules_est": 198.17, "duration_seconds": 9.847, "sample_count": 84}, "timestamp": "2026-01-26T11:56:04.593985"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9173.26, "latencies_ms": [9173.26], "images_per_second": 0.109, "prompt_tokens": 44, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The kites are flying in the air above the ground, with some closer to the camera and others further away. The kites are spread out across the sky, with some flying higher than others. The kites are flying in different directions, creating a dynamic and colorful scene.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20942.3, "ram_available_mb": 41898.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20976.7, "ram_available_mb": 41864.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.179}, "power_stats": {"power_gpu_soc_mean_watts": 20.152, "power_cpu_cv_mean_watts": 1.77, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 71.179, "power_watts_avg": 20.152, "energy_joules_est": 184.87, "duration_seconds": 9.174, "sample_count": 78}, "timestamp": "2026-01-26T11:56:15.785145"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9034.616, "latencies_ms": [9034.616], "images_per_second": 0.111, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image captures a lively scene of a kite festival in a park, with numerous colorful kites flying high in the sky. People are gathered on the grassy field, enjoying the outdoor activity and the vibrant display of kites.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20976.7, "ram_available_mb": 41864.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20977.2, "ram_available_mb": 41863.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.25}, "power_stats": {"power_gpu_soc_mean_watts": 20.482, "power_cpu_cv_mean_watts": 1.696, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 72.25, "power_watts_avg": 20.482, "energy_joules_est": 185.06, "duration_seconds": 9.035, "sample_count": 76}, "timestamp": "2026-01-26T11:56:26.849903"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7061.966, "latencies_ms": [7061.966], "images_per_second": 0.142, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The kites in the image are predominantly red, white, and purple, with some blue accents. They are flying in a cloudy sky, suggesting overcast weather conditions.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20977.2, "ram_available_mb": 41863.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21008.6, "ram_available_mb": 41832.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.767}, "power_stats": {"power_gpu_soc_mean_watts": 21.462, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.629, "gpu_utilization_percent_mean": 72.767, "power_watts_avg": 21.462, "energy_joules_est": 151.58, "duration_seconds": 7.063, "sample_count": 60}, "timestamp": "2026-01-26T11:56:35.956842"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11655.041, "latencies_ms": [11655.041], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man and a young boy are sitting on the floor, sharing a slice of pizza. The man is wearing a blue shirt and the boy is wearing a blue sweater. They are both focused on the pizza, with the man holding the slice and the boy reaching for it. The pizza is placed on a red and white striped tablecloth", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20931.1, "ram_available_mb": 41909.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20967.1, "ram_available_mb": 41873.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.129}, "power_stats": {"power_gpu_soc_mean_watts": 19.136, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.613, "gpu_utilization_percent_mean": 70.129, "power_watts_avg": 19.136, "energy_joules_est": 223.04, "duration_seconds": 11.656, "sample_count": 101}, "timestamp": "2026-01-26T11:56:49.650773"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9036.823, "latencies_ms": [9036.823], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Man: 1\n- Child: 1\n- Pizza slice: 1\n- Pizza box: 1\n- Toy: 1\n- Bottle: 1\n- Chair: 1\n- Couch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20967.1, "ram_available_mb": 41873.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20967.8, "ram_available_mb": 41873.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.909}, "power_stats": {"power_gpu_soc_mean_watts": 20.461, "power_cpu_cv_mean_watts": 1.7, "power_sys_5v0_mean_watts": 8.574, "gpu_utilization_percent_mean": 71.909, "power_watts_avg": 20.461, "energy_joules_est": 184.92, "duration_seconds": 9.037, "sample_count": 77}, "timestamp": "2026-01-26T11:57:00.729724"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11595.316, "latencies_ms": [11595.316], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a young child with curly hair is seated on the floor, holding a slice of pizza up to their face, appearing to take a bite. Behind the child, an adult with a bald head and a beard is sitting cross-legged on the floor, wearing a blue patterned jacket and jeans, holding a bottle of water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20967.8, "ram_available_mb": 41873.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20968.3, "ram_available_mb": 41872.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.061}, "power_stats": {"power_gpu_soc_mean_watts": 19.231, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 71.061, "power_watts_avg": 19.231, "energy_joules_est": 223.0, "duration_seconds": 11.596, "sample_count": 99}, "timestamp": "2026-01-26T11:57:14.339906"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7774.804, "latencies_ms": [7774.804], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A man and a young child are sitting on the floor, sharing a slice of pizza. The man is wearing a blue patterned jacket and the child is wearing a blue sweatshirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20968.3, "ram_available_mb": 41872.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20968.5, "ram_available_mb": 41872.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.045}, "power_stats": {"power_gpu_soc_mean_watts": 21.221, "power_cpu_cv_mean_watts": 1.583, "power_sys_5v0_mean_watts": 8.573, "gpu_utilization_percent_mean": 73.045, "power_watts_avg": 21.221, "energy_joules_est": 165.0, "duration_seconds": 7.775, "sample_count": 66}, "timestamp": "2026-01-26T11:57:24.173628"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7772.828, "latencies_ms": [7772.828], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a man and a child sitting indoors with a bottle of water on the table. The man is wearing a blue patterned jacket and the child is in a blue sweatshirt.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20968.5, "ram_available_mb": 41872.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21013.8, "ram_available_mb": 41827.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.409}, "power_stats": {"power_gpu_soc_mean_watts": 20.894, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 72.409, "power_watts_avg": 20.894, "energy_joules_est": 162.42, "duration_seconds": 7.773, "sample_count": 66}, "timestamp": "2026-01-26T11:57:33.991952"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11590.666, "latencies_ms": [11590.666], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is sitting in a camping chair, enjoying a meal outdoors. She is holding a hot dog in her hand and has a plate of chips in front of her. The woman appears to be eating the hot dog while looking at the camera. The setting is a camping area, with a backpack placed nearby, indicating that she might be", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 20931.9, "ram_available_mb": 41909.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20973.3, "ram_available_mb": 41867.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.101}, "power_stats": {"power_gpu_soc_mean_watts": 19.325, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 71.101, "power_watts_avg": 19.325, "energy_joules_est": 224.0, "duration_seconds": 11.591, "sample_count": 99}, "timestamp": "2026-01-26T11:57:47.624035"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8023.01, "latencies_ms": [8023.01], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "person: 1, plate: 1, tortilla chip: 1, bite of food: 1, chair: 1, backpack: 1, rocks: 5, branches: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.3, "ram_available_mb": 41867.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21009.3, "ram_available_mb": 41831.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.162}, "power_stats": {"power_gpu_soc_mean_watts": 20.989, "power_cpu_cv_mean_watts": 1.613, "power_sys_5v0_mean_watts": 8.579, "gpu_utilization_percent_mean": 73.162, "power_watts_avg": 20.989, "energy_joules_est": 168.41, "duration_seconds": 8.024, "sample_count": 68}, "timestamp": "2026-01-26T11:57:57.672198"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8965.788, "latencies_ms": [8965.788], "images_per_second": 0.112, "prompt_tokens": 44, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The person is seated in the foreground on the left side of the image, holding a sandwich near their mouth. In the background, there is a plate with what appears to be tortilla chips on the right side, placed on the ground near some rocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21009.3, "ram_available_mb": 41831.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21010.5, "ram_available_mb": 41830.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.526}, "power_stats": {"power_gpu_soc_mean_watts": 20.212, "power_cpu_cv_mean_watts": 1.738, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 71.526, "power_watts_avg": 20.212, "energy_joules_est": 181.23, "duration_seconds": 8.966, "sample_count": 76}, "timestamp": "2026-01-26T11:58:08.699077"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8363.495, "latencies_ms": [8363.495], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A person is sitting in a camping chair outdoors at night, eating a hot dog and enjoying a plate of tortilla chips. The setting appears to be a campsite with a natural, outdoor environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20938.8, "ram_available_mb": 41902.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21048.8, "ram_available_mb": 41792.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.028}, "power_stats": {"power_gpu_soc_mean_watts": 20.734, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 72.028, "power_watts_avg": 20.734, "energy_joules_est": 173.42, "duration_seconds": 8.364, "sample_count": 71}, "timestamp": "2026-01-26T11:58:19.111745"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8346.792, "latencies_ms": [8346.792], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a person sitting outdoors at night, illuminated by the light of a campfire. The person is wearing a striped, long-sleeved shirt and is seated on a blue camping chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21048.8, "ram_available_mb": 41792.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20951.4, "ram_available_mb": 41889.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.371}, "power_stats": {"power_gpu_soc_mean_watts": 20.524, "power_cpu_cv_mean_watts": 1.692, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 72.371, "power_watts_avg": 20.524, "energy_joules_est": 171.32, "duration_seconds": 8.347, "sample_count": 70}, "timestamp": "2026-01-26T11:58:29.484517"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11607.163, "latencies_ms": [11607.163], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large group of people gathered around a dining table, enjoying a meal together. There are at least 13 people in the scene, with some sitting and others standing. The table is filled with various food items, including multiple plates of food, bowls, and cups. \n\nThe dining table is covered with a purple tablecloth", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 20951.4, "ram_available_mb": 41889.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21043.3, "ram_available_mb": 41797.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.051}, "power_stats": {"power_gpu_soc_mean_watts": 19.321, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 71.051, "power_watts_avg": 19.321, "energy_joules_est": 224.27, "duration_seconds": 11.608, "sample_count": 99}, "timestamp": "2026-01-26T11:58:43.148575"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10415.597, "latencies_ms": [10415.597], "images_per_second": 0.096, "prompt_tokens": 39, "response_tokens_est": 67, "n_tiles": 16, "output_text": "- People: 15\n- Plates: 20\n- Glasses: 10\n- Wine glasses: 5\n- Wine bottles: 2\n- Silverware: 15\n- Forks: 10\n- Knives: 10", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20938.9, "ram_available_mb": 41902.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20999.5, "ram_available_mb": 41841.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.273}, "power_stats": {"power_gpu_soc_mean_watts": 19.865, "power_cpu_cv_mean_watts": 1.774, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 71.273, "power_watts_avg": 19.865, "energy_joules_est": 206.92, "duration_seconds": 10.416, "sample_count": 88}, "timestamp": "2026-01-26T11:58:55.598912"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11585.417, "latencies_ms": [11585.417], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a long dining table is filled with plates of food, glasses, and cutlery, indicating a family gathering. The people are seated around the table, with some standing in the background, suggesting a casual and intimate atmosphere. The table is the central focus of the image, with the people arranged around it, creating a sense of togeth", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20999.5, "ram_available_mb": 41841.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20940.2, "ram_available_mb": 41900.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.909}, "power_stats": {"power_gpu_soc_mean_watts": 19.305, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.63, "gpu_utilization_percent_mean": 70.909, "power_watts_avg": 19.305, "energy_joules_est": 223.67, "duration_seconds": 11.586, "sample_count": 99}, "timestamp": "2026-01-26T11:59:09.215552"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8238.878, "latencies_ms": [8238.878], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A large group of people are gathered around a long dining table, enjoying a meal together. The table is filled with various dishes, drinks, and utensils, indicating a festive or celebratory occasion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.2, "ram_available_mb": 41900.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20972.6, "ram_available_mb": 41868.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.028}, "power_stats": {"power_gpu_soc_mean_watts": 20.907, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.569, "gpu_utilization_percent_mean": 73.028, "power_watts_avg": 20.907, "energy_joules_est": 172.26, "duration_seconds": 8.24, "sample_count": 71}, "timestamp": "2026-01-26T11:59:19.510453"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11580.831, "latencies_ms": [11580.831], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a group of people gathered around a long dining table covered with a purple tablecloth. The table is set with various dishes, glasses, and utensils, indicating a meal is being shared. The lighting in the room is warm and natural, coming from the windows in the background, creating a cozy atmosphere. The people are dressed in casual", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 20972.6, "ram_available_mb": 41868.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21022.9, "ram_available_mb": 41818.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.869}, "power_stats": {"power_gpu_soc_mean_watts": 19.241, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 68.869, "power_watts_avg": 19.241, "energy_joules_est": 222.84, "duration_seconds": 11.581, "sample_count": 99}, "timestamp": "2026-01-26T11:59:33.159524"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11586.87, "latencies_ms": [11586.87], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment from a baseball game. The central focus is on the batter, who is in the midst of swinging at a pitch. The catcher is crouched behind him, ready to catch the ball. The umpire stands behind the catcher, observing the play closely. The batter is wearing a black shirt and grey pants, while the catch", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 20941.3, "ram_available_mb": 41899.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.4, "ram_used_mb": 21003.9, "ram_available_mb": 41837.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.612}, "power_stats": {"power_gpu_soc_mean_watts": 19.391, "power_cpu_cv_mean_watts": 1.977, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 70.612, "power_watts_avg": 19.391, "energy_joules_est": 224.7, "duration_seconds": 11.588, "sample_count": 98}, "timestamp": "2026-01-26T11:59:46.797361"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9488.047, "latencies_ms": [9488.047], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "players: 4, catcher: 1, umpire: 1, batter: 1, pitcher: 1, home plate: 1, base: 1, fence: 1, trash can: 1, bench: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20941.8, "ram_available_mb": 41899.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 20979.2, "ram_available_mb": 41861.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.506}, "power_stats": {"power_gpu_soc_mean_watts": 20.231, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 8.587, "gpu_utilization_percent_mean": 71.506, "power_watts_avg": 20.231, "energy_joules_est": 191.97, "duration_seconds": 9.489, "sample_count": 81}, "timestamp": "2026-01-26T11:59:58.310386"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11600.521, "latencies_ms": [11600.521], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a baseball player is sliding into a base, with the base and home plate clearly visible. The catcher and umpire are in the middle ground, closely positioned behind the sliding player, indicating the action is taking place at home plate. In the background, other players are scattered around the field, with some standing near the fence and others on the ben", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20979.2, "ram_available_mb": 41861.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21021.7, "ram_available_mb": 41819.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.265}, "power_stats": {"power_gpu_soc_mean_watts": 19.33, "power_cpu_cv_mean_watts": 1.875, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 70.265, "power_watts_avg": 19.33, "energy_joules_est": 224.25, "duration_seconds": 11.601, "sample_count": 98}, "timestamp": "2026-01-26T12:00:11.944091"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8446.529, "latencies_ms": [8446.529], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image captures a moment during a baseball game on a field with a dirt infield and a grassy outfield. A player is sliding into home plate while another player, the catcher, is attempting to tag him out.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21021.7, "ram_available_mb": 41819.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21021.9, "ram_available_mb": 41819.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.597}, "power_stats": {"power_gpu_soc_mean_watts": 20.893, "power_cpu_cv_mean_watts": 1.64, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 72.597, "power_watts_avg": 20.893, "energy_joules_est": 176.49, "duration_seconds": 8.447, "sample_count": 72}, "timestamp": "2026-01-26T12:00:22.430004"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10742.027, "latencies_ms": [10742.027], "images_per_second": 0.093, "prompt_tokens": 36, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image captures a baseball game in progress under clear skies, with the sun casting shadows on the field, which is a vibrant green with a red dirt infield. The players are wearing a mix of dark and light-colored uniforms, and the stands in the background are mostly empty with a few spectators.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21021.9, "ram_available_mb": 41819.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20986.8, "ram_available_mb": 41854.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.198}, "power_stats": {"power_gpu_soc_mean_watts": 19.553, "power_cpu_cv_mean_watts": 1.852, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 70.198, "power_watts_avg": 19.553, "energy_joules_est": 210.05, "duration_seconds": 10.743, "sample_count": 91}, "timestamp": "2026-01-26T12:00:35.228115"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12399.928, "latencies_ms": [12399.928], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a dynamic scene at a skatepark. A skateboarder, clad in a black t-shirt and helmet, is in mid-air, performing a trick on a concrete ramp. The skateboarder's arms are outstretched, aiding in balance, and their legs are bent at the knees, ready to land smooth", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 20986.8, "ram_available_mb": 41854.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21018.2, "ram_available_mb": 41822.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.368}, "power_stats": {"power_gpu_soc_mean_watts": 21.507, "power_cpu_cv_mean_watts": 1.82, "power_sys_5v0_mean_watts": 8.876, "gpu_utilization_percent_mean": 72.368, "power_watts_avg": 21.507, "energy_joules_est": 266.7, "duration_seconds": 12.401, "sample_count": 106}, "timestamp": "2026-01-26T12:00:49.683117"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8792.641, "latencies_ms": [8792.641], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "skateboard: 1, person: 1, fence: 2, trees: 3, grass: 2, buildings: 1, sky: 1, shadow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20929.9, "ram_available_mb": 41911.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20944.2, "ram_available_mb": 41896.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.122}, "power_stats": {"power_gpu_soc_mean_watts": 23.167, "power_cpu_cv_mean_watts": 1.423, "power_sys_5v0_mean_watts": 8.709, "gpu_utilization_percent_mean": 78.122, "power_watts_avg": 23.167, "energy_joules_est": 203.71, "duration_seconds": 8.793, "sample_count": 74}, "timestamp": "2026-01-26T12:01:00.498728"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11889.626, "latencies_ms": [11889.626], "images_per_second": 0.084, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, performing a trick on a concrete ramp. The railing is in the middle ground, and the grassy area with trees is in the background. The shadow of the skateboarder is cast on the ramp, indicating the light source is coming from the upper left side of the image.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20944.2, "ram_available_mb": 41896.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21027.4, "ram_available_mb": 41813.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.545}, "power_stats": {"power_gpu_soc_mean_watts": 21.82, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 8.878, "gpu_utilization_percent_mean": 74.545, "power_watts_avg": 21.82, "energy_joules_est": 259.45, "duration_seconds": 11.89, "sample_count": 101}, "timestamp": "2026-01-26T12:01:14.410598"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7720.895, "latencies_ms": [7720.895], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "A person is skateboarding at a skate park, performing a trick on a rail. The skate park is surrounded by trees and grassy areas.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20970.7, "ram_available_mb": 41870.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 21001.6, "ram_available_mb": 41839.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 79.538}, "power_stats": {"power_gpu_soc_mean_watts": 23.981, "power_cpu_cv_mean_watts": 1.293, "power_sys_5v0_mean_watts": 8.699, "gpu_utilization_percent_mean": 79.538, "power_watts_avg": 23.981, "energy_joules_est": 185.17, "duration_seconds": 7.722, "sample_count": 65}, "timestamp": "2026-01-26T12:01:24.166258"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8144.922, "latencies_ms": [8144.922], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image captures a skateboarder in mid-air, performing a trick on a concrete ramp. The lighting is bright and natural, suggesting it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20931.4, "ram_available_mb": 41909.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21022.0, "ram_available_mb": 41818.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.623}, "power_stats": {"power_gpu_soc_mean_watts": 23.402, "power_cpu_cv_mean_watts": 1.415, "power_sys_5v0_mean_watts": 8.812, "gpu_utilization_percent_mean": 76.623, "power_watts_avg": 23.402, "energy_joules_est": 190.62, "duration_seconds": 8.146, "sample_count": 69}, "timestamp": "2026-01-26T12:01:34.326200"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11580.25, "latencies_ms": [11580.25], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with a plate of food, including a sandwich, fries, and a salad. The sandwich is placed on a white plate, accompanied by a side of fries and a small salad. The fries are golden and appear crispy, while the salad consists of lettuce, tomato, and possibly other vegetables. There are", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20939.9, "ram_available_mb": 41901.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20967.0, "ram_available_mb": 41873.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.561}, "power_stats": {"power_gpu_soc_mean_watts": 19.379, "power_cpu_cv_mean_watts": 1.883, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 70.561, "power_watts_avg": 19.379, "energy_joules_est": 224.43, "duration_seconds": 11.581, "sample_count": 98}, "timestamp": "2026-01-26T12:01:47.936349"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9831.212, "latencies_ms": [9831.212], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "- Fries: 12\n- Hamburger: 1\n- Tomato: 2\n- Lettuce: 2\n- Pickles: 1\n- Mayonnaise: 2\n- Ketchup: 2\n- Glass of water: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20967.0, "ram_available_mb": 41873.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20967.0, "ram_available_mb": 41873.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.048}, "power_stats": {"power_gpu_soc_mean_watts": 20.15, "power_cpu_cv_mean_watts": 1.756, "power_sys_5v0_mean_watts": 8.619, "gpu_utilization_percent_mean": 72.048, "power_watts_avg": 20.15, "energy_joules_est": 198.11, "duration_seconds": 9.832, "sample_count": 83}, "timestamp": "2026-01-26T12:01:59.779924"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11584.477, "latencies_ms": [11584.477], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "On the left side of the image, there is a plate with a burger and fries, which is positioned in the foreground and appears to be the main focus of the meal. In the background, there is a plate with a salad, containing lettuce, tomato, and pickles, which is slightly less prominent. The bottles of condiments are placed on the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20967.0, "ram_available_mb": 41873.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21047.8, "ram_available_mb": 41793.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.41}, "power_stats": {"power_gpu_soc_mean_watts": 19.323, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 70.41, "power_watts_avg": 19.323, "energy_joules_est": 223.86, "duration_seconds": 11.585, "sample_count": 100}, "timestamp": "2026-01-26T12:02:13.426880"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9040.985, "latencies_ms": [9040.985], "images_per_second": 0.111, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows a meal consisting of a hamburger, French fries, and a side salad with a slice of lemon on a table. There are also condiment bottles and a glass of water on the table, suggesting a dining setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.2, "ram_available_mb": 41908.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20977.1, "ram_available_mb": 41863.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.769}, "power_stats": {"power_gpu_soc_mean_watts": 20.337, "power_cpu_cv_mean_watts": 1.704, "power_sys_5v0_mean_watts": 8.608, "gpu_utilization_percent_mean": 72.769, "power_watts_avg": 20.337, "energy_joules_est": 183.88, "duration_seconds": 9.042, "sample_count": 78}, "timestamp": "2026-01-26T12:02:24.525687"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9954.349, "latencies_ms": [9954.349], "images_per_second": 0.1, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows a meal on a table with a white plate containing a hamburger, fries, and a side of pickles and tomato, accompanied by a glass of water and condiment bottles. The lighting appears to be artificial, and the table has a textured white tablecloth.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20977.1, "ram_available_mb": 41863.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21009.5, "ram_available_mb": 41831.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.523}, "power_stats": {"power_gpu_soc_mean_watts": 19.769, "power_cpu_cv_mean_watts": 1.825, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 71.523, "power_watts_avg": 19.769, "energy_joules_est": 196.8, "duration_seconds": 9.955, "sample_count": 86}, "timestamp": "2026-01-26T12:02:36.498085"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10203.73, "latencies_ms": [10203.73], "images_per_second": 0.098, "prompt_tokens": 24, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image shows a red motorcycle with a sidecar parked on a paved area. The motorcycle has a black seat and a windshield. In the background, there are palm trees and a wooden fence, suggesting a coastal or tropical location. The sky is clear and blue, indicating good weather.", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21009.5, "ram_available_mb": 41831.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20932.3, "ram_available_mb": 41908.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.909}, "power_stats": {"power_gpu_soc_mean_watts": 19.784, "power_cpu_cv_mean_watts": 1.824, "power_sys_5v0_mean_watts": 8.662, "gpu_utilization_percent_mean": 70.909, "power_watts_avg": 19.784, "energy_joules_est": 201.88, "duration_seconds": 10.204, "sample_count": 88}, "timestamp": "2026-01-26T12:02:48.739920"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8272.832, "latencies_ms": [8272.832], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "motorcycle: 1, palm tree: 3, fence post: 1, fence: 1, sand dune: 1, grass: 1, sky: 1, shadow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.3, "ram_available_mb": 41908.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20941.0, "ram_available_mb": 41899.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.338}, "power_stats": {"power_gpu_soc_mean_watts": 20.672, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.568, "gpu_utilization_percent_mean": 73.338, "power_watts_avg": 20.672, "energy_joules_est": 171.03, "duration_seconds": 8.273, "sample_count": 71}, "timestamp": "2026-01-26T12:02:59.046687"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8479.615, "latencies_ms": [8479.615], "images_per_second": 0.118, "prompt_tokens": 44, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The motorcycle is parked in the foreground on the right side of the image, near the center. The palm trees are in the background, behind the motorcycle, and there is a wooden fence to the left of the motorcycle.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20941.0, "ram_available_mb": 41899.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20972.4, "ram_available_mb": 41868.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.625}, "power_stats": {"power_gpu_soc_mean_watts": 20.498, "power_cpu_cv_mean_watts": 1.712, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 71.625, "power_watts_avg": 20.498, "energy_joules_est": 173.83, "duration_seconds": 8.48, "sample_count": 72}, "timestamp": "2026-01-26T12:03:09.569084"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8673.873, "latencies_ms": [8673.873], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A red motorcycle with a sidecar is parked on a paved area near a wooden fence, with palm trees and a sandy beach in the background. The sky is clear and blue, suggesting a sunny day at the beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20972.4, "ram_available_mb": 41868.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20989.9, "ram_available_mb": 41851.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.88}, "power_stats": {"power_gpu_soc_mean_watts": 20.671, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 72.88, "power_watts_avg": 20.671, "energy_joules_est": 179.31, "duration_seconds": 8.675, "sample_count": 75}, "timestamp": "2026-01-26T12:03:20.275053"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6530.315, "latencies_ms": [6530.315], "images_per_second": 0.153, "prompt_tokens": 36, "response_tokens_est": 35, "n_tiles": 16, "output_text": "The motorcycle is red and black, parked on a sunny day with clear blue skies. It has a black leather seat and a silver exhaust pipe.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 20925.5, "ram_available_mb": 41915.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20967.9, "ram_available_mb": 41873.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.327}, "power_stats": {"power_gpu_soc_mean_watts": 21.644, "power_cpu_cv_mean_watts": 1.47, "power_sys_5v0_mean_watts": 8.608, "gpu_utilization_percent_mean": 74.327, "power_watts_avg": 21.644, "energy_joules_est": 141.36, "duration_seconds": 6.531, "sample_count": 55}, "timestamp": "2026-01-26T12:03:28.832731"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11570.048, "latencies_ms": [11570.048], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of quiet introspection. A man, dressed in a formal black suit and tie, stands against a stark white wall. His face is turned away from the camera, suggesting a moment of deep thought or perhaps a desire for privacy. The lighting in the room is dim, casting a soft glow on his suit and creating a sense of solitude. The white", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 20967.9, "ram_available_mb": 41873.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20945.1, "ram_available_mb": 41895.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.614}, "power_stats": {"power_gpu_soc_mean_watts": 19.311, "power_cpu_cv_mean_watts": 1.902, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 69.614, "power_watts_avg": 19.311, "energy_joules_est": 223.44, "duration_seconds": 11.571, "sample_count": 101}, "timestamp": "2026-01-26T12:03:42.454572"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7312.703, "latencies_ms": [7312.703], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20945.1, "ram_available_mb": 41895.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20968.4, "ram_available_mb": 41872.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.097}, "power_stats": {"power_gpu_soc_mean_watts": 21.497, "power_cpu_cv_mean_watts": 1.543, "power_sys_5v0_mean_watts": 8.587, "gpu_utilization_percent_mean": 74.097, "power_watts_avg": 21.497, "energy_joules_est": 157.22, "duration_seconds": 7.313, "sample_count": 62}, "timestamp": "2026-01-26T12:03:51.784832"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7114.183, "latencies_ms": [7114.183], "images_per_second": 0.141, "prompt_tokens": 44, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The man is standing in the foreground with a dark suit jacket and a patterned tie. The light switch is on the wall in the background, slightly to the left of the man.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20968.4, "ram_available_mb": 41872.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.6, "ram_used_mb": 20942.1, "ram_available_mb": 41898.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.533}, "power_stats": {"power_gpu_soc_mean_watts": 21.341, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.608, "gpu_utilization_percent_mean": 72.533, "power_watts_avg": 21.341, "energy_joules_est": 151.84, "duration_seconds": 7.115, "sample_count": 60}, "timestamp": "2026-01-26T12:04:00.949439"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6991.22, "latencies_ms": [6991.22], "images_per_second": 0.143, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A man in a dark suit with a white shirt and a patterned tie is standing in a dimly lit room. The light switch is visible on the wall behind him.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20942.1, "ram_available_mb": 41898.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21006.5, "ram_available_mb": 41834.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.695}, "power_stats": {"power_gpu_soc_mean_watts": 21.658, "power_cpu_cv_mean_watts": 1.492, "power_sys_5v0_mean_watts": 8.543, "gpu_utilization_percent_mean": 74.695, "power_watts_avg": 21.658, "energy_joules_est": 151.43, "duration_seconds": 6.992, "sample_count": 59}, "timestamp": "2026-01-26T12:04:09.957492"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7454.45, "latencies_ms": [7454.45], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image shows a person wearing a dark suit with a white shirt and a patterned tie. The lighting is dim, highlighting the person's attire and creating a moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20946.2, "ram_available_mb": 41894.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20969.6, "ram_available_mb": 41871.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.905}, "power_stats": {"power_gpu_soc_mean_watts": 21.09, "power_cpu_cv_mean_watts": 1.614, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 72.905, "power_watts_avg": 21.09, "energy_joules_est": 157.23, "duration_seconds": 7.455, "sample_count": 63}, "timestamp": "2026-01-26T12:04:19.470707"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11623.135, "latencies_ms": [11623.135], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a gray and white cat is peacefully sleeping on a pair of shoes. The cat is curled up, with its head resting on the shoes, and its eyes closed. The shoes are placed on a wooden floor, and the cat is positioned in the center of the image. The background features a white wall, providing a neutral backdrop that contrast", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 20969.6, "ram_available_mb": 41871.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21016.6, "ram_available_mb": 41824.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.082}, "power_stats": {"power_gpu_soc_mean_watts": 19.279, "power_cpu_cv_mean_watts": 1.883, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 71.082, "power_watts_avg": 19.279, "energy_joules_est": 224.1, "duration_seconds": 11.624, "sample_count": 98}, "timestamp": "2026-01-26T12:04:33.116023"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4696.219, "latencies_ms": [4696.219], "images_per_second": 0.213, "prompt_tokens": 39, "response_tokens_est": 17, "n_tiles": 16, "output_text": "shoe: 2\ncat: 1\nwall: 1\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20944.7, "ram_available_mb": 41896.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 21005.4, "ram_available_mb": 41835.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_gpu_soc_mean_watts": 24.451, "power_cpu_cv_mean_watts": 1.006, "power_sys_5v0_mean_watts": 8.482, "gpu_utilization_percent_mean": 81.0, "power_watts_avg": 24.451, "energy_joules_est": 114.84, "duration_seconds": 4.697, "sample_count": 39}, "timestamp": "2026-01-26T12:04:39.858469"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11575.686, "latencies_ms": [11575.686], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The cat is resting on top of a shoe, which is placed on the ground in the foreground of the image. The shoe is positioned to the left of the image, and there is a white wall in the background. The cat is near the front edge of the image, and the shoe is in the immediate foreground, suggesting that the cat is closer to the camera", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20951.8, "ram_available_mb": 41889.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20986.4, "ram_available_mb": 41854.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.293}, "power_stats": {"power_gpu_soc_mean_watts": 19.326, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.629, "gpu_utilization_percent_mean": 70.293, "power_watts_avg": 19.326, "energy_joules_est": 223.73, "duration_seconds": 11.576, "sample_count": 99}, "timestamp": "2026-01-26T12:04:53.456255"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7758.508, "latencies_ms": [7758.508], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A cat is sleeping peacefully on top of a pair of shoes. The shoes are placed against a white wall, and the cat appears to be snuggled up against the shoe laces.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20986.4, "ram_available_mb": 41854.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20987.6, "ram_available_mb": 41853.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.03}, "power_stats": {"power_gpu_soc_mean_watts": 21.235, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.584, "gpu_utilization_percent_mean": 74.03, "power_watts_avg": 21.235, "energy_joules_est": 164.77, "duration_seconds": 7.759, "sample_count": 67}, "timestamp": "2026-01-26T12:05:03.243048"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11571.888, "latencies_ms": [11571.888], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a gray and white cat with a white chest and paws, sleeping on a pair of blue and white sneakers. The cat is resting its head on the tongue of the shoe, with its eyes closed and a peaceful expression on its face. The sneakers are placed on a wooden surface, and the background is a plain white wall. The light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20987.6, "ram_available_mb": 41853.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20987.9, "ram_available_mb": 41853.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.01}, "power_stats": {"power_gpu_soc_mean_watts": 19.271, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 70.01, "power_watts_avg": 19.271, "energy_joules_est": 223.01, "duration_seconds": 11.573, "sample_count": 98}, "timestamp": "2026-01-26T12:05:16.828768"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11573.032, "latencies_ms": [11573.032], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene on a city street. Dominating the frame is a large green dump truck, its body adorned with a red and white striped pattern on the front bumper. The truck is in motion, driving on the right side of the road, as indicated by the white line on the left side of the road. \n\nOn", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20987.9, "ram_available_mb": 41853.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21002.1, "ram_available_mb": 41838.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.65}, "power_stats": {"power_gpu_soc_mean_watts": 19.298, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 70.65, "power_watts_avg": 19.298, "energy_joules_est": 223.35, "duration_seconds": 11.574, "sample_count": 100}, "timestamp": "2026-01-26T12:05:30.438516"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7893.826, "latencies_ms": [7893.826], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "truck: 1, people: 3, buildings: 2, trees: 2, vehicles: 2, traffic lights: 1, signs: 2, pedestrians: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20939.2, "ram_available_mb": 41901.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21001.4, "ram_available_mb": 41839.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.582}, "power_stats": {"power_gpu_soc_mean_watts": 21.072, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.564, "gpu_utilization_percent_mean": 73.582, "power_watts_avg": 21.072, "energy_joules_est": 166.35, "duration_seconds": 7.894, "sample_count": 67}, "timestamp": "2026-01-26T12:05:40.353700"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9163.355, "latencies_ms": [9163.355], "images_per_second": 0.109, "prompt_tokens": 44, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The green dump truck is in the foreground on the right side of the image, with a pedestrian crossing in front of it. In the background, there is a red bus on the left side and a building with the sign 'Change Tower' on the right side.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21001.4, "ram_available_mb": 41839.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21011.3, "ram_available_mb": 41829.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.808}, "power_stats": {"power_gpu_soc_mean_watts": 20.098, "power_cpu_cv_mean_watts": 1.755, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 70.808, "power_watts_avg": 20.098, "energy_joules_est": 184.18, "duration_seconds": 9.164, "sample_count": 78}, "timestamp": "2026-01-26T12:05:51.541435"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8924.744, "latencies_ms": [8924.744], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "A green Isuzu truck with a red and white striped front bumper is on a city street. There are two people sitting on the back of the truck, one wearing a yellow hat and the other wearing a green jacket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.9, "ram_available_mb": 41908.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20949.5, "ram_available_mb": 41891.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.447}, "power_stats": {"power_gpu_soc_mean_watts": 20.575, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.567, "gpu_utilization_percent_mean": 72.447, "power_watts_avg": 20.575, "energy_joules_est": 183.64, "duration_seconds": 8.925, "sample_count": 76}, "timestamp": "2026-01-26T12:06:02.500481"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8817.378, "latencies_ms": [8817.378], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "A green Isuzu FTR truck with a red and white striped front bumper is on the road. The truck has a license plate that reads \"77-6921\" and is marked with the logo of \"Thailand\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20949.5, "ram_available_mb": 41891.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20952.9, "ram_available_mb": 41888.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.973}, "power_stats": {"power_gpu_soc_mean_watts": 20.117, "power_cpu_cv_mean_watts": 1.719, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 71.973, "power_watts_avg": 20.117, "energy_joules_est": 177.39, "duration_seconds": 8.818, "sample_count": 75}, "timestamp": "2026-01-26T12:06:13.352883"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12378.682, "latencies_ms": [12378.682], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a bird is standing on a rocky shore near a river. The bird is positioned in the center of the scene, surrounded by numerous rocks. The river appears to be shallow, with the bird standing on one of the rocks. The rocks are scattered throughout the scene, with some closer to the bird and others further away. The bird seems to be enjoying its time", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20952.9, "ram_available_mb": 41888.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21024.3, "ram_available_mb": 41816.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.314}, "power_stats": {"power_gpu_soc_mean_watts": 21.36, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.813, "gpu_utilization_percent_mean": 72.314, "power_watts_avg": 21.36, "energy_joules_est": 264.42, "duration_seconds": 12.379, "sample_count": 105}, "timestamp": "2026-01-26T12:06:27.769950"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7567.806, "latencies_ms": [7567.806], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 35, "n_tiles": 16, "output_text": "rocks: numerous\nwater: river\ntrees: greenery\nbridge: overpass\ncars: on bridge\nbird: standing on rocks\nfish: in water", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20945.7, "ram_available_mb": 41895.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 20932.9, "ram_available_mb": 41908.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.031}, "power_stats": {"power_gpu_soc_mean_watts": 23.549, "power_cpu_cv_mean_watts": 1.345, "power_sys_5v0_mean_watts": 8.675, "gpu_utilization_percent_mean": 78.031, "power_watts_avg": 23.549, "energy_joules_est": 178.23, "duration_seconds": 7.568, "sample_count": 64}, "timestamp": "2026-01-26T12:06:37.397372"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12202.596, "latencies_ms": [12202.596], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a rocky riverbed with various sized rocks scattered across the water's surface. Further back, on the riverbank, there is a bridge spanning across the river, with vegetation growing on the banks. The bird is standing on the rocks in the middle ground, closer to the camera than the bridge, but further away from the camera than the rocks", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.9, "ram_available_mb": 41908.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21033.5, "ram_available_mb": 41807.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.269}, "power_stats": {"power_gpu_soc_mean_watts": 21.315, "power_cpu_cv_mean_watts": 1.832, "power_sys_5v0_mean_watts": 8.836, "gpu_utilization_percent_mean": 72.269, "power_watts_avg": 21.315, "energy_joules_est": 260.11, "duration_seconds": 12.203, "sample_count": 104}, "timestamp": "2026-01-26T12:06:51.649750"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7676.369, "latencies_ms": [7676.369], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The image depicts a serene riverbank with a bridge in the background. A bird is standing on the rocks in the shallow water, possibly searching for food.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21033.5, "ram_available_mb": 41807.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21056.2, "ram_available_mb": 41784.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.323}, "power_stats": {"power_gpu_soc_mean_watts": 23.475, "power_cpu_cv_mean_watts": 1.355, "power_sys_5v0_mean_watts": 8.663, "gpu_utilization_percent_mean": 78.323, "power_watts_avg": 23.475, "energy_joules_est": 180.22, "duration_seconds": 7.677, "sample_count": 65}, "timestamp": "2026-01-26T12:07:01.350271"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7947.185, "latencies_ms": [7947.185], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image depicts a serene river scene with a bridge in the background and a clear sky above. The river is filled with rocks and boulders, and the water appears calm and still.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21032.9, "ram_available_mb": 41808.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.621}, "power_stats": {"power_gpu_soc_mean_watts": 22.828, "power_cpu_cv_mean_watts": 1.516, "power_sys_5v0_mean_watts": 8.815, "gpu_utilization_percent_mean": 76.621, "power_watts_avg": 22.828, "energy_joules_est": 181.43, "duration_seconds": 7.948, "sample_count": 66}, "timestamp": "2026-01-26T12:07:11.313535"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11600.888, "latencies_ms": [11600.888], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a row of parked motor scooters lined up along the side of a street. There are at least 13 scooters visible, with some positioned closer to the curb and others further back. The scooters are of various sizes and colors, creating a diverse and visually interesting scene.\n\nIn addition to the scooters, there are", "error": null, "sys_before": {"cpu_percent": 6.1, "ram_used_mb": 20927.7, "ram_available_mb": 41913.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20974.8, "ram_available_mb": 41866.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.426}, "power_stats": {"power_gpu_soc_mean_watts": 19.299, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.648, "gpu_utilization_percent_mean": 69.426, "power_watts_avg": 19.299, "energy_joules_est": 223.9, "duration_seconds": 11.602, "sample_count": 101}, "timestamp": "2026-01-26T12:07:24.984017"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8684.293, "latencies_ms": [8684.293], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "- scooters: 10\n- people: 5\n- buildings: 2\n- windows: 14\n- doors: 2\n- signs: 3\n- trees: 1\n- plants: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20974.8, "ram_available_mb": 41866.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20974.8, "ram_available_mb": 41866.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.041}, "power_stats": {"power_gpu_soc_mean_watts": 20.776, "power_cpu_cv_mean_watts": 1.667, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 73.041, "power_watts_avg": 20.776, "energy_joules_est": 180.44, "duration_seconds": 8.685, "sample_count": 73}, "timestamp": "2026-01-26T12:07:35.677753"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11582.025, "latencies_ms": [11582.025], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a row of scooters parked on the side of the street, with the closest ones being nearest to the viewer and the farthest ones being the most distant. The scooters are parked in front of a building with a red awning, which is located in the background. There are also a few people walking on the sidewalk in", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20974.8, "ram_available_mb": 41866.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21024.2, "ram_available_mb": 41816.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.727}, "power_stats": {"power_gpu_soc_mean_watts": 19.214, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.683, "gpu_utilization_percent_mean": 68.727, "power_watts_avg": 19.214, "energy_joules_est": 222.55, "duration_seconds": 11.583, "sample_count": 99}, "timestamp": "2026-01-26T12:07:49.299634"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8783.666, "latencies_ms": [8783.666], "images_per_second": 0.114, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image depicts a street scene with a row of parked scooters and motorcycles in front of a building with a red awning. There are several people walking on the sidewalk and a few individuals standing near the parked vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20938.1, "ram_available_mb": 41902.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20991.7, "ram_available_mb": 41849.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.645}, "power_stats": {"power_gpu_soc_mean_watts": 20.637, "power_cpu_cv_mean_watts": 1.691, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 72.645, "power_watts_avg": 20.637, "energy_joules_est": 181.28, "duration_seconds": 8.784, "sample_count": 76}, "timestamp": "2026-01-26T12:08:00.114115"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7768.178, "latencies_ms": [7768.178], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a row of parked scooters in front of a building with a red awning. The scooters are mostly black and silver, and the building has a beige facade with green trim.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20926.8, "ram_available_mb": 41914.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.8, "ram_used_mb": 20938.5, "ram_available_mb": 41902.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.015}, "power_stats": {"power_gpu_soc_mean_watts": 20.985, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 8.639, "gpu_utilization_percent_mean": 73.015, "power_watts_avg": 20.985, "energy_joules_est": 163.03, "duration_seconds": 7.769, "sample_count": 66}, "timestamp": "2026-01-26T12:08:09.923751"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11124.48, "latencies_ms": [11124.48], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The image shows a close-up of a person's hand holding a piece of broccoli. The broccoli is green with a few brown spots, indicating it might be slightly overripe or cooked. The background is blurred, but it appears to be a kitchen setting with a dark-colored surface, possibly a countertop.", "error": null, "sys_before": {"cpu_percent": 17.4, "ram_used_mb": 20938.5, "ram_available_mb": 41902.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.6, "ram_used_mb": 20945.2, "ram_available_mb": 41895.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.8}, "power_stats": {"power_gpu_soc_mean_watts": 19.489, "power_cpu_cv_mean_watts": 1.947, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 70.8, "power_watts_avg": 19.489, "energy_joules_est": 216.82, "duration_seconds": 11.125, "sample_count": 95}, "timestamp": "2026-01-26T12:08:23.087991"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7313.423, "latencies_ms": [7313.423], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20945.2, "ram_available_mb": 41895.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 20960.4, "ram_available_mb": 41880.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.435}, "power_stats": {"power_gpu_soc_mean_watts": 21.577, "power_cpu_cv_mean_watts": 1.847, "power_sys_5v0_mean_watts": 8.592, "gpu_utilization_percent_mean": 74.435, "power_watts_avg": 21.577, "energy_joules_est": 157.82, "duration_seconds": 7.314, "sample_count": 62}, "timestamp": "2026-01-26T12:08:32.425980"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11589.791, "latencies_ms": [11589.791], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The main object, which is a piece of broccoli, is held in the foreground of the image, appearing large and in focus. It is positioned near the center of the image, with a blurred background that suggests a kitchen setting with a tiled backsplash. The broccoli is held up by a human hand, which is in the foreground and occupies", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20960.4, "ram_available_mb": 41880.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.3, "ram_used_mb": 21022.6, "ram_available_mb": 41818.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.212}, "power_stats": {"power_gpu_soc_mean_watts": 19.217, "power_cpu_cv_mean_watts": 2.066, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 70.212, "power_watts_avg": 19.217, "energy_joules_est": 222.73, "duration_seconds": 11.59, "sample_count": 99}, "timestamp": "2026-01-26T12:08:46.072240"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6987.374, "latencies_ms": [6987.374], "images_per_second": 0.143, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A person is holding a piece of broccoli with a mushroom attached to it, against a backdrop of a kitchen with a black pot and a tiled wall.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20969.1, "ram_available_mb": 41871.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.7, "ram_used_mb": 20964.5, "ram_available_mb": 41876.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.949}, "power_stats": {"power_gpu_soc_mean_watts": 21.656, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.586, "gpu_utilization_percent_mean": 73.949, "power_watts_avg": 21.656, "energy_joules_est": 151.33, "duration_seconds": 6.988, "sample_count": 59}, "timestamp": "2026-01-26T12:08:55.101404"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9725.903, "latencies_ms": [9725.903], "images_per_second": 0.103, "prompt_tokens": 36, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image shows a close-up of a hand holding a piece of broccoli with a dark, possibly roasted, mushroom on top. The lighting is bright and appears to be coming from the upper left, casting a shadow to the right of the broccoli and mushroom.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20964.5, "ram_available_mb": 41876.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 21022.9, "ram_available_mb": 41818.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.229}, "power_stats": {"power_gpu_soc_mean_watts": 19.897, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.61, "gpu_utilization_percent_mean": 71.229, "power_watts_avg": 19.897, "energy_joules_est": 193.53, "duration_seconds": 9.727, "sample_count": 83}, "timestamp": "2026-01-26T12:09:06.866416"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11597.209, "latencies_ms": [11597.209], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two individuals captured in a moment of joy and camaraderie. The person on the left, clad in a black jacket, is playfully covering the other person's face with their own. The person on the right, wearing a green jacket adorned with a fur hood, is laughing heartily, their face pressed against the", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 21022.9, "ram_available_mb": 41818.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20978.3, "ram_available_mb": 41862.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.091}, "power_stats": {"power_gpu_soc_mean_watts": 19.359, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 70.091, "power_watts_avg": 19.359, "energy_joules_est": 224.52, "duration_seconds": 11.598, "sample_count": 99}, "timestamp": "2026-01-26T12:09:20.496056"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6529.89, "latencies_ms": [6529.89], "images_per_second": 0.153, "prompt_tokens": 39, "response_tokens_est": 33, "n_tiles": 16, "output_text": "face: 2\nhair: 2\ncoat: 2\nhat: 2\nfur: 2\nperson: 2\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20978.3, "ram_available_mb": 41862.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21036.2, "ram_available_mb": 41804.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.673}, "power_stats": {"power_gpu_soc_mean_watts": 22.27, "power_cpu_cv_mean_watts": 1.405, "power_sys_5v0_mean_watts": 8.535, "gpu_utilization_percent_mean": 75.673, "power_watts_avg": 22.27, "energy_joules_est": 145.43, "duration_seconds": 6.531, "sample_count": 55}, "timestamp": "2026-01-26T12:09:29.044305"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7918.092, "latencies_ms": [7918.092], "images_per_second": 0.126, "prompt_tokens": 44, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The person on the left is in the foreground and appears to be facing the camera, while the person on the right is slightly behind and to the right of the first person, both are in the middle ground of the image.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21036.2, "ram_available_mb": 41804.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20948.8, "ram_available_mb": 41892.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.485}, "power_stats": {"power_gpu_soc_mean_watts": 20.845, "power_cpu_cv_mean_watts": 1.654, "power_sys_5v0_mean_watts": 8.61, "gpu_utilization_percent_mean": 74.485, "power_watts_avg": 20.845, "energy_joules_est": 165.07, "duration_seconds": 7.919, "sample_count": 68}, "timestamp": "2026-01-26T12:09:39.017846"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11789.428, "latencies_ms": [11789.428], "images_per_second": 0.085, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of playful interaction between two individuals, likely in an urban setting, as suggested by the blurred background that hints at a busy street or public space. One person is wearing a dark jacket and a fur-lined hat, while the other is partially obscured by the jacket, creating a sense of movement and spontaneity in the scene.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20948.8, "ram_available_mb": 41892.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20991.3, "ram_available_mb": 41849.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.088}, "power_stats": {"power_gpu_soc_mean_watts": 19.309, "power_cpu_cv_mean_watts": 1.864, "power_sys_5v0_mean_watts": 8.603, "gpu_utilization_percent_mean": 71.088, "power_watts_avg": 19.309, "energy_joules_est": 227.65, "duration_seconds": 11.79, "sample_count": 102}, "timestamp": "2026-01-26T12:09:52.832640"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9375.335, "latencies_ms": [9375.335], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image features a person wearing a dark jacket with a fur-lined hood, and the lighting appears to be artificial, possibly from an indoor source. The background is blurred, but there seems to be a hint of a snowy environment, suggesting cold weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20991.3, "ram_available_mb": 41849.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21015.7, "ram_available_mb": 41825.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.987}, "power_stats": {"power_gpu_soc_mean_watts": 20.096, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 70.987, "power_watts_avg": 20.096, "energy_joules_est": 188.42, "duration_seconds": 9.376, "sample_count": 80}, "timestamp": "2026-01-26T12:10:04.245176"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11591.938, "latencies_ms": [11591.938], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a man standing on a tennis court, holding a tennis racket and preparing to hit a tennis ball. He is positioned near the center of the court, with the ball located slightly to his right. The court is surrounded by numerous chairs, arranged in rows, which are likely for spectators or players to rest during breaks. The man appears focused and ready to engage", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20961.2, "ram_available_mb": 41879.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21009.2, "ram_available_mb": 41831.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.772}, "power_stats": {"power_gpu_soc_mean_watts": 19.295, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 69.772, "power_watts_avg": 19.295, "energy_joules_est": 223.68, "duration_seconds": 11.593, "sample_count": 101}, "timestamp": "2026-01-26T12:10:17.895466"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9481.977, "latencies_ms": [9481.977], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Chair: 11\n- Tennis ball: 1\n- Tennis racket: 1\n- Tennis player: 1\n- Shoe: 1\n- Sock: 1\n- Wristband: 1\n- Wristband: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20946.5, "ram_available_mb": 41894.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21013.3, "ram_available_mb": 41827.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.481}, "power_stats": {"power_gpu_soc_mean_watts": 20.381, "power_cpu_cv_mean_watts": 1.735, "power_sys_5v0_mean_watts": 8.603, "gpu_utilization_percent_mean": 72.481, "power_watts_avg": 20.381, "energy_joules_est": 193.27, "duration_seconds": 9.483, "sample_count": 81}, "timestamp": "2026-01-26T12:10:29.415206"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11576.362, "latencies_ms": [11576.362], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a tennis player is positioned on the left side of the image, holding a tennis racket and preparing to hit a tennis ball that is near the center of the image. The background consists of multiple rows of white stadium seats, which are arranged in a pattern that recedes towards the right side of the image. The tennis court itself is a vibrant blue", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21013.3, "ram_available_mb": 41827.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21017.0, "ram_available_mb": 41823.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.47}, "power_stats": {"power_gpu_soc_mean_watts": 19.157, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 69.47, "power_watts_avg": 19.157, "energy_joules_est": 221.78, "duration_seconds": 11.577, "sample_count": 100}, "timestamp": "2026-01-26T12:10:43.022322"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5948.094, "latencies_ms": [5948.094], "images_per_second": 0.168, "prompt_tokens": 37, "response_tokens_est": 28, "n_tiles": 16, "output_text": "A man is playing tennis on a blue court with a yellow ball in the air. There are multiple white chairs in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.7, "ram_available_mb": 41879.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21024.0, "ram_available_mb": 41816.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.6}, "power_stats": {"power_gpu_soc_mean_watts": 22.933, "power_cpu_cv_mean_watts": 1.337, "power_sys_5v0_mean_watts": 8.512, "gpu_utilization_percent_mean": 75.6, "power_watts_avg": 22.933, "energy_joules_est": 136.42, "duration_seconds": 5.949, "sample_count": 50}, "timestamp": "2026-01-26T12:10:50.991022"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6230.968, "latencies_ms": [6230.968], "images_per_second": 0.16, "prompt_tokens": 36, "response_tokens_est": 32, "n_tiles": 16, "output_text": "The tennis player is wearing a white sleeveless shirt and black shorts. The court is blue with white lines marking the boundaries.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21024.0, "ram_available_mb": 41816.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21024.7, "ram_available_mb": 41816.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.906}, "power_stats": {"power_gpu_soc_mean_watts": 21.836, "power_cpu_cv_mean_watts": 1.457, "power_sys_5v0_mean_watts": 8.563, "gpu_utilization_percent_mean": 74.906, "power_watts_avg": 21.836, "energy_joules_est": 136.07, "duration_seconds": 6.232, "sample_count": 53}, "timestamp": "2026-01-26T12:10:59.265749"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11567.467, "latencies_ms": [11567.467], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a serene indoor setting. At the center of the scene is a **glass vase** with a **red and orange** color scheme, filled with water and adorned with **yellow flowers**. The vase is placed on a **green glass coaster**, which is situated on a **wooden table**. To the left of the vase, there", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 20952.0, "ram_available_mb": 41888.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20986.6, "ram_available_mb": 41854.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.38}, "power_stats": {"power_gpu_soc_mean_watts": 19.376, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 70.38, "power_watts_avg": 19.376, "energy_joules_est": 224.14, "duration_seconds": 11.568, "sample_count": 100}, "timestamp": "2026-01-26T12:11:12.886733"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8017.4, "latencies_ms": [8017.4], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "candle: 1, glass vase: 1, plate: 1, string lights: multiple, wooden frame: 1, wall: 1, table: 1, candle holder: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20986.6, "ram_available_mb": 41854.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21020.8, "ram_available_mb": 41820.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.162}, "power_stats": {"power_gpu_soc_mean_watts": 21.036, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.575, "gpu_utilization_percent_mean": 74.162, "power_watts_avg": 21.036, "energy_joules_est": 168.67, "duration_seconds": 8.018, "sample_count": 68}, "timestamp": "2026-01-26T12:11:22.939014"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8266.697, "latencies_ms": [8266.697], "images_per_second": 0.121, "prompt_tokens": 44, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The glass vase is placed in the foreground on the right side of the image, while the white candle is in the foreground on the left side. The candle is positioned closer to the viewer than the vase.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21020.8, "ram_available_mb": 41820.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20970.4, "ram_available_mb": 41870.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.286}, "power_stats": {"power_gpu_soc_mean_watts": 20.61, "power_cpu_cv_mean_watts": 1.67, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 72.286, "power_watts_avg": 20.61, "energy_joules_est": 170.39, "duration_seconds": 8.267, "sample_count": 70}, "timestamp": "2026-01-26T12:11:33.218780"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7996.585, "latencies_ms": [7996.585], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a decorative setting with a glass vase containing orange flowers placed on a glass coaster. The scene is illuminated by a string of white fairy lights, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20970.4, "ram_available_mb": 41870.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20974.9, "ram_available_mb": 41866.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.309}, "power_stats": {"power_gpu_soc_mean_watts": 21.115, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.57, "gpu_utilization_percent_mean": 73.309, "power_watts_avg": 21.115, "energy_joules_est": 168.86, "duration_seconds": 7.997, "sample_count": 68}, "timestamp": "2026-01-26T12:11:43.236168"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10746.066, "latencies_ms": [10746.066], "images_per_second": 0.093, "prompt_tokens": 36, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image features a clear glass vase with a red interior, placed on a green coaster. The vase contains orange flowers, and there is a white candle to the left of the vase. The background is a plain white wall, and there are string lights on either side of the vase, creating a warm and cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20974.9, "ram_available_mb": 41866.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20975.1, "ram_available_mb": 41865.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.044}, "power_stats": {"power_gpu_soc_mean_watts": 19.564, "power_cpu_cv_mean_watts": 1.839, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 71.044, "power_watts_avg": 19.564, "energy_joules_est": 210.25, "duration_seconds": 10.747, "sample_count": 91}, "timestamp": "2026-01-26T12:11:55.999593"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11606.297, "latencies_ms": [11606.297], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a man in a room with a brown couch. He is wearing a black shirt and is bending over, possibly adjusting something on the couch. The room is equipped with a tripod and a light, suggesting that the man might be preparing for a photoshoot or a video recording. The man's face is blurred", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 20975.1, "ram_available_mb": 41865.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20975.6, "ram_available_mb": 41865.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.398}, "power_stats": {"power_gpu_soc_mean_watts": 19.343, "power_cpu_cv_mean_watts": 1.879, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 70.398, "power_watts_avg": 19.343, "energy_joules_est": 224.51, "duration_seconds": 11.607, "sample_count": 98}, "timestamp": "2026-01-26T12:12:09.632729"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10048.792, "latencies_ms": [10048.792], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "- Couch: 1\n\n- Jacket: 1\n\n- Microphone stand: 1\n\n- Cable: 1\n\n- Suitcase: 1\n\n- Lighting equipment: 1\n\n- Chair: 1\n\n- Table: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20975.6, "ram_available_mb": 41865.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20976.8, "ram_available_mb": 41864.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.141}, "power_stats": {"power_gpu_soc_mean_watts": 20.093, "power_cpu_cv_mean_watts": 1.747, "power_sys_5v0_mean_watts": 8.588, "gpu_utilization_percent_mean": 72.141, "power_watts_avg": 20.093, "energy_joules_est": 201.92, "duration_seconds": 10.049, "sample_count": 85}, "timestamp": "2026-01-26T12:12:21.713637"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9858.466, "latencies_ms": [9858.466], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "In the foreground, there is a person bending over, seemingly interacting with the equipment. Behind them, another person is standing and appears to be operating a lighting setup. The lighting equipment is positioned to the left of the scene, while the person operating it is to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20976.8, "ram_available_mb": 41864.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21016.5, "ram_available_mb": 41824.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.024}, "power_stats": {"power_gpu_soc_mean_watts": 19.875, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 71.024, "power_watts_avg": 19.875, "energy_joules_est": 195.95, "duration_seconds": 9.859, "sample_count": 84}, "timestamp": "2026-01-26T12:12:33.589709"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8606.045, "latencies_ms": [8606.045], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "In a room with a beige carpet, a man is bending over a couch while another man stands behind him, holding a white umbrella. There is a suitcase on the floor and a coat hanging on a chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20960.4, "ram_available_mb": 41880.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21012.6, "ram_available_mb": 41828.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.342}, "power_stats": {"power_gpu_soc_mean_watts": 20.525, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.561, "gpu_utilization_percent_mean": 73.342, "power_watts_avg": 20.525, "energy_joules_est": 176.65, "duration_seconds": 8.607, "sample_count": 73}, "timestamp": "2026-01-26T12:12:44.221131"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7228.766, "latencies_ms": [7228.766], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image shows a person in a room with a wooden floor and a beige carpet. There is a white umbrella in the background and a brown jacket hanging on a chair.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21012.6, "ram_available_mb": 41828.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20980.7, "ram_available_mb": 41860.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.098}, "power_stats": {"power_gpu_soc_mean_watts": 21.116, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 8.602, "gpu_utilization_percent_mean": 74.098, "power_watts_avg": 21.116, "energy_joules_est": 152.66, "duration_seconds": 7.229, "sample_count": 61}, "timestamp": "2026-01-26T12:12:53.473645"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12376.0, "latencies_ms": [12376.0], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photo, a woman is the central figure, exuding a sense of joy and relaxation. She is adorned in a wide-brimmed hat that casts a shadow over her face, adding an air of mystery to her persona. Her eyes are closed, and a smile graces her face, suggesting she is savoring the moment. In her right", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20980.7, "ram_available_mb": 41860.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.0, "ram_used_mb": 21010.6, "ram_available_mb": 41830.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.393}, "power_stats": {"power_gpu_soc_mean_watts": 21.49, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 8.931, "gpu_utilization_percent_mean": 73.393, "power_watts_avg": 21.49, "energy_joules_est": 265.97, "duration_seconds": 12.377, "sample_count": 107}, "timestamp": "2026-01-26T12:13:07.889937"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9342.985, "latencies_ms": [9342.985], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "hat: 1, earring: 1, necklace: 1, striped top: 1, cigarette: 1, hand: 1, bracelet: 1, smile: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20946.2, "ram_available_mb": 41894.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.6, "ram_used_mb": 20942.6, "ram_available_mb": 41898.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.354}, "power_stats": {"power_gpu_soc_mean_watts": 23.0, "power_cpu_cv_mean_watts": 1.611, "power_sys_5v0_mean_watts": 8.741, "gpu_utilization_percent_mean": 77.354, "power_watts_avg": 23.0, "energy_joules_est": 214.9, "duration_seconds": 9.344, "sample_count": 79}, "timestamp": "2026-01-26T12:13:19.264300"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12598.253, "latencies_ms": [12598.253], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The woman is positioned in the foreground, wearing a wide-brimmed hat that extends to the left side of the frame, suggesting she is facing towards the right. The cigarette is held near her mouth, slightly to the right side, indicating she is smoking. The background is plain and unobtrusive, putting the focus entirely on her.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20942.6, "ram_available_mb": 41898.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.3, "ram_used_mb": 21033.7, "ram_available_mb": 41807.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.551}, "power_stats": {"power_gpu_soc_mean_watts": 21.642, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.868, "gpu_utilization_percent_mean": 73.551, "power_watts_avg": 21.642, "energy_joules_est": 272.66, "duration_seconds": 12.599, "sample_count": 107}, "timestamp": "2026-01-26T12:13:33.878992"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11210.326, "latencies_ms": [11210.326], "images_per_second": 0.089, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "A woman is seen wearing a wide-brimmed hat and a striped tank top, with a necklace and a bracelet on her wrist, smiling and holding a cigarette in her hand. The image is in black and white, giving it a timeless and classic feel.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20947.3, "ram_available_mb": 41893.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.1, "ram_used_mb": 21018.2, "ram_available_mb": 41822.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.537}, "power_stats": {"power_gpu_soc_mean_watts": 22.237, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 8.807, "gpu_utilization_percent_mean": 75.537, "power_watts_avg": 22.237, "energy_joules_est": 249.3, "duration_seconds": 11.211, "sample_count": 95}, "timestamp": "2026-01-26T12:13:47.150661"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9188.892, "latencies_ms": [9188.892], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image is in black and white, featuring a person wearing a wide-brimmed hat and a striped tank top. The lighting appears to be natural, casting soft shadows on the person's face and hat.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20954.7, "ram_available_mb": 41886.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.5, "ram_used_mb": 21054.5, "ram_available_mb": 41786.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.41}, "power_stats": {"power_gpu_soc_mean_watts": 22.861, "power_cpu_cv_mean_watts": 1.653, "power_sys_5v0_mean_watts": 8.854, "gpu_utilization_percent_mean": 77.41, "power_watts_avg": 22.861, "energy_joules_est": 210.08, "duration_seconds": 9.19, "sample_count": 78}, "timestamp": "2026-01-26T12:13:58.368872"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11590.973, "latencies_ms": [11590.973], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two zebras grazing in a grassy field. One zebra is located on the left side of the field, while the other is on the right side. They are both eating grass and appear to be enjoying their meal. The field is surrounded by a rock wall, providing a natural boundary for the zebras. The scene captures", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20965.8, "ram_available_mb": 41875.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20961.6, "ram_available_mb": 41879.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.879}, "power_stats": {"power_gpu_soc_mean_watts": 19.357, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 70.879, "power_watts_avg": 19.357, "energy_joules_est": 224.38, "duration_seconds": 11.592, "sample_count": 99}, "timestamp": "2026-01-26T12:14:12.002239"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7788.028, "latencies_ms": [7788.028], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "zebra: 2, rock: 1, tree: 1, grass: many, dirt path: 1, stone wall: 1, building: 1, shadow: 2", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20961.6, "ram_available_mb": 41879.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20956.8, "ram_available_mb": 41884.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.197}, "power_stats": {"power_gpu_soc_mean_watts": 21.248, "power_cpu_cv_mean_watts": 1.571, "power_sys_5v0_mean_watts": 8.587, "gpu_utilization_percent_mean": 74.197, "power_watts_avg": 21.248, "energy_joules_est": 165.49, "duration_seconds": 7.789, "sample_count": 66}, "timestamp": "2026-01-26T12:14:21.829894"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10304.613, "latencies_ms": [10304.613], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, there are two zebras grazing on the grass. One zebra is positioned slightly behind the other, with the one in the back being closer to the left side of the image. The background features a rocky wall and some trees, creating a natural enclosure for the zebras.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20956.8, "ram_available_mb": 41884.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20959.7, "ram_available_mb": 41881.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.466}, "power_stats": {"power_gpu_soc_mean_watts": 19.703, "power_cpu_cv_mean_watts": 1.838, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 70.466, "power_watts_avg": 19.703, "energy_joules_est": 203.05, "duration_seconds": 10.305, "sample_count": 88}, "timestamp": "2026-01-26T12:14:34.190719"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7074.33, "latencies_ms": [7074.33], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "Two zebras are grazing in a grassy enclosure with a rocky wall and trees in the background. The sun is shining, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20959.7, "ram_available_mb": 41881.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20950.8, "ram_available_mb": 41890.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.6}, "power_stats": {"power_gpu_soc_mean_watts": 21.856, "power_cpu_cv_mean_watts": 1.481, "power_sys_5v0_mean_watts": 8.552, "gpu_utilization_percent_mean": 74.6, "power_watts_avg": 21.856, "energy_joules_est": 154.63, "duration_seconds": 7.075, "sample_count": 60}, "timestamp": "2026-01-26T12:14:43.322096"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7476.09, "latencies_ms": [7476.09], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image features two zebras grazing in a grassy field with a backdrop of trees and a rocky wall. The lighting is natural and bright, suggesting it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20950.8, "ram_available_mb": 41890.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21016.8, "ram_available_mb": 41824.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.603}, "power_stats": {"power_gpu_soc_mean_watts": 20.93, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.601, "gpu_utilization_percent_mean": 72.603, "power_watts_avg": 20.93, "energy_joules_est": 156.49, "duration_seconds": 7.477, "sample_count": 63}, "timestamp": "2026-01-26T12:14:52.853820"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11585.302, "latencies_ms": [11585.302], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of urban decay and neglect. Dominating the frame is a fire hydrant, its once vibrant orange color now faded to a dull brown, a testament to the passage of time. The hydrant, showing signs of rust and wear, stands on a sidewalk, its once gleaming surface now marred by the elements. \n\nA", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 20952.3, "ram_available_mb": 41888.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20947.8, "ram_available_mb": 41893.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.02}, "power_stats": {"power_gpu_soc_mean_watts": 19.236, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 70.02, "power_watts_avg": 19.236, "energy_joules_est": 222.87, "duration_seconds": 11.586, "sample_count": 99}, "timestamp": "2026-01-26T12:15:06.474578"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7551.174, "latencies_ms": [7551.174], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "Fire hydrant: 1, chain: 2, cap: 1, step: 1, stone: 1, plant: 1, leaf: 1, flower: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20947.8, "ram_available_mb": 41893.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20946.7, "ram_available_mb": 41894.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.312}, "power_stats": {"power_gpu_soc_mean_watts": 21.616, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 74.312, "power_watts_avg": 21.616, "energy_joules_est": 163.24, "duration_seconds": 7.552, "sample_count": 64}, "timestamp": "2026-01-26T12:15:16.062938"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11160.067, "latencies_ms": [11160.067], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The fire hydrant is positioned in the foreground of the image, appearing large and detailed. It is situated on the left side of the frame, with a blurred background that includes a stone wall and some greenery. The hydrant is also the main focus, with other elements like the chain and cap being secondary and placed in the mid-ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20946.7, "ram_available_mb": 41894.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21009.6, "ram_available_mb": 41831.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.635}, "power_stats": {"power_gpu_soc_mean_watts": 19.6, "power_cpu_cv_mean_watts": 1.876, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 71.635, "power_watts_avg": 19.6, "energy_joules_est": 218.75, "duration_seconds": 11.161, "sample_count": 96}, "timestamp": "2026-01-26T12:15:29.249191"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6431.156, "latencies_ms": [6431.156], "images_per_second": 0.155, "prompt_tokens": 37, "response_tokens_est": 32, "n_tiles": 16, "output_text": "The image shows an old, rusted fire hydrant on a sidewalk. It is located next to a wall with a painted green hedge design.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21009.6, "ram_available_mb": 41831.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 21049.6, "ram_available_mb": 41791.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.815}, "power_stats": {"power_gpu_soc_mean_watts": 22.533, "power_cpu_cv_mean_watts": 1.379, "power_sys_5v0_mean_watts": 8.534, "gpu_utilization_percent_mean": 75.815, "power_watts_avg": 22.533, "energy_joules_est": 144.93, "duration_seconds": 6.432, "sample_count": 54}, "timestamp": "2026-01-26T12:15:37.711313"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8303.269, "latencies_ms": [8303.269], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The fire hydrant is a faded orange color with a black top and is situated on a concrete step. It appears to be an older model, with visible rust and wear, indicating it has been in use for a considerable amount of time.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20964.2, "ram_available_mb": 41876.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20962.0, "ram_available_mb": 41878.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.915}, "power_stats": {"power_gpu_soc_mean_watts": 20.895, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 70.915, "power_watts_avg": 20.895, "energy_joules_est": 173.51, "duration_seconds": 8.304, "sample_count": 71}, "timestamp": "2026-01-26T12:15:48.048556"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11576.632, "latencies_ms": [11576.632], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two brown bears walking on a dirt road. The bear in the foreground is walking towards the camera, while the other bear is walking away from the camera. The road they are walking on is surrounded by dry grass and shrubs. The bears appear to be in a natural habitat, possibly a forest or a wildlife reserve. The image does not contain", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20962.0, "ram_available_mb": 41878.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21003.0, "ram_available_mb": 41837.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.41}, "power_stats": {"power_gpu_soc_mean_watts": 19.297, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.679, "gpu_utilization_percent_mean": 70.41, "power_watts_avg": 19.297, "energy_joules_est": 223.41, "duration_seconds": 11.577, "sample_count": 100}, "timestamp": "2026-01-26T12:16:01.653896"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7431.361, "latencies_ms": [7431.361], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "bear: 2, road: 1, vegetation: 1, ground: 1, sky: 1, sun: 1, shadow: 1, rocks: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21003.0, "ram_available_mb": 41837.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21019.7, "ram_available_mb": 41821.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.841}, "power_stats": {"power_gpu_soc_mean_watts": 21.494, "power_cpu_cv_mean_watts": 1.544, "power_sys_5v0_mean_watts": 8.579, "gpu_utilization_percent_mean": 73.841, "power_watts_avg": 21.494, "energy_joules_est": 159.74, "duration_seconds": 7.432, "sample_count": 63}, "timestamp": "2026-01-26T12:16:11.111453"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9287.397, "latencies_ms": [9287.397], "images_per_second": 0.108, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "In the foreground, there is a large brown bear walking towards the camera, with another bear slightly behind it and to the left, both positioned on a rocky terrain. The background features a barren landscape with dry grass and shrubs, suggesting a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21019.7, "ram_available_mb": 41821.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20968.9, "ram_available_mb": 41872.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.962}, "power_stats": {"power_gpu_soc_mean_watts": 20.137, "power_cpu_cv_mean_watts": 1.753, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 70.962, "power_watts_avg": 20.137, "energy_joules_est": 187.03, "duration_seconds": 9.288, "sample_count": 79}, "timestamp": "2026-01-26T12:16:22.414431"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7241.796, "latencies_ms": [7241.796], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "Two brown bears are walking on a rocky terrain with dry grass in the background. The bears appear to be in a natural habitat, possibly a forest or wilderness area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20968.9, "ram_available_mb": 41872.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21019.7, "ram_available_mb": 41821.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.623}, "power_stats": {"power_gpu_soc_mean_watts": 21.379, "power_cpu_cv_mean_watts": 1.522, "power_sys_5v0_mean_watts": 8.543, "gpu_utilization_percent_mean": 74.623, "power_watts_avg": 21.379, "energy_joules_est": 154.84, "duration_seconds": 7.242, "sample_count": 61}, "timestamp": "2026-01-26T12:16:31.708475"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7553.664, "latencies_ms": [7553.664], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image features two brown bears in a natural setting with a clear sky. The bears are walking on a dirt ground with sparse vegetation, and the lighting suggests it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20939.8, "ram_available_mb": 41901.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20949.7, "ram_available_mb": 41891.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.516}, "power_stats": {"power_gpu_soc_mean_watts": 20.89, "power_cpu_cv_mean_watts": 1.626, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 73.516, "power_watts_avg": 20.89, "energy_joules_est": 157.81, "duration_seconds": 7.554, "sample_count": 64}, "timestamp": "2026-01-26T12:16:41.311636"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11667.002, "latencies_ms": [11667.002], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a lush garden, a young child with blonde hair is engrossed in a playful activity. The child, dressed in a crisp white shirt and beige pants, is kneeling on the ground, their hands reaching into a black bucket filled with gray gravel. The bucket, which is placed on the ground, is the center of the child'", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20949.7, "ram_available_mb": 41891.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20961.4, "ram_available_mb": 41879.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.378}, "power_stats": {"power_gpu_soc_mean_watts": 18.975, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 70.378, "power_watts_avg": 18.975, "energy_joules_est": 221.39, "duration_seconds": 11.668, "sample_count": 98}, "timestamp": "2026-01-26T12:16:55.010102"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7573.997, "latencies_ms": [7573.997], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "bucket: 1, shovel: 1, child: 1, tie: 1, leaves: many, ground: 1, sunlight: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.4, "ram_available_mb": 41879.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21004.8, "ram_available_mb": 41836.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.094}, "power_stats": {"power_gpu_soc_mean_watts": 21.334, "power_cpu_cv_mean_watts": 1.545, "power_sys_5v0_mean_watts": 8.543, "gpu_utilization_percent_mean": 73.094, "power_watts_avg": 21.334, "energy_joules_est": 161.6, "duration_seconds": 7.575, "sample_count": 64}, "timestamp": "2026-01-26T12:17:04.619082"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11666.8, "latencies_ms": [11666.8], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a young child is kneeling on the ground, reaching into a metal tub filled with dark material, likely soil or sand. The child is positioned to the left of the image, with their body facing the camera. In the background, there is a dense arrangement of dark green leaves, possibly from a hedge or shrub, which provides a contrasting backdrop to", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21004.8, "ram_available_mb": 41836.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20954.6, "ram_available_mb": 41886.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.17}, "power_stats": {"power_gpu_soc_mean_watts": 19.36, "power_cpu_cv_mean_watts": 1.873, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 71.17, "power_watts_avg": 19.36, "energy_joules_est": 225.88, "duration_seconds": 11.668, "sample_count": 100}, "timestamp": "2026-01-26T12:17:18.310845"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8697.752, "latencies_ms": [8697.752], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A young child is playing in a large metal tub filled with black sand, surrounded by green foliage. The child is wearing a white shirt and a colorful tie, and appears to be enjoying the sensory experience of the sand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20954.6, "ram_available_mb": 41886.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21006.1, "ram_available_mb": 41834.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.986}, "power_stats": {"power_gpu_soc_mean_watts": 20.831, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 8.575, "gpu_utilization_percent_mean": 71.986, "power_watts_avg": 20.831, "energy_joules_est": 181.2, "duration_seconds": 8.698, "sample_count": 74}, "timestamp": "2026-01-26T12:17:29.027774"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11656.751, "latencies_ms": [11656.751], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a child with light-colored hair, wearing a white shirt and a colorful tie with a pattern of leaves and the letter \"B\". The child is crouched down and appears to be playing with a black substance in a metal tub, with a shovel nearby. The lighting is natural and bright, suggesting that the photo was taken outdoors", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21006.1, "ram_available_mb": 41834.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21007.0, "ram_available_mb": 41833.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.566}, "power_stats": {"power_gpu_soc_mean_watts": 19.326, "power_cpu_cv_mean_watts": 1.872, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 70.566, "power_watts_avg": 19.326, "energy_joules_est": 225.29, "duration_seconds": 11.657, "sample_count": 99}, "timestamp": "2026-01-26T12:17:42.704006"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11591.954, "latencies_ms": [11591.954], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a desert landscape, where a small table is set up in the middle of the sandy terrain. The table is adorned with various items, including a bottle of beer, a can of soda, and a bouquet of yellow flowers. A teddy bear, dressed in a red shirt, is seated on the table, adding a", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 21007.0, "ram_available_mb": 41833.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.0, "ram_used_mb": 21014.2, "ram_available_mb": 41826.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.224}, "power_stats": {"power_gpu_soc_mean_watts": 19.37, "power_cpu_cv_mean_watts": 1.883, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 70.224, "power_watts_avg": 19.37, "energy_joules_est": 224.55, "duration_seconds": 11.593, "sample_count": 98}, "timestamp": "2026-01-26T12:17:56.331729"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7301.681, "latencies_ms": [7301.681], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1, object: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20942.5, "ram_available_mb": 41898.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 20991.3, "ram_available_mb": 41849.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.081}, "power_stats": {"power_gpu_soc_mean_watts": 21.596, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 8.572, "gpu_utilization_percent_mean": 74.081, "power_watts_avg": 21.596, "energy_joules_est": 157.7, "duration_seconds": 7.302, "sample_count": 62}, "timestamp": "2026-01-26T12:18:05.659332"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10440.44, "latencies_ms": [10440.44], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a table with various items on it, positioned slightly to the right of the center of the image. The background consists of a vast, open desert landscape that extends to the horizon. The table and its contents are in the middle ground, creating a sense of depth against the expansive backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20991.3, "ram_available_mb": 41849.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 20998.6, "ram_available_mb": 41842.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.798}, "power_stats": {"power_gpu_soc_mean_watts": 19.655, "power_cpu_cv_mean_watts": 2.074, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 71.798, "power_watts_avg": 19.655, "energy_joules_est": 205.22, "duration_seconds": 10.441, "sample_count": 89}, "timestamp": "2026-01-26T12:18:18.119270"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10743.384, "latencies_ms": [10743.384], "images_per_second": 0.093, "prompt_tokens": 37, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image depicts a desert-like setting with a small table holding various items, including a bottle of beer and a can of soda. The table is surrounded by a few stuffed animals and a couch with a red cross symbol on it, suggesting a makeshift camp or gathering spot in a remote location.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20998.6, "ram_available_mb": 41842.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.7, "ram_used_mb": 21038.3, "ram_available_mb": 41802.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.615}, "power_stats": {"power_gpu_soc_mean_watts": 19.824, "power_cpu_cv_mean_watts": 1.949, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 71.615, "power_watts_avg": 19.824, "energy_joules_est": 212.99, "duration_seconds": 10.744, "sample_count": 91}, "timestamp": "2026-01-26T12:18:30.890438"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8033.04, "latencies_ms": [8033.04], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image depicts a desert landscape with a clear sky and bright sunlight casting shadows on the ground. A table with various items on it, including a bottle and a can, is set up in the foreground.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21038.3, "ram_available_mb": 41802.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 21064.9, "ram_available_mb": 41776.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.681}, "power_stats": {"power_gpu_soc_mean_watts": 20.679, "power_cpu_cv_mean_watts": 1.752, "power_sys_5v0_mean_watts": 8.629, "gpu_utilization_percent_mean": 72.681, "power_watts_avg": 20.679, "energy_joules_est": 166.13, "duration_seconds": 8.034, "sample_count": 69}, "timestamp": "2026-01-26T12:18:40.961567"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12392.355, "latencies_ms": [12392.355], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a harbor with several boats docked at a pier. There are two boats prominently visible in the scene, one on the left side and another on the right side of the image. The boats are of different sizes and are docked close to each other.\n\nIn addition to the boats, there are a few people present in the harbor. One person is standing", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 21064.9, "ram_available_mb": 41776.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 21065.8, "ram_available_mb": 41775.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.544, "power_cpu_cv_mean_watts": 1.873, "power_sys_5v0_mean_watts": 8.916, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 21.544, "energy_joules_est": 266.99, "duration_seconds": 12.393, "sample_count": 106}, "timestamp": "2026-01-26T12:18:55.411848"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8633.822, "latencies_ms": [8633.822], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "boat: 2\nfish: 0\nperson: 2\nmountain: 1\nhouse: 0\nboats: 2\nwater: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20960.9, "ram_available_mb": 41880.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 21022.8, "ram_available_mb": 41818.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.73}, "power_stats": {"power_gpu_soc_mean_watts": 23.29, "power_cpu_cv_mean_watts": 1.439, "power_sys_5v0_mean_watts": 8.755, "gpu_utilization_percent_mean": 78.73, "power_watts_avg": 23.29, "energy_joules_est": 201.1, "duration_seconds": 8.634, "sample_count": 74}, "timestamp": "2026-01-26T12:19:06.075726"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12710.111, "latencies_ms": [12710.111], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two boats docked at a pier, with one boat appearing larger and more detailed than the other. The larger boat is positioned to the left of the smaller boat. In the background, there are more boats on the water and a hill with houses on it. The pier extends from the bottom right corner of the image towards the center, where the boats are dock", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21022.8, "ram_available_mb": 41818.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21033.3, "ram_available_mb": 41807.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.148}, "power_stats": {"power_gpu_soc_mean_watts": 21.594, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.869, "gpu_utilization_percent_mean": 72.148, "power_watts_avg": 21.594, "energy_joules_est": 274.47, "duration_seconds": 12.711, "sample_count": 108}, "timestamp": "2026-01-26T12:19:20.817453"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9921.955, "latencies_ms": [9921.955], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image depicts a serene harbor scene with several boats docked at a pier. The boats are moored to the pier, and there are a few people visible on the pier, possibly attending to their boats or enjoying the view.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21033.3, "ram_available_mb": 41807.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21034.0, "ram_available_mb": 41806.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.734, "power_cpu_cv_mean_watts": 1.534, "power_sys_5v0_mean_watts": 8.77, "gpu_utilization_percent_mean": 77.0, "power_watts_avg": 22.734, "energy_joules_est": 225.58, "duration_seconds": 9.923, "sample_count": 84}, "timestamp": "2026-01-26T12:19:32.771847"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11162.112, "latencies_ms": [11162.112], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image features a serene harbor scene with boats docked at a wooden pier. The boats are painted in various colors, including green and white, and are equipped with fishing gear. The sky is overcast, and the water appears calm, reflecting the muted colors of the surroundings.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21034.0, "ram_available_mb": 41806.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21068.2, "ram_available_mb": 41772.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.526}, "power_stats": {"power_gpu_soc_mean_watts": 22.07, "power_cpu_cv_mean_watts": 1.686, "power_sys_5v0_mean_watts": 8.872, "gpu_utilization_percent_mean": 74.526, "power_watts_avg": 22.07, "energy_joules_est": 246.36, "duration_seconds": 11.163, "sample_count": 95}, "timestamp": "2026-01-26T12:19:45.955437"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11592.01, "latencies_ms": [11592.01], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is captured in the act of taking a bite out of a hot dog. The hot dog, with its golden-brown bun, is held in the person's right hand, and their left hand is holding a pair of chopsticks. The person's mouth is open wide, ready to take a bite. The background is blur", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20945.7, "ram_available_mb": 41895.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20954.4, "ram_available_mb": 41886.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.949}, "power_stats": {"power_gpu_soc_mean_watts": 19.35, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.661, "gpu_utilization_percent_mean": 70.949, "power_watts_avg": 19.35, "energy_joules_est": 224.32, "duration_seconds": 11.593, "sample_count": 99}, "timestamp": "2026-01-26T12:19:59.576845"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7541.772, "latencies_ms": [7541.772], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "face: 1, mouth: 1, tongue: 1, chin: 1, nose: 1, ear: 1, eye: 1, cheek: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20954.4, "ram_available_mb": 41886.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21001.7, "ram_available_mb": 41839.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.938}, "power_stats": {"power_gpu_soc_mean_watts": 21.435, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 74.938, "power_watts_avg": 21.435, "energy_joules_est": 161.67, "duration_seconds": 7.542, "sample_count": 64}, "timestamp": "2026-01-26T12:20:09.150605"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9329.699, "latencies_ms": [9329.699], "images_per_second": 0.107, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The person is in the foreground, holding a hot dog with their right hand, which is near their mouth, indicating they are about to eat it. The background is blurred, but it appears to be an outdoor setting with lights that could suggest a street or public area.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21001.7, "ram_available_mb": 41839.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20949.6, "ram_available_mb": 41891.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.316}, "power_stats": {"power_gpu_soc_mean_watts": 19.879, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.617, "gpu_utilization_percent_mean": 70.316, "power_watts_avg": 19.879, "energy_joules_est": 185.48, "duration_seconds": 9.33, "sample_count": 79}, "timestamp": "2026-01-26T12:20:20.509138"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8110.277, "latencies_ms": [8110.277], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A person is seen holding a hot dog with their mouth wide open, as if they are about to take a bite. The background is blurred, but it appears to be an outdoor setting with some lights visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20949.6, "ram_available_mb": 41891.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21000.0, "ram_available_mb": 41840.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.855}, "power_stats": {"power_gpu_soc_mean_watts": 21.001, "power_cpu_cv_mean_watts": 1.63, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 72.855, "power_watts_avg": 21.001, "energy_joules_est": 170.34, "duration_seconds": 8.111, "sample_count": 69}, "timestamp": "2026-01-26T12:20:30.639199"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8020.712, "latencies_ms": [8020.712], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image features a person with short hair, wearing a dark jacket, holding a hot dog with a bite taken out of it. The lighting is dim with a warm tone, suggesting an evening or nighttime setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.0, "ram_available_mb": 41900.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20995.5, "ram_available_mb": 41845.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.059}, "power_stats": {"power_gpu_soc_mean_watts": 20.584, "power_cpu_cv_mean_watts": 1.637, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 73.059, "power_watts_avg": 20.584, "energy_joules_est": 165.11, "duration_seconds": 8.021, "sample_count": 68}, "timestamp": "2026-01-26T12:20:40.701109"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11571.171, "latencies_ms": [11571.171], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two individuals standing in a room. The person on the left is wearing a gray dress and has blonde hair. The person on the right is dressed in a black suit and is holding a martini glass. The background of the image features a white wall and a door. The image does not contain any text. The relative positions of the objects are such that the", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 20995.5, "ram_available_mb": 41845.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20960.7, "ram_available_mb": 41880.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.27}, "power_stats": {"power_gpu_soc_mean_watts": 19.158, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 70.27, "power_watts_avg": 19.158, "energy_joules_est": 221.69, "duration_seconds": 11.572, "sample_count": 100}, "timestamp": "2026-01-26T12:20:54.298950"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7582.281, "latencies_ms": [7582.281], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "woman: 1, man: 1, dress: 1, wine glass: 1, room: 1, door: 1, curtain: 1, wall: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20960.7, "ram_available_mb": 41880.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20939.7, "ram_available_mb": 41901.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.465, "power_cpu_cv_mean_watts": 1.546, "power_sys_5v0_mean_watts": 8.562, "gpu_utilization_percent_mean": 74.0, "power_watts_avg": 21.465, "energy_joules_est": 162.77, "duration_seconds": 7.583, "sample_count": 65}, "timestamp": "2026-01-26T12:21:03.927939"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9482.456, "latencies_ms": [9482.456], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The man is standing to the right of the woman, slightly closer to the camera, creating a sense of depth in the image. The woman is positioned to the left of the man, and both are standing in the foreground with a blurred background that suggests an indoor setting.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20939.7, "ram_available_mb": 41901.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21000.8, "ram_available_mb": 41840.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.012}, "power_stats": {"power_gpu_soc_mean_watts": 20.084, "power_cpu_cv_mean_watts": 1.749, "power_sys_5v0_mean_watts": 8.618, "gpu_utilization_percent_mean": 71.012, "power_watts_avg": 20.084, "energy_joules_est": 190.46, "duration_seconds": 9.483, "sample_count": 81}, "timestamp": "2026-01-26T12:21:15.448317"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8490.206, "latencies_ms": [8490.206], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A man and a woman are standing together in a room, with the man holding a glass of wine. The woman is wearing a grey dress and the man is wearing a black suit with a white shirt and a maroon tie.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21000.8, "ram_available_mb": 41840.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21001.3, "ram_available_mb": 41839.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.556}, "power_stats": {"power_gpu_soc_mean_watts": 20.777, "power_cpu_cv_mean_watts": 1.634, "power_sys_5v0_mean_watts": 8.569, "gpu_utilization_percent_mean": 72.556, "power_watts_avg": 20.777, "energy_joules_est": 176.42, "duration_seconds": 8.491, "sample_count": 72}, "timestamp": "2026-01-26T12:21:25.969126"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8210.362, "latencies_ms": [8210.362], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a man and a woman in an indoor setting with soft, warm lighting. The man is wearing a dark suit with a white shirt and a dark tie, while the woman is dressed in a grey dress.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21001.3, "ram_available_mb": 41839.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20977.7, "ram_available_mb": 41863.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.732}, "power_stats": {"power_gpu_soc_mean_watts": 20.716, "power_cpu_cv_mean_watts": 1.663, "power_sys_5v0_mean_watts": 8.619, "gpu_utilization_percent_mean": 72.732, "power_watts_avg": 20.716, "energy_joules_est": 170.1, "duration_seconds": 8.211, "sample_count": 71}, "timestamp": "2026-01-26T12:21:36.197973"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12335.9, "latencies_ms": [12335.9], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a blue shelf with various objects on it, including a vase, cups, and a bowl. The shelf is placed on a dirt ground, and there are other items scattered around the area. A chair can be seen in the background, and a dining table is also present in the scene.\n\nThere are multiple cups placed on the shelf", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20977.7, "ram_available_mb": 41863.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20942.5, "ram_available_mb": 41898.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.879}, "power_stats": {"power_gpu_soc_mean_watts": 21.32, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.825, "gpu_utilization_percent_mean": 72.879, "power_watts_avg": 21.32, "energy_joules_est": 263.01, "duration_seconds": 12.336, "sample_count": 107}, "timestamp": "2026-01-26T12:21:50.558654"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8154.788, "latencies_ms": [8154.788], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "object: 1, object: 2, object: 3, object: 4, object: 5, object: 6, object: 7, object: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20942.5, "ram_available_mb": 41898.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20973.4, "ram_available_mb": 41867.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.899}, "power_stats": {"power_gpu_soc_mean_watts": 23.172, "power_cpu_cv_mean_watts": 1.415, "power_sys_5v0_mean_watts": 8.677, "gpu_utilization_percent_mean": 75.899, "power_watts_avg": 23.172, "energy_joules_est": 188.98, "duration_seconds": 8.155, "sample_count": 69}, "timestamp": "2026-01-26T12:22:00.727184"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10908.323, "latencies_ms": [10908.323], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, there is a blue shelf with various objects placed on it, including a silver teapot and a small blue box. Behind the shelf, there is a wooden chair and a white metal stand. To the right of the shelf, there is a green table with a wooden object on top.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 20973.4, "ram_available_mb": 41867.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21018.1, "ram_available_mb": 41822.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.196}, "power_stats": {"power_gpu_soc_mean_watts": 21.77, "power_cpu_cv_mean_watts": 1.749, "power_sys_5v0_mean_watts": 8.842, "gpu_utilization_percent_mean": 74.196, "power_watts_avg": 21.77, "energy_joules_est": 237.49, "duration_seconds": 10.909, "sample_count": 92}, "timestamp": "2026-01-26T12:22:13.649199"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10338.532, "latencies_ms": [10338.532], "images_per_second": 0.097, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a blue wooden cabinet with various objects on top, including a vase, cups, and a card. The cabinet is placed on a concrete floor, and there are other furniture pieces and items in the background, suggesting a cluttered and disorganized space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20938.3, "ram_available_mb": 41902.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20974.1, "ram_available_mb": 41866.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.195}, "power_stats": {"power_gpu_soc_mean_watts": 22.201, "power_cpu_cv_mean_watts": 1.61, "power_sys_5v0_mean_watts": 8.74, "gpu_utilization_percent_mean": 75.195, "power_watts_avg": 22.201, "energy_joules_est": 229.54, "duration_seconds": 10.339, "sample_count": 87}, "timestamp": "2026-01-26T12:22:26.027871"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7912.82, "latencies_ms": [7912.82], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image features a blue wooden cabinet with a green top, placed outdoors under natural lighting. Various objects, including a silver teapot, are displayed on the cabinet's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20974.1, "ram_available_mb": 41866.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21033.0, "ram_available_mb": 41807.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.999, "power_cpu_cv_mean_watts": 1.511, "power_sys_5v0_mean_watts": 8.861, "gpu_utilization_percent_mean": 76.0, "power_watts_avg": 22.999, "energy_joules_est": 182.0, "duration_seconds": 7.913, "sample_count": 67}, "timestamp": "2026-01-26T12:22:35.962676"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11585.826, "latencies_ms": [11585.826], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a white plate with several pieces of bread covered in cheese, placed on a dining table. The bread appears to be toasted, and the cheese is melted on top of it. The table also has a keyboard and a mouse, suggesting that the setting might be a workspace or a casual dining area. The focus of the image is on the che", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 21033.0, "ram_available_mb": 41807.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 21037.1, "ram_available_mb": 41803.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.459}, "power_stats": {"power_gpu_soc_mean_watts": 19.32, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 70.459, "power_watts_avg": 19.32, "energy_joules_est": 223.85, "duration_seconds": 11.586, "sample_count": 98}, "timestamp": "2026-01-26T12:22:49.582145"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7896.692, "latencies_ms": [7896.692], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "cheese: 5\ncracker: 5\nplate: 1\nkeyboard: 1\nmouse: 1\nbowl: 1\ndrink: 1\nbook: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20933.1, "ram_available_mb": 41907.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.9, "ram_used_mb": 21000.7, "ram_available_mb": 41840.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.418}, "power_stats": {"power_gpu_soc_mean_watts": 21.117, "power_cpu_cv_mean_watts": 1.697, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 72.418, "power_watts_avg": 21.117, "energy_joules_est": 166.77, "duration_seconds": 7.897, "sample_count": 67}, "timestamp": "2026-01-26T12:22:59.494349"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11237.825, "latencies_ms": [11237.825], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground, there are four pieces of bread with cheese on top, arranged in a square pattern on a white plate. In the background, there is a blurred keyboard and a mouse, suggesting that the table is in a room with a computer setup. The bread is in the near foreground, while the keyboard and mouse are in the far background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20938.1, "ram_available_mb": 41902.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 9.4, "ram_used_mb": 21010.6, "ram_available_mb": 41830.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.526}, "power_stats": {"power_gpu_soc_mean_watts": 19.368, "power_cpu_cv_mean_watts": 2.128, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 69.526, "power_watts_avg": 19.368, "energy_joules_est": 217.67, "duration_seconds": 11.239, "sample_count": 95}, "timestamp": "2026-01-26T12:23:12.784780"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8488.023, "latencies_ms": [8488.023], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a white plate with four pieces of bread topped with cheese, placed on a table. In the background, there is a keyboard and a mouse, suggesting that the setting might be a workspace or a computer desk.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21010.6, "ram_available_mb": 41830.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.6, "ram_used_mb": 21017.4, "ram_available_mb": 41823.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.083}, "power_stats": {"power_gpu_soc_mean_watts": 20.616, "power_cpu_cv_mean_watts": 1.824, "power_sys_5v0_mean_watts": 8.571, "gpu_utilization_percent_mean": 73.083, "power_watts_avg": 20.616, "energy_joules_est": 175.0, "duration_seconds": 8.489, "sample_count": 72}, "timestamp": "2026-01-26T12:23:23.298736"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8820.762, "latencies_ms": [8820.762], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows a white plate with four pieces of food, each topped with a white, creamy substance. The lighting in the image is warm and appears to be coming from the left side, casting a soft glow on the plate and its contents.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 20954.8, "ram_available_mb": 41886.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 20956.9, "ram_available_mb": 41884.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.387}, "power_stats": {"power_gpu_soc_mean_watts": 20.335, "power_cpu_cv_mean_watts": 1.879, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 71.387, "power_watts_avg": 20.335, "energy_joules_est": 179.39, "duration_seconds": 8.822, "sample_count": 75}, "timestamp": "2026-01-26T12:23:34.179575"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12415.189, "latencies_ms": [12415.189], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing against a dark gray background. He is dressed in a black suit and tie, and he is wearing glasses. His hair is styled in a messy manner. The man is adjusting his tie, which is adorned with a series of colorful lights. These lights are arranged in a vertical line and are illuminated in a gradient", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 20956.9, "ram_available_mb": 41884.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.5, "ram_used_mb": 21044.2, "ram_available_mb": 41796.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.632}, "power_stats": {"power_gpu_soc_mean_watts": 21.47, "power_cpu_cv_mean_watts": 1.956, "power_sys_5v0_mean_watts": 8.932, "gpu_utilization_percent_mean": 72.632, "power_watts_avg": 21.47, "energy_joules_est": 266.57, "duration_seconds": 12.416, "sample_count": 106}, "timestamp": "2026-01-26T12:23:48.618725"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11524.964, "latencies_ms": [11524.964], "images_per_second": 0.087, "prompt_tokens": 39, "response_tokens_est": 67, "n_tiles": 16, "output_text": "- Man: 1\n\n- Glasses: 1\n\n- Tie: 1\n\n- Jacket: 1\n\n- Pocket: 1\n\n- Ring: 1\n\n- Beads on tie: 1\n\n- Lights on tie: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20954.0, "ram_available_mb": 41886.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20947.7, "ram_available_mb": 41893.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.398}, "power_stats": {"power_gpu_soc_mean_watts": 22.197, "power_cpu_cv_mean_watts": 1.687, "power_sys_5v0_mean_watts": 8.808, "gpu_utilization_percent_mean": 75.398, "power_watts_avg": 22.197, "energy_joules_est": 255.83, "duration_seconds": 11.526, "sample_count": 98}, "timestamp": "2026-01-26T12:24:02.155930"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12675.385, "latencies_ms": [12675.385], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The man is positioned in the foreground, standing against a dark background. He is wearing a black suit jacket and a white shirt, and is adjusting a tie with a unique feature: it has lights along its length. The lights on the tie are arranged in a vertical line, with the topmost light being red, followed by orange, yellow, green, blue, ind", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20947.7, "ram_available_mb": 41893.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20970.7, "ram_available_mb": 41870.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.972}, "power_stats": {"power_gpu_soc_mean_watts": 21.638, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.893, "gpu_utilization_percent_mean": 72.972, "power_watts_avg": 21.638, "energy_joules_est": 274.28, "duration_seconds": 12.676, "sample_count": 109}, "timestamp": "2026-01-26T12:24:16.861171"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9670.252, "latencies_ms": [9670.252], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A man in a suit is adjusting a tie with a unique design, featuring a series of lights that change colors. The setting appears to be a studio with a dark background, emphasizing the vibrant colors of the lights on the tie.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20970.7, "ram_available_mb": 41870.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21043.3, "ram_available_mb": 41797.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.244}, "power_stats": {"power_gpu_soc_mean_watts": 22.941, "power_cpu_cv_mean_watts": 1.547, "power_sys_5v0_mean_watts": 8.787, "gpu_utilization_percent_mean": 77.244, "power_watts_avg": 22.941, "energy_joules_est": 221.86, "duration_seconds": 9.671, "sample_count": 82}, "timestamp": "2026-01-26T12:24:28.564265"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11147.608, "latencies_ms": [11147.608], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image features a person dressed in a formal black suit with a white shirt and a tie that has a unique design. The tie appears to be made of a reflective material with a vertical arrangement of lights that change colors from red to green to yellow, creating a vibrant and dynamic effect against the dark background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20956.6, "ram_available_mb": 41884.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21079.3, "ram_available_mb": 41761.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.625}, "power_stats": {"power_gpu_soc_mean_watts": 22.045, "power_cpu_cv_mean_watts": 1.714, "power_sys_5v0_mean_watts": 8.858, "gpu_utilization_percent_mean": 74.625, "power_watts_avg": 22.045, "energy_joules_est": 245.76, "duration_seconds": 11.148, "sample_count": 96}, "timestamp": "2026-01-26T12:24:41.759933"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11588.03, "latencies_ms": [11588.03], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a woman crossing the street at a crosswalk during the evening. She is wearing a brown jacket and carrying a handbag. The street is illuminated by a streetlight, and there are several other people in the scene, some of whom are also carrying handbags.\n\nThere are multiple traffic lights visible in the image, with one located near the woman", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 20974.7, "ram_available_mb": 41866.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20969.3, "ram_available_mb": 41871.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.612}, "power_stats": {"power_gpu_soc_mean_watts": 19.257, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 69.612, "power_watts_avg": 19.257, "energy_joules_est": 223.16, "duration_seconds": 11.589, "sample_count": 98}, "timestamp": "2026-01-26T12:24:55.377849"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7662.205, "latencies_ms": [7662.205], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "pedestrian: 1, traffic light: 1, building: 2, window: multiple, streetlight: 1, sign: 1, restaurant: 1, car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20969.3, "ram_available_mb": 41871.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21036.0, "ram_available_mb": 41804.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.6}, "power_stats": {"power_gpu_soc_mean_watts": 21.554, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.576, "gpu_utilization_percent_mean": 73.6, "power_watts_avg": 21.554, "energy_joules_est": 165.17, "duration_seconds": 7.663, "sample_count": 65}, "timestamp": "2026-01-26T12:25:05.079726"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10789.771, "latencies_ms": [10789.771], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "In the foreground, there is a person crossing the street, positioned near the center of the image. The traffic light is in the background, slightly to the right of the person. The building with the sign 'TADURIA' is in the background, further to the right, and appears to be taller than the other buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20948.9, "ram_available_mb": 41891.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20966.6, "ram_available_mb": 41874.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.901}, "power_stats": {"power_gpu_soc_mean_watts": 19.405, "power_cpu_cv_mean_watts": 1.856, "power_sys_5v0_mean_watts": 8.653, "gpu_utilization_percent_mean": 71.901, "power_watts_avg": 19.405, "energy_joules_est": 209.39, "duration_seconds": 10.79, "sample_count": 91}, "timestamp": "2026-01-26T12:25:17.914349"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8590.085, "latencies_ms": [8590.085], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image depicts a street scene at night with a pedestrian crossing the road. The pedestrian is wearing a brown jacket and carrying a black bag, and there is a traffic light on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20966.6, "ram_available_mb": 41874.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21060.2, "ram_available_mb": 41780.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.405}, "power_stats": {"power_gpu_soc_mean_watts": 20.705, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 73.405, "power_watts_avg": 20.705, "energy_joules_est": 177.87, "duration_seconds": 8.591, "sample_count": 74}, "timestamp": "2026-01-26T12:25:28.528387"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7849.679, "latencies_ms": [7849.679], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image depicts a night scene with artificial lighting, including street lamps and building lights, casting a warm glow on the scene. The sky is dark, indicating it is either late evening or night time.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20998.4, "ram_available_mb": 41842.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20945.4, "ram_available_mb": 41895.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.075}, "power_stats": {"power_gpu_soc_mean_watts": 21.028, "power_cpu_cv_mean_watts": 1.643, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 73.075, "power_watts_avg": 21.028, "energy_joules_est": 165.08, "duration_seconds": 7.85, "sample_count": 67}, "timestamp": "2026-01-26T12:25:38.396092"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11589.382, "latencies_ms": [11589.382], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young woman is skillfully riding a wave on a blue surfboard. She is wearing a bikini and appears to be enjoying the thrilling experience. There are three other people in the water, with one person lying on a surfboard, another person standing on a surfboard, and the third person is in the water without a surf", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 20945.4, "ram_available_mb": 41895.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21003.0, "ram_available_mb": 41837.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.469}, "power_stats": {"power_gpu_soc_mean_watts": 19.391, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 70.469, "power_watts_avg": 19.391, "energy_joules_est": 224.74, "duration_seconds": 11.59, "sample_count": 98}, "timestamp": "2026-01-26T12:25:52.022609"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7777.146, "latencies_ms": [7777.146], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "girl: 1, surfboard: 1, wave: 1, water: 1, person: 3, swimsuit: 2, arm: 1, leg: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21003.0, "ram_available_mb": 41837.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21012.5, "ram_available_mb": 41828.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.215}, "power_stats": {"power_gpu_soc_mean_watts": 21.318, "power_cpu_cv_mean_watts": 1.583, "power_sys_5v0_mean_watts": 8.59, "gpu_utilization_percent_mean": 74.215, "power_watts_avg": 21.318, "energy_joules_est": 165.81, "duration_seconds": 7.778, "sample_count": 65}, "timestamp": "2026-01-26T12:26:01.813916"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11532.779, "latencies_ms": [11532.779], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is surfing on a blue surfboard, riding a wave towards the right side of the image. In the background, there are two other individuals; one is lying on a surfboard further out at sea, and the other is standing on a surfboard closer to the shore, holding a blue surfboard. The main surfer is", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21012.5, "ram_available_mb": 41828.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21013.0, "ram_available_mb": 41827.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.56}, "power_stats": {"power_gpu_soc_mean_watts": 19.313, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 70.56, "power_watts_avg": 19.313, "energy_joules_est": 222.74, "duration_seconds": 11.533, "sample_count": 100}, "timestamp": "2026-01-26T12:26:15.408607"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9120.597, "latencies_ms": [9120.597], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "A young girl is surfing on a wave in the ocean, with two other people in the background, one of whom is holding a surfboard. The girl is wearing a bikini and appears to be enjoying herself as she rides the wave.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21013.0, "ram_available_mb": 41827.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20943.1, "ram_available_mb": 41897.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.244}, "power_stats": {"power_gpu_soc_mean_watts": 20.584, "power_cpu_cv_mean_watts": 1.719, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 73.244, "power_watts_avg": 20.584, "energy_joules_est": 187.75, "duration_seconds": 9.121, "sample_count": 78}, "timestamp": "2026-01-26T12:26:26.588391"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9126.726, "latencies_ms": [9126.726], "images_per_second": 0.11, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image captures a vibrant scene at the beach with a young girl in a purple bikini riding a wave on a blue surfboard. The water is a clear blue-green, and the sunlight is bright, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20943.1, "ram_available_mb": 41897.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21005.3, "ram_available_mb": 41835.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.208}, "power_stats": {"power_gpu_soc_mean_watts": 19.808, "power_cpu_cv_mean_watts": 1.773, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 72.208, "power_watts_avg": 19.808, "energy_joules_est": 180.79, "duration_seconds": 9.127, "sample_count": 77}, "timestamp": "2026-01-26T12:26:37.725886"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 9645.62, "latencies_ms": [9645.62], "images_per_second": 0.104, "prompt_tokens": 24, "response_tokens_est": 62, "n_tiles": 16, "output_text": "In the image, a man is standing in a fenced area, reaching out to pet an elephant. The elephant is standing on a concrete platform, and the man is wearing a white shirt. There are a few other people in the background, possibly observing the interaction.", "error": null, "sys_before": {"cpu_percent": 19.2, "ram_used_mb": 21005.3, "ram_available_mb": 41835.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21006.8, "ram_available_mb": 41834.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.22}, "power_stats": {"power_gpu_soc_mean_watts": 19.994, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 71.22, "power_watts_avg": 19.994, "energy_joules_est": 192.87, "duration_seconds": 9.646, "sample_count": 82}, "timestamp": "2026-01-26T12:26:49.402635"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7941.533, "latencies_ms": [7941.533], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "elephant: 1, fence: 2, hand: 1, trunk: 1, watch: 1, shirt: 1, belt: 1, trees: multiple", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21006.8, "ram_available_mb": 41834.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20945.5, "ram_available_mb": 41895.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.618}, "power_stats": {"power_gpu_soc_mean_watts": 20.87, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.546, "gpu_utilization_percent_mean": 73.618, "power_watts_avg": 20.87, "energy_joules_est": 165.75, "duration_seconds": 7.942, "sample_count": 68}, "timestamp": "2026-01-26T12:26:59.397065"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11610.891, "latencies_ms": [11610.891], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is standing to the right of a fence, reaching out to touch an elephant on the left side of the image. The elephant is positioned behind the fence, in the middle ground of the image, and is facing the person. The background is filled with greenery and another person is visible behind the fence, slightly to the right", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20945.5, "ram_available_mb": 41895.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20933.9, "ram_available_mb": 41907.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.05}, "power_stats": {"power_gpu_soc_mean_watts": 19.187, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 70.05, "power_watts_avg": 19.187, "energy_joules_est": 222.79, "duration_seconds": 11.611, "sample_count": 101}, "timestamp": "2026-01-26T12:27:13.035960"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8343.281, "latencies_ms": [8343.281], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "In the image, a person is feeding an elephant with a stick through a fence in a zoo enclosure. The elephant is standing on a concrete platform, and there are trees and a building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20933.9, "ram_available_mb": 41907.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20991.8, "ram_available_mb": 41849.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.845}, "power_stats": {"power_gpu_soc_mean_watts": 20.933, "power_cpu_cv_mean_watts": 1.646, "power_sys_5v0_mean_watts": 8.575, "gpu_utilization_percent_mean": 72.845, "power_watts_avg": 20.933, "energy_joules_est": 174.66, "duration_seconds": 8.344, "sample_count": 71}, "timestamp": "2026-01-26T12:27:23.398306"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9384.745, "latencies_ms": [9384.745], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image shows a scene with an elephant and a human interacting. The elephant is grey, and the human is wearing a white shirt and dark pants. The lighting appears to be natural daylight, and the weather seems to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20991.8, "ram_available_mb": 41849.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21014.0, "ram_available_mb": 41826.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.722}, "power_stats": {"power_gpu_soc_mean_watts": 20.062, "power_cpu_cv_mean_watts": 1.778, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 71.722, "power_watts_avg": 20.062, "energy_joules_est": 188.29, "duration_seconds": 9.385, "sample_count": 79}, "timestamp": "2026-01-26T12:27:34.794975"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11605.26, "latencies_ms": [11605.26], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a brown dog with a white chest is sitting on a bed, surrounded by a mess of clothes and various items. The dog appears to be looking directly at the camera, giving the impression that it is posing for the photo. The bed is covered with clothes, and there are multiple items scattered around, including a pillow and a jacket. The scene", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 20961.1, "ram_available_mb": 41879.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.3, "ram_used_mb": 21014.8, "ram_available_mb": 41826.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.745}, "power_stats": {"power_gpu_soc_mean_watts": 19.332, "power_cpu_cv_mean_watts": 1.977, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 69.745, "power_watts_avg": 19.332, "energy_joules_est": 224.37, "duration_seconds": 11.606, "sample_count": 98}, "timestamp": "2026-01-26T12:27:48.460302"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9521.489, "latencies_ms": [9521.489], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Bed: 1\n- Blanket: 1\n- Pillow: 1\n- Pillowcase: 1\n- Clothes: 1\n- Bags: 1\n- Envelope: 1\n- Dog: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20944.7, "ram_available_mb": 41896.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.2, "ram_used_mb": 20945.8, "ram_available_mb": 41895.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.519}, "power_stats": {"power_gpu_soc_mean_watts": 20.261, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.586, "gpu_utilization_percent_mean": 71.519, "power_watts_avg": 20.261, "energy_joules_est": 192.93, "duration_seconds": 9.522, "sample_count": 81}, "timestamp": "2026-01-26T12:28:00.014680"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7666.574, "latencies_ms": [7666.574], "images_per_second": 0.13, "prompt_tokens": 44, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The dog is sitting in the foreground on the left side of the image, near the center. The bed is in the background, with various items scattered on it, including a brown pillow on the right side.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20945.8, "ram_available_mb": 41895.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.1, "ram_used_mb": 21013.6, "ram_available_mb": 41827.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.138}, "power_stats": {"power_gpu_soc_mean_watts": 21.15, "power_cpu_cv_mean_watts": 1.804, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 73.138, "power_watts_avg": 21.15, "energy_joules_est": 162.17, "duration_seconds": 7.667, "sample_count": 65}, "timestamp": "2026-01-26T12:28:09.703515"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6613.071, "latencies_ms": [6613.071], "images_per_second": 0.151, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "A dog is sitting on a bed surrounded by a pile of clothes and a pillow. The bed appears to be unmade and the room looks messy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20950.9, "ram_available_mb": 41890.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.5, "ram_used_mb": 20951.8, "ram_available_mb": 41889.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.875}, "power_stats": {"power_gpu_soc_mean_watts": 22.272, "power_cpu_cv_mean_watts": 1.558, "power_sys_5v0_mean_watts": 8.57, "gpu_utilization_percent_mean": 75.875, "power_watts_avg": 22.272, "energy_joules_est": 147.3, "duration_seconds": 6.614, "sample_count": 56}, "timestamp": "2026-01-26T12:28:18.328058"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11316.349, "latencies_ms": [11316.349], "images_per_second": 0.088, "prompt_tokens": 36, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The image features a dog sitting amidst a pile of clothes and bags. The dog appears to be a dark-colored breed with a shiny coat. The clothes and bags are of various colors and materials, including a brown leather bag and a yellow garment. The lighting in the image is bright, suggesting it was taken indoors.", "error": null, "sys_before": {"cpu_percent": 30.0, "ram_used_mb": 20951.8, "ram_available_mb": 41889.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 9.1, "ram_used_mb": 20957.1, "ram_available_mb": 41883.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.125}, "power_stats": {"power_gpu_soc_mean_watts": 19.387, "power_cpu_cv_mean_watts": 1.939, "power_sys_5v0_mean_watts": 8.696, "gpu_utilization_percent_mean": 70.125, "power_watts_avg": 19.387, "energy_joules_est": 219.4, "duration_seconds": 11.317, "sample_count": 96}, "timestamp": "2026-01-26T12:28:31.696247"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11584.446, "latencies_ms": [11584.446], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seated at a desk in an office environment. He is dressed in a white shirt and a blue tie, and he is holding a pen in his right hand, poised to write on a piece of paper that is placed on the desk. His left hand is raised to his chin, suggesting a moment of contemplation or deep thought.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 20957.1, "ram_available_mb": 41883.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 21016.2, "ram_available_mb": 41824.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.194}, "power_stats": {"power_gpu_soc_mean_watts": 19.385, "power_cpu_cv_mean_watts": 1.961, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 70.194, "power_watts_avg": 19.385, "energy_joules_est": 224.58, "duration_seconds": 11.585, "sample_count": 98}, "timestamp": "2026-01-26T12:28:45.303430"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7662.062, "latencies_ms": [7662.062], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "laptop: 1, pen: 1, paper: 1, clipboard: 1, glasses: 1, shirt: 1, tie: 1, background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20962.7, "ram_available_mb": 41878.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20996.6, "ram_available_mb": 41844.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.631}, "power_stats": {"power_gpu_soc_mean_watts": 21.367, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.571, "gpu_utilization_percent_mean": 73.631, "power_watts_avg": 21.367, "energy_joules_est": 163.73, "duration_seconds": 7.663, "sample_count": 65}, "timestamp": "2026-01-26T12:28:54.982643"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11532.047, "latencies_ms": [11532.047], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The laptop is positioned to the left of the image, placed on a wooden desk in the foreground. A person, dressed in a white shirt and blue tie, is seated at the desk, with their right hand resting on their chin and their left hand holding a pen, suggesting they are in the process of writing or reviewing documents. The background is blur", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20996.6, "ram_available_mb": 41844.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20995.1, "ram_available_mb": 41845.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.031}, "power_stats": {"power_gpu_soc_mean_watts": 19.398, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 70.031, "power_watts_avg": 19.398, "energy_joules_est": 223.71, "duration_seconds": 11.533, "sample_count": 97}, "timestamp": "2026-01-26T12:29:08.540649"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8102.403, "latencies_ms": [8102.403], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A man in a white shirt and blue tie is sitting at a desk with a laptop and papers in front of him, looking thoughtful. He is holding a pen in his hand and appears to be deep in thought.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20995.1, "ram_available_mb": 41845.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20995.9, "ram_available_mb": 41845.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.812}, "power_stats": {"power_gpu_soc_mean_watts": 21.077, "power_cpu_cv_mean_watts": 1.619, "power_sys_5v0_mean_watts": 8.591, "gpu_utilization_percent_mean": 72.812, "power_watts_avg": 21.077, "energy_joules_est": 170.79, "duration_seconds": 8.103, "sample_count": 69}, "timestamp": "2026-01-26T12:29:18.695931"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11580.145, "latencies_ms": [11580.145], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The image shows a person sitting at a desk with a laptop open in front of them. The person is wearing a white shirt and a blue tie. The desk appears to be made of wood and there is a pen in the person's hand. The background is blurred but it seems to be an office or workspace with natural light coming in from the window.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20995.9, "ram_available_mb": 41845.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20994.4, "ram_available_mb": 41846.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.414}, "power_stats": {"power_gpu_soc_mean_watts": 19.123, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 70.414, "power_watts_avg": 19.123, "energy_joules_est": 221.46, "duration_seconds": 11.581, "sample_count": 99}, "timestamp": "2026-01-26T12:29:32.315509"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12400.704, "latencies_ms": [12400.704], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a clear blue sky, where a large, full moon is visible in the bottom left corner, casting a soft glow. A commercial airplane, painted in white with a red and blue tail, is seen flying from the right to the left of the frame. The airplane's wings are fully extended, indicating it is in the process of taking", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20994.4, "ram_available_mb": 41846.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21018.6, "ram_available_mb": 41822.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.171}, "power_stats": {"power_gpu_soc_mean_watts": 21.534, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.905, "gpu_utilization_percent_mean": 73.171, "power_watts_avg": 21.534, "energy_joules_est": 267.05, "duration_seconds": 12.401, "sample_count": 105}, "timestamp": "2026-01-26T12:29:46.762403"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5078.973, "latencies_ms": [5078.973], "images_per_second": 0.197, "prompt_tokens": 39, "response_tokens_est": 11, "n_tiles": 16, "output_text": "moon: 1, airplane: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20955.8, "ram_available_mb": 41885.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 21038.7, "ram_available_mb": 41802.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 86.791}, "power_stats": {"power_gpu_soc_mean_watts": 26.444, "power_cpu_cv_mean_watts": 0.717, "power_sys_5v0_mean_watts": 8.571, "gpu_utilization_percent_mean": 86.791, "power_watts_avg": 26.444, "energy_joules_est": 134.33, "duration_seconds": 5.08, "sample_count": 43}, "timestamp": "2026-01-26T12:29:53.861972"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9440.315, "latencies_ms": [9440.315], "images_per_second": 0.106, "prompt_tokens": 44, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The airplane is flying in the background, higher up in the sky compared to the moon, which is closer to the viewer and appears larger. The moon is positioned to the left of the airplane from the perspective of the viewer.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20948.6, "ram_available_mb": 41892.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21006.8, "ram_available_mb": 41834.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.062}, "power_stats": {"power_gpu_soc_mean_watts": 22.783, "power_cpu_cv_mean_watts": 1.576, "power_sys_5v0_mean_watts": 8.856, "gpu_utilization_percent_mean": 77.062, "power_watts_avg": 22.783, "energy_joules_est": 215.1, "duration_seconds": 9.441, "sample_count": 80}, "timestamp": "2026-01-26T12:30:05.324645"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9905.479, "latencies_ms": [9905.479], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image captures a serene scene of a clear blue sky with a full moon visible in the bottom left corner. A commercial airplane with its landing gear extended is seen flying from left to right, indicating it is approaching an airport for landing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21006.8, "ram_available_mb": 41834.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21008.5, "ram_available_mb": 41832.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.25}, "power_stats": {"power_gpu_soc_mean_watts": 22.724, "power_cpu_cv_mean_watts": 1.563, "power_sys_5v0_mean_watts": 8.788, "gpu_utilization_percent_mean": 77.25, "power_watts_avg": 22.724, "energy_joules_est": 225.11, "duration_seconds": 9.906, "sample_count": 84}, "timestamp": "2026-01-26T12:30:17.254018"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10012.874, "latencies_ms": [10012.874], "images_per_second": 0.1, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image features a clear blue sky with a large, pale orange moon visible in the lower left corner. An airplane with a predominantly white body and blue tail is captured in flight, with its landing gear extended, suggesting it is either taking off or landing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20945.0, "ram_available_mb": 41895.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21029.2, "ram_available_mb": 41811.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.479, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.858, "gpu_utilization_percent_mean": 76.0, "power_watts_avg": 22.479, "energy_joules_est": 225.09, "duration_seconds": 10.013, "sample_count": 85}, "timestamp": "2026-01-26T12:30:29.316564"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12432.585, "latencies_ms": [12432.585], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is captured in the midst of performing a skateboard trick at a skate park. He is wearing a tie-dye shirt and black pants, and his skateboard is adorned with a vibrant design. The skate park itself is a concrete ramp, and the background is filled with palm trees and a play", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 20946.2, "ram_available_mb": 41894.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21012.1, "ram_available_mb": 41828.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.962}, "power_stats": {"power_gpu_soc_mean_watts": 21.51, "power_cpu_cv_mean_watts": 1.809, "power_sys_5v0_mean_watts": 8.912, "gpu_utilization_percent_mean": 72.962, "power_watts_avg": 21.51, "energy_joules_est": 267.44, "duration_seconds": 12.433, "sample_count": 106}, "timestamp": "2026-01-26T12:30:43.786280"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9169.514, "latencies_ms": [9169.514], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "palm tree: 3, skateboard: 1, person: 1, wall: 1, building: 1, tree: 2, sky: 1, umbrella: 1", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 21012.1, "ram_available_mb": 41828.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20932.9, "ram_available_mb": 41908.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.038}, "power_stats": {"power_gpu_soc_mean_watts": 22.879, "power_cpu_cv_mean_watts": 1.485, "power_sys_5v0_mean_watts": 8.73, "gpu_utilization_percent_mean": 77.038, "power_watts_avg": 22.879, "energy_joules_est": 209.8, "duration_seconds": 9.17, "sample_count": 79}, "timestamp": "2026-01-26T12:30:54.981860"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12445.502, "latencies_ms": [12445.502], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, performing a trick on a ramp. In the background, there are palm trees and a playground structure, indicating the skate park is located in an outdoor recreational area. The skateboarder is near the edge of the ramp, suggesting they are in the process of executing a jump or trick.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.9, "ram_available_mb": 41908.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20995.3, "ram_available_mb": 41845.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.84}, "power_stats": {"power_gpu_soc_mean_watts": 21.78, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 8.893, "gpu_utilization_percent_mean": 73.84, "power_watts_avg": 21.78, "energy_joules_est": 271.08, "duration_seconds": 12.446, "sample_count": 106}, "timestamp": "2026-01-26T12:31:09.472745"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8425.48, "latencies_ms": [8425.48], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A person is performing a skateboard trick on a ramp at a skate park. The skateboarder is wearing a tie-dye shirt and black pants.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20995.3, "ram_available_mb": 41845.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 21037.3, "ram_available_mb": 41803.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.69}, "power_stats": {"power_gpu_soc_mean_watts": 23.536, "power_cpu_cv_mean_watts": 1.404, "power_sys_5v0_mean_watts": 8.76, "gpu_utilization_percent_mean": 78.69, "power_watts_avg": 23.536, "energy_joules_est": 198.32, "duration_seconds": 8.426, "sample_count": 71}, "timestamp": "2026-01-26T12:31:19.959692"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9436.741, "latencies_ms": [9436.741], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The skateboarder is wearing a tie-dye shirt with a mix of purple, blue, and white colors. The lighting is natural, suggesting it is daytime, and the weather appears to be partly cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21037.3, "ram_available_mb": 41803.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21053.8, "ram_available_mb": 41787.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.362}, "power_stats": {"power_gpu_soc_mean_watts": 22.644, "power_cpu_cv_mean_watts": 1.561, "power_sys_5v0_mean_watts": 8.855, "gpu_utilization_percent_mean": 76.362, "power_watts_avg": 22.644, "energy_joules_est": 213.7, "duration_seconds": 9.437, "sample_count": 80}, "timestamp": "2026-01-26T12:31:31.458442"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11629.636, "latencies_ms": [11629.636], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a sheep with a thick, shaggy coat of wool, standing behind a wire fence. The sheep is facing the camera, and its expression is calm and attentive. The fence is made of metal wires, and it encloses the sheep, suggesting that it is in a pen or a farm. The background of the image reveals", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 20990.2, "ram_available_mb": 41850.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20991.4, "ram_available_mb": 41849.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.818}, "power_stats": {"power_gpu_soc_mean_watts": 19.313, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 70.818, "power_watts_avg": 19.313, "energy_joules_est": 224.62, "duration_seconds": 11.63, "sample_count": 99}, "timestamp": "2026-01-26T12:31:45.133297"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7924.253, "latencies_ms": [7924.253], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "wire: 10, sheep: 1, wool: 1, grass: 1, trees: 10, sky: 1, rocks: 1, fence post: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20934.5, "ram_available_mb": 41906.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20985.7, "ram_available_mb": 41855.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.485}, "power_stats": {"power_gpu_soc_mean_watts": 21.09, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.575, "gpu_utilization_percent_mean": 73.485, "power_watts_avg": 21.09, "energy_joules_est": 167.13, "duration_seconds": 7.925, "sample_count": 68}, "timestamp": "2026-01-26T12:31:55.116141"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11595.081, "latencies_ms": [11595.081], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The sheep is positioned in the foreground of the image, appearing large and in focus, while the background consists of a natural landscape with trees and a grassy area, which appears smaller and less detailed. The wire fence is in the foreground, closer to the viewer, and the sheep is behind it, indicating that the fence is between the viewer and the sheep.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20932.4, "ram_available_mb": 41908.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21005.7, "ram_available_mb": 41835.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.03}, "power_stats": {"power_gpu_soc_mean_watts": 19.277, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 70.03, "power_watts_avg": 19.277, "energy_joules_est": 223.53, "duration_seconds": 11.596, "sample_count": 99}, "timestamp": "2026-01-26T12:32:08.761974"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8225.376, "latencies_ms": [8225.376], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A sheep is lying down in a wire fenced area with a lush green background of trees and grass. The sheep appears to be resting or possibly sleeping, with its woolly body partially covered by the fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21005.7, "ram_available_mb": 41835.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21042.1, "ram_available_mb": 41798.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.217}, "power_stats": {"power_gpu_soc_mean_watts": 21.055, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.584, "gpu_utilization_percent_mean": 73.217, "power_watts_avg": 21.055, "energy_joules_est": 173.2, "duration_seconds": 8.226, "sample_count": 69}, "timestamp": "2026-01-26T12:32:19.014404"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10203.183, "latencies_ms": [10203.183], "images_per_second": 0.098, "prompt_tokens": 36, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image features a sheep with a thick, shaggy coat of wool, predominantly in a light brown color, with some white areas. The lighting is natural and soft, suggesting an overcast day, and the sheep is behind a wire fence, indicating it is likely in a farm or rural setting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20930.2, "ram_available_mb": 41910.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.455}, "power_stats": {"power_gpu_soc_mean_watts": 19.524, "power_cpu_cv_mean_watts": 1.829, "power_sys_5v0_mean_watts": 8.639, "gpu_utilization_percent_mean": 70.455, "power_watts_avg": 19.524, "energy_joules_est": 199.22, "duration_seconds": 10.204, "sample_count": 88}, "timestamp": "2026-01-26T12:32:31.247320"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11643.719, "latencies_ms": [11643.719], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a close-up view of a Nokia phone, bathed in a soft, golden hue. The phone's back cover, a pristine white, contrasts with the black buttons that are visible. The camera lens, a silver circle, is the focal point of the image, its lens cap resting on top. The phone is rest", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 21015.7, "ram_available_mb": 41825.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.76}, "power_stats": {"power_gpu_soc_mean_watts": 19.287, "power_cpu_cv_mean_watts": 1.934, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 69.76, "power_watts_avg": 19.287, "energy_joules_est": 224.59, "duration_seconds": 11.644, "sample_count": 100}, "timestamp": "2026-01-26T12:32:44.965212"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8141.612, "latencies_ms": [8141.612], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "camera: 1, flash: 1, volume rocker: 1, power button: 1, camera lens: 1, camera button: 1, camera icon: 1, logo: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20935.7, "ram_available_mb": 41905.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 20999.0, "ram_available_mb": 41841.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.319}, "power_stats": {"power_gpu_soc_mean_watts": 21.047, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.606, "gpu_utilization_percent_mean": 73.319, "power_watts_avg": 21.047, "energy_joules_est": 171.37, "duration_seconds": 8.142, "sample_count": 69}, "timestamp": "2026-01-26T12:32:55.168772"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11600.927, "latencies_ms": [11600.927], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The camera is positioned in the foreground of the image, with its buttons and lens clearly visible. In the background, there is a blurred image of a colorful object, possibly a logo or a picture, which is not the main focus of the image. The main object, the camera, is in sharp focus and appears to be the central subject of the photograph.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20945.6, "ram_available_mb": 41895.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 20998.1, "ram_available_mb": 41842.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.112}, "power_stats": {"power_gpu_soc_mean_watts": 19.207, "power_cpu_cv_mean_watts": 1.883, "power_sys_5v0_mean_watts": 8.68, "gpu_utilization_percent_mean": 70.112, "power_watts_avg": 19.207, "energy_joules_est": 222.83, "duration_seconds": 11.602, "sample_count": 98}, "timestamp": "2026-01-26T12:33:08.797701"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8000.539, "latencies_ms": [8000.539], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a close-up of a smartphone with a shiny, metallic surface. The phone has a camera lens in the center, a flash, and a power button on the top right corner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20998.1, "ram_available_mb": 41842.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21016.3, "ram_available_mb": 41824.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.059}, "power_stats": {"power_gpu_soc_mean_watts": 20.963, "power_cpu_cv_mean_watts": 1.613, "power_sys_5v0_mean_watts": 8.573, "gpu_utilization_percent_mean": 73.059, "power_watts_avg": 20.963, "energy_joules_est": 167.73, "duration_seconds": 8.001, "sample_count": 68}, "timestamp": "2026-01-26T12:33:18.859568"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11542.067, "latencies_ms": [11542.067], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a close-up of a device with a metallic finish, featuring a prominent circular button with a red and white logo in the center, which appears to be illuminated. The device is resting on a surface with a dark background, and the lighting highlights the reflective surface and the buttons, creating a contrast between the shiny metallic parts and the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21016.3, "ram_available_mb": 41824.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21017.6, "ram_available_mb": 41823.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.449}, "power_stats": {"power_gpu_soc_mean_watts": 19.392, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 69.449, "power_watts_avg": 19.392, "energy_joules_est": 223.84, "duration_seconds": 11.543, "sample_count": 98}, "timestamp": "2026-01-26T12:33:32.464917"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12435.417, "latencies_ms": [12435.417], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing in a kitchen, holding a wine glass and smiling. She is wearing a black dress and black heels. The kitchen features a refrigerator, an oven, and a sink. There are also several bottles placed around the kitchen, with one near the refrigerator and another close to the sink. A bowl can be", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20945.9, "ram_available_mb": 41895.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21025.0, "ram_available_mb": 41815.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.774}, "power_stats": {"power_gpu_soc_mean_watts": 21.515, "power_cpu_cv_mean_watts": 1.813, "power_sys_5v0_mean_watts": 8.908, "gpu_utilization_percent_mean": 72.774, "power_watts_avg": 21.515, "energy_joules_est": 267.56, "duration_seconds": 12.436, "sample_count": 106}, "timestamp": "2026-01-26T12:33:46.936068"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9674.736, "latencies_ms": [9674.736], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "refrigerator: 1, woman: 1, wine bottle: 1, wine glass: 1, bowl: 1, knife block: 1, tiles: 1, drawer: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21025.0, "ram_available_mb": 41815.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21026.0, "ram_available_mb": 41814.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.927}, "power_stats": {"power_gpu_soc_mean_watts": 22.821, "power_cpu_cv_mean_watts": 1.543, "power_sys_5v0_mean_watts": 8.782, "gpu_utilization_percent_mean": 76.927, "power_watts_avg": 22.821, "energy_joules_est": 220.8, "duration_seconds": 9.675, "sample_count": 82}, "timestamp": "2026-01-26T12:33:58.633120"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11187.315, "latencies_ms": [11187.315], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The woman is standing in the foreground, positioned between the refrigerator and the kitchen cabinets. The refrigerator is in the background, with the kitchen cabinets to the right of it. The woman is standing near the refrigerator, with the cabinets further away in the background.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20944.8, "ram_available_mb": 41896.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20990.3, "ram_available_mb": 41850.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.937}, "power_stats": {"power_gpu_soc_mean_watts": 22.105, "power_cpu_cv_mean_watts": 1.702, "power_sys_5v0_mean_watts": 8.884, "gpu_utilization_percent_mean": 74.937, "power_watts_avg": 22.105, "energy_joules_est": 247.31, "duration_seconds": 11.188, "sample_count": 95}, "timestamp": "2026-01-26T12:34:11.841665"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7932.62, "latencies_ms": [7932.62], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "A woman is standing in a kitchen, holding a glass of champagne and smiling. The kitchen has wooden cabinets and a stainless steel refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20928.4, "ram_available_mb": 41912.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 21022.2, "ram_available_mb": 41818.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 79.403}, "power_stats": {"power_gpu_soc_mean_watts": 23.829, "power_cpu_cv_mean_watts": 1.35, "power_sys_5v0_mean_watts": 8.748, "gpu_utilization_percent_mean": 79.403, "power_watts_avg": 23.829, "energy_joules_est": 189.04, "duration_seconds": 7.933, "sample_count": 67}, "timestamp": "2026-01-26T12:34:21.804281"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9416.835, "latencies_ms": [9416.835], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a person standing in a kitchen with wooden cabinets and a stainless steel refrigerator. The person is wearing a black dress with sparkles and black heels, and is holding a glass with a yellow drink.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20939.2, "ram_available_mb": 41901.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21018.0, "ram_available_mb": 41822.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.062}, "power_stats": {"power_gpu_soc_mean_watts": 22.823, "power_cpu_cv_mean_watts": 1.571, "power_sys_5v0_mean_watts": 8.883, "gpu_utilization_percent_mean": 76.062, "power_watts_avg": 22.823, "energy_joules_est": 214.93, "duration_seconds": 9.417, "sample_count": 80}, "timestamp": "2026-01-26T12:34:33.261042"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11576.577, "latencies_ms": [11576.577], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment on a road, viewed through the reflection of a round mirror. The mirror, attached to a black pole, is the main focus of the image. It reflects a yellow school bus, a white car, and a blue sky, creating a mirror image of the scene. The bus and car are in motion, as suggested by the blurred background. The pole", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 20936.2, "ram_available_mb": 41904.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20997.1, "ram_available_mb": 41843.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.394}, "power_stats": {"power_gpu_soc_mean_watts": 19.363, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.661, "gpu_utilization_percent_mean": 70.394, "power_watts_avg": 19.363, "energy_joules_est": 224.17, "duration_seconds": 11.577, "sample_count": 99}, "timestamp": "2026-01-26T12:34:46.897070"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7444.571, "latencies_ms": [7444.571], "images_per_second": 0.134, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "bus: 1, car: 1, traffic light: 2, building: 1, sky: 1, clouds: 1, road: 1, mirror: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20935.2, "ram_available_mb": 41905.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20991.9, "ram_available_mb": 41849.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.919}, "power_stats": {"power_gpu_soc_mean_watts": 21.533, "power_cpu_cv_mean_watts": 1.523, "power_sys_5v0_mean_watts": 8.574, "gpu_utilization_percent_mean": 74.919, "power_watts_avg": 21.533, "energy_joules_est": 160.32, "duration_seconds": 7.445, "sample_count": 62}, "timestamp": "2026-01-26T12:34:56.357376"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11616.976, "latencies_ms": [11616.976], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a close-up of a rearview mirror reflecting a yellow school bus on the road behind it. The bus is positioned in the middle ground of the reflection, with other vehicles and a building visible in the background. The mirror is mounted on the right side of the image, and the reflection shows the bus moving away from the camera's point of view", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20929.5, "ram_available_mb": 41911.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20991.5, "ram_available_mb": 41849.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.869}, "power_stats": {"power_gpu_soc_mean_watts": 19.174, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.661, "gpu_utilization_percent_mean": 70.869, "power_watts_avg": 19.174, "energy_joules_est": 222.76, "duration_seconds": 11.618, "sample_count": 99}, "timestamp": "2026-01-26T12:35:09.991857"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9034.27, "latencies_ms": [9034.27], "images_per_second": 0.111, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows a rearview mirror of a vehicle, likely a car or a bus, reflecting the road ahead. The mirror captures the image of a yellow school bus and other vehicles on the road, indicating that the vehicle is on a busy road or highway.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20991.5, "ram_available_mb": 41849.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21009.7, "ram_available_mb": 41831.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.342}, "power_stats": {"power_gpu_soc_mean_watts": 20.571, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 8.614, "gpu_utilization_percent_mean": 72.342, "power_watts_avg": 20.571, "energy_joules_est": 185.86, "duration_seconds": 9.035, "sample_count": 76}, "timestamp": "2026-01-26T12:35:21.039198"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7316.949, "latencies_ms": [7316.949], "images_per_second": 0.137, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The mirror shows a yellow school bus with a reflection of a car and a building in the background. The sky appears to be cloudy, and the lighting suggests it might be an overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21009.7, "ram_available_mb": 41831.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21010.0, "ram_available_mb": 41830.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.629}, "power_stats": {"power_gpu_soc_mean_watts": 21.277, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 72.629, "power_watts_avg": 21.277, "energy_joules_est": 155.7, "duration_seconds": 7.318, "sample_count": 62}, "timestamp": "2026-01-26T12:35:30.401925"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11561.507, "latencies_ms": [11561.507], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a gray cat and a brown dog are sitting on a wooden table, looking at a potted plant. The cat is positioned on the right side of the table, while the dog is on the left side. They both appear to be curious about the plant, possibly observing its growth or the soil inside. The potted plant is placed in the center of the table,", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20939.7, "ram_available_mb": 41901.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20940.6, "ram_available_mb": 41900.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.364}, "power_stats": {"power_gpu_soc_mean_watts": 19.38, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 71.364, "power_watts_avg": 19.38, "energy_joules_est": 224.08, "duration_seconds": 11.562, "sample_count": 99}, "timestamp": "2026-01-26T12:35:44.000814"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7419.95, "latencies_ms": [7419.95], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "cat: 1, dog: 1, window: 1, plant: 1, pot: 1, soil: 1, label: 1, sunlight: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.6, "ram_available_mb": 41900.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21001.5, "ram_available_mb": 41839.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.111}, "power_stats": {"power_gpu_soc_mean_watts": 21.558, "power_cpu_cv_mean_watts": 1.557, "power_sys_5v0_mean_watts": 8.602, "gpu_utilization_percent_mean": 74.111, "power_watts_avg": 21.558, "energy_joules_est": 159.97, "duration_seconds": 7.421, "sample_count": 63}, "timestamp": "2026-01-26T12:35:53.438222"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9631.633, "latencies_ms": [9631.633], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 62, "n_tiles": 16, "output_text": "In the foreground, there is a potted plant on the right side of the image, which is near the cat that is on the right side. The dog is in the background, standing on its hind legs on the left side of the image, and appears to be looking at the plant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20948.0, "ram_available_mb": 41892.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20952.0, "ram_available_mb": 41888.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.512}, "power_stats": {"power_gpu_soc_mean_watts": 20.001, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 71.512, "power_watts_avg": 20.001, "energy_joules_est": 192.66, "duration_seconds": 9.632, "sample_count": 82}, "timestamp": "2026-01-26T12:36:05.098897"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8357.396, "latencies_ms": [8357.396], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A gray cat is sitting on a wooden table, looking at a small potted plant with a dog standing behind it, seemingly curious about the plant. The scene takes place near a window with a view of a green lawn outside.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20952.0, "ram_available_mb": 41888.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21012.2, "ram_available_mb": 41828.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.254}, "power_stats": {"power_gpu_soc_mean_watts": 20.892, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 73.254, "power_watts_avg": 20.892, "energy_joules_est": 174.62, "duration_seconds": 8.358, "sample_count": 71}, "timestamp": "2026-01-26T12:36:15.480736"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7662.467, "latencies_ms": [7662.467], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image features a gray cat and a brown dog standing on a wooden surface, with a potted plant in front of them. The lighting in the room is natural, coming from the large window in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21012.2, "ram_available_mb": 41828.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20992.9, "ram_available_mb": 41848.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.848}, "power_stats": {"power_gpu_soc_mean_watts": 20.708, "power_cpu_cv_mean_watts": 1.644, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 73.848, "power_watts_avg": 20.708, "energy_joules_est": 158.69, "duration_seconds": 7.663, "sample_count": 66}, "timestamp": "2026-01-26T12:36:25.170200"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11604.164, "latencies_ms": [11604.164], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two women playing soccer on a field. One woman is wearing a blue jersey and is in the process of kicking the soccer ball, while the other woman is wearing a yellow jersey and is running towards the ball. The soccer ball is located in the left side of the image, and the players are positioned in the center", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20937.7, "ram_available_mb": 41903.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20943.0, "ram_available_mb": 41897.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.303}, "power_stats": {"power_gpu_soc_mean_watts": 19.297, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.673, "gpu_utilization_percent_mean": 70.303, "power_watts_avg": 19.297, "energy_joules_est": 223.94, "duration_seconds": 11.605, "sample_count": 99}, "timestamp": "2026-01-26T12:36:38.837700"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7684.425, "latencies_ms": [7684.425], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "ball: 1, player: 2, team jersey: 1, player: 2, player: 1, player: 1, player: 1, player: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20943.0, "ram_available_mb": 41897.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20995.4, "ram_available_mb": 41845.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.938}, "power_stats": {"power_gpu_soc_mean_watts": 21.316, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.596, "gpu_utilization_percent_mean": 73.938, "power_watts_avg": 21.316, "energy_joules_est": 163.81, "duration_seconds": 7.685, "sample_count": 65}, "timestamp": "2026-01-26T12:36:48.546233"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11577.421, "latencies_ms": [11577.421], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The main object, a person in a blue sports jersey, is in the foreground and appears to be running towards the left side of the image, holding a soccer ball. Another person, wearing a yellow jersey, is in the background and seems to be slightly behind and to the right of the main object. The background is out of focus, emphasizing the main object", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20942.0, "ram_available_mb": 41898.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 20944.5, "ram_available_mb": 41896.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.778}, "power_stats": {"power_gpu_soc_mean_watts": 19.258, "power_cpu_cv_mean_watts": 2.022, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 70.778, "power_watts_avg": 19.258, "energy_joules_est": 222.97, "duration_seconds": 11.578, "sample_count": 99}, "timestamp": "2026-01-26T12:37:02.177111"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8280.121, "latencies_ms": [8280.121], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A female soccer player in a blue jersey is in possession of the ball and appears to be running with it. Another player in a yellow jersey is in the background, possibly preparing to challenge for the ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20944.5, "ram_available_mb": 41896.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21002.9, "ram_available_mb": 41838.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.268}, "power_stats": {"power_gpu_soc_mean_watts": 20.652, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.559, "gpu_utilization_percent_mean": 73.268, "power_watts_avg": 20.652, "energy_joules_est": 171.01, "duration_seconds": 8.281, "sample_count": 71}, "timestamp": "2026-01-26T12:37:12.495814"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11548.223, "latencies_ms": [11548.223], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a person in a blue sports jersey with the word \"ACRONIS\" prominently displayed across the chest, indicating a possible sponsorship or branding. The jersey has a V-neck design and is paired with shorts, suggesting a casual or athletic setting. The person is holding a soccer ball, which is white", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21002.9, "ram_available_mb": 41838.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20938.2, "ram_available_mb": 41902.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.4}, "power_stats": {"power_gpu_soc_mean_watts": 19.308, "power_cpu_cv_mean_watts": 1.918, "power_sys_5v0_mean_watts": 8.653, "gpu_utilization_percent_mean": 69.4, "power_watts_avg": 19.308, "energy_joules_est": 222.98, "duration_seconds": 11.549, "sample_count": 100}, "timestamp": "2026-01-26T12:37:26.081999"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11555.516, "latencies_ms": [11555.516], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two giraffes in a zoo enclosure. The giraffe on the left is standing near a wooden fence, while the one on the right is standing near a metal fence. Both giraffes are facing different directions, with the left one looking towards the left side of the image and the right one looking towards the right side. The en", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20938.2, "ram_available_mb": 41902.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20985.1, "ram_available_mb": 41855.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.594}, "power_stats": {"power_gpu_soc_mean_watts": 19.34, "power_cpu_cv_mean_watts": 1.902, "power_sys_5v0_mean_watts": 8.668, "gpu_utilization_percent_mean": 69.594, "power_watts_avg": 19.34, "energy_joules_est": 223.5, "duration_seconds": 11.556, "sample_count": 101}, "timestamp": "2026-01-26T12:37:39.686525"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7661.908, "latencies_ms": [7661.908], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "giraffe: 2, fence: 1, tree: multiple, grass: large area, path: small dirt area, enclosure: large, wall: wooden, building: not visible", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20931.8, "ram_available_mb": 41909.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20992.2, "ram_available_mb": 41848.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.446}, "power_stats": {"power_gpu_soc_mean_watts": 21.242, "power_cpu_cv_mean_watts": 1.57, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 73.446, "power_watts_avg": 21.242, "energy_joules_est": 162.77, "duration_seconds": 7.663, "sample_count": 65}, "timestamp": "2026-01-26T12:37:49.413582"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9867.322, "latencies_ms": [9867.322], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "In the foreground, there is a giraffe standing on the left side of a wooden fence, while another giraffe is standing on the right side of the same fence. The background is filled with lush green trees, creating a natural and serene environment for the giraffes.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20938.5, "ram_available_mb": 41902.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20935.8, "ram_available_mb": 41905.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.131}, "power_stats": {"power_gpu_soc_mean_watts": 19.839, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 71.131, "power_watts_avg": 19.839, "energy_joules_est": 195.77, "duration_seconds": 9.868, "sample_count": 84}, "timestamp": "2026-01-26T12:38:01.318640"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7908.259, "latencies_ms": [7908.259], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "Two giraffes are standing in a grassy enclosure with a wooden fence, surrounded by trees. One giraffe is bending down to eat grass, while the other stands tall and looks around.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 20935.8, "ram_available_mb": 41905.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20989.4, "ram_available_mb": 41851.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.045}, "power_stats": {"power_gpu_soc_mean_watts": 21.166, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.591, "gpu_utilization_percent_mean": 73.045, "power_watts_avg": 21.166, "energy_joules_est": 167.4, "duration_seconds": 7.909, "sample_count": 67}, "timestamp": "2026-01-26T12:38:11.278431"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7327.746, "latencies_ms": [7327.746], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image shows two giraffes in a grassy enclosure with a wooden fence. The lighting appears to be natural daylight, and the weather seems to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20926.2, "ram_available_mb": 41914.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20990.0, "ram_available_mb": 41850.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.629}, "power_stats": {"power_gpu_soc_mean_watts": 21.194, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 72.629, "power_watts_avg": 21.194, "energy_joules_est": 155.32, "duration_seconds": 7.328, "sample_count": 62}, "timestamp": "2026-01-26T12:38:20.627061"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12429.879, "latencies_ms": [12429.879], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of travel preparation. A gray suitcase, standing upright, is the central focus of the scene. It's equipped with a handle and wheels, suggesting it's ready for a journey. To the left of the suitcase, a black trash bag is casually placed, perhaps indicating the end of a meal or the need for", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20936.5, "ram_available_mb": 41904.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20923.9, "ram_available_mb": 41917.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.305}, "power_stats": {"power_gpu_soc_mean_watts": 21.565, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.91, "gpu_utilization_percent_mean": 73.305, "power_watts_avg": 21.565, "energy_joules_est": 268.07, "duration_seconds": 12.431, "sample_count": 105}, "timestamp": "2026-01-26T12:38:35.086914"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7028.397, "latencies_ms": [7028.397], "images_per_second": 0.142, "prompt_tokens": 39, "response_tokens_est": 28, "n_tiles": 16, "output_text": "bag: 2\nsuitcase: 1\nbox: 1\ncarpet: 1\ncurtain: 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20923.9, "ram_available_mb": 41917.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 20990.1, "ram_available_mb": 41850.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 80.917}, "power_stats": {"power_gpu_soc_mean_watts": 24.455, "power_cpu_cv_mean_watts": 1.201, "power_sys_5v0_mean_watts": 8.695, "gpu_utilization_percent_mean": 80.917, "power_watts_avg": 24.455, "energy_joules_est": 171.9, "duration_seconds": 7.029, "sample_count": 60}, "timestamp": "2026-01-26T12:38:44.135288"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12674.514, "latencies_ms": [12674.514], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a suitcase standing upright with its handle extended to the right, suggesting it is ready to be picked up. Behind the suitcase, there is a trash bin to its left, partially obscured by the suitcase. The trash bin appears to be on the same floor level as the suitcase. The background consists of a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20990.1, "ram_available_mb": 41850.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20991.3, "ram_available_mb": 41849.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.927}, "power_stats": {"power_gpu_soc_mean_watts": 21.654, "power_cpu_cv_mean_watts": 1.788, "power_sys_5v0_mean_watts": 8.915, "gpu_utilization_percent_mean": 72.927, "power_watts_avg": 21.654, "energy_joules_est": 274.47, "duration_seconds": 12.675, "sample_count": 109}, "timestamp": "2026-01-26T12:38:58.821390"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11541.641, "latencies_ms": [11541.641], "images_per_second": 0.087, "prompt_tokens": 37, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image shows a collection of luggage items, including a suitcase and a duffel bag, placed on a carpeted floor against a backdrop of sheer curtains. It appears to be a hotel room or a similar setting, possibly indicating that someone is preparing to leave or has just arrived.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20991.3, "ram_available_mb": 41849.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20937.0, "ram_available_mb": 41903.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.46}, "power_stats": {"power_gpu_soc_mean_watts": 22.095, "power_cpu_cv_mean_watts": 1.681, "power_sys_5v0_mean_watts": 8.805, "gpu_utilization_percent_mean": 74.46, "power_watts_avg": 22.095, "energy_joules_est": 255.03, "duration_seconds": 11.542, "sample_count": 100}, "timestamp": "2026-01-26T12:39:12.407297"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10667.94, "latencies_ms": [10667.94], "images_per_second": 0.094, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image is in black and white, featuring a suitcase, a bag, and a trash can against a backdrop of sheer curtains. The materials appear to be typical for travel items, with the suitcase and bag made of fabric and the trash bag made of plastic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20937.0, "ram_available_mb": 41903.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21025.9, "ram_available_mb": 41815.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.066}, "power_stats": {"power_gpu_soc_mean_watts": 22.315, "power_cpu_cv_mean_watts": 1.663, "power_sys_5v0_mean_watts": 8.881, "gpu_utilization_percent_mean": 75.066, "power_watts_avg": 22.315, "energy_joules_est": 238.07, "duration_seconds": 10.669, "sample_count": 91}, "timestamp": "2026-01-26T12:39:25.100148"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11605.263, "latencies_ms": [11605.263], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man wearing a red bandana and a blue shirt is standing in a forest, watching two horses walking down a rocky path. The man is positioned on the right side of the image, while the horses are located on the left side. The man appears to be observing the horses as they walk past him.\n\nThere are several other people in the", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 20937.1, "ram_available_mb": 41903.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20992.5, "ram_available_mb": 41848.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.576}, "power_stats": {"power_gpu_soc_mean_watts": 19.346, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 70.576, "power_watts_avg": 19.346, "energy_joules_est": 224.53, "duration_seconds": 11.606, "sample_count": 99}, "timestamp": "2026-01-26T12:39:38.760578"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8775.369, "latencies_ms": [8775.369], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "rock: 10\n\ntree: 5\n\nhorse: 2\n\nman: 1\n\nbackpack: 1\n\ngrass: 1\n\ndirt: 1\n\nmountain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20937.5, "ram_available_mb": 41903.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20986.6, "ram_available_mb": 41854.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.605}, "power_stats": {"power_gpu_soc_mean_watts": 20.637, "power_cpu_cv_mean_watts": 1.691, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 73.605, "power_watts_avg": 20.637, "energy_joules_est": 181.11, "duration_seconds": 8.776, "sample_count": 76}, "timestamp": "2026-01-26T12:39:49.567307"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11599.652, "latencies_ms": [11599.652], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person wearing a blue shirt and a red bandana on their head, looking towards the left side of the image. In the background, there are two horses being ridden by a person wearing a hat, walking along a rocky trail in a wooded area. The person with the blue shirt appears to be observing the horses from a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20933.2, "ram_available_mb": 41907.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20951.3, "ram_available_mb": 41889.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.551}, "power_stats": {"power_gpu_soc_mean_watts": 19.306, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 70.551, "power_watts_avg": 19.306, "energy_joules_est": 223.95, "duration_seconds": 11.6, "sample_count": 98}, "timestamp": "2026-01-26T12:40:03.184404"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6181.614, "latencies_ms": [6181.614], "images_per_second": 0.162, "prompt_tokens": 37, "response_tokens_est": 30, "n_tiles": 16, "output_text": "In a forested area with a rocky path, a person is riding a horse, while another person with a backpack is watching.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20951.3, "ram_available_mb": 41889.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 20992.0, "ram_available_mb": 41848.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.096}, "power_stats": {"power_gpu_soc_mean_watts": 22.731, "power_cpu_cv_mean_watts": 1.37, "power_sys_5v0_mean_watts": 8.54, "gpu_utilization_percent_mean": 76.096, "power_watts_avg": 22.731, "energy_joules_est": 140.53, "duration_seconds": 6.182, "sample_count": 52}, "timestamp": "2026-01-26T12:40:11.388832"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6984.968, "latencies_ms": [6984.968], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image shows a bright and sunny day in a forested area with clear skies. The lighting is natural and strong, indicating it might be midday or early afternoon.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20992.0, "ram_available_mb": 41848.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21012.8, "ram_available_mb": 41828.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.983}, "power_stats": {"power_gpu_soc_mean_watts": 21.473, "power_cpu_cv_mean_watts": 1.54, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 72.983, "power_watts_avg": 21.473, "energy_joules_est": 150.0, "duration_seconds": 6.986, "sample_count": 59}, "timestamp": "2026-01-26T12:40:20.430237"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11602.357, "latencies_ms": [11602.357], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photo, a man is riding a horse on a bridge. The man is wearing a jacket and is holding the reins of the horse, guiding it as they move across the bridge. The horse is positioned in the center of the image, with the man standing on its back. The bridge appears to be made of wood and spans across the scene", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 20966.1, "ram_available_mb": 41874.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20980.1, "ram_available_mb": 41860.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.283}, "power_stats": {"power_gpu_soc_mean_watts": 19.332, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 70.283, "power_watts_avg": 19.332, "energy_joules_est": 224.31, "duration_seconds": 11.603, "sample_count": 99}, "timestamp": "2026-01-26T12:40:34.061731"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10194.609, "latencies_ms": [10194.609], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. Man: 1\n2. Horse: 1\n3. Stick: 1\n4. Sweater: 1\n5. Rider: 1\n6. Blanket: 1\n7. Ground: 1\n8. Sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20980.1, "ram_available_mb": 41860.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20930.2, "ram_available_mb": 41910.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.31}, "power_stats": {"power_gpu_soc_mean_watts": 19.998, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 8.596, "gpu_utilization_percent_mean": 71.31, "power_watts_avg": 19.998, "energy_joules_est": 203.88, "duration_seconds": 10.195, "sample_count": 87}, "timestamp": "2026-01-26T12:40:46.295069"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10430.427, "latencies_ms": [10430.427], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The person is standing on the back of the horse, which is in the foreground of the image. The background is blurred, but it appears to be a flat landscape with no other objects in the immediate vicinity. The horse and rider are the main focus of the image, with the background serving as a backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20930.2, "ram_available_mb": 41910.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.6, "ram_used_mb": 20987.8, "ram_available_mb": 41853.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.156}, "power_stats": {"power_gpu_soc_mean_watts": 19.635, "power_cpu_cv_mean_watts": 1.868, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 71.156, "power_watts_avg": 19.635, "energy_joules_est": 204.81, "duration_seconds": 10.431, "sample_count": 90}, "timestamp": "2026-01-26T12:40:58.786353"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6935.141, "latencies_ms": [6935.141], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A man is riding a horse, and the horse is moving at a fast pace. The man is wearing a jacket with the word \"Rio\" on it.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 20934.4, "ram_available_mb": 41906.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 21004.7, "ram_available_mb": 41836.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.103}, "power_stats": {"power_gpu_soc_mean_watts": 22.16, "power_cpu_cv_mean_watts": 1.877, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 75.103, "power_watts_avg": 22.16, "energy_joules_est": 153.69, "duration_seconds": 6.936, "sample_count": 58}, "timestamp": "2026-01-26T12:41:07.747534"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6557.578, "latencies_ms": [6557.578], "images_per_second": 0.152, "prompt_tokens": 36, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The image is in black and white, with a high contrast between the light and dark areas. The weather appears to be overcast, as the sky is filled with clouds.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21004.7, "ram_available_mb": 41836.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 20952.0, "ram_available_mb": 41888.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.909}, "power_stats": {"power_gpu_soc_mean_watts": 22.089, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.704, "gpu_utilization_percent_mean": 74.909, "power_watts_avg": 22.089, "energy_joules_est": 144.86, "duration_seconds": 6.558, "sample_count": 55}, "timestamp": "2026-01-26T12:41:16.319794"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11575.962, "latencies_ms": [11575.962], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a group of geese swimming in a calm lake. There are at least six geese visible in the water, with some of them closer to the foreground and others further away. The geese are floating peacefully, enjoying the tranquility of the water.\n\nThe lake is surrounded by a lush green landscape, with trees", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 20952.0, "ram_available_mb": 41888.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20940.8, "ram_available_mb": 41900.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.86}, "power_stats": {"power_gpu_soc_mean_watts": 19.349, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 70.86, "power_watts_avg": 19.349, "energy_joules_est": 224.0, "duration_seconds": 11.577, "sample_count": 100}, "timestamp": "2026-01-26T12:41:29.970036"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4441.463, "latencies_ms": [4441.463], "images_per_second": 0.225, "prompt_tokens": 39, "response_tokens_est": 15, "n_tiles": 16, "output_text": "geese: 5, trees: numerous, grasses: dense", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20940.8, "ram_available_mb": 41900.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 21001.8, "ram_available_mb": 41839.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 81.216}, "power_stats": {"power_gpu_soc_mean_watts": 24.955, "power_cpu_cv_mean_watts": 0.984, "power_sys_5v0_mean_watts": 8.51, "gpu_utilization_percent_mean": 81.216, "power_watts_avg": 24.955, "energy_joules_est": 110.85, "duration_seconds": 4.442, "sample_count": 37}, "timestamp": "2026-01-26T12:41:36.471045"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11578.011, "latencies_ms": [11578.011], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are three ducks swimming in the water, with one duck slightly closer to the viewer than the others. In the background, there are trees and grasses along the edge of the water, creating a natural border for the scene. The ducks are positioned in the center of the image, with the trees and grasses appearing further away, creating a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21001.8, "ram_available_mb": 41839.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21000.5, "ram_available_mb": 41840.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.071}, "power_stats": {"power_gpu_soc_mean_watts": 19.309, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.648, "gpu_utilization_percent_mean": 71.071, "power_watts_avg": 19.309, "energy_joules_est": 223.57, "duration_seconds": 11.579, "sample_count": 99}, "timestamp": "2026-01-26T12:41:50.088381"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7441.975, "latencies_ms": [7441.975], "images_per_second": 0.134, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A group of geese is swimming in a calm body of water surrounded by lush greenery and tall grasses. The tranquil scene depicts a peaceful moment in nature.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20947.1, "ram_available_mb": 41893.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21007.4, "ram_available_mb": 41833.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.889}, "power_stats": {"power_gpu_soc_mean_watts": 21.406, "power_cpu_cv_mean_watts": 1.557, "power_sys_5v0_mean_watts": 8.554, "gpu_utilization_percent_mean": 73.889, "power_watts_avg": 21.406, "energy_joules_est": 159.32, "duration_seconds": 7.443, "sample_count": 63}, "timestamp": "2026-01-26T12:41:59.572984"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7113.576, "latencies_ms": [7113.576], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image features a serene body of water with a group of geese swimming in it. The water is reflecting the surrounding trees and grass, creating a calm and peaceful atmosphere.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21007.4, "ram_available_mb": 41833.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20955.4, "ram_available_mb": 41885.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.82}, "power_stats": {"power_gpu_soc_mean_watts": 21.242, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.609, "gpu_utilization_percent_mean": 73.82, "power_watts_avg": 21.242, "energy_joules_est": 151.12, "duration_seconds": 7.114, "sample_count": 61}, "timestamp": "2026-01-26T12:42:08.747492"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11569.652, "latencies_ms": [11569.652], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a cat with a coat of white and orange fur is perched on the hood of a black Mercedes-Benz car. The car is parked in front of a brick building, and a green fence can be seen in the background. The cat appears to be looking down at the car, perhaps curious about its surroundings or simply enjoying the view from", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 20955.4, "ram_available_mb": 41885.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20996.6, "ram_available_mb": 41844.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.673}, "power_stats": {"power_gpu_soc_mean_watts": 19.401, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.681, "gpu_utilization_percent_mean": 69.673, "power_watts_avg": 19.401, "energy_joules_est": 224.48, "duration_seconds": 11.57, "sample_count": 98}, "timestamp": "2026-01-26T12:42:22.351121"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7782.429, "latencies_ms": [7782.429], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "car: 1\nmercedes logo: 1\nwindow: 4\ngrill: 1\ncat: 1\nbuilding: 1\nplant: 1\nflower: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20942.3, "ram_available_mb": 41898.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21005.9, "ram_available_mb": 41835.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.909}, "power_stats": {"power_gpu_soc_mean_watts": 21.287, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.57, "gpu_utilization_percent_mean": 73.909, "power_watts_avg": 21.287, "energy_joules_est": 165.68, "duration_seconds": 7.783, "sample_count": 66}, "timestamp": "2026-01-26T12:42:32.146147"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7568.995, "latencies_ms": [7568.995], "images_per_second": 0.132, "prompt_tokens": 44, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The cat is sitting on the hood of a black Mercedes car, which is in the foreground of the image. In the background, there is a building with windows and a fence with green plants.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 21005.9, "ram_available_mb": 41835.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21019.6, "ram_available_mb": 41821.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.062}, "power_stats": {"power_gpu_soc_mean_watts": 20.996, "power_cpu_cv_mean_watts": 1.626, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 72.062, "power_watts_avg": 20.996, "energy_joules_est": 158.93, "duration_seconds": 7.57, "sample_count": 64}, "timestamp": "2026-01-26T12:42:41.732086"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9097.63, "latencies_ms": [9097.63], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "A cat is sitting on the hood of a black Mercedes-Benz car, with its reflection visible in the glossy surface. The car is parked in front of a building with a brick facade and a green fence with plants behind it.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21019.6, "ram_available_mb": 41821.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20950.5, "ram_available_mb": 41890.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.355}, "power_stats": {"power_gpu_soc_mean_watts": 20.374, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 72.355, "power_watts_avg": 20.374, "energy_joules_est": 185.37, "duration_seconds": 9.098, "sample_count": 76}, "timestamp": "2026-01-26T12:42:52.846576"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7771.14, "latencies_ms": [7771.14], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image features a black Mercedes-Benz car with a cat sitting on the hood. The car is parked in front of a building with a brick facade and a green fence with plants behind it.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20950.5, "ram_available_mb": 41890.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20996.4, "ram_available_mb": 41844.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.5}, "power_stats": {"power_gpu_soc_mean_watts": 20.923, "power_cpu_cv_mean_watts": 1.644, "power_sys_5v0_mean_watts": 8.613, "gpu_utilization_percent_mean": 71.5, "power_watts_avg": 20.923, "energy_joules_est": 162.61, "duration_seconds": 7.772, "sample_count": 66}, "timestamp": "2026-01-26T12:43:02.638666"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11589.152, "latencies_ms": [11589.152], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a snowboarder is captured in mid-air, performing a daring trick on a snowy mountain. The snowboarder is wearing a brown jacket and yellow pants, and is holding onto a snowboard with both hands. The snowboarder is suspended in the air, with the snowboard angled upwards, indicating a jump or a trick.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 20943.2, "ram_available_mb": 41897.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20938.8, "ram_available_mb": 41902.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.899}, "power_stats": {"power_gpu_soc_mean_watts": 19.329, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 69.899, "power_watts_avg": 19.329, "energy_joules_est": 224.02, "duration_seconds": 11.59, "sample_count": 99}, "timestamp": "2026-01-26T12:43:16.257786"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9607.976, "latencies_ms": [9607.976], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "sky: 1\n\nsnowboard: 1\n\nsnowflakes: numerous\n\nsnow: 1\n\nsnowboarder: 1\n\nsweater: 1\n\npants: 1\n\njacket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20938.8, "ram_available_mb": 41902.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20944.5, "ram_available_mb": 41896.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.036}, "power_stats": {"power_gpu_soc_mean_watts": 20.188, "power_cpu_cv_mean_watts": 1.746, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 72.036, "power_watts_avg": 20.188, "energy_joules_est": 193.98, "duration_seconds": 9.609, "sample_count": 83}, "timestamp": "2026-01-26T12:43:27.892361"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9058.041, "latencies_ms": [9058.041], "images_per_second": 0.11, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The snowboarder is in the foreground, performing a trick in the air above a snowy hill. The clear blue sky forms the background, and the snowflakes are scattered throughout the image, indicating that the photo was taken on a cold, snowy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20944.5, "ram_available_mb": 41896.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20996.4, "ram_available_mb": 41844.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.455}, "power_stats": {"power_gpu_soc_mean_watts": 20.234, "power_cpu_cv_mean_watts": 1.752, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 71.455, "power_watts_avg": 20.234, "energy_joules_est": 183.29, "duration_seconds": 9.059, "sample_count": 77}, "timestamp": "2026-01-26T12:43:38.978416"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9055.061, "latencies_ms": [9055.061], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "A snowboarder is captured in mid-air, performing a jump above a snowy hill under a clear blue sky. The snowboarder is wearing a brown jacket and yellow pants, and is in a dynamic pose with arms outstretched.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20934.0, "ram_available_mb": 41906.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20937.6, "ram_available_mb": 41903.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.403}, "power_stats": {"power_gpu_soc_mean_watts": 20.4, "power_cpu_cv_mean_watts": 1.71, "power_sys_5v0_mean_watts": 8.574, "gpu_utilization_percent_mean": 73.403, "power_watts_avg": 20.4, "energy_joules_est": 184.74, "duration_seconds": 9.056, "sample_count": 77}, "timestamp": "2026-01-26T12:43:50.047965"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7994.138, "latencies_ms": [7994.138], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The snowboarder is wearing a brown jacket and bright yellow pants, and is performing a trick in the air. The sky is a clear blue, and there are snowflakes falling around the snowboarder.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20937.6, "ram_available_mb": 41903.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20996.3, "ram_available_mb": 41844.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.913}, "power_stats": {"power_gpu_soc_mean_watts": 20.753, "power_cpu_cv_mean_watts": 1.688, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 72.913, "power_watts_avg": 20.753, "energy_joules_est": 165.92, "duration_seconds": 7.995, "sample_count": 69}, "timestamp": "2026-01-26T12:44:00.094831"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11537.474, "latencies_ms": [11537.474], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a quaint, small bathroom bathed in soft light. Dominating the scene is a white toilet, its lid slightly ajar, revealing a glimpse of the interior. The toilet is positioned against a pristine white wall, which is adorned with a single light switch, casting a warm glow on the surroundings", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20933.5, "ram_available_mb": 41907.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20972.2, "ram_available_mb": 41868.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.75}, "power_stats": {"power_gpu_soc_mean_watts": 19.3, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 70.75, "power_watts_avg": 19.3, "energy_joules_est": 222.69, "duration_seconds": 11.538, "sample_count": 100}, "timestamp": "2026-01-26T12:44:13.676530"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10484.925, "latencies_ms": [10484.925], "images_per_second": 0.095, "prompt_tokens": 39, "response_tokens_est": 68, "n_tiles": 16, "output_text": "- Toilet: 1\n\n- Bathtub: 1\n\n- Pipes: 10\n\n- Chain: 1\n\n- Tiles: 1\n\n- Door: 1\n\n- Light fixture: 1\n\n- Graffiti: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20972.2, "ram_available_mb": 41868.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20987.4, "ram_available_mb": 41853.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.378}, "power_stats": {"power_gpu_soc_mean_watts": 19.985, "power_cpu_cv_mean_watts": 1.819, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 71.378, "power_watts_avg": 19.985, "energy_joules_est": 209.55, "duration_seconds": 10.486, "sample_count": 90}, "timestamp": "2026-01-26T12:44:26.200921"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11676.259, "latencies_ms": [11676.259], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a toilet positioned on the left side, which is relatively close to the viewer. Behind the toilet, there is a white bathtub on the right side, which is further away from the viewer. The pipes and tanks are mounted on the wall in the background, creating a sense of depth in the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20926.3, "ram_available_mb": 41914.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.7}, "power_stats": {"power_gpu_soc_mean_watts": 19.465, "power_cpu_cv_mean_watts": 1.877, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 69.7, "power_watts_avg": 19.465, "energy_joules_est": 227.29, "duration_seconds": 11.677, "sample_count": 100}, "timestamp": "2026-01-26T12:44:39.911502"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11783.067, "latencies_ms": [11783.067], "images_per_second": 0.085, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a small, old-fashioned bathroom with a toilet and a bathtub. The toilet has a unique design with a wooden seat and a white tank, while the bathtub is white and appears to be made of porcelain. The walls are painted white, and there are multiple pipes running along the walls, giving the room", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20934.1, "ram_available_mb": 41906.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20925.0, "ram_available_mb": 41915.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.804}, "power_stats": {"power_gpu_soc_mean_watts": 19.543, "power_cpu_cv_mean_watts": 1.868, "power_sys_5v0_mean_watts": 8.608, "gpu_utilization_percent_mean": 70.804, "power_watts_avg": 19.543, "energy_joules_est": 230.29, "duration_seconds": 11.784, "sample_count": 102}, "timestamp": "2026-01-26T12:44:53.720586"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11636.58, "latencies_ms": [11636.58], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a bathroom with a vintage or industrial aesthetic, featuring a white toilet with a wooden seat and lid, a white bathtub with metal fixtures, and a complex network of white pipes and chains hanging from the ceiling. The lighting is dim, and the walls are painted in a light color, possibly white or cre", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20925.0, "ram_available_mb": 41915.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20991.7, "ram_available_mb": 41849.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.556}, "power_stats": {"power_gpu_soc_mean_watts": 19.35, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 70.556, "power_watts_avg": 19.35, "energy_joules_est": 225.18, "duration_seconds": 11.637, "sample_count": 99}, "timestamp": "2026-01-26T12:45:07.409681"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11589.054, "latencies_ms": [11589.054], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a bustling city, a bronze statue of a man stands tall on a stone pedestal. The man, frozen in a moment of joy, is depicted flying a kite. The kite, a vibrant spectacle of colors, is shaped like a fish and soars high in the sky. The statue and the kite are positioned", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20936.7, "ram_available_mb": 41904.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.0, "ram_used_mb": 20988.9, "ram_available_mb": 41852.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.99}, "power_stats": {"power_gpu_soc_mean_watts": 19.136, "power_cpu_cv_mean_watts": 2.03, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 69.99, "power_watts_avg": 19.136, "energy_joules_est": 221.78, "duration_seconds": 11.59, "sample_count": 99}, "timestamp": "2026-01-26T12:45:21.045532"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10995.702, "latencies_ms": [10995.702], "images_per_second": 0.091, "prompt_tokens": 39, "response_tokens_est": 72, "n_tiles": 16, "output_text": "1. Kite: 1\n2. Statue: 1\n3. Building: 1\n4. Skyscraper: 1\n5. Clouds: 1\n6. Sculpture: 1\n7. Statue base: 1\n8. Sculpture base: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20935.5, "ram_available_mb": 41905.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20930.3, "ram_available_mb": 41910.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.596}, "power_stats": {"power_gpu_soc_mean_watts": 19.788, "power_cpu_cv_mean_watts": 1.818, "power_sys_5v0_mean_watts": 8.603, "gpu_utilization_percent_mean": 71.596, "power_watts_avg": 19.788, "energy_joules_est": 217.6, "duration_seconds": 10.996, "sample_count": 94}, "timestamp": "2026-01-26T12:45:34.064691"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11658.317, "latencies_ms": [11658.317], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a statue of a person holding a kite, which is positioned near the top right corner of the image. The kite is flying in the air, slightly above the statue's head. In the background, there is a tall building with a glass facade, and the sky is visible above it. The statue appears to be in front of the building", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20930.3, "ram_available_mb": 41910.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20990.0, "ram_available_mb": 41850.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.77}, "power_stats": {"power_gpu_soc_mean_watts": 19.355, "power_cpu_cv_mean_watts": 1.881, "power_sys_5v0_mean_watts": 8.639, "gpu_utilization_percent_mean": 69.77, "power_watts_avg": 19.355, "energy_joules_est": 225.66, "duration_seconds": 11.659, "sample_count": 100}, "timestamp": "2026-01-26T12:45:47.743492"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7645.519, "latencies_ms": [7645.519], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A statue of a person holding a kite is located in front of a modern building with glass windows. The kite is flying high in the sky, and the person appears to be enjoying the activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20990.0, "ram_available_mb": 41850.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20936.0, "ram_available_mb": 41904.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.385}, "power_stats": {"power_gpu_soc_mean_watts": 21.464, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 74.385, "power_watts_avg": 21.464, "energy_joules_est": 164.12, "duration_seconds": 7.646, "sample_count": 65}, "timestamp": "2026-01-26T12:45:57.442988"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8187.876, "latencies_ms": [8187.876], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a statue of a person holding a kite, with the kite displaying a colorful pattern. The statue is located in front of a modern building with a glass facade, and the sky appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20936.0, "ram_available_mb": 41904.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20992.4, "ram_available_mb": 41848.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.114}, "power_stats": {"power_gpu_soc_mean_watts": 20.823, "power_cpu_cv_mean_watts": 1.67, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 73.114, "power_watts_avg": 20.823, "energy_joules_est": 170.51, "duration_seconds": 8.189, "sample_count": 70}, "timestamp": "2026-01-26T12:46:07.680877"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11568.422, "latencies_ms": [11568.422], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a table filled with a variety of fresh vegetables and fruits. There are several bowls and baskets containing different types of produce. The table is covered with a diverse assortment of vegetables, including carrots, potatoes, and broccoli. \n\nIn addition to the vegetables, there are also some fruits present, such as st", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20938.9, "ram_available_mb": 41901.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21002.8, "ram_available_mb": 41838.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.929}, "power_stats": {"power_gpu_soc_mean_watts": 19.375, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 69.929, "power_watts_avg": 19.375, "energy_joules_est": 224.15, "duration_seconds": 11.569, "sample_count": 98}, "timestamp": "2026-01-26T12:46:21.305279"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9477.928, "latencies_ms": [9477.928], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "strawberries: 20, broccoli: 1, cucumber: 1, radishes: 12, carrots: 5, potatoes: 8, green beans: 10, asparagus: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20949.3, "ram_available_mb": 41891.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20941.4, "ram_available_mb": 41899.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.5}, "power_stats": {"power_gpu_soc_mean_watts": 20.212, "power_cpu_cv_mean_watts": 1.742, "power_sys_5v0_mean_watts": 8.588, "gpu_utilization_percent_mean": 72.5, "power_watts_avg": 20.212, "energy_joules_est": 191.58, "duration_seconds": 9.479, "sample_count": 82}, "timestamp": "2026-01-26T12:46:32.801814"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10526.934, "latencies_ms": [10526.934], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there are bright red strawberries in a wooden bowl on the left side of the image. Behind them, in the middle ground, are green beans in a white plastic bag. In the background, there are various vegetables including radishes, carrots, and asparagus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20941.4, "ram_available_mb": 41899.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20994.5, "ram_available_mb": 41846.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.344}, "power_stats": {"power_gpu_soc_mean_watts": 19.665, "power_cpu_cv_mean_watts": 1.841, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 70.344, "power_watts_avg": 19.665, "energy_joules_est": 207.02, "duration_seconds": 10.528, "sample_count": 90}, "timestamp": "2026-01-26T12:46:45.371621"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10533.881, "latencies_ms": [10533.881], "images_per_second": 0.095, "prompt_tokens": 37, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image showcases a variety of fresh vegetables and fruits arranged on a table, including strawberries, broccoli, carrots, beets, and asparagus. The vegetables are displayed in different containers, such as bowls and bags, creating an appealing and colorful presentation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20994.5, "ram_available_mb": 41846.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20982.4, "ram_available_mb": 41858.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.189}, "power_stats": {"power_gpu_soc_mean_watts": 19.852, "power_cpu_cv_mean_watts": 1.797, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 71.189, "power_watts_avg": 19.852, "energy_joules_est": 209.13, "duration_seconds": 10.534, "sample_count": 90}, "timestamp": "2026-01-26T12:46:57.937416"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9249.048, "latencies_ms": [9249.048], "images_per_second": 0.108, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image features a variety of fresh produce, including vibrant red strawberries, green asparagus, and pink radishes, all arranged on a wooden surface. The lighting is bright and natural, highlighting the freshness and colors of the vegetables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20982.4, "ram_available_mb": 41858.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21041.6, "ram_available_mb": 41799.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.709}, "power_stats": {"power_gpu_soc_mean_watts": 19.98, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 71.709, "power_watts_avg": 19.98, "energy_joules_est": 184.81, "duration_seconds": 9.25, "sample_count": 79}, "timestamp": "2026-01-26T12:47:09.225269"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11600.841, "latencies_ms": [11600.841], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of joy and camaraderie among three individuals, likely friends, as they engage in a shared activity. They are comfortably seated on a vibrant, patterned couch, each holding a gaming controller, suggesting they are playing a video game together. The room is dimly lit, with a soft glow emanating from the television", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20938.0, "ram_available_mb": 41902.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20997.6, "ram_available_mb": 41843.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.152}, "power_stats": {"power_gpu_soc_mean_watts": 19.325, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 70.152, "power_watts_avg": 19.325, "energy_joules_est": 224.2, "duration_seconds": 11.601, "sample_count": 99}, "timestamp": "2026-01-26T12:47:22.854256"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8129.996, "latencies_ms": [8129.996], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "people: 3\ngame controllers: 2\ncouch: 1\npillows: 2\nbed: 1\nremote control: 2\nwii console: 1\ntv: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20997.6, "ram_available_mb": 41843.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21015.1, "ram_available_mb": 41825.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.754}, "power_stats": {"power_gpu_soc_mean_watts": 20.967, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 72.754, "power_watts_avg": 20.967, "energy_joules_est": 170.48, "duration_seconds": 8.131, "sample_count": 69}, "timestamp": "2026-01-26T12:47:33.045771"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11586.548, "latencies_ms": [11586.548], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person on the left side of the image, holding a gaming controller and facing towards the right side of the image where another person is seated on a couch. The third person is on the far right, also holding a controller and facing the left. The couch is positioned in the middle ground of the image, and there is a bed with", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20953.3, "ram_available_mb": 41887.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20950.9, "ram_available_mb": 41890.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.02}, "power_stats": {"power_gpu_soc_mean_watts": 19.307, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 71.02, "power_watts_avg": 19.307, "energy_joules_est": 223.71, "duration_seconds": 11.587, "sample_count": 99}, "timestamp": "2026-01-26T12:47:46.661432"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8694.377, "latencies_ms": [8694.377], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "Three people are sitting on a couch in a dimly lit room, each holding a gaming controller and laughing, suggesting they are playing a video game together. The room has a casual, homey atmosphere with a bed in the background.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20950.9, "ram_available_mb": 41890.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20954.7, "ram_available_mb": 41886.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.189}, "power_stats": {"power_gpu_soc_mean_watts": 20.575, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.608, "gpu_utilization_percent_mean": 73.189, "power_watts_avg": 20.575, "energy_joules_est": 178.9, "duration_seconds": 8.695, "sample_count": 74}, "timestamp": "2026-01-26T12:47:57.382409"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8461.363, "latencies_ms": [8461.363], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows three individuals in a dimly lit room, with one person holding a camera with a flash, suggesting low light conditions. The room has a cozy atmosphere with a patterned couch and a pile of pillows in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20954.7, "ram_available_mb": 41886.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20935.8, "ram_available_mb": 41905.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.681}, "power_stats": {"power_gpu_soc_mean_watts": 20.293, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 8.617, "gpu_utilization_percent_mean": 70.681, "power_watts_avg": 20.293, "energy_joules_est": 171.72, "duration_seconds": 8.462, "sample_count": 72}, "timestamp": "2026-01-26T12:48:07.863967"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11613.372, "latencies_ms": [11613.372], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of cows resting in a lush green field. There are at least five cows visible, with one cow lying down in the foreground and the others scattered throughout the field. The cows are of different sizes and are enjoying the grassy area. The scene is peaceful and serene, showcasing the natural environment where the cows are gra", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 20935.8, "ram_available_mb": 41905.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20995.2, "ram_available_mb": 41845.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.182}, "power_stats": {"power_gpu_soc_mean_watts": 19.259, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 70.182, "power_watts_avg": 19.259, "energy_joules_est": 223.67, "duration_seconds": 11.614, "sample_count": 99}, "timestamp": "2026-01-26T12:48:21.506717"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7703.692, "latencies_ms": [7703.692], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "tree: 1\ncows: 5\ngrass: many\nfield: 1\nsky: 1\nleaves: many\nstones: 0\nbirds: 0", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20933.2, "ram_available_mb": 41907.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20997.0, "ram_available_mb": 41843.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.864}, "power_stats": {"power_gpu_soc_mean_watts": 21.032, "power_cpu_cv_mean_watts": 1.577, "power_sys_5v0_mean_watts": 8.546, "gpu_utilization_percent_mean": 73.864, "power_watts_avg": 21.032, "energy_joules_est": 162.04, "duration_seconds": 7.704, "sample_count": 66}, "timestamp": "2026-01-26T12:48:31.232222"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11566.987, "latencies_ms": [11566.987], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a lone cow lying on the grass, closer to the viewer than the other cows. The cows are spread out in the background, with some lying down and others standing, all at a distance from the viewer. The tree trunk is in the foreground on the right side of the image, while the cows are scattered in the middle", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20943.4, "ram_available_mb": 41897.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20936.8, "ram_available_mb": 41904.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.06}, "power_stats": {"power_gpu_soc_mean_watts": 19.285, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.648, "gpu_utilization_percent_mean": 70.06, "power_watts_avg": 19.285, "energy_joules_est": 223.08, "duration_seconds": 11.568, "sample_count": 100}, "timestamp": "2026-01-26T12:48:44.817509"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7753.192, "latencies_ms": [7753.192], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A group of cows is resting in a lush green field, with one cow lying down close to a tree trunk. The cows appear to be at ease and enjoying the peaceful environment.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20936.8, "ram_available_mb": 41904.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20999.3, "ram_available_mb": 41841.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.537}, "power_stats": {"power_gpu_soc_mean_watts": 21.269, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.563, "gpu_utilization_percent_mean": 74.537, "power_watts_avg": 21.269, "energy_joules_est": 164.92, "duration_seconds": 7.754, "sample_count": 67}, "timestamp": "2026-01-26T12:48:54.600884"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7223.543, "latencies_ms": [7223.543], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image features a lush green field with a clear sky, indicating a sunny day. A tree with a rough bark texture is prominently visible on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20939.0, "ram_available_mb": 41901.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20932.0, "ram_available_mb": 41908.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.492}, "power_stats": {"power_gpu_soc_mean_watts": 21.239, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 73.492, "power_watts_avg": 21.239, "energy_joules_est": 153.44, "duration_seconds": 7.224, "sample_count": 61}, "timestamp": "2026-01-26T12:49:03.866508"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11613.174, "latencies_ms": [11613.174], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a black and white photograph of a large group of young boys, likely students, posing for a group photo. They are all dressed in formal attire, with some wearing ties. The boys are arranged in rows, with some sitting on the ground and others standing. The photograph appears to be from Goodmayes Boys' School, as indicated by the text at the bottom", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 20932.0, "ram_available_mb": 41908.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20937.7, "ram_available_mb": 41903.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.55}, "power_stats": {"power_gpu_soc_mean_watts": 19.22, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.63, "gpu_utilization_percent_mean": 70.55, "power_watts_avg": 19.22, "energy_joules_est": 223.22, "duration_seconds": 11.614, "sample_count": 100}, "timestamp": "2026-01-26T12:49:17.502318"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11782.496, "latencies_ms": [11782.496], "images_per_second": 0.085, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "group of boys: 50, boys wearing ties: 30, boys wearing suits: 10, boys wearing dress uniforms: 10, boys wearing sports uniforms: 10, boys wearing military uniforms: 10, boys wearing casual clothes: 10, boys wearing hats: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20937.7, "ram_available_mb": 41903.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.6, "ram_used_mb": 20992.6, "ram_available_mb": 41848.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.554}, "power_stats": {"power_gpu_soc_mean_watts": 19.431, "power_cpu_cv_mean_watts": 2.041, "power_sys_5v0_mean_watts": 8.618, "gpu_utilization_percent_mean": 70.554, "power_watts_avg": 19.431, "energy_joules_est": 228.96, "duration_seconds": 11.783, "sample_count": 101}, "timestamp": "2026-01-26T12:49:31.306461"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11563.135, "latencies_ms": [11563.135], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, the group of boys is arranged in two distinct formations. The larger group is seated in the foreground, with boys spaced out evenly across the ground, while the smaller group is standing in the background, positioned behind the seated boys, creating a sense of depth. The boys in the background appear to be slightly smaller due to the perspective, indicating they are", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20992.6, "ram_available_mb": 41848.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20950.6, "ram_available_mb": 41890.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.99}, "power_stats": {"power_gpu_soc_mean_watts": 19.186, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 69.99, "power_watts_avg": 19.186, "energy_joules_est": 221.86, "duration_seconds": 11.564, "sample_count": 100}, "timestamp": "2026-01-26T12:49:44.903553"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9114.259, "latencies_ms": [9114.259], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image is a black and white photograph of a large group of students at Goodmayes Boys' School, taken in April 1929. The students are arranged in rows, with some sitting and others standing, and they are all dressed in formal attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20950.6, "ram_available_mb": 41890.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20953.3, "ram_available_mb": 41887.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.935}, "power_stats": {"power_gpu_soc_mean_watts": 20.527, "power_cpu_cv_mean_watts": 1.7, "power_sys_5v0_mean_watts": 8.598, "gpu_utilization_percent_mean": 72.935, "power_watts_avg": 20.527, "energy_joules_est": 187.1, "duration_seconds": 9.115, "sample_count": 77}, "timestamp": "2026-01-26T12:49:56.033301"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8356.202, "latencies_ms": [8356.202], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image is a black and white photograph, indicating it was taken in an era before color photography was common. The lighting is even, suggesting it was taken on a clear day, and the attire of the individuals suggests a formal occasion.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20953.3, "ram_available_mb": 41887.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20929.6, "ram_available_mb": 41911.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.127}, "power_stats": {"power_gpu_soc_mean_watts": 20.567, "power_cpu_cv_mean_watts": 1.691, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 72.127, "power_watts_avg": 20.567, "energy_joules_est": 171.87, "duration_seconds": 8.357, "sample_count": 71}, "timestamp": "2026-01-26T12:50:06.423662"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11603.739, "latencies_ms": [11603.739], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene of a kite soaring high in the sky. The kite, a striking combination of orange and blue, is tilted slightly to the left, adding a dynamic element to the composition. It's flying over a lush green field, which is dotted with trees and buildings in the background, suggesting a park or a similar recreational", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 20929.6, "ram_available_mb": 41911.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21011.3, "ram_available_mb": 41829.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.636}, "power_stats": {"power_gpu_soc_mean_watts": 19.285, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 69.636, "power_watts_avg": 19.285, "energy_joules_est": 223.79, "duration_seconds": 11.604, "sample_count": 99}, "timestamp": "2026-01-26T12:50:20.074693"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6770.645, "latencies_ms": [6770.645], "images_per_second": 0.148, "prompt_tokens": 39, "response_tokens_est": 35, "n_tiles": 16, "output_text": "kite: 1, cloud: multiple, tree: multiple, building: multiple, grass: multiple, sky: multiple, person: 1, car: multiple", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20941.1, "ram_available_mb": 41899.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20935.3, "ram_available_mb": 41905.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.81}, "power_stats": {"power_gpu_soc_mean_watts": 21.733, "power_cpu_cv_mean_watts": 1.476, "power_sys_5v0_mean_watts": 8.542, "gpu_utilization_percent_mean": 75.81, "power_watts_avg": 21.733, "energy_joules_est": 147.16, "duration_seconds": 6.771, "sample_count": 58}, "timestamp": "2026-01-26T12:50:28.857383"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10885.103, "latencies_ms": [10885.103], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The kite is flying in the sky, positioned in the upper right quadrant of the image, while the buildings are situated in the lower left quadrant, indicating that the kite is in the background relative to the buildings. The trees are scattered throughout the image, with some closer to the buildings and others further away, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20935.3, "ram_available_mb": 41905.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20933.1, "ram_available_mb": 41907.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.796}, "power_stats": {"power_gpu_soc_mean_watts": 19.5, "power_cpu_cv_mean_watts": 1.864, "power_sys_5v0_mean_watts": 8.63, "gpu_utilization_percent_mean": 70.796, "power_watts_avg": 19.5, "energy_joules_est": 212.27, "duration_seconds": 10.886, "sample_count": 93}, "timestamp": "2026-01-26T12:50:41.802160"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7769.033, "latencies_ms": [7769.033], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A colorful kite is flying high in the sky above a park with trees and buildings in the background. The kite appears to be a rainbow-colored butterfly or dragonfly design.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20933.1, "ram_available_mb": 41907.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21008.7, "ram_available_mb": 41832.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.061}, "power_stats": {"power_gpu_soc_mean_watts": 21.282, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.559, "gpu_utilization_percent_mean": 74.061, "power_watts_avg": 21.282, "energy_joules_est": 165.35, "duration_seconds": 7.77, "sample_count": 66}, "timestamp": "2026-01-26T12:50:51.624492"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7781.341, "latencies_ms": [7781.341], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The kite in the image has a gradient of colors, transitioning from orange to blue to purple. It is flying high in the sky with a backdrop of fluffy white clouds and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21008.7, "ram_available_mb": 41832.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21009.7, "ram_available_mb": 41831.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.515}, "power_stats": {"power_gpu_soc_mean_watts": 20.858, "power_cpu_cv_mean_watts": 1.637, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 71.515, "power_watts_avg": 20.858, "energy_joules_est": 162.32, "duration_seconds": 7.782, "sample_count": 66}, "timestamp": "2026-01-26T12:51:01.423726"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11577.754, "latencies_ms": [11577.754], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of anticipation, a pizza resting in a cardboard box, waiting to be savored. The pizza, with its golden brown crust, is the star of the show. It's generously topped with a layer of melted cheese that has turned a light golden color, indicating it's been baked to perfection.", "error": null, "sys_before": {"cpu_percent": 5.4, "ram_used_mb": 20937.9, "ram_available_mb": 41903.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20928.1, "ram_available_mb": 41912.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.941}, "power_stats": {"power_gpu_soc_mean_watts": 19.267, "power_cpu_cv_mean_watts": 1.91, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 69.941, "power_watts_avg": 19.267, "energy_joules_est": 223.08, "duration_seconds": 11.578, "sample_count": 101}, "timestamp": "2026-01-26T12:51:15.055395"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8576.966, "latencies_ms": [8576.966], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "pizza: 1\nbox: 1\ncheese: 1\ntomato sauce: 1\npepperoni: 1\nmushroom: 1\nolive: 1\nbasil: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20928.1, "ram_available_mb": 41912.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20935.0, "ram_available_mb": 41905.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.863}, "power_stats": {"power_gpu_soc_mean_watts": 20.791, "power_cpu_cv_mean_watts": 1.661, "power_sys_5v0_mean_watts": 8.57, "gpu_utilization_percent_mean": 72.863, "power_watts_avg": 20.791, "energy_joules_est": 178.34, "duration_seconds": 8.578, "sample_count": 73}, "timestamp": "2026-01-26T12:51:25.651424"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11578.602, "latencies_ms": [11578.602], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The pizza is centrally located in the image, occupying the majority of the space. It is placed within a cardboard pizza box, which is positioned on a flat surface that appears to be a table or countertop. The pizza is in the foreground, making it the main focus of the image, while the cardboard box provides a sense of depth, indicating that", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20935.0, "ram_available_mb": 41905.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20976.7, "ram_available_mb": 41864.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.768}, "power_stats": {"power_gpu_soc_mean_watts": 19.287, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 69.768, "power_watts_avg": 19.287, "energy_joules_est": 223.33, "duration_seconds": 11.579, "sample_count": 99}, "timestamp": "2026-01-26T12:51:39.245119"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7534.486, "latencies_ms": [7534.486], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A large pizza with melted cheese and tomato sauce is placed in a cardboard pizza box. The pizza appears to be freshly baked and ready to be served.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20976.7, "ram_available_mb": 41864.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20926.9, "ram_available_mb": 41914.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.594}, "power_stats": {"power_gpu_soc_mean_watts": 21.441, "power_cpu_cv_mean_watts": 1.532, "power_sys_5v0_mean_watts": 8.551, "gpu_utilization_percent_mean": 73.594, "power_watts_avg": 21.441, "energy_joules_est": 161.56, "duration_seconds": 7.535, "sample_count": 64}, "timestamp": "2026-01-26T12:51:48.832546"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9962.118, "latencies_ms": [9962.118], "images_per_second": 0.1, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The pizza in the image has a golden-brown crust with a generous amount of melted cheese on top, which is slightly browned in spots. It appears to be a pepperoni pizza, as there are visible slices of pepperoni scattered across the cheese.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20926.9, "ram_available_mb": 41914.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20989.7, "ram_available_mb": 41851.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.718}, "power_stats": {"power_gpu_soc_mean_watts": 19.732, "power_cpu_cv_mean_watts": 1.822, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 70.718, "power_watts_avg": 19.732, "energy_joules_est": 196.58, "duration_seconds": 9.963, "sample_count": 85}, "timestamp": "2026-01-26T12:52:00.840160"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11589.223, "latencies_ms": [11589.223], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two women are sitting on a refrigerator placed on the sidewalk. One woman is sitting on the left side of the refrigerator, while the other woman is sitting on the right side. They both appear to be enjoying their time together, possibly having a conversation or sharing a drink.\n\nThere are two cups visible in the scene, one near", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 20934.7, "ram_available_mb": 41906.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20937.5, "ram_available_mb": 41903.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.808}, "power_stats": {"power_gpu_soc_mean_watts": 19.358, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 69.808, "power_watts_avg": 19.358, "energy_joules_est": 224.36, "duration_seconds": 11.59, "sample_count": 99}, "timestamp": "2026-01-26T12:52:14.493274"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8920.484, "latencies_ms": [8920.484], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Woman: 2\n- Refrigerator: 1\n- Beer: 2\n- Cup: 2\n- Street: 1\n- Sidewalk: 1\n- Pavement: 1\n- Chair: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20937.5, "ram_available_mb": 41903.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20989.7, "ram_available_mb": 41851.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.224}, "power_stats": {"power_gpu_soc_mean_watts": 20.587, "power_cpu_cv_mean_watts": 1.691, "power_sys_5v0_mean_watts": 8.574, "gpu_utilization_percent_mean": 72.224, "power_watts_avg": 20.587, "energy_joules_est": 183.66, "duration_seconds": 8.921, "sample_count": 76}, "timestamp": "2026-01-26T12:52:25.467297"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9607.745, "latencies_ms": [9607.745], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The woman sitting on the left is positioned in the foreground and is closer to the camera than the woman sitting in the fridge. The fridge is located on the right side of the image, near the curb, and is further away from the camera than the woman sitting on the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20989.7, "ram_available_mb": 41851.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20949.6, "ram_available_mb": 41891.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.122}, "power_stats": {"power_gpu_soc_mean_watts": 20.016, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 71.122, "power_watts_avg": 20.016, "energy_joules_est": 192.32, "duration_seconds": 9.608, "sample_count": 82}, "timestamp": "2026-01-26T12:52:37.103612"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7218.749, "latencies_ms": [7218.749], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "Two women are sitting on a small refrigerator placed on the sidewalk. One woman is talking on her cell phone while the other woman is sitting inside the refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20949.6, "ram_available_mb": 41891.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20952.0, "ram_available_mb": 41888.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.459}, "power_stats": {"power_gpu_soc_mean_watts": 21.697, "power_cpu_cv_mean_watts": 1.509, "power_sys_5v0_mean_watts": 8.543, "gpu_utilization_percent_mean": 74.459, "power_watts_avg": 21.697, "energy_joules_est": 156.64, "duration_seconds": 7.219, "sample_count": 61}, "timestamp": "2026-01-26T12:52:46.370596"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7352.544, "latencies_ms": [7352.544], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image shows a sunny day with clear skies, as indicated by the bright lighting and shadows cast on the ground. The weather appears to be mild, suitable for outdoor activities.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20952.0, "ram_available_mb": 41888.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20992.5, "ram_available_mb": 41848.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.339}, "power_stats": {"power_gpu_soc_mean_watts": 21.21, "power_cpu_cv_mean_watts": 1.568, "power_sys_5v0_mean_watts": 8.595, "gpu_utilization_percent_mean": 73.339, "power_watts_avg": 21.21, "energy_joules_est": 155.96, "duration_seconds": 7.353, "sample_count": 62}, "timestamp": "2026-01-26T12:52:55.764939"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11572.512, "latencies_ms": [11572.512], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man wearing a hat is sitting at a dining table, enjoying a meal of hot dogs. There are several hot dogs on the table, with some placed in buns and others laid out on a tray. The man appears to be focused on his food, possibly taking a bite or preparing to eat. The scene suggests a casual outdoor", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 20992.5, "ram_available_mb": 41848.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20930.8, "ram_available_mb": 41910.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.582}, "power_stats": {"power_gpu_soc_mean_watts": 19.297, "power_cpu_cv_mean_watts": 1.879, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 70.582, "power_watts_avg": 19.297, "energy_joules_est": 223.33, "duration_seconds": 11.573, "sample_count": 98}, "timestamp": "2026-01-26T12:53:09.364893"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8944.166, "latencies_ms": [8944.166], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Hot dog: 10\n- Bun: 10\n- Man: 1\n- Hat: 1\n- Chair: 1\n- Tray: 1\n- Grass: 1\n- Grill: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20930.8, "ram_available_mb": 41910.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20927.5, "ram_available_mb": 41913.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.461}, "power_stats": {"power_gpu_soc_mean_watts": 20.624, "power_cpu_cv_mean_watts": 1.696, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 72.461, "power_watts_avg": 20.624, "energy_joules_est": 184.48, "duration_seconds": 8.945, "sample_count": 76}, "timestamp": "2026-01-26T12:53:20.360875"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9616.383, "latencies_ms": [9616.383], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "In the foreground, there is a tray of hot dogs with one being held by a person. The person is seated to the right of the tray, wearing a green shirt and a straw hat. The background shows a grassy area, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20927.5, "ram_available_mb": 41913.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.6, "ram_used_mb": 21003.0, "ram_available_mb": 41837.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.634}, "power_stats": {"power_gpu_soc_mean_watts": 20.03, "power_cpu_cv_mean_watts": 1.957, "power_sys_5v0_mean_watts": 8.63, "gpu_utilization_percent_mean": 71.634, "power_watts_avg": 20.03, "energy_joules_est": 192.63, "duration_seconds": 9.617, "sample_count": 82}, "timestamp": "2026-01-26T12:53:31.997953"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9402.78, "latencies_ms": [9402.78], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "A man wearing a straw hat and a green polo shirt is sitting at a table with a tray of hot dogs on aluminum foil. He appears to be enjoying a meal outdoors, possibly at a picnic or outdoor event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21003.0, "ram_available_mb": 41837.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.5, "ram_used_mb": 21059.4, "ram_available_mb": 41781.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.325}, "power_stats": {"power_gpu_soc_mean_watts": 20.33, "power_cpu_cv_mean_watts": 2.132, "power_sys_5v0_mean_watts": 8.609, "gpu_utilization_percent_mean": 71.325, "power_watts_avg": 20.33, "energy_joules_est": 191.17, "duration_seconds": 9.403, "sample_count": 80}, "timestamp": "2026-01-26T12:53:43.434011"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9681.676, "latencies_ms": [9681.676], "images_per_second": 0.103, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image shows a person wearing a straw hat and a green polo shirt, sitting in a white chair outdoors. The person is holding a tray with several hot dogs, some of which have red and some with black toppings, on a foil-lined tray.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20955.0, "ram_available_mb": 41885.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20959.1, "ram_available_mb": 41881.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.807}, "power_stats": {"power_gpu_soc_mean_watts": 20.048, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 71.807, "power_watts_avg": 20.048, "energy_joules_est": 194.11, "duration_seconds": 9.682, "sample_count": 83}, "timestamp": "2026-01-26T12:53:55.151022"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11615.557, "latencies_ms": [11615.557], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a living room with a couch and a desk. The couch is positioned on the right side of the room, while the desk is on the left side. A chair is placed in front of the desk, and a laptop is placed on the desk. The room also contains a bookshelf filled with numerous books, adding a cozy and intellectual atmosphere", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20959.1, "ram_available_mb": 41881.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21003.7, "ram_available_mb": 41837.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.202}, "power_stats": {"power_gpu_soc_mean_watts": 19.315, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 70.202, "power_watts_avg": 19.315, "energy_joules_est": 224.37, "duration_seconds": 11.616, "sample_count": 99}, "timestamp": "2026-01-26T12:54:08.819375"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10408.309, "latencies_ms": [10408.309], "images_per_second": 0.096, "prompt_tokens": 39, "response_tokens_est": 67, "n_tiles": 16, "output_text": "- Chair: 1\n- Book: multiple (exact count not possible)\n- Laptop: 1\n- Cords: multiple (exact count not possible)\n- Star decoration: 1\n- Fan: 1\n- Couch: 1\n- Backpack: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21003.7, "ram_available_mb": 41837.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21013.4, "ram_available_mb": 41827.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.773}, "power_stats": {"power_gpu_soc_mean_watts": 19.897, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 71.773, "power_watts_avg": 19.897, "energy_joules_est": 207.11, "duration_seconds": 10.409, "sample_count": 88}, "timestamp": "2026-01-26T12:54:21.243586"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11575.429, "latencies_ms": [11575.429], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden desk with a laptop and a chair positioned to the left side of the desk. The chair is in front of the desk, suggesting it is meant for sitting while using the laptop. In the background, there is a bookshelf filled with books, and a white radiator is placed to the right side of the room. The book", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21013.4, "ram_available_mb": 41827.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21011.9, "ram_available_mb": 41829.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.505}, "power_stats": {"power_gpu_soc_mean_watts": 19.285, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 70.505, "power_watts_avg": 19.285, "energy_joules_est": 223.24, "duration_seconds": 11.576, "sample_count": 99}, "timestamp": "2026-01-26T12:54:34.854629"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8208.686, "latencies_ms": [8208.686], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a desk and a chair, where a person is working on a laptop. There are several books scattered around the room, and a backpack is placed on the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21011.9, "ram_available_mb": 41829.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21012.4, "ram_available_mb": 41828.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.886}, "power_stats": {"power_gpu_soc_mean_watts": 20.998, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 72.886, "power_watts_avg": 20.998, "energy_joules_est": 172.38, "duration_seconds": 8.209, "sample_count": 70}, "timestamp": "2026-01-26T12:54:45.075596"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7997.24, "latencies_ms": [7997.24], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The room is dimly lit with natural light coming from the window on the left. The walls are painted in a light beige color, and the furniture includes a wooden desk and a bookshelf filled with various books.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21012.4, "ram_available_mb": 41828.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21013.6, "ram_available_mb": 41827.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.294}, "power_stats": {"power_gpu_soc_mean_watts": 20.739, "power_cpu_cv_mean_watts": 1.659, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 72.294, "power_watts_avg": 20.739, "energy_joules_est": 165.87, "duration_seconds": 7.998, "sample_count": 68}, "timestamp": "2026-01-26T12:54:55.100705"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11652.883, "latencies_ms": [11652.883], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a verdant landscape, two majestic elephants stand in a field of tall grass, their gray bodies contrasting with the greenery around them. The elephant on the left, slightly ahead of its companion, is adorned with a black saddle on its back, adding a touch of color to its otherwise monochrome appearance. The elephant on", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 21013.6, "ram_available_mb": 41827.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21002.4, "ram_available_mb": 41838.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.931}, "power_stats": {"power_gpu_soc_mean_watts": 19.06, "power_cpu_cv_mean_watts": 1.894, "power_sys_5v0_mean_watts": 8.606, "gpu_utilization_percent_mean": 69.931, "power_watts_avg": 19.06, "energy_joules_est": 222.12, "duration_seconds": 11.653, "sample_count": 101}, "timestamp": "2026-01-26T12:55:08.784405"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7100.82, "latencies_ms": [7100.82], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 38, "n_tiles": 16, "output_text": "elephant: 2, grass: numerous, trees: scattered, sky: hazy, clouds: visible, mountains: distant, water: not visible, birds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20939.8, "ram_available_mb": 41901.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20990.2, "ram_available_mb": 41850.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.983}, "power_stats": {"power_gpu_soc_mean_watts": 21.688, "power_cpu_cv_mean_watts": 1.487, "power_sys_5v0_mean_watts": 8.55, "gpu_utilization_percent_mean": 73.983, "power_watts_avg": 21.688, "energy_joules_est": 154.02, "duration_seconds": 7.101, "sample_count": 60}, "timestamp": "2026-01-26T12:55:17.903538"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10299.799, "latencies_ms": [10299.799], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The two elephants are positioned in the background of the image, standing close to each other in a grassy field. They appear to be near the center of the image, with trees and shrubs in the foreground. The elephants are farther away from the camera compared to the vegetation in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20990.2, "ram_available_mb": 41850.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20945.7, "ram_available_mb": 41895.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.636}, "power_stats": {"power_gpu_soc_mean_watts": 19.708, "power_cpu_cv_mean_watts": 1.833, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 68.636, "power_watts_avg": 19.708, "energy_joules_est": 203.0, "duration_seconds": 10.3, "sample_count": 88}, "timestamp": "2026-01-26T12:55:30.243835"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6634.672, "latencies_ms": [6634.672], "images_per_second": 0.151, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "Two elephants are standing in a grassy field with trees in the background. They appear to be interacting with each other, possibly playing or communicating.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20945.7, "ram_available_mb": 41895.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20943.6, "ram_available_mb": 41897.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.702}, "power_stats": {"power_gpu_soc_mean_watts": 21.954, "power_cpu_cv_mean_watts": 1.453, "power_sys_5v0_mean_watts": 8.54, "gpu_utilization_percent_mean": 75.702, "power_watts_avg": 21.954, "energy_joules_est": 145.67, "duration_seconds": 6.635, "sample_count": 57}, "timestamp": "2026-01-26T12:55:38.903124"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7335.322, "latencies_ms": [7335.322], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "Two elephants are standing in a grassy field with green vegetation around them. The sky is overcast, and the lighting is soft, giving the scene a calm and serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20943.6, "ram_available_mb": 41897.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21010.8, "ram_available_mb": 41830.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.429}, "power_stats": {"power_gpu_soc_mean_watts": 21.104, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.61, "gpu_utilization_percent_mean": 73.429, "power_watts_avg": 21.104, "energy_joules_est": 154.82, "duration_seconds": 7.336, "sample_count": 63}, "timestamp": "2026-01-26T12:55:48.257511"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11589.444, "latencies_ms": [11589.444], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing in a grassy field, holding a Frisbee in his right hand. He is wearing a baseball cap and appears to be preparing to throw the Frisbee. Another person is visible in the background, standing further away from the main subject. The scene suggests that the man is participating in a game of Frisbee or", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21010.8, "ram_available_mb": 41830.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20949.8, "ram_available_mb": 41891.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.384}, "power_stats": {"power_gpu_soc_mean_watts": 19.344, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 70.384, "power_watts_avg": 19.344, "energy_joules_est": 224.2, "duration_seconds": 11.59, "sample_count": 99}, "timestamp": "2026-01-26T12:56:01.877725"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7553.261, "latencies_ms": [7553.261], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "person: 2, frisbee: 1, bottle: 1, trees: many, grass: field, sky: clear, sun: visible, shirtless: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20949.8, "ram_available_mb": 41891.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20991.4, "ram_available_mb": 41849.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.469}, "power_stats": {"power_gpu_soc_mean_watts": 21.38, "power_cpu_cv_mean_watts": 1.557, "power_sys_5v0_mean_watts": 8.557, "gpu_utilization_percent_mean": 73.469, "power_watts_avg": 21.38, "energy_joules_est": 161.5, "duration_seconds": 7.554, "sample_count": 64}, "timestamp": "2026-01-26T12:56:11.486867"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11350.568, "latencies_ms": [11350.568], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a man standing with his left arm extended, holding a frisbee in his right hand. He is wearing a cap, sunglasses, and shorts. In the background, there is another person who appears to be running towards the left side of the image. The background is filled with trees and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20936.2, "ram_available_mb": 41904.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20998.1, "ram_available_mb": 41842.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.25}, "power_stats": {"power_gpu_soc_mean_watts": 19.376, "power_cpu_cv_mean_watts": 1.876, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 70.25, "power_watts_avg": 19.376, "energy_joules_est": 219.94, "duration_seconds": 11.351, "sample_count": 96}, "timestamp": "2026-01-26T12:56:24.874319"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7085.289, "latencies_ms": [7085.289], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A man is standing in a grassy field, throwing a frisbee while another person is in the background. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20944.2, "ram_available_mb": 41896.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20996.0, "ram_available_mb": 41844.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.983}, "power_stats": {"power_gpu_soc_mean_watts": 21.762, "power_cpu_cv_mean_watts": 1.501, "power_sys_5v0_mean_watts": 8.55, "gpu_utilization_percent_mean": 74.983, "power_watts_avg": 21.762, "energy_joules_est": 154.2, "duration_seconds": 7.086, "sample_count": 60}, "timestamp": "2026-01-26T12:56:33.977125"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8568.456, "latencies_ms": [8568.456], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a sunny day with clear blue skies and bright sunlight casting shadows on the ground. The man is wearing a white cap and sunglasses, and he is shirtless, revealing a muscular torso.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20996.0, "ram_available_mb": 41844.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20960.0, "ram_available_mb": 41880.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.027}, "power_stats": {"power_gpu_soc_mean_watts": 20.438, "power_cpu_cv_mean_watts": 1.716, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 71.027, "power_watts_avg": 20.438, "energy_joules_est": 175.13, "duration_seconds": 8.569, "sample_count": 73}, "timestamp": "2026-01-26T12:56:44.560827"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11564.63, "latencies_ms": [11564.63], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young boy is sitting at a dining table, cutting a cake with a knife. The cake is placed on a plate, and the boy is focused on cutting it. The table is covered with a dining tablecloth, and there are several cups and a knife on the table. The boy is wearing a blue shirt, and he", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 20960.0, "ram_available_mb": 41880.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21017.6, "ram_available_mb": 41823.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.653}, "power_stats": {"power_gpu_soc_mean_watts": 19.317, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 70.653, "power_watts_avg": 19.317, "energy_joules_est": 223.41, "duration_seconds": 11.565, "sample_count": 98}, "timestamp": "2026-01-26T12:56:58.147933"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10753.544, "latencies_ms": [10753.544], "images_per_second": 0.093, "prompt_tokens": 39, "response_tokens_est": 70, "n_tiles": 16, "output_text": "- Cake: 1\n\n- Knife: 1\n\n- Frosting: 1\n\n- Plates: 2\n\n- Tablecloth: 1\n\n- Cake pan: 1\n\n- Cake decorations: 1\n\n- Cake topper: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21017.6, "ram_available_mb": 41823.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21029.6, "ram_available_mb": 41811.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.429}, "power_stats": {"power_gpu_soc_mean_watts": 19.928, "power_cpu_cv_mean_watts": 1.808, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 71.429, "power_watts_avg": 19.928, "energy_joules_est": 214.31, "duration_seconds": 10.754, "sample_count": 91}, "timestamp": "2026-01-26T12:57:10.918800"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10645.654, "latencies_ms": [10645.654], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there is a cake on a table with a knife being used to cut it. The cake is placed near the center of the table, and there are plates and a fork nearby. In the background, there is a person wearing a blue shirt, who appears to be cutting the cake.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21029.6, "ram_available_mb": 41811.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21044.3, "ram_available_mb": 41796.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.722}, "power_stats": {"power_gpu_soc_mean_watts": 19.718, "power_cpu_cv_mean_watts": 1.815, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 70.722, "power_watts_avg": 19.718, "energy_joules_est": 209.92, "duration_seconds": 10.646, "sample_count": 90}, "timestamp": "2026-01-26T12:57:23.590523"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7574.438, "latencies_ms": [7574.438], "images_per_second": 0.132, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A young boy in a blue sports jersey is cutting a chocolate cake with a knife. The cake is decorated with a baseball theme, featuring a baseball bat and ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21044.3, "ram_available_mb": 41796.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21045.7, "ram_available_mb": 41795.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.594}, "power_stats": {"power_gpu_soc_mean_watts": 21.311, "power_cpu_cv_mean_watts": 1.563, "power_sys_5v0_mean_watts": 8.56, "gpu_utilization_percent_mean": 74.594, "power_watts_avg": 21.311, "energy_joules_est": 161.43, "duration_seconds": 7.575, "sample_count": 64}, "timestamp": "2026-01-26T12:57:33.191022"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10637.668, "latencies_ms": [10637.668], "images_per_second": 0.094, "prompt_tokens": 36, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image shows a person wearing a blue sports jersey with a white logo, cutting into a chocolate cake that is shaped like a baseball bat and ball. The cake is placed on a colorful tablecloth with a dinosaur pattern, and there is a knife in the person's hand.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20940.5, "ram_available_mb": 41900.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 21006.3, "ram_available_mb": 41834.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.011}, "power_stats": {"power_gpu_soc_mean_watts": 19.687, "power_cpu_cv_mean_watts": 1.828, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 71.011, "power_watts_avg": 19.687, "energy_joules_est": 209.44, "duration_seconds": 10.638, "sample_count": 90}, "timestamp": "2026-01-26T12:57:45.856946"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11600.439, "latencies_ms": [11600.439], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two zebras are standing close to each other, with one zebra partially visible on the left side and the other zebra occupying the majority of the frame. They are standing near a metal fence, which is likely part of their enclosure. The zebras appear to be looking in the same direction, possibly observing something of interest. The scene", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20951.3, "ram_available_mb": 41889.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20989.9, "ram_available_mb": 41851.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.622}, "power_stats": {"power_gpu_soc_mean_watts": 19.343, "power_cpu_cv_mean_watts": 1.965, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 70.622, "power_watts_avg": 19.343, "energy_joules_est": 224.4, "duration_seconds": 11.601, "sample_count": 98}, "timestamp": "2026-01-26T12:57:59.494104"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9384.885, "latencies_ms": [9384.885], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 58, "n_tiles": 16, "output_text": "zebra: 2, pipe: 1, rock: 1, leaves: 1, background: 1, zebra's mane: 1, zebra's eye: 1, zebra's ear: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20928.2, "ram_available_mb": 41912.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20991.6, "ram_available_mb": 41849.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.6}, "power_stats": {"power_gpu_soc_mean_watts": 20.346, "power_cpu_cv_mean_watts": 1.726, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 71.6, "power_watts_avg": 20.346, "energy_joules_est": 190.96, "duration_seconds": 9.386, "sample_count": 80}, "timestamp": "2026-01-26T12:58:10.938701"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11616.623, "latencies_ms": [11616.623], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The zebra on the left is in the foreground, appearing larger and more detailed, while the zebra on the right is in the background, appearing smaller and less detailed. The zebra on the left is positioned near the camera, making its face the main focus of the image, whereas the zebra on the right is positioned further away, behind a metal bar", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20938.2, "ram_available_mb": 41902.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20989.5, "ram_available_mb": 41851.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.59}, "power_stats": {"power_gpu_soc_mean_watts": 19.142, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 70.59, "power_watts_avg": 19.142, "energy_joules_est": 222.38, "duration_seconds": 11.617, "sample_count": 100}, "timestamp": "2026-01-26T12:58:24.618114"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8235.521, "latencies_ms": [8235.521], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "Two zebras are standing close to each other, with one facing the camera and the other partially visible in the background. They appear to be in a zoo enclosure, as there is a metal fence visible in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20927.8, "ram_available_mb": 41913.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20994.7, "ram_available_mb": 41846.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.394}, "power_stats": {"power_gpu_soc_mean_watts": 20.748, "power_cpu_cv_mean_watts": 1.64, "power_sys_5v0_mean_watts": 8.565, "gpu_utilization_percent_mean": 73.394, "power_watts_avg": 20.748, "energy_joules_est": 170.88, "duration_seconds": 8.236, "sample_count": 71}, "timestamp": "2026-01-26T12:58:34.878672"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7902.77, "latencies_ms": [7902.77], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The zebra's stripes are black and white, and the lighting appears to be natural sunlight. The zebra is standing next to a metal pole, and there is a rock visible in the background.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20941.4, "ram_available_mb": 41899.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20934.8, "ram_available_mb": 41906.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.582}, "power_stats": {"power_gpu_soc_mean_watts": 20.827, "power_cpu_cv_mean_watts": 1.654, "power_sys_5v0_mean_watts": 8.609, "gpu_utilization_percent_mean": 72.582, "power_watts_avg": 20.827, "energy_joules_est": 164.6, "duration_seconds": 7.903, "sample_count": 67}, "timestamp": "2026-01-26T12:58:44.835865"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11598.878, "latencies_ms": [11598.878], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment at the La Spezia Centrale train station in Italy. The station, bathed in the soft glow of daylight, is a symphony of grayscale hues. The platform, constructed from brick, stretches out in the foreground, leading the eye towards the train tracks that disappear into the distance. \n\nTwo trains, one on each", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20934.8, "ram_available_mb": 41906.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20941.4, "ram_available_mb": 41899.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.333}, "power_stats": {"power_gpu_soc_mean_watts": 19.293, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 70.333, "power_watts_avg": 19.293, "energy_joules_est": 223.79, "duration_seconds": 11.6, "sample_count": 99}, "timestamp": "2026-01-26T12:58:58.464761"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7649.438, "latencies_ms": [7649.438], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "platform: 1, bench: 1, train: 1, train tracks: 2, train car: 1, building: 1, sign: 1, mountains: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20941.4, "ram_available_mb": 41899.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20997.8, "ram_available_mb": 41843.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.277}, "power_stats": {"power_gpu_soc_mean_watts": 21.315, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 74.277, "power_watts_avg": 21.315, "energy_joules_est": 163.06, "duration_seconds": 7.65, "sample_count": 65}, "timestamp": "2026-01-26T12:59:08.145685"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11553.565, "latencies_ms": [11553.565], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The train tracks extend from the foreground into the background, converging towards the center of the image where a train is positioned on the right side, suggesting it is moving away from the viewer. A bench is placed in the foreground on the left side, indicating it is closer to the viewer than the train. The overhead structure spans across the middle of the image, connecting", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20942.6, "ram_available_mb": 41898.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20986.7, "ram_available_mb": 41854.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.222}, "power_stats": {"power_gpu_soc_mean_watts": 19.303, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 69.222, "power_watts_avg": 19.303, "energy_joules_est": 223.03, "duration_seconds": 11.554, "sample_count": 99}, "timestamp": "2026-01-26T12:59:21.724357"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7530.522, "latencies_ms": [7530.522], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image depicts a train station named \"La Spezia Centrale\" with a train on the tracks and a bench on the platform. The station appears to be quiet with no visible passengers.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20925.0, "ram_available_mb": 41915.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20994.4, "ram_available_mb": 41846.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.222}, "power_stats": {"power_gpu_soc_mean_watts": 21.527, "power_cpu_cv_mean_watts": 1.556, "power_sys_5v0_mean_watts": 8.586, "gpu_utilization_percent_mean": 74.222, "power_watts_avg": 21.527, "energy_joules_est": 162.12, "duration_seconds": 7.531, "sample_count": 63}, "timestamp": "2026-01-26T12:59:31.281202"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5500.347, "latencies_ms": [5500.347], "images_per_second": 0.182, "prompt_tokens": 36, "response_tokens_est": 26, "n_tiles": 16, "output_text": "The image is a black and white photograph of a train station. The station has a brick platform and a metal roof structure.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20933.4, "ram_available_mb": 41907.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 20987.6, "ram_available_mb": 41853.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.64, "power_cpu_cv_mean_watts": 1.311, "power_sys_5v0_mean_watts": 8.574, "gpu_utilization_percent_mean": 76.0, "power_watts_avg": 22.64, "energy_joules_est": 124.54, "duration_seconds": 5.501, "sample_count": 47}, "timestamp": "2026-01-26T12:59:38.816138"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11625.65, "latencies_ms": [11625.65], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is sitting on a red surfboard in the middle of the ocean. The surfer is wearing a black wetsuit and is facing away from the camera, looking out towards the horizon. The sky above is filled with dark, ominous clouds, suggesting an impending storm. The ocean is a deep blue-green color, and the waves are g", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 20934.0, "ram_available_mb": 41906.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20990.6, "ram_available_mb": 41850.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.465}, "power_stats": {"power_gpu_soc_mean_watts": 19.167, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 70.465, "power_watts_avg": 19.167, "energy_joules_est": 222.84, "duration_seconds": 11.626, "sample_count": 101}, "timestamp": "2026-01-26T12:59:52.506627"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7081.956, "latencies_ms": [7081.956], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 38, "n_tiles": 16, "output_text": "person: 1, surfboard: 1, ocean: multiple, waves: multiple, sky: multiple, clouds: multiple, sun: 1, horizon: 1", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20936.4, "ram_available_mb": 41904.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20989.0, "ram_available_mb": 41851.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.083}, "power_stats": {"power_gpu_soc_mean_watts": 21.77, "power_cpu_cv_mean_watts": 1.507, "power_sys_5v0_mean_watts": 8.564, "gpu_utilization_percent_mean": 74.083, "power_watts_avg": 21.77, "energy_joules_est": 154.19, "duration_seconds": 7.083, "sample_count": 60}, "timestamp": "2026-01-26T13:00:01.636092"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9501.791, "latencies_ms": [9501.791], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The person is seated on a surfboard in the foreground, positioned near the water's edge. The ocean extends towards the horizon, with the sky above and the beach in the far background. The clouds are closer to the horizon, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20935.4, "ram_available_mb": 41905.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20991.0, "ram_available_mb": 41849.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.05}, "power_stats": {"power_gpu_soc_mean_watts": 20.055, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 71.05, "power_watts_avg": 20.055, "energy_joules_est": 190.57, "duration_seconds": 9.502, "sample_count": 80}, "timestamp": "2026-01-26T13:00:13.149385"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8677.187, "latencies_ms": [8677.187], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A person is sitting on a red surfboard in the ocean, facing away from the camera, with a dramatic cloudy sky in the background. The sun is setting, casting a warm glow on the horizon and creating a serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20991.0, "ram_available_mb": 41849.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21012.2, "ram_available_mb": 41828.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.342}, "power_stats": {"power_gpu_soc_mean_watts": 20.756, "power_cpu_cv_mean_watts": 1.655, "power_sys_5v0_mean_watts": 8.606, "gpu_utilization_percent_mean": 73.342, "power_watts_avg": 20.756, "energy_joules_est": 180.12, "duration_seconds": 8.678, "sample_count": 73}, "timestamp": "2026-01-26T13:00:23.851315"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9477.283, "latencies_ms": [9477.283], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image features a surfer in a black wetsuit sitting on a red surfboard, with the ocean in the background under a cloudy sky. The lighting is dim, with the sun setting or rising, casting a warm glow on the horizon and creating a moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21012.2, "ram_available_mb": 41828.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21030.9, "ram_available_mb": 41810.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.086}, "power_stats": {"power_gpu_soc_mean_watts": 19.874, "power_cpu_cv_mean_watts": 1.789, "power_sys_5v0_mean_watts": 8.621, "gpu_utilization_percent_mean": 71.086, "power_watts_avg": 19.874, "energy_joules_est": 188.36, "duration_seconds": 9.478, "sample_count": 81}, "timestamp": "2026-01-26T13:00:35.371651"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11531.669, "latencies_ms": [11531.669], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man and a woman are sitting together on a train, enjoying a meal. The man is holding chopsticks, while the woman is holding a sandwich. They are both seated on a train seat, with the man on the left side and the woman on the right side.\n\nThe dining table in front of them is filled with various food", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20935.4, "ram_available_mb": 41905.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20989.5, "ram_available_mb": 41851.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.59}, "power_stats": {"power_gpu_soc_mean_watts": 19.291, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.666, "gpu_utilization_percent_mean": 70.59, "power_watts_avg": 19.291, "energy_joules_est": 222.47, "duration_seconds": 11.532, "sample_count": 100}, "timestamp": "2026-01-26T13:00:48.967329"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9978.57, "latencies_ms": [9978.57], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Chair: 2\n2. Tray: 1\n3. Chopsticks: 2\n4. Food: 5\n5. Train car: 1\n6. Window: 1\n7. Curtain: 1\n8. Sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20936.1, "ram_available_mb": 41904.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20983.7, "ram_available_mb": 41857.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.118}, "power_stats": {"power_gpu_soc_mean_watts": 20.135, "power_cpu_cv_mean_watts": 1.761, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 71.118, "power_watts_avg": 20.135, "energy_joules_est": 200.93, "duration_seconds": 9.979, "sample_count": 85}, "timestamp": "2026-01-26T13:01:00.996408"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11630.527, "latencies_ms": [11630.527], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a man and a woman seated next to each other on a train, with the man on the left and the woman on the right. They are both holding chopsticks and appear to be eating from a tray of food in front of them. In the background, there is a yellow sign on the right side of the image and a window with", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20983.7, "ram_available_mb": 41857.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21013.7, "ram_available_mb": 41827.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.37}, "power_stats": {"power_gpu_soc_mean_watts": 19.379, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 70.37, "power_watts_avg": 19.379, "energy_joules_est": 225.4, "duration_seconds": 11.631, "sample_count": 100}, "timestamp": "2026-01-26T13:01:14.658053"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7339.528, "latencies_ms": [7339.528], "images_per_second": 0.136, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A man and a woman are sitting in a train, enjoying a meal together. The woman is holding a tray with various food items, including sushi and other dishes.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20933.6, "ram_available_mb": 41907.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21001.7, "ram_available_mb": 41839.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.629}, "power_stats": {"power_gpu_soc_mean_watts": 21.714, "power_cpu_cv_mean_watts": 1.523, "power_sys_5v0_mean_watts": 8.555, "gpu_utilization_percent_mean": 73.629, "power_watts_avg": 21.714, "energy_joules_est": 159.38, "duration_seconds": 7.34, "sample_count": 62}, "timestamp": "2026-01-26T13:01:24.051163"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11654.233, "latencies_ms": [11654.233], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a man and a woman seated in a train, with the man holding chopsticks and the woman holding a piece of food. They are both smiling and appear to be enjoying their meal. The train has a yellow sign on the right side, and the seats are blue and white striped. The lighting in the image is bright, and the materials of", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20940.0, "ram_available_mb": 41900.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20929.0, "ram_available_mb": 41911.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.434}, "power_stats": {"power_gpu_soc_mean_watts": 19.392, "power_cpu_cv_mean_watts": 1.872, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 70.434, "power_watts_avg": 19.392, "energy_joules_est": 226.01, "duration_seconds": 11.655, "sample_count": 99}, "timestamp": "2026-01-26T13:01:37.733363"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11598.91, "latencies_ms": [11598.91], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two men are walking down a sidewalk at night. One man is wearing a white shirt and black pants, while the other is wearing a black suit and a pink tie. They are both walking in the same direction, and the man in the suit is slightly ahead of the other. The sidewalk is lined with a metal railing, and there", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20929.0, "ram_available_mb": 41911.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 20935.9, "ram_available_mb": 41905.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.899}, "power_stats": {"power_gpu_soc_mean_watts": 19.344, "power_cpu_cv_mean_watts": 1.973, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 70.899, "power_watts_avg": 19.344, "energy_joules_est": 224.38, "duration_seconds": 11.599, "sample_count": 99}, "timestamp": "2026-01-26T13:01:51.359107"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9493.541, "latencies_ms": [9493.541], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- People: 2\n\n- Pole: 2\n\n- Building: 1\n\n- Sign: 1\n\n- Car: 1\n\n- Street light: 1\n\n- Sidewalk: 1\n\n- Stairs: 1", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20935.9, "ram_available_mb": 41905.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21000.0, "ram_available_mb": 41840.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.763}, "power_stats": {"power_gpu_soc_mean_watts": 20.334, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 71.763, "power_watts_avg": 20.334, "energy_joules_est": 193.05, "duration_seconds": 9.494, "sample_count": 80}, "timestamp": "2026-01-26T13:02:02.886886"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8048.581, "latencies_ms": [8048.581], "images_per_second": 0.124, "prompt_tokens": 44, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The two men are walking on a sidewalk in the foreground of the image. The building with the sign 'Hierro Y Albero' is in the background, and there are street lamps and cars visible further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21000.0, "ram_available_mb": 41840.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20929.4, "ram_available_mb": 41911.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.103}, "power_stats": {"power_gpu_soc_mean_watts": 20.722, "power_cpu_cv_mean_watts": 1.636, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 72.103, "power_watts_avg": 20.722, "energy_joules_est": 166.79, "duration_seconds": 8.049, "sample_count": 68}, "timestamp": "2026-01-26T13:02:12.981810"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10503.234, "latencies_ms": [10503.234], "images_per_second": 0.095, "prompt_tokens": 37, "response_tokens_est": 68, "n_tiles": 16, "output_text": "Two men are walking on a sidewalk at night, one wearing a white shirt and black pants, the other wearing a black suit and a pink striped tie. They appear to be in a city setting, with a building in the background that has a sign that reads \"Hierro Y Albero\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20929.4, "ram_available_mb": 41911.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20997.6, "ram_available_mb": 41843.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.945}, "power_stats": {"power_gpu_soc_mean_watts": 19.852, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 71.945, "power_watts_avg": 19.852, "energy_joules_est": 208.52, "duration_seconds": 10.504, "sample_count": 91}, "timestamp": "2026-01-26T13:02:25.506139"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7889.826, "latencies_ms": [7889.826], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image depicts two individuals in business attire walking on a street at night. The lighting is dim with street lamps casting a yellow glow, and the buildings in the background have a warm, artificial light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20997.6, "ram_available_mb": 41843.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21028.6, "ram_available_mb": 41812.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.701}, "power_stats": {"power_gpu_soc_mean_watts": 20.869, "power_cpu_cv_mean_watts": 1.654, "power_sys_5v0_mean_watts": 8.614, "gpu_utilization_percent_mean": 72.701, "power_watts_avg": 20.869, "energy_joules_est": 164.67, "duration_seconds": 7.89, "sample_count": 67}, "timestamp": "2026-01-26T13:02:35.459596"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11558.346, "latencies_ms": [11558.346], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a man standing in a wine cellar, holding a wine bottle and a wine glass. He is wearing a gray shirt and appears to be cleaning the wine bottle with a cloth. The wine cellar is filled with numerous wine bottles, some of which are placed on shelves and others are scattered around the room. \n\nThere are two wine", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20923.5, "ram_available_mb": 41917.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20988.8, "ram_available_mb": 41852.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.03}, "power_stats": {"power_gpu_soc_mean_watts": 19.255, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.639, "gpu_utilization_percent_mean": 70.03, "power_watts_avg": 19.255, "energy_joules_est": 222.57, "duration_seconds": 11.559, "sample_count": 99}, "timestamp": "2026-01-26T13:02:49.064605"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9651.647, "latencies_ms": [9651.647], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- Wine bottles: 15\n- Wine glasses: 3\n- Glass: 1\n- Table: 1\n- Menu: 1\n- Chair: 1\n- Cabinet: 1\n- Wine rack: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20988.8, "ram_available_mb": 41852.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21001.5, "ram_available_mb": 41839.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.841}, "power_stats": {"power_gpu_soc_mean_watts": 20.124, "power_cpu_cv_mean_watts": 1.718, "power_sys_5v0_mean_watts": 8.589, "gpu_utilization_percent_mean": 71.841, "power_watts_avg": 20.124, "energy_joules_est": 194.24, "duration_seconds": 9.652, "sample_count": 82}, "timestamp": "2026-01-26T13:03:00.774402"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11656.08, "latencies_ms": [11656.08], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a man is standing and holding a wine bottle with a cloth, positioned near a large glass and a smaller wine glass on the table in front of him. In the background, there are multiple wine bottles on shelves, suggesting a wine cellar or a wine tasting room. The man is closer to the camera than the wine bottles on the", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21001.5, "ram_available_mb": 41839.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20934.4, "ram_available_mb": 41906.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.48}, "power_stats": {"power_gpu_soc_mean_watts": 19.453, "power_cpu_cv_mean_watts": 1.881, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 70.48, "power_watts_avg": 19.453, "energy_joules_est": 226.76, "duration_seconds": 11.657, "sample_count": 100}, "timestamp": "2026-01-26T13:03:14.452637"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8841.532, "latencies_ms": [8841.532], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "A man is standing in a wine cellar, holding a bottle of wine and cleaning a glass with a cloth. The cellar is filled with wine bottles on shelves and a person's hand is visible holding a glass of wine.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20934.4, "ram_available_mb": 41906.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20991.1, "ram_available_mb": 41849.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.48}, "power_stats": {"power_gpu_soc_mean_watts": 20.668, "power_cpu_cv_mean_watts": 1.681, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 71.48, "power_watts_avg": 20.668, "energy_joules_est": 182.75, "duration_seconds": 8.842, "sample_count": 75}, "timestamp": "2026-01-26T13:03:25.318869"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9366.947, "latencies_ms": [9366.947], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image features a man in a gray sweater standing in a dimly lit room with wooden furniture and walls. There are multiple wine bottles displayed on shelves in the background, and a person's hand is holding a wine glass with a red liquid, possibly wine.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20929.4, "ram_available_mb": 41911.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20987.5, "ram_available_mb": 41853.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.089}, "power_stats": {"power_gpu_soc_mean_watts": 20.163, "power_cpu_cv_mean_watts": 1.738, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 72.089, "power_watts_avg": 20.163, "energy_joules_est": 188.88, "duration_seconds": 9.368, "sample_count": 79}, "timestamp": "2026-01-26T13:03:36.725810"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11599.568, "latencies_ms": [11599.568], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a tennis player is captured in the midst of a powerful swing, his body arched back and his arm extended upwards. He is dressed in a crisp white shirt and shorts, a classic attire for the sport. His blue wristband adds a pop of color to his ensemble. The tennis racket, gripped firmly in his hand, is", "error": null, "sys_before": {"cpu_percent": 19.0, "ram_used_mb": 20934.0, "ram_available_mb": 41906.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20997.1, "ram_available_mb": 41843.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.465}, "power_stats": {"power_gpu_soc_mean_watts": 19.114, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 70.465, "power_watts_avg": 19.114, "energy_joules_est": 221.73, "duration_seconds": 11.6, "sample_count": 99}, "timestamp": "2026-01-26T13:03:50.363781"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7817.774, "latencies_ms": [7817.774], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "grass: numerous\nwhite lines: numerous\nracket: 1\nball: 1\nperson: 1\nshirt: 1\nshorts: 1\nwristband: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20935.3, "ram_available_mb": 41905.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20987.7, "ram_available_mb": 41853.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.515}, "power_stats": {"power_gpu_soc_mean_watts": 21.173, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.558, "gpu_utilization_percent_mean": 73.515, "power_watts_avg": 21.173, "energy_joules_est": 165.54, "duration_seconds": 7.818, "sample_count": 66}, "timestamp": "2026-01-26T13:04:00.198554"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11642.978, "latencies_ms": [11642.978], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The tennis player is positioned in the foreground, appearing large and in focus, while the tennis ball is in the background, slightly out of focus. The player is near the ground, and the ball is elevated above the court, indicating a high shot. The lines on the court create a sense of depth, with the player closer to the viewer and the lines leading to the background.", "error": null, "sys_before": {"cpu_percent": 23.1, "ram_used_mb": 20987.7, "ram_available_mb": 41853.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20942.7, "ram_available_mb": 41898.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.525}, "power_stats": {"power_gpu_soc_mean_watts": 19.484, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 70.525, "power_watts_avg": 19.484, "energy_joules_est": 226.86, "duration_seconds": 11.644, "sample_count": 101}, "timestamp": "2026-01-26T13:04:13.877385"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8917.571, "latencies_ms": [8917.571], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "A tennis player is captured in the midst of a powerful serve on a grass court, with a tennis ball in the air and a racket in motion. The player is wearing a white shirt and shorts, and the court is marked with white lines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20942.7, "ram_available_mb": 41898.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20935.0, "ram_available_mb": 41905.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.44}, "power_stats": {"power_gpu_soc_mean_watts": 20.715, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 72.44, "power_watts_avg": 20.715, "energy_joules_est": 184.74, "duration_seconds": 8.918, "sample_count": 75}, "timestamp": "2026-01-26T13:04:24.830555"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9035.729, "latencies_ms": [9035.729], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image captures a moment of intense action on a vibrant green tennis court, bathed in natural daylight. The player, dressed in white, is in the midst of a powerful serve, with the tennis ball suspended in the air above his head.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20935.0, "ram_available_mb": 41905.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21034.8, "ram_available_mb": 41806.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.513}, "power_stats": {"power_gpu_soc_mean_watts": 20.184, "power_cpu_cv_mean_watts": 1.729, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 72.513, "power_watts_avg": 20.184, "energy_joules_est": 182.39, "duration_seconds": 9.036, "sample_count": 78}, "timestamp": "2026-01-26T13:04:35.920941"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11599.978, "latencies_ms": [11599.978], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a white and orange cat is the main subject. The cat is standing on a wooden shelf, its body facing the television screen. The television, which is turned on, displays a man in a suit. The man appears to be in a room with a blue wall and a window. The cat seems to be watching the television, perhaps intrigued by the", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 21034.8, "ram_available_mb": 41806.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20973.8, "ram_available_mb": 41867.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.818}, "power_stats": {"power_gpu_soc_mean_watts": 19.323, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 70.818, "power_watts_avg": 19.323, "energy_joules_est": 224.16, "duration_seconds": 11.601, "sample_count": 99}, "timestamp": "2026-01-26T13:04:49.562819"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8614.723, "latencies_ms": [8614.723], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "Television: 1\nCat: 1\nTv stand: 1\nDvd player: 1\nCable box: 1\nRemote control: 1\nCoffee cup: 1\nWall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.8, "ram_available_mb": 41867.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20994.5, "ram_available_mb": 41846.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.342}, "power_stats": {"power_gpu_soc_mean_watts": 20.502, "power_cpu_cv_mean_watts": 1.672, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 73.342, "power_watts_avg": 20.502, "energy_joules_est": 176.63, "duration_seconds": 8.615, "sample_count": 73}, "timestamp": "2026-01-26T13:05:00.207574"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9420.982, "latencies_ms": [9420.982], "images_per_second": 0.106, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "In the foreground, there is a cat standing on a wooden entertainment center. The entertainment center is positioned in front of a television that is mounted on the wall in the background. The cat is closer to the camera than the television, making it the main focus of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20932.6, "ram_available_mb": 41908.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20993.5, "ram_available_mb": 41847.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.951}, "power_stats": {"power_gpu_soc_mean_watts": 20.049, "power_cpu_cv_mean_watts": 1.769, "power_sys_5v0_mean_watts": 8.673, "gpu_utilization_percent_mean": 71.951, "power_watts_avg": 20.049, "energy_joules_est": 188.89, "duration_seconds": 9.422, "sample_count": 81}, "timestamp": "2026-01-26T13:05:11.650410"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7209.683, "latencies_ms": [7209.683], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A cat is standing on a wooden entertainment center, with a television displaying a man in a suit in the background. The entertainment center has a DVD player and a cup on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20933.2, "ram_available_mb": 41907.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20929.3, "ram_available_mb": 41911.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.279}, "power_stats": {"power_gpu_soc_mean_watts": 21.687, "power_cpu_cv_mean_watts": 1.509, "power_sys_5v0_mean_watts": 8.549, "gpu_utilization_percent_mean": 74.279, "power_watts_avg": 21.687, "energy_joules_est": 156.37, "duration_seconds": 7.21, "sample_count": 61}, "timestamp": "2026-01-26T13:05:20.907251"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6534.634, "latencies_ms": [6534.634], "images_per_second": 0.153, "prompt_tokens": 36, "response_tokens_est": 35, "n_tiles": 16, "output_text": "The television is displaying a black and white image of a man in a suit. The cat is white with orange patches and is standing on a wooden entertainment center.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20929.3, "ram_available_mb": 41911.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20993.2, "ram_available_mb": 41847.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.509}, "power_stats": {"power_gpu_soc_mean_watts": 21.71, "power_cpu_cv_mean_watts": 1.477, "power_sys_5v0_mean_watts": 8.619, "gpu_utilization_percent_mean": 74.509, "power_watts_avg": 21.71, "energy_joules_est": 141.88, "duration_seconds": 6.535, "sample_count": 55}, "timestamp": "2026-01-26T13:05:29.494126"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11609.022, "latencies_ms": [11609.022], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a blue circular sign with a white silhouette of a person and a bicycle, indicating a shared path for both pedestrians and cyclists. The sign is affixed to a metal pole, which also holds a rectangular sign with Chinese characters. The pole is positioned on a street, with a tree providing a natural backdrop to the", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20938.3, "ram_available_mb": 41902.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20989.7, "ram_available_mb": 41851.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.636}, "power_stats": {"power_gpu_soc_mean_watts": 19.322, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 70.636, "power_watts_avg": 19.322, "energy_joules_est": 224.32, "duration_seconds": 11.61, "sample_count": 99}, "timestamp": "2026-01-26T13:05:43.137412"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10275.357, "latencies_ms": [10275.357], "images_per_second": 0.097, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "1. Sign: 1\n2. Bicycle symbol: 1\n3. Pedestrian symbol: 1\n4. Traffic light: 1\n5. Street light: 1\n6. Tree: 1\n7. Sky: 1\n8. Sun: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20927.8, "ram_available_mb": 41913.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.9, "ram_used_mb": 20932.0, "ram_available_mb": 41908.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.839}, "power_stats": {"power_gpu_soc_mean_watts": 20.093, "power_cpu_cv_mean_watts": 1.928, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 71.839, "power_watts_avg": 20.093, "energy_joules_est": 206.48, "duration_seconds": 10.276, "sample_count": 87}, "timestamp": "2026-01-26T13:05:55.432785"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10472.266, "latencies_ms": [10472.266], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The sign with the pedestrian and bicycle symbol is in the background, mounted on a pole that is positioned in the foreground. The sign with the Chinese characters is in the foreground, attached to the same pole. The background is a clear sky with some green foliage from trees partially visible behind the pole.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20932.0, "ram_available_mb": 41908.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 10.0, "ram_used_mb": 20937.3, "ram_available_mb": 41903.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.899}, "power_stats": {"power_gpu_soc_mean_watts": 19.532, "power_cpu_cv_mean_watts": 2.275, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 70.899, "power_watts_avg": 19.532, "energy_joules_est": 204.56, "duration_seconds": 10.473, "sample_count": 89}, "timestamp": "2026-01-26T13:06:07.947927"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9513.976, "latencies_ms": [9513.976], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a blue circular sign with a white pictogram of a person and a bicycle, indicating a shared path for pedestrians and cyclists. Below it, there is a rectangular sign with Japanese characters, likely providing additional information or instructions for the path.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20937.3, "ram_available_mb": 41903.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 20944.1, "ram_available_mb": 41896.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.988}, "power_stats": {"power_gpu_soc_mean_watts": 20.104, "power_cpu_cv_mean_watts": 1.833, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 71.988, "power_watts_avg": 20.104, "energy_joules_est": 191.29, "duration_seconds": 9.515, "sample_count": 81}, "timestamp": "2026-01-26T13:06:19.522692"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7345.448, "latencies_ms": [7345.448], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The sign is circular with a blue background and features symbols of a pedestrian and a bicycle. It is mounted on a metal pole with a rectangular sign below it that has Chinese characters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20944.1, "ram_available_mb": 41896.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21000.6, "ram_available_mb": 41840.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.145}, "power_stats": {"power_gpu_soc_mean_watts": 21.243, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 73.145, "power_watts_avg": 21.243, "energy_joules_est": 156.05, "duration_seconds": 7.346, "sample_count": 62}, "timestamp": "2026-01-26T13:06:28.895242"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11594.589, "latencies_ms": [11594.589], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a woman sitting at a dining table in a restaurant. She is holding a slice of pizza in her hand, and there are two other pizza slices on the table. The table is surrounded by chairs, and a book is placed on the table as well. The woman appears to be enjoying her meal, possibly taking a break to read the book. The", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20945.6, "ram_available_mb": 41895.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20998.9, "ram_available_mb": 41842.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.408}, "power_stats": {"power_gpu_soc_mean_watts": 19.376, "power_cpu_cv_mean_watts": 1.879, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 70.408, "power_watts_avg": 19.376, "energy_joules_est": 224.67, "duration_seconds": 11.595, "sample_count": 98}, "timestamp": "2026-01-26T13:06:42.563571"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10320.547, "latencies_ms": [10320.547], "images_per_second": 0.097, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "- Chairs: 10\n\n- Tables: 3\n\n- Glasses: 2\n\n- Pizza slices: 1\n\n- Napkin: 1\n\n- Book: 1\n\n- Water glass: 1\n\n- Paper: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20998.9, "ram_available_mb": 41842.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21020.1, "ram_available_mb": 41820.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.057}, "power_stats": {"power_gpu_soc_mean_watts": 19.915, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.614, "gpu_utilization_percent_mean": 72.057, "power_watts_avg": 19.915, "energy_joules_est": 205.55, "duration_seconds": 10.321, "sample_count": 88}, "timestamp": "2026-01-26T13:06:54.916390"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10862.363, "latencies_ms": [10862.363], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, there is a table with a glass of water and a slice of pizza, indicating a meal in progress. The person is seated at the table, which is in the middle ground of the image. In the background, there are other tables and chairs, suggesting this is a dining area with multiple seating options.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21020.1, "ram_available_mb": 41820.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21021.1, "ram_available_mb": 41819.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.62}, "power_stats": {"power_gpu_soc_mean_watts": 19.613, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.662, "gpu_utilization_percent_mean": 70.62, "power_watts_avg": 19.613, "energy_joules_est": 213.06, "duration_seconds": 10.863, "sample_count": 92}, "timestamp": "2026-01-26T13:07:07.799449"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8677.791, "latencies_ms": [8677.791], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A person is sitting at a table in a restaurant, with a slice of pizza on their plate and a glass of water in front of them. The restaurant has wooden tables and chairs, and there are other tables and patrons in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20949.2, "ram_available_mb": 41891.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20936.8, "ram_available_mb": 41904.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.635}, "power_stats": {"power_gpu_soc_mean_watts": 20.757, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 72.635, "power_watts_avg": 20.757, "energy_joules_est": 180.14, "duration_seconds": 8.679, "sample_count": 74}, "timestamp": "2026-01-26T13:07:18.511055"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10789.605, "latencies_ms": [10789.605], "images_per_second": 0.093, "prompt_tokens": 36, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image shows an indoor setting with warm lighting, likely from hanging lamps or ceiling lights, creating a cozy atmosphere. The walls are adorned with various framed pictures and a notice board, and the furniture, including chairs and tables, appears to be made of wood, giving the space a rustic feel.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20936.8, "ram_available_mb": 41904.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20997.2, "ram_available_mb": 41843.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.075}, "power_stats": {"power_gpu_soc_mean_watts": 19.332, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 70.075, "power_watts_avg": 19.332, "energy_joules_est": 208.6, "duration_seconds": 10.79, "sample_count": 93}, "timestamp": "2026-01-26T13:07:31.341370"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11589.895, "latencies_ms": [11589.895], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with a variety of food items spread across its surface. There are several bowls containing different dishes, including a plate of broccoli and cauliflower, a plate of rice, and a plate of meat. A spoon is also present on the table, placed near the broccoli and cauliflower. \n\nIn addition to", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20997.2, "ram_available_mb": 41843.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21015.2, "ram_available_mb": 41825.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.697}, "power_stats": {"power_gpu_soc_mean_watts": 19.365, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 69.697, "power_watts_avg": 19.365, "energy_joules_est": 224.45, "duration_seconds": 11.591, "sample_count": 99}, "timestamp": "2026-01-26T13:07:44.975779"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8258.939, "latencies_ms": [8258.939], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "bowl: 3, spoon: 1, plate: 3, dish: 3, glass: 1, butter: 1, box: 1, aluminum foil: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21015.2, "ram_available_mb": 41825.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21016.2, "ram_available_mb": 41824.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.845, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 20.845, "energy_joules_est": 172.17, "duration_seconds": 8.26, "sample_count": 70}, "timestamp": "2026-01-26T13:07:55.277070"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11600.189, "latencies_ms": [11600.189], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a plate of food with a tomato-based sauce and rice, which is near the center of the image. To the right, there is a bowl of green vegetables, and further back, there is a plate with what appears to be a curry dish. In the background, there is a box of crackers and a bowl of", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20944.4, "ram_available_mb": 41896.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20937.9, "ram_available_mb": 41903.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.01}, "power_stats": {"power_gpu_soc_mean_watts": 19.293, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.675, "gpu_utilization_percent_mean": 70.01, "power_watts_avg": 19.293, "energy_joules_est": 223.81, "duration_seconds": 11.601, "sample_count": 99}, "timestamp": "2026-01-26T13:08:08.925415"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9700.542, "latencies_ms": [9700.542], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image shows a dining table with a variety of food items including a bowl of broccoli, a plate of rice with vegetables, and a plate of dumplings. There is also a glass of water and a box of 100g visible on the table.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20937.9, "ram_available_mb": 41903.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20997.9, "ram_available_mb": 41843.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.854}, "power_stats": {"power_gpu_soc_mean_watts": 20.138, "power_cpu_cv_mean_watts": 1.752, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 71.854, "power_watts_avg": 20.138, "energy_joules_est": 195.36, "duration_seconds": 9.701, "sample_count": 82}, "timestamp": "2026-01-26T13:08:20.642259"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11432.626, "latencies_ms": [11432.626], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The image shows a meal spread on a wooden table with a mix of warm and cool colors, such as the green of the broccoli, the white of the cauliflower, and the orange of the tomato-based dish. The lighting appears to be artificial, as indicated by the reflections on the table and the shadows cast by the objects.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20944.4, "ram_available_mb": 41896.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21017.0, "ram_available_mb": 41823.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.602}, "power_stats": {"power_gpu_soc_mean_watts": 19.255, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 70.602, "power_watts_avg": 19.255, "energy_joules_est": 220.15, "duration_seconds": 11.433, "sample_count": 98}, "timestamp": "2026-01-26T13:08:34.122908"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11598.748, "latencies_ms": [11598.748], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a busy city street scene with a mix of vehicles and pedestrians. There are several cars, a bus, and a truck on the street, with some cars stopped at a traffic light. The traffic light is located on the left side of the scene, and there are multiple traffic lights visible in the image.\n\nA person can be seen walking on the side", "error": null, "sys_before": {"cpu_percent": 11.5, "ram_used_mb": 20953.6, "ram_available_mb": 41887.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21014.3, "ram_available_mb": 41826.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.755}, "power_stats": {"power_gpu_soc_mean_watts": 19.341, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 69.755, "power_watts_avg": 19.341, "energy_joules_est": 224.34, "duration_seconds": 11.599, "sample_count": 98}, "timestamp": "2026-01-26T13:08:47.773074"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9506.599, "latencies_ms": [9506.599], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Cars: 6\n- Buses: 1\n- Trucks: 1\n- Trees: 4\n- Buildings: 3\n- Traffic lights: 1\n- Signs: 3\n- Pedestrians: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20942.4, "ram_available_mb": 41898.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20939.7, "ram_available_mb": 41901.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.593}, "power_stats": {"power_gpu_soc_mean_watts": 20.157, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.592, "gpu_utilization_percent_mean": 71.593, "power_watts_avg": 20.157, "energy_joules_est": 191.64, "duration_seconds": 9.507, "sample_count": 81}, "timestamp": "2026-01-26T13:08:59.309175"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11366.927, "latencies_ms": [11366.927], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a red car positioned near the center of the image, slightly to the right. Behind the red car, there is a silver car and a black car, both of which are further away from the viewer. In the background, there is a bus that is located behind the red car and is also further away from the viewer.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20939.7, "ram_available_mb": 41901.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20974.6, "ram_available_mb": 41866.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.583}, "power_stats": {"power_gpu_soc_mean_watts": 19.336, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 69.583, "power_watts_avg": 19.336, "energy_joules_est": 219.81, "duration_seconds": 11.368, "sample_count": 96}, "timestamp": "2026-01-26T13:09:12.735335"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9846.51, "latencies_ms": [9846.51], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image depicts a busy urban street scene with multiple vehicles, including a bus, cars, and a truck, navigating through traffic. Pedestrians can be seen walking along the sidewalks, and there are various buildings in the background, creating a bustling city atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20974.6, "ram_available_mb": 41866.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21041.7, "ram_available_mb": 41799.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.56}, "power_stats": {"power_gpu_soc_mean_watts": 20.136, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.606, "gpu_utilization_percent_mean": 71.56, "power_watts_avg": 20.136, "energy_joules_est": 198.28, "duration_seconds": 9.847, "sample_count": 84}, "timestamp": "2026-01-26T13:09:24.624644"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10204.147, "latencies_ms": [10204.147], "images_per_second": 0.098, "prompt_tokens": 36, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image shows a busy urban street scene with a mix of vehicles, including a green and yellow bus, cars, and a white van with a \"GOLD SEAL PLUMBING\" sign. The weather appears to be overcast, and the lighting is natural, suggesting it might be a cloudy day.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20954.9, "ram_available_mb": 41886.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21012.8, "ram_available_mb": 41828.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.747}, "power_stats": {"power_gpu_soc_mean_watts": 19.712, "power_cpu_cv_mean_watts": 1.817, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 70.747, "power_watts_avg": 19.712, "energy_joules_est": 201.16, "duration_seconds": 10.205, "sample_count": 87}, "timestamp": "2026-01-26T13:09:36.854703"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11606.355, "latencies_ms": [11606.355], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a black Toshiba laptop placed on a white table. The laptop is open, displaying a desktop screen with a blue background and a bright light source in the center. A silver pen is lying next to the laptop, and a black cell phone is positioned to the right of the laptop. A person in a red shirt is standing behind the table, partially visible in the", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20951.0, "ram_available_mb": 41889.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21014.4, "ram_available_mb": 41826.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.758}, "power_stats": {"power_gpu_soc_mean_watts": 19.339, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 69.758, "power_watts_avg": 19.339, "energy_joules_est": 224.47, "duration_seconds": 11.607, "sample_count": 99}, "timestamp": "2026-01-26T13:09:50.519856"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7426.643, "latencies_ms": [7426.643], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "laptop: 1, pen: 1, smartphone: 1, camera: 1, table: 1, person: 1, chair: 1, screen: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20937.1, "ram_available_mb": 41903.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20992.5, "ram_available_mb": 41848.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.079}, "power_stats": {"power_gpu_soc_mean_watts": 21.418, "power_cpu_cv_mean_watts": 1.556, "power_sys_5v0_mean_watts": 8.614, "gpu_utilization_percent_mean": 74.079, "power_watts_avg": 21.418, "energy_joules_est": 159.08, "duration_seconds": 7.427, "sample_count": 63}, "timestamp": "2026-01-26T13:09:59.965402"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10339.653, "latencies_ms": [10339.653], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The laptop is positioned in the foreground on a white surface, with a smartphone placed to its right side. A pen is lying vertically on the left side of the laptop, close to the edge of the table. In the background, there appears to be a partial view of a person wearing a red shirt.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20992.5, "ram_available_mb": 41848.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 21014.9, "ram_available_mb": 41826.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.202}, "power_stats": {"power_gpu_soc_mean_watts": 19.574, "power_cpu_cv_mean_watts": 1.848, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 71.202, "power_watts_avg": 19.574, "energy_joules_est": 202.4, "duration_seconds": 10.34, "sample_count": 89}, "timestamp": "2026-01-26T13:10:12.326699"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8049.104, "latencies_ms": [8049.104], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "A black Toshiba laptop is open on a white table, displaying a desktop wallpaper with a bright light source. Next to the laptop, there is a silver pen and a black smartphone lying on the table.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21014.9, "ram_available_mb": 41826.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.6, "ram_used_mb": 21043.0, "ram_available_mb": 41797.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.045}, "power_stats": {"power_gpu_soc_mean_watts": 20.882, "power_cpu_cv_mean_watts": 2.025, "power_sys_5v0_mean_watts": 8.621, "gpu_utilization_percent_mean": 74.045, "power_watts_avg": 20.882, "energy_joules_est": 168.09, "duration_seconds": 8.05, "sample_count": 67}, "timestamp": "2026-01-26T13:10:22.425477"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8253.431, "latencies_ms": [8253.431], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a black Toshiba laptop on a white surface with a bright light reflecting off its screen. There is a silver pen lying next to the laptop, and a black smartphone is placed to the right of the laptop.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21043.0, "ram_available_mb": 41797.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21043.5, "ram_available_mb": 41797.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.623}, "power_stats": {"power_gpu_soc_mean_watts": 20.437, "power_cpu_cv_mean_watts": 1.664, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 72.623, "power_watts_avg": 20.437, "energy_joules_est": 168.69, "duration_seconds": 8.254, "sample_count": 69}, "timestamp": "2026-01-26T13:10:32.726730"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11626.271, "latencies_ms": [11626.271], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a desk with a computer setup, including a desktop computer and a laptop. The desktop computer is positioned on the left side of the desk, while the laptop is placed on the right side. A keyboard and a mouse are also present on the desk, with the keyboard located in front of the desktop computer and the mouse in front of the laptop.\n\nThere are", "error": null, "sys_before": {"cpu_percent": 13.6, "ram_used_mb": 20944.9, "ram_available_mb": 41896.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20997.3, "ram_available_mb": 41843.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.333}, "power_stats": {"power_gpu_soc_mean_watts": 19.314, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 70.333, "power_watts_avg": 19.314, "energy_joules_est": 224.56, "duration_seconds": 11.627, "sample_count": 99}, "timestamp": "2026-01-26T13:10:46.387693"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9152.821, "latencies_ms": [9152.821], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Computer monitor: 1\n- Keyboard: 1\n- Laptop: 1\n- Water bottle: 1\n- Books: 10\n- Cable: 1\n- Computer mouse: 1\n- Wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20997.3, "ram_available_mb": 41843.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21014.8, "ram_available_mb": 41826.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.846}, "power_stats": {"power_gpu_soc_mean_watts": 20.34, "power_cpu_cv_mean_watts": 1.724, "power_sys_5v0_mean_watts": 8.616, "gpu_utilization_percent_mean": 71.846, "power_watts_avg": 20.34, "energy_joules_est": 186.18, "duration_seconds": 9.153, "sample_count": 78}, "timestamp": "2026-01-26T13:10:57.555067"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11603.631, "latencies_ms": [11603.631], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The computer monitor is positioned on the left side of the desk, closer to the foreground, while the laptop is on the right side, slightly further away. The keyboard and mouse are in the center of the desk, with the keyboard being closer to the viewer than the mouse. The books are stacked on the left side of the desk, near the window, and the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21014.8, "ram_available_mb": 41826.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20975.1, "ram_available_mb": 41865.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.949}, "power_stats": {"power_gpu_soc_mean_watts": 19.223, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 69.949, "power_watts_avg": 19.223, "energy_joules_est": 223.07, "duration_seconds": 11.604, "sample_count": 99}, "timestamp": "2026-01-26T13:11:11.182086"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8466.713, "latencies_ms": [8466.713], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a home office setup with a desk containing a desktop computer with a monitor, keyboard, and mouse. There is also a laptop and a water bottle on the desk, along with a few books and other office supplies.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20975.1, "ram_available_mb": 41865.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20945.8, "ram_available_mb": 41895.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.417}, "power_stats": {"power_gpu_soc_mean_watts": 20.644, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.61, "gpu_utilization_percent_mean": 73.417, "power_watts_avg": 20.644, "energy_joules_est": 174.8, "duration_seconds": 8.467, "sample_count": 72}, "timestamp": "2026-01-26T13:11:21.703229"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7670.085, "latencies_ms": [7670.085], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows an indoor setting with a desk that has a computer monitor, a keyboard, a mouse, and a laptop. The room has a window with blinds, and the walls are a light color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20945.8, "ram_available_mb": 41895.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21001.3, "ram_available_mb": 41839.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.015}, "power_stats": {"power_gpu_soc_mean_watts": 20.959, "power_cpu_cv_mean_watts": 1.625, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 72.015, "power_watts_avg": 20.959, "energy_joules_est": 160.77, "duration_seconds": 7.671, "sample_count": 65}, "timestamp": "2026-01-26T13:11:31.395854"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11551.569, "latencies_ms": [11551.569], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a thrilling moment of a skateboarder performing a trick in mid-air. The skateboarder is in the center of the scene, skillfully balancing on his skateboard while flying through the air. \n\nThere are several people around the skateboarder, some of them holding cell phones to capture the exciting moment. In total", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 20939.3, "ram_available_mb": 41901.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20999.8, "ram_available_mb": 41841.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.485}, "power_stats": {"power_gpu_soc_mean_watts": 19.279, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 69.485, "power_watts_avg": 19.279, "energy_joules_est": 222.72, "duration_seconds": 11.552, "sample_count": 99}, "timestamp": "2026-01-26T13:11:45.008040"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8388.066, "latencies_ms": [8388.066], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "skateboarder: 1, skateboard: 1, camera: 3, spectators: 10, barrier: 1, ramp: 1, logo: 2, logo: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20999.8, "ram_available_mb": 41841.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20960.7, "ram_available_mb": 41880.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.169}, "power_stats": {"power_gpu_soc_mean_watts": 20.834, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 73.169, "power_watts_avg": 20.834, "energy_joules_est": 174.77, "duration_seconds": 8.389, "sample_count": 71}, "timestamp": "2026-01-26T13:11:55.428631"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11618.557, "latencies_ms": [11618.557], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a skateboarder is captured in mid-air, performing a trick above a rail, with a crowd of spectators and photographers in the background. The skateboarder is positioned near the rail, indicating proximity to the obstacle being used for the trick. The photographers are standing at various distances from the rail, with some closer to the action", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20960.7, "ram_available_mb": 41880.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20956.8, "ram_available_mb": 41884.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.394}, "power_stats": {"power_gpu_soc_mean_watts": 19.554, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.687, "gpu_utilization_percent_mean": 70.394, "power_watts_avg": 19.554, "energy_joules_est": 227.21, "duration_seconds": 11.619, "sample_count": 99}, "timestamp": "2026-01-26T13:12:09.060350"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9971.992, "latencies_ms": [9971.992], "images_per_second": 0.1, "prompt_tokens": 37, "response_tokens_est": 63, "n_tiles": 16, "output_text": "A skateboarder is performing a trick in mid-air above a ramp, with a crowd of spectators and photographers capturing the moment. The setting appears to be an indoor skate park or a similar venue, with a high ceiling and a large audience watching the performance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20956.8, "ram_available_mb": 41884.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20946.9, "ram_available_mb": 41894.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.333}, "power_stats": {"power_gpu_soc_mean_watts": 20.124, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 8.617, "gpu_utilization_percent_mean": 72.333, "power_watts_avg": 20.124, "energy_joules_est": 200.69, "duration_seconds": 9.973, "sample_count": 84}, "timestamp": "2026-01-26T13:12:21.062851"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11620.934, "latencies_ms": [11620.934], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a skateboarder performing a trick in mid-air with a dark background and a pattern of red and white lines. The skateboarder is wearing a black shirt and white pants, and the skateboard is green and white. The audience is visible in the foreground, with some people holding up their phones to take pictures. The lighting in", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20946.9, "ram_available_mb": 41894.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21007.8, "ram_available_mb": 41833.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.697}, "power_stats": {"power_gpu_soc_mean_watts": 19.456, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.711, "gpu_utilization_percent_mean": 70.697, "power_watts_avg": 19.456, "energy_joules_est": 226.11, "duration_seconds": 11.622, "sample_count": 99}, "timestamp": "2026-01-26T13:12:34.697398"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11564.632, "latencies_ms": [11564.632], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a red fire hydrant with a smiley face painted on it, located on the side of a street. The fire hydrant is positioned near a tree, adding a touch of nature to the urban setting. In the background, there are several cars parked along the street, indicating that this is a populated area. The fire hydrant's cheerful face adds a play", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20952.8, "ram_available_mb": 41888.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20945.6, "ram_available_mb": 41895.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.959}, "power_stats": {"power_gpu_soc_mean_watts": 19.247, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 69.959, "power_watts_avg": 19.247, "energy_joules_est": 222.6, "duration_seconds": 11.565, "sample_count": 98}, "timestamp": "2026-01-26T13:12:48.298835"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9409.524, "latencies_ms": [9409.524], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 58, "n_tiles": 16, "output_text": "- Fire hydrant: 1\n- Tree: 1\n- Cars: 3\n- Buildings: 2\n- Sidewalk: 1\n- Road: 1\n- Bike lane: 1\n- Parking meter: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20945.6, "ram_available_mb": 41895.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21003.5, "ram_available_mb": 41837.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.263}, "power_stats": {"power_gpu_soc_mean_watts": 20.456, "power_cpu_cv_mean_watts": 1.711, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 71.263, "power_watts_avg": 20.456, "energy_joules_est": 192.49, "duration_seconds": 9.41, "sample_count": 80}, "timestamp": "2026-01-26T13:12:59.728078"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9598.026, "latencies_ms": [9598.026], "images_per_second": 0.104, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The fire hydrant is located in the foreground of the image, positioned on the right side. It is near the curb of a street, with a yellow line running parallel to it. In the background, there are cars parked along the street and trees lining the sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21003.5, "ram_available_mb": 41837.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20959.7, "ram_available_mb": 41881.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.39}, "power_stats": {"power_gpu_soc_mean_watts": 20.053, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 8.677, "gpu_utilization_percent_mean": 72.39, "power_watts_avg": 20.053, "energy_joules_est": 192.48, "duration_seconds": 9.599, "sample_count": 82}, "timestamp": "2026-01-26T13:13:11.368316"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7912.304, "latencies_ms": [7912.304], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A red fire hydrant with a smiley face painted on it is located on the side of a street. The hydrant is positioned near a tree and there are cars parked on the street in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20959.7, "ram_available_mb": 41881.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20954.3, "ram_available_mb": 41886.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.985}, "power_stats": {"power_gpu_soc_mean_watts": 21.311, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.573, "gpu_utilization_percent_mean": 72.985, "power_watts_avg": 21.311, "energy_joules_est": 168.63, "duration_seconds": 7.913, "sample_count": 67}, "timestamp": "2026-01-26T13:13:21.342772"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8318.456, "latencies_ms": [8318.456], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The fire hydrant in the image is bright red with a black top and has a smiley face drawn on it. It is located on the side of a street with a yellow curb and is surrounded by trees and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20954.3, "ram_available_mb": 41886.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20942.5, "ram_available_mb": 41898.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.371}, "power_stats": {"power_gpu_soc_mean_watts": 20.764, "power_cpu_cv_mean_watts": 1.658, "power_sys_5v0_mean_watts": 8.66, "gpu_utilization_percent_mean": 73.371, "power_watts_avg": 20.764, "energy_joules_est": 172.74, "duration_seconds": 8.319, "sample_count": 70}, "timestamp": "2026-01-26T13:13:31.696300"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11591.645, "latencies_ms": [11591.645], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a green luggage cart loaded with various suitcases and trunks. The cart is positioned in front of a green door, and the suitcases are stacked on top of each other, occupying most of the cart's space. The suitcases come in different sizes and colors, creating a visually interesting scene.\n\nThere are at least nine suitcases visible", "error": null, "sys_before": {"cpu_percent": 10.7, "ram_used_mb": 20942.5, "ram_available_mb": 41898.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21000.7, "ram_available_mb": 41840.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.04}, "power_stats": {"power_gpu_soc_mean_watts": 19.316, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 71.04, "power_watts_avg": 19.316, "energy_joules_est": 223.92, "duration_seconds": 11.592, "sample_count": 99}, "timestamp": "2026-01-26T13:13:45.334843"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8105.026, "latencies_ms": [8105.026], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "cart: 1\ntrunk: 1\nsuitcase: 1\nsuitcase: 1\nsuitcase: 1\nsuitcase: 1\nsuitcase: 1\nsuitcase: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21000.7, "ram_available_mb": 41840.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21005.9, "ram_available_mb": 41835.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.058}, "power_stats": {"power_gpu_soc_mean_watts": 21.058, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 73.058, "power_watts_avg": 21.058, "energy_joules_est": 170.69, "duration_seconds": 8.106, "sample_count": 69}, "timestamp": "2026-01-26T13:13:55.458199"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9168.637, "latencies_ms": [9168.637], "images_per_second": 0.109, "prompt_tokens": 44, "response_tokens_est": 58, "n_tiles": 16, "output_text": "In the foreground, there is a green cart loaded with various old suitcases. The suitcases are stacked on top of each other, with the largest at the bottom and the smallest on top. In the background, there is a green door and a poster on the wall.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21005.9, "ram_available_mb": 41835.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21054.5, "ram_available_mb": 41786.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.679}, "power_stats": {"power_gpu_soc_mean_watts": 20.18, "power_cpu_cv_mean_watts": 1.755, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 71.679, "power_watts_avg": 20.18, "energy_joules_est": 185.04, "duration_seconds": 9.169, "sample_count": 78}, "timestamp": "2026-01-26T13:14:06.647582"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8320.332, "latencies_ms": [8320.332], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image depicts a collection of old, worn suitcases stacked on top of each other on a green cart. The cart is placed in front of a green door, and there is a poster on the wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20947.7, "ram_available_mb": 41893.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20996.6, "ram_available_mb": 41844.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.93}, "power_stats": {"power_gpu_soc_mean_watts": 20.938, "power_cpu_cv_mean_watts": 1.668, "power_sys_5v0_mean_watts": 8.596, "gpu_utilization_percent_mean": 72.93, "power_watts_avg": 20.938, "energy_joules_est": 174.23, "duration_seconds": 8.321, "sample_count": 71}, "timestamp": "2026-01-26T13:14:17.023386"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9850.523, "latencies_ms": [9850.523], "images_per_second": 0.102, "prompt_tokens": 36, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image features a collection of old, worn suitcases in various colors such as brown, blue, and green, stacked on a green cart. The suitcases appear to be made of leather and metal, and the cart is placed in front of a green door with a poster on the wall behind it.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20996.6, "ram_available_mb": 41844.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.3, "ram_used_mb": 21016.8, "ram_available_mb": 41824.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.238}, "power_stats": {"power_gpu_soc_mean_watts": 19.729, "power_cpu_cv_mean_watts": 2.073, "power_sys_5v0_mean_watts": 8.666, "gpu_utilization_percent_mean": 70.238, "power_watts_avg": 19.729, "energy_joules_est": 194.35, "duration_seconds": 9.851, "sample_count": 84}, "timestamp": "2026-01-26T13:14:28.890066"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11569.231, "latencies_ms": [11569.231], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young girl is standing in front of a couch, holding a Wii remote in her hand. She appears to be playing a video game, possibly on a Nintendo Wii console. The girl is wearing a pink dress, and her attention is focused on the game she is playing. The couch is positioned behind her, and the girl is standing close", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21016.8, "ram_available_mb": 41824.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.5, "ram_used_mb": 20970.1, "ram_available_mb": 41870.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.408}, "power_stats": {"power_gpu_soc_mean_watts": 19.361, "power_cpu_cv_mean_watts": 2.193, "power_sys_5v0_mean_watts": 8.71, "gpu_utilization_percent_mean": 69.408, "power_watts_avg": 19.361, "energy_joules_est": 224.0, "duration_seconds": 11.57, "sample_count": 98}, "timestamp": "2026-01-26T13:14:42.510624"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8225.831, "latencies_ms": [8225.831], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "couch: 1\ndress: 1\nflowers: 5\ncontroller: 1\nwindows: 2\nblinds: 2\ncurtains: 1\narmchair: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20970.1, "ram_available_mb": 41870.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21025.5, "ram_available_mb": 41815.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.643}, "power_stats": {"power_gpu_soc_mean_watts": 21.009, "power_cpu_cv_mean_watts": 1.612, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 72.643, "power_watts_avg": 21.009, "energy_joules_est": 172.83, "duration_seconds": 8.226, "sample_count": 70}, "timestamp": "2026-01-26T13:14:52.757447"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9855.621, "latencies_ms": [9855.621], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The girl is standing in front of a couch, holding a Wii controller in her right hand, which is positioned near her body. The couch is behind her, occupying the background of the image. The girl appears to be in the foreground, with the focus on her and the controller.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20947.1, "ram_available_mb": 41893.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20946.1, "ram_available_mb": 41894.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.583}, "power_stats": {"power_gpu_soc_mean_watts": 19.908, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 70.583, "power_watts_avg": 19.908, "energy_joules_est": 196.22, "duration_seconds": 9.856, "sample_count": 84}, "timestamp": "2026-01-26T13:15:04.646294"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7755.071, "latencies_ms": [7755.071], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A young girl in a pink dress is standing in front of a couch, holding a Wii remote control in her hands. She appears to be playing a video game or possibly preparing to play one.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20946.1, "ram_available_mb": 41894.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20999.8, "ram_available_mb": 41841.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.379}, "power_stats": {"power_gpu_soc_mean_watts": 21.299, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.601, "gpu_utilization_percent_mean": 73.379, "power_watts_avg": 21.299, "energy_joules_est": 165.19, "duration_seconds": 7.756, "sample_count": 66}, "timestamp": "2026-01-26T13:15:14.446737"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7422.204, "latencies_ms": [7422.204], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image shows a young girl in a pink dress with a floral pattern, standing in a room with white blinds on the window. She is holding a white Wii remote control in her hands.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20946.3, "ram_available_mb": 41894.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21014.9, "ram_available_mb": 41826.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.952}, "power_stats": {"power_gpu_soc_mean_watts": 21.324, "power_cpu_cv_mean_watts": 1.594, "power_sys_5v0_mean_watts": 8.68, "gpu_utilization_percent_mean": 73.952, "power_watts_avg": 21.324, "energy_joules_est": 158.29, "duration_seconds": 7.423, "sample_count": 62}, "timestamp": "2026-01-26T13:15:23.895909"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11630.142, "latencies_ms": [11630.142], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a home office setup. Dominating the space is a glass desk, which houses a laptop and a keyboard, suggesting a workspace. A black office chair is positioned in front of the desk, ready for use. The desk is situated against a white wall, and a window with white blinds is visible in the background, allowing natural light to", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 20953.2, "ram_available_mb": 41887.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20952.8, "ram_available_mb": 41888.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.505}, "power_stats": {"power_gpu_soc_mean_watts": 19.282, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 69.505, "power_watts_avg": 19.282, "energy_joules_est": 224.27, "duration_seconds": 11.631, "sample_count": 99}, "timestamp": "2026-01-26T13:15:37.548339"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10964.663, "latencies_ms": [10964.663], "images_per_second": 0.091, "prompt_tokens": 39, "response_tokens_est": 72, "n_tiles": 16, "output_text": "- Computer monitor: 1\n\n- Keyboard: 1\n\n- Computer mouse: 1\n\n- Headphones: 1\n\n- Glass desk: 1\n\n- Glass desk surface: 1\n\n- Glass desk edge: 1\n\n- Wire: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20952.8, "ram_available_mb": 41888.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21008.2, "ram_available_mb": 41832.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.558}, "power_stats": {"power_gpu_soc_mean_watts": 19.694, "power_cpu_cv_mean_watts": 1.832, "power_sys_5v0_mean_watts": 8.648, "gpu_utilization_percent_mean": 71.558, "power_watts_avg": 19.694, "energy_joules_est": 215.95, "duration_seconds": 10.965, "sample_count": 95}, "timestamp": "2026-01-26T13:15:50.558263"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10671.071, "latencies_ms": [10671.071], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The laptop is positioned on the left side of the glass desk, which is in the foreground of the image. The chair is placed in front of the desk, occupying the right side of the desk space. The computer tower on the floor is in the background, behind the desk and to the left of the chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20945.6, "ram_available_mb": 41895.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21012.4, "ram_available_mb": 41828.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.275}, "power_stats": {"power_gpu_soc_mean_watts": 19.578, "power_cpu_cv_mean_watts": 1.864, "power_sys_5v0_mean_watts": 8.676, "gpu_utilization_percent_mean": 69.275, "power_watts_avg": 19.578, "energy_joules_est": 208.93, "duration_seconds": 10.672, "sample_count": 91}, "timestamp": "2026-01-26T13:16:03.262085"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8695.96, "latencies_ms": [8695.96], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image depicts a home office setting with a glass desk, a laptop, a keyboard, and a pair of headphones. There is a chair with a black strap and a trash can with a brown bag on the floor.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20940.3, "ram_available_mb": 41900.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21010.7, "ram_available_mb": 41830.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.743}, "power_stats": {"power_gpu_soc_mean_watts": 20.695, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.619, "gpu_utilization_percent_mean": 71.743, "power_watts_avg": 20.695, "energy_joules_est": 179.98, "duration_seconds": 8.697, "sample_count": 74}, "timestamp": "2026-01-26T13:16:13.982099"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8149.914, "latencies_ms": [8149.914], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The room has a wooden floor and white walls, with a window covered by white blinds. The desk is made of glass and metal, and there is a laptop, a keyboard, and a pair of headphones on it.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 20932.3, "ram_available_mb": 41908.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21016.3, "ram_available_mb": 41824.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.087}, "power_stats": {"power_gpu_soc_mean_watts": 20.651, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 72.087, "power_watts_avg": 20.651, "energy_joules_est": 168.32, "duration_seconds": 8.151, "sample_count": 69}, "timestamp": "2026-01-26T13:16:24.196858"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11598.886, "latencies_ms": [11598.886], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a close-up of a pizza on a white plate, which is placed on a table with a red and white checkered tablecloth. The pizza has a golden-brown crust and is topped with red tomato sauce, melted cheese, and slices of yellow bell pepper. The bell pepper slices are arranged in a", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20944.3, "ram_available_mb": 41896.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21012.9, "ram_available_mb": 41828.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.626}, "power_stats": {"power_gpu_soc_mean_watts": 19.326, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 70.626, "power_watts_avg": 19.326, "energy_joules_est": 224.17, "duration_seconds": 11.6, "sample_count": 99}, "timestamp": "2026-01-26T13:16:37.845002"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8362.893, "latencies_ms": [8362.893], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "pizza: 1\npepperoni: 8\nmushrooms: 5\nonions: 3\ntomatoes: 4\ncheese: 1\nfork: 1\nplate: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20940.8, "ram_available_mb": 41900.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21010.0, "ram_available_mb": 41830.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.592}, "power_stats": {"power_gpu_soc_mean_watts": 20.86, "power_cpu_cv_mean_watts": 1.634, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 72.592, "power_watts_avg": 20.86, "energy_joules_est": 174.46, "duration_seconds": 8.364, "sample_count": 71}, "timestamp": "2026-01-26T13:16:48.228568"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10893.065, "latencies_ms": [10893.065], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The pizza is positioned in the foreground of the image, centered on the plate which is placed on a tablecloth. The fork is held above the pizza, indicating it is ready to be eaten. The background is less distinct but appears to be a red and white checkered tablecloth, suggesting a casual dining setting.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20955.0, "ram_available_mb": 41885.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21030.6, "ram_available_mb": 41810.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.086}, "power_stats": {"power_gpu_soc_mean_watts": 19.467, "power_cpu_cv_mean_watts": 1.863, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 69.086, "power_watts_avg": 19.467, "energy_joules_est": 212.07, "duration_seconds": 10.894, "sample_count": 93}, "timestamp": "2026-01-26T13:17:01.148788"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8661.443, "latencies_ms": [8661.443], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a pizza with various toppings on a white plate, which is placed on a table with a red and white checkered tablecloth. A person's hand is visible, holding a fork, ready to eat the pizza.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21030.6, "ram_available_mb": 41810.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21032.0, "ram_available_mb": 41808.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.459}, "power_stats": {"power_gpu_soc_mean_watts": 20.77, "power_cpu_cv_mean_watts": 1.687, "power_sys_5v0_mean_watts": 8.66, "gpu_utilization_percent_mean": 73.459, "power_watts_avg": 20.77, "energy_joules_est": 179.91, "duration_seconds": 8.662, "sample_count": 74}, "timestamp": "2026-01-26T13:17:11.840165"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11549.126, "latencies_ms": [11549.126], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The pizza is topped with red tomato sauce, melted cheese, slices of yellow bell pepper, and mushrooms, all resting on a thin, golden crust. The lighting in the image is warm, suggesting an indoor setting with artificial light, and the pizza is served on a white plate with a red and white checkered tablecl", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20944.3, "ram_available_mb": 41896.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20991.2, "ram_available_mb": 41849.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.182}, "power_stats": {"power_gpu_soc_mean_watts": 19.21, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.689, "gpu_utilization_percent_mean": 70.182, "power_watts_avg": 19.21, "energy_joules_est": 221.87, "duration_seconds": 11.55, "sample_count": 99}, "timestamp": "2026-01-26T13:17:25.426108"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12427.179, "latencies_ms": [12427.179], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a white and red bus parked on the side of a street. The bus is a part of the Metropolitan Transit System, as indicated by the sign on the front. Several people are visible inside the bus, waiting for their ride or preparing to board. \n\nThere are also a few cars parked or driving nearby, with one car located on the right side of", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20937.7, "ram_available_mb": 41903.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21026.5, "ram_available_mb": 41814.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.057}, "power_stats": {"power_gpu_soc_mean_watts": 21.604, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.947, "gpu_utilization_percent_mean": 73.057, "power_watts_avg": 21.604, "energy_joules_est": 268.49, "duration_seconds": 12.428, "sample_count": 106}, "timestamp": "2026-01-26T13:17:39.893846"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9356.453, "latencies_ms": [9356.453], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "Bus: 1\nSeat: 10\nWindow: 6\nPassenger: 5\nHandbag: 1\nSign: 1\nWheelchair symbol: 1\nFlag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20947.9, "ram_available_mb": 41893.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21004.0, "ram_available_mb": 41836.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.418}, "power_stats": {"power_gpu_soc_mean_watts": 22.967, "power_cpu_cv_mean_watts": 1.479, "power_sys_5v0_mean_watts": 8.755, "gpu_utilization_percent_mean": 77.418, "power_watts_avg": 22.967, "energy_joules_est": 214.9, "duration_seconds": 9.357, "sample_count": 79}, "timestamp": "2026-01-26T13:17:51.264737"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11071.821, "latencies_ms": [11071.821], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The bus is in the foreground of the image, positioned on the left side, and appears to be moving towards the right. There are other vehicles in the background, including a car on the far right. The bus is closer to the camera than the other vehicles, making it the main focus of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21004.0, "ram_available_mb": 41836.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20930.1, "ram_available_mb": 41910.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.277}, "power_stats": {"power_gpu_soc_mean_watts": 22.194, "power_cpu_cv_mean_watts": 1.677, "power_sys_5v0_mean_watts": 8.884, "gpu_utilization_percent_mean": 73.277, "power_watts_avg": 22.194, "energy_joules_est": 245.74, "duration_seconds": 11.073, "sample_count": 94}, "timestamp": "2026-01-26T13:18:04.366590"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9365.555, "latencies_ms": [9365.555], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A white and red Metropolitan Transit System bus is parked on the side of a street with trees in the background. The bus has the number 805 and the destination \"DOWNTOWN\" displayed on the front.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20930.1, "ram_available_mb": 41910.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20931.4, "ram_available_mb": 41909.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.367}, "power_stats": {"power_gpu_soc_mean_watts": 23.037, "power_cpu_cv_mean_watts": 1.499, "power_sys_5v0_mean_watts": 8.786, "gpu_utilization_percent_mean": 76.367, "power_watts_avg": 23.037, "energy_joules_est": 215.77, "duration_seconds": 9.366, "sample_count": 79}, "timestamp": "2026-01-26T13:18:15.764104"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7710.456, "latencies_ms": [7710.456], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The bus is predominantly white with red and blue accents. It is a sunny day, as indicated by the shadows and bright lighting on the bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20931.4, "ram_available_mb": 41909.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 20930.6, "ram_available_mb": 41910.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.273}, "power_stats": {"power_gpu_soc_mean_watts": 23.578, "power_cpu_cv_mean_watts": 1.4, "power_sys_5v0_mean_watts": 8.842, "gpu_utilization_percent_mean": 78.273, "power_watts_avg": 23.578, "energy_joules_est": 181.81, "duration_seconds": 7.711, "sample_count": 66}, "timestamp": "2026-01-26T13:18:25.520581"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12443.623, "latencies_ms": [12443.623], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility on a baseball field. A black and gold baseball glove, with its laces loosely tied, rests on the ground. The glove is positioned next to a black pole, suggesting it might be a part of a fence or a boundary marker. A navy blue baseball cap, adorned with a white logo,", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 20930.6, "ram_available_mb": 41910.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 21013.3, "ram_available_mb": 41827.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.34}, "power_stats": {"power_gpu_soc_mean_watts": 21.558, "power_cpu_cv_mean_watts": 1.85, "power_sys_5v0_mean_watts": 8.935, "gpu_utilization_percent_mean": 72.34, "power_watts_avg": 21.558, "energy_joules_est": 268.27, "duration_seconds": 12.444, "sample_count": 106}, "timestamp": "2026-01-26T13:18:40.033459"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10284.565, "latencies_ms": [10284.565], "images_per_second": 0.097, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "baseball cap: 1, baseball glove: 1, baseball cleat: 1, baseball bat: 1, baseball: 1, baseball glove laces: 1, baseball cap logo: 1, baseball cleat laces: 1", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20926.8, "ram_available_mb": 41914.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21016.0, "ram_available_mb": 41824.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.443}, "power_stats": {"power_gpu_soc_mean_watts": 22.571, "power_cpu_cv_mean_watts": 1.646, "power_sys_5v0_mean_watts": 8.835, "gpu_utilization_percent_mean": 76.443, "power_watts_avg": 22.571, "energy_joules_est": 232.15, "duration_seconds": 10.285, "sample_count": 88}, "timestamp": "2026-01-26T13:18:52.370417"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10285.482, "latencies_ms": [10285.482], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The baseball cap is positioned in the foreground, resting on top of the baseball glove which is lying on the ground. The glove is near the base of a metal pole, suggesting it is placed on a surface that is at a lower elevation than the pole.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21016.0, "ram_available_mb": 41824.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20995.9, "ram_available_mb": 41844.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.562}, "power_stats": {"power_gpu_soc_mean_watts": 22.275, "power_cpu_cv_mean_watts": 1.641, "power_sys_5v0_mean_watts": 8.857, "gpu_utilization_percent_mean": 75.562, "power_watts_avg": 22.275, "energy_joules_est": 229.12, "duration_seconds": 10.286, "sample_count": 89}, "timestamp": "2026-01-26T13:19:04.669253"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9796.536, "latencies_ms": [9796.536], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A baseball cap with a logo is placed on top of a worn baseball glove, which is lying on the ground. The setting appears to be an outdoor area, possibly a baseball field, as suggested by the presence of the glove and cap.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20995.9, "ram_available_mb": 41844.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20994.5, "ram_available_mb": 41846.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.277}, "power_stats": {"power_gpu_soc_mean_watts": 22.738, "power_cpu_cv_mean_watts": 1.528, "power_sys_5v0_mean_watts": 8.801, "gpu_utilization_percent_mean": 77.277, "power_watts_avg": 22.738, "energy_joules_est": 222.77, "duration_seconds": 9.797, "sample_count": 83}, "timestamp": "2026-01-26T13:19:16.486682"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8826.645, "latencies_ms": [8826.645], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The baseball cap is navy blue with a white logo, and the glove is black with gold accents. The image has a warm, natural lighting, likely from the sun, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20994.5, "ram_available_mb": 41846.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21007.7, "ram_available_mb": 41833.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.413}, "power_stats": {"power_gpu_soc_mean_watts": 23.12, "power_cpu_cv_mean_watts": 1.521, "power_sys_5v0_mean_watts": 8.876, "gpu_utilization_percent_mean": 76.413, "power_watts_avg": 23.12, "energy_joules_est": 204.09, "duration_seconds": 8.827, "sample_count": 75}, "timestamp": "2026-01-26T13:19:27.357812"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11627.959, "latencies_ms": [11627.959], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a surfer is skillfully riding a wave on a sunny day. The surfer is wearing a red shirt and is positioned on a white surfboard. The wave, a beautiful shade of blue-green, is breaking to the right of the surfer. The surfer is leaning into the wave, demonstrating their control and balance.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20925.9, "ram_available_mb": 41915.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20990.7, "ram_available_mb": 41850.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.152}, "power_stats": {"power_gpu_soc_mean_watts": 19.311, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.686, "gpu_utilization_percent_mean": 70.152, "power_watts_avg": 19.311, "energy_joules_est": 224.56, "duration_seconds": 11.629, "sample_count": 99}, "timestamp": "2026-01-26T13:19:41.057008"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8783.124, "latencies_ms": [8783.124], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "wave: 1\nsurfboard: 1\nsurfer: 1\nred shirt: 1\nblack shorts: 1\nnumber 7: 1\ngreen stripe: 1\nwhite foam: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20937.4, "ram_available_mb": 41903.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20979.8, "ram_available_mb": 41861.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.867}, "power_stats": {"power_gpu_soc_mean_watts": 20.676, "power_cpu_cv_mean_watts": 1.697, "power_sys_5v0_mean_watts": 8.629, "gpu_utilization_percent_mean": 72.867, "power_watts_avg": 20.676, "energy_joules_est": 181.61, "duration_seconds": 8.784, "sample_count": 75}, "timestamp": "2026-01-26T13:19:51.857356"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11541.609, "latencies_ms": [11541.609], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is breaking towards the right side of the image. The wave originates in the background and extends towards the left, creating a dynamic spatial relationship where the surfer is near the wave's crest. The water's surface in the background appears calmer, indicating it is farther away from the surfer's", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20979.8, "ram_available_mb": 41861.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20941.6, "ram_available_mb": 41899.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.68}, "power_stats": {"power_gpu_soc_mean_watts": 19.362, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.694, "gpu_utilization_percent_mean": 70.68, "power_watts_avg": 19.362, "energy_joules_est": 223.48, "duration_seconds": 11.542, "sample_count": 100}, "timestamp": "2026-01-26T13:20:05.425566"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7084.314, "latencies_ms": [7084.314], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A surfer in a red shirt is riding a wave on a sunny day. The ocean is a beautiful shade of blue and the wave is breaking to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20941.6, "ram_available_mb": 41899.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20946.3, "ram_available_mb": 41894.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.767}, "power_stats": {"power_gpu_soc_mean_watts": 21.68, "power_cpu_cv_mean_watts": 1.507, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 74.767, "power_watts_avg": 21.68, "energy_joules_est": 153.6, "duration_seconds": 7.085, "sample_count": 60}, "timestamp": "2026-01-26T13:20:14.531770"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8336.974, "latencies_ms": [8336.974], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The surfer is wearing a red shirt and is riding a wave that is a vibrant shade of blue-green. The water is splashing around the surfer, indicating movement and the dynamic nature of the sport.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20946.3, "ram_available_mb": 41894.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20989.8, "ram_available_mb": 41851.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.408}, "power_stats": {"power_gpu_soc_mean_watts": 20.589, "power_cpu_cv_mean_watts": 1.696, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 71.408, "power_watts_avg": 20.589, "energy_joules_est": 171.66, "duration_seconds": 8.338, "sample_count": 71}, "timestamp": "2026-01-26T13:20:24.901286"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11568.155, "latencies_ms": [11568.155], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "This image captures a quaint, small bathroom bathed in the soft glow of a single light source. The walls, adorned with gray tiles, provide a neutral backdrop to the scene. On the left, a white toilet stands, its lid closed, exuding an air of cleanliness and order. Adjacent to it, a white sink", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 20989.8, "ram_available_mb": 41851.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21009.7, "ram_available_mb": 41831.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.163}, "power_stats": {"power_gpu_soc_mean_watts": 19.409, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 70.163, "power_watts_avg": 19.409, "energy_joules_est": 224.54, "duration_seconds": 11.569, "sample_count": 98}, "timestamp": "2026-01-26T13:20:38.514977"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9625.223, "latencies_ms": [9625.223], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "toilet: 1, sink: 1, toilet paper: 6, toilet brush: 1, garbage can: 1, trash bag: 1, toilet paper roll: 1, toilet paper holder: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21009.7, "ram_available_mb": 41831.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21023.1, "ram_available_mb": 41817.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.707}, "power_stats": {"power_gpu_soc_mean_watts": 20.16, "power_cpu_cv_mean_watts": 1.742, "power_sys_5v0_mean_watts": 8.613, "gpu_utilization_percent_mean": 71.707, "power_watts_avg": 20.16, "energy_joules_est": 194.06, "duration_seconds": 9.626, "sample_count": 82}, "timestamp": "2026-01-26T13:20:50.184329"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9288.469, "latencies_ms": [9288.469], "images_per_second": 0.108, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The toilet is positioned in the foreground on the left side of the image, while the sink is in the background on the right side. There is a clear spatial separation between the two main objects, with the toilet being closer to the viewer than the sink.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21023.1, "ram_available_mb": 41817.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20952.7, "ram_available_mb": 41888.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.734}, "power_stats": {"power_gpu_soc_mean_watts": 20.14, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 71.734, "power_watts_avg": 20.14, "energy_joules_est": 187.08, "duration_seconds": 9.289, "sample_count": 79}, "timestamp": "2026-01-26T13:21:01.504261"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7561.793, "latencies_ms": [7561.793], "images_per_second": 0.132, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image depicts a bathroom with a toilet and a sink. The toilet is positioned on the left side of the image, while the sink is on the right side.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20952.7, "ram_available_mb": 41888.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20987.7, "ram_available_mb": 41853.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.554}, "power_stats": {"power_gpu_soc_mean_watts": 21.189, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.595, "gpu_utilization_percent_mean": 74.554, "power_watts_avg": 21.189, "energy_joules_est": 160.24, "duration_seconds": 7.562, "sample_count": 65}, "timestamp": "2026-01-26T13:21:11.086271"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7788.442, "latencies_ms": [7788.442], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image is in black and white, featuring a toilet and a sink in a bathroom. The tiles on the wall are made of marble, and there is a picture frame hanging above the sink.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20987.7, "ram_available_mb": 41853.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21002.1, "ram_available_mb": 41838.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.061}, "power_stats": {"power_gpu_soc_mean_watts": 20.772, "power_cpu_cv_mean_watts": 1.631, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 72.061, "power_watts_avg": 20.772, "energy_joules_est": 161.8, "duration_seconds": 7.789, "sample_count": 66}, "timestamp": "2026-01-26T13:21:20.895122"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11611.623, "latencies_ms": [11611.623], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a white clock tower standing tall against a clear blue sky. The tower, adorned with a green dome, is the centerpiece of the image. It's surrounded by a white fence, adding a touch of symmetry to the composition. \n\nIn the foreground, a tiled roof with a pointed archway adds an", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 20948.9, "ram_available_mb": 41892.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20956.6, "ram_available_mb": 41884.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.21}, "power_stats": {"power_gpu_soc_mean_watts": 19.266, "power_cpu_cv_mean_watts": 1.877, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 69.21, "power_watts_avg": 19.266, "energy_joules_est": 223.72, "duration_seconds": 11.612, "sample_count": 100}, "timestamp": "2026-01-26T13:21:34.556382"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10419.337, "latencies_ms": [10419.337], "images_per_second": 0.096, "prompt_tokens": 39, "response_tokens_est": 67, "n_tiles": 16, "output_text": "1. Dome: 1\n2. Clock: 1\n3. Bell: 3\n4. Roof tiles: 100\n5. Trees: 1\n6. Fence: 1\n7. Pole: 1\n8. Building: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20956.6, "ram_available_mb": 41884.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20991.1, "ram_available_mb": 41849.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.944}, "power_stats": {"power_gpu_soc_mean_watts": 19.763, "power_cpu_cv_mean_watts": 1.798, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 70.944, "power_watts_avg": 19.763, "energy_joules_est": 205.93, "duration_seconds": 10.42, "sample_count": 89}, "timestamp": "2026-01-26T13:21:46.990498"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11577.507, "latencies_ms": [11577.507], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The ornate structure on the left is in the foreground and appears to be closer to the viewer than the white tower with a blue dome in the background. The tower is situated on a higher elevation, as indicated by its position relative to the surrounding trees and the clear blue sky. The roof tiles are in the foreground, covering the entire bottom of the image, while the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20991.1, "ram_available_mb": 41849.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20947.9, "ram_available_mb": 41893.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_gpu_soc_mean_watts": 19.353, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.69, "gpu_utilization_percent_mean": 70.0, "power_watts_avg": 19.353, "energy_joules_est": 224.07, "duration_seconds": 11.578, "sample_count": 98}, "timestamp": "2026-01-26T13:22:00.590989"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11805.209, "latencies_ms": [11805.209], "images_per_second": 0.085, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a white clock tower with a green dome, standing tall against a clear blue sky. The tower is adorned with a clock face, and a weather vane on top, indicating the direction of the wind. The tower is nestled amidst lush green trees, adding a touch of nature to the man-made structure. The perspective of", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20947.9, "ram_available_mb": 41893.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20955.6, "ram_available_mb": 41885.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.21}, "power_stats": {"power_gpu_soc_mean_watts": 19.37, "power_cpu_cv_mean_watts": 1.869, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 70.21, "power_watts_avg": 19.37, "energy_joules_est": 228.68, "duration_seconds": 11.806, "sample_count": 100}, "timestamp": "2026-01-26T13:22:14.437290"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6297.023, "latencies_ms": [6297.023], "images_per_second": 0.159, "prompt_tokens": 36, "response_tokens_est": 33, "n_tiles": 16, "output_text": "The image features a clear blue sky and a white clock tower with a green dome. The roof of the building is covered in terracotta tiles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20955.6, "ram_available_mb": 41885.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 20995.3, "ram_available_mb": 41845.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.074}, "power_stats": {"power_gpu_soc_mean_watts": 22.089, "power_cpu_cv_mean_watts": 1.475, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 75.074, "power_watts_avg": 22.089, "energy_joules_est": 139.11, "duration_seconds": 6.298, "sample_count": 54}, "timestamp": "2026-01-26T13:22:22.770038"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11597.581, "latencies_ms": [11597.581], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene moment in the wild, featuring a group of elephants in their natural habitat. The elephant in the foreground, a majestic creature with a dark gray skin, is the focal point of the image. Its trunk is extended, reaching out towards the camera, as if inviting us into its world. The elephant's t", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20995.3, "ram_available_mb": 41845.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21003.0, "ram_available_mb": 41837.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.929}, "power_stats": {"power_gpu_soc_mean_watts": 19.414, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.685, "gpu_utilization_percent_mean": 69.929, "power_watts_avg": 19.414, "energy_joules_est": 225.17, "duration_seconds": 11.598, "sample_count": 98}, "timestamp": "2026-01-26T13:22:36.396497"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8054.043, "latencies_ms": [8054.043], "images_per_second": 0.124, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "elephant: 5, tree: 10, grass: 15, dirt: 20, path: 1, water: 0, rock: 3, bush: 4", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20931.1, "ram_available_mb": 41909.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.4, "ram_used_mb": 20995.2, "ram_available_mb": 41845.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.552}, "power_stats": {"power_gpu_soc_mean_watts": 20.98, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.614, "gpu_utilization_percent_mean": 73.552, "power_watts_avg": 20.98, "energy_joules_est": 168.99, "duration_seconds": 8.055, "sample_count": 67}, "timestamp": "2026-01-26T13:22:46.465307"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10910.957, "latencies_ms": [10910.957], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "In the foreground, there is a single elephant standing on the left side of the image, facing towards the right. Behind it, there is a group of elephants, with one partially visible on the right side and others in the background. The elephants are positioned in a natural setting with trees and vegetation surrounding them.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20941.7, "ram_available_mb": 41899.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 10.3, "ram_used_mb": 21014.5, "ram_available_mb": 41826.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.172}, "power_stats": {"power_gpu_soc_mean_watts": 19.544, "power_cpu_cv_mean_watts": 2.354, "power_sys_5v0_mean_watts": 8.723, "gpu_utilization_percent_mean": 70.172, "power_watts_avg": 19.544, "energy_joules_est": 213.26, "duration_seconds": 10.912, "sample_count": 93}, "timestamp": "2026-01-26T13:22:59.435232"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9376.657, "latencies_ms": [9376.657], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "A herd of elephants is gathered in a natural, wooded area, with one elephant in the foreground looking directly at the camera. The elephants appear to be in a relaxed and social environment, possibly a gathering or a resting spot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20951.3, "ram_available_mb": 41889.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 10.8, "ram_used_mb": 20963.6, "ram_available_mb": 41877.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.7}, "power_stats": {"power_gpu_soc_mean_watts": 20.394, "power_cpu_cv_mean_watts": 2.376, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 71.7, "power_watts_avg": 20.394, "energy_joules_est": 191.24, "duration_seconds": 9.377, "sample_count": 80}, "timestamp": "2026-01-26T13:23:10.871044"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8239.98, "latencies_ms": [8239.98], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image features a group of elephants with a prominent one in the foreground that has its trunk extended towards the camera. The lighting is natural and soft, suggesting the photo was taken during the day in a shaded area.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20963.6, "ram_available_mb": 41877.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 11.4, "ram_used_mb": 21031.6, "ram_available_mb": 41809.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.6}, "power_stats": {"power_gpu_soc_mean_watts": 20.705, "power_cpu_cv_mean_watts": 2.641, "power_sys_5v0_mean_watts": 8.734, "gpu_utilization_percent_mean": 72.6, "power_watts_avg": 20.705, "energy_joules_est": 170.62, "duration_seconds": 8.241, "sample_count": 70}, "timestamp": "2026-01-26T13:23:21.154248"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11540.061, "latencies_ms": [11540.061], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility in a kitchen, where a white refrigerator stands as the centerpiece. The refrigerator, pristine and clean, is open, revealing its empty shelves and drawers. The top shelf is devoid of any items, while the bottom shelf houses a carton of eggs and a bottle", "error": null, "sys_before": {"cpu_percent": 12.9, "ram_used_mb": 21031.6, "ram_available_mb": 41809.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 12.6, "ram_used_mb": 21050.2, "ram_available_mb": 41790.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.677}, "power_stats": {"power_gpu_soc_mean_watts": 19.287, "power_cpu_cv_mean_watts": 2.891, "power_sys_5v0_mean_watts": 8.759, "gpu_utilization_percent_mean": 69.677, "power_watts_avg": 19.287, "energy_joules_est": 222.59, "duration_seconds": 11.541, "sample_count": 99}, "timestamp": "2026-01-26T13:23:34.768807"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11820.796, "latencies_ms": [11820.796], "images_per_second": 0.085, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "- Drawer: 1\n\n- Shelf: 1\n\n- Shelf: 1\n\n- Shelf: 1\n\n- Shelf: 1\n\n- Shelf: 1\n\n- Shelf: 1\n\n- Shelf: 1\n\n- Shelf: 1\n\n- Shelf: 1\n\n", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20971.1, "ram_available_mb": 41869.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.7, "ram_used_mb": 21018.8, "ram_available_mb": 41822.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.118}, "power_stats": {"power_gpu_soc_mean_watts": 19.569, "power_cpu_cv_mean_watts": 2.774, "power_sys_5v0_mean_watts": 8.709, "gpu_utilization_percent_mean": 69.118, "power_watts_avg": 19.569, "energy_joules_est": 231.33, "duration_seconds": 11.821, "sample_count": 102}, "timestamp": "2026-01-26T13:23:48.618583"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11012.084, "latencies_ms": [11012.084], "images_per_second": 0.091, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The refrigerator is the main object in the foreground, with its door open revealing the interior. The shelves and drawers are empty, with the top shelf containing a carton of eggs and a bottle of condiment. The background is less distinct, but appears to be the interior of a kitchen or a similar setting.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20957.9, "ram_available_mb": 41883.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21007.9, "ram_available_mb": 41832.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.106}, "power_stats": {"power_gpu_soc_mean_watts": 19.567, "power_cpu_cv_mean_watts": 1.848, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 71.106, "power_watts_avg": 19.567, "energy_joules_est": 215.49, "duration_seconds": 11.013, "sample_count": 94}, "timestamp": "2026-01-26T13:24:01.667493"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9085.539, "latencies_ms": [9085.539], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows an open refrigerator with its door wide open, revealing the empty shelves and drawers inside. The refrigerator appears to be in a kitchen, as there is a tiled floor and a wall visible in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21007.9, "ram_available_mb": 41832.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21024.9, "ram_available_mb": 41816.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.351}, "power_stats": {"power_gpu_soc_mean_watts": 20.482, "power_cpu_cv_mean_watts": 1.684, "power_sys_5v0_mean_watts": 8.639, "gpu_utilization_percent_mean": 72.351, "power_watts_avg": 20.482, "energy_joules_est": 186.1, "duration_seconds": 9.086, "sample_count": 77}, "timestamp": "2026-01-26T13:24:12.775660"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10072.428, "latencies_ms": [10072.428], "images_per_second": 0.099, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows an open refrigerator with a light interior, possibly white or light-colored, and a brown or beige exterior. The refrigerator is placed in a room with a tiled floor, and the lighting appears to be artificial, coming from the top of the fridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21024.9, "ram_available_mb": 41816.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21066.3, "ram_available_mb": 41774.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.279}, "power_stats": {"power_gpu_soc_mean_watts": 19.864, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 71.279, "power_watts_avg": 19.864, "energy_joules_est": 200.09, "duration_seconds": 10.073, "sample_count": 86}, "timestamp": "2026-01-26T13:24:24.875909"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12429.929, "latencies_ms": [12429.929], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a close-up view of a bunch of bananas resting on a shelf. The bananas are yellow, indicating they are ripe and ready to eat. They are arranged in a neat row, with each banana slightly overlapping the one next to it. The shelf they are on is metallic, reflecting a soft purple light that adds", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 21011.9, "ram_available_mb": 41829.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21033.0, "ram_available_mb": 41807.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.962}, "power_stats": {"power_gpu_soc_mean_watts": 21.589, "power_cpu_cv_mean_watts": 1.808, "power_sys_5v0_mean_watts": 8.931, "gpu_utilization_percent_mean": 72.962, "power_watts_avg": 21.589, "energy_joules_est": 268.36, "duration_seconds": 12.431, "sample_count": 106}, "timestamp": "2026-01-26T13:24:39.348261"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4625.859, "latencies_ms": [4625.859], "images_per_second": 0.216, "prompt_tokens": 39, "response_tokens_est": 7, "n_tiles": 16, "output_text": "banana: 8\n", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20971.3, "ram_available_mb": 41869.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 21025.9, "ram_available_mb": 41815.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 89.605}, "power_stats": {"power_gpu_soc_mean_watts": 27.24, "power_cpu_cv_mean_watts": 0.547, "power_sys_5v0_mean_watts": 8.562, "gpu_utilization_percent_mean": 89.605, "power_watts_avg": 27.24, "energy_joules_est": 126.03, "duration_seconds": 4.626, "sample_count": 38}, "timestamp": "2026-01-26T13:24:45.989203"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12715.165, "latencies_ms": [12715.165], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The bananas are arranged in a diagonal line from the bottom left to the top right of the image, creating a sense of depth. The left side of the image is blurred, making it difficult to discern any details, while the right side is in focus, showing the individual bananas and their sticker labels clearly. The bananas in the foreground appear larger and more detailed compared", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20960.3, "ram_available_mb": 41880.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20967.1, "ram_available_mb": 41873.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.046}, "power_stats": {"power_gpu_soc_mean_watts": 21.73, "power_cpu_cv_mean_watts": 1.778, "power_sys_5v0_mean_watts": 8.902, "gpu_utilization_percent_mean": 74.046, "power_watts_avg": 21.73, "energy_joules_est": 276.31, "duration_seconds": 12.716, "sample_count": 108}, "timestamp": "2026-01-26T13:25:00.762265"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9832.095, "latencies_ms": [9832.095], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a bunch of bananas with a sticker on them, placed on a surface with a blurred background. The focus is on the bananas, and the background is out of focus, making it difficult to determine the exact setting.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20967.1, "ram_available_mb": 41873.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21051.5, "ram_available_mb": 41789.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.663}, "power_stats": {"power_gpu_soc_mean_watts": 22.791, "power_cpu_cv_mean_watts": 1.538, "power_sys_5v0_mean_watts": 8.798, "gpu_utilization_percent_mean": 76.663, "power_watts_avg": 22.791, "energy_joules_est": 224.1, "duration_seconds": 9.833, "sample_count": 83}, "timestamp": "2026-01-26T13:25:12.648232"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11036.834, "latencies_ms": [11036.834], "images_per_second": 0.091, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image features a close-up of a bunch of bananas with a focus on their yellow skin, which has a slight greenish tint at the top, indicating ripeness. The lighting in the image is soft and diffused, casting gentle shadows and highlighting the smooth texture of the bananas.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21051.5, "ram_available_mb": 41789.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21027.1, "ram_available_mb": 41813.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.137}, "power_stats": {"power_gpu_soc_mean_watts": 22.133, "power_cpu_cv_mean_watts": 1.697, "power_sys_5v0_mean_watts": 8.897, "gpu_utilization_percent_mean": 74.137, "power_watts_avg": 22.133, "energy_joules_est": 244.29, "duration_seconds": 11.037, "sample_count": 95}, "timestamp": "2026-01-26T13:25:25.701788"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12223.713, "latencies_ms": [12223.713], "images_per_second": 0.082, "prompt_tokens": 24, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The image captures a city street scene with three large, orange, cylindrical objects that resemble fire hydrants. They are placed on a sidewalk, and the street is covered with snow. In the background, there are buildings, a traffic light, and a few people walking. The sky is overcast, and the overall atmosphere is cold and wintery.", "error": null, "sys_before": {"cpu_percent": 9.7, "ram_used_mb": 20965.0, "ram_available_mb": 41875.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20964.1, "ram_available_mb": 41876.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.25}, "power_stats": {"power_gpu_soc_mean_watts": 21.603, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 73.25, "power_watts_avg": 21.603, "energy_joules_est": 264.08, "duration_seconds": 12.224, "sample_count": 104}, "timestamp": "2026-01-26T13:25:40.003757"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 12735.214, "latencies_ms": [12735.214], "images_per_second": 0.079, "prompt_tokens": 39, "response_tokens_est": 77, "n_tiles": 16, "output_text": "object: 3, count: 3\nobject: trees, count: 4\nobject: buildings, count: 3\nobject: street, count: 1\nobject: snow, count: 1\nobject: traffic lights, count: 2\nobject: sidewalk, count: 1\nobject: traffic cones, count: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20964.1, "ram_available_mb": 41876.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21020.5, "ram_available_mb": 41820.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.454}, "power_stats": {"power_gpu_soc_mean_watts": 21.804, "power_cpu_cv_mean_watts": 1.723, "power_sys_5v0_mean_watts": 8.843, "gpu_utilization_percent_mean": 73.454, "power_watts_avg": 21.804, "energy_joules_est": 277.69, "duration_seconds": 12.736, "sample_count": 108}, "timestamp": "2026-01-26T13:25:54.787095"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12736.574, "latencies_ms": [12736.574], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are three large, copper-colored bollards with circular designs on top, positioned close to each other and standing on a paved area that appears to be a sidewalk. In the background, there is a large, ornate building with many windows and a sign that reads 'IGN', which is situated behind the bollards and appears to be", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21020.5, "ram_available_mb": 41820.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21021.5, "ram_available_mb": 41819.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.587}, "power_stats": {"power_gpu_soc_mean_watts": 21.681, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 8.885, "gpu_utilization_percent_mean": 72.587, "power_watts_avg": 21.681, "energy_joules_est": 276.16, "duration_seconds": 12.737, "sample_count": 109}, "timestamp": "2026-01-26T13:26:09.584741"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 12278.185, "latencies_ms": [12278.185], "images_per_second": 0.081, "prompt_tokens": 37, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The image depicts a city street with a row of large, ornate, copper-colored fire hydrants in the foreground, each with a unique design on top. The background features a mix of modern and older buildings, and the ground is covered with a layer of snow, suggesting a cold, winter day in an urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21021.5, "ram_available_mb": 41819.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20976.7, "ram_available_mb": 41864.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.337}, "power_stats": {"power_gpu_soc_mean_watts": 21.918, "power_cpu_cv_mean_watts": 1.701, "power_sys_5v0_mean_watts": 8.829, "gpu_utilization_percent_mean": 74.337, "power_watts_avg": 21.918, "energy_joules_est": 269.13, "duration_seconds": 12.279, "sample_count": 104}, "timestamp": "2026-01-26T13:26:23.888185"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10865.914, "latencies_ms": [10865.914], "images_per_second": 0.092, "prompt_tokens": 36, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image features three large, copper-colored bollards with circular designs on top, set against a backdrop of a snowy urban street. The buildings in the background have a mix of architectural styles, with the prominent building in the center displaying a mix of brickwork and large windows.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20976.7, "ram_available_mb": 41864.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21020.6, "ram_available_mb": 41820.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.564}, "power_stats": {"power_gpu_soc_mean_watts": 22.031, "power_cpu_cv_mean_watts": 1.69, "power_sys_5v0_mean_watts": 8.871, "gpu_utilization_percent_mean": 74.564, "power_watts_avg": 22.031, "energy_joules_est": 239.4, "duration_seconds": 10.867, "sample_count": 94}, "timestamp": "2026-01-26T13:26:36.788332"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12404.695, "latencies_ms": [12404.695], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a jockey is seen riding a brown horse on a dirt track. The jockey, dressed in a yellow and green uniform, is wearing a white helmet. The horse, adorned with a black bridle, is in motion, its legs lifted off the ground, indicating a gallop. The track is marked by a white fence on the right", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 20957.3, "ram_available_mb": 41883.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21025.9, "ram_available_mb": 41815.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.84}, "power_stats": {"power_gpu_soc_mean_watts": 21.577, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 72.84, "power_watts_avg": 21.577, "energy_joules_est": 267.67, "duration_seconds": 12.405, "sample_count": 106}, "timestamp": "2026-01-26T13:26:51.257347"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11230.653, "latencies_ms": [11230.653], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "- Horse: 1\n\n- Jockey: 1\n\n- Cart: 1\n\n- Number 8 sign: 1\n\n- Grass: 1\n\n- Fence: 1\n\n- Cart track: 1\n\n- Signs: 2", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20963.9, "ram_available_mb": 41877.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21029.5, "ram_available_mb": 41811.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.579}, "power_stats": {"power_gpu_soc_mean_watts": 22.208, "power_cpu_cv_mean_watts": 1.639, "power_sys_5v0_mean_watts": 8.811, "gpu_utilization_percent_mean": 73.579, "power_watts_avg": 22.208, "energy_joules_est": 249.42, "duration_seconds": 11.231, "sample_count": 95}, "timestamp": "2026-01-26T13:27:04.515485"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12793.723, "latencies_ms": [12793.723], "images_per_second": 0.078, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person wearing a helmet and a yellow and green outfit, riding a brown horse with the number 8 on its bridle. The horse is in motion, likely racing, on a dirt track. In the background, there are banners with the words \"LANDS STUD,\" \"MAGNUM,\" and \"WINST", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21029.5, "ram_available_mb": 41811.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21029.9, "ram_available_mb": 41810.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.694}, "power_stats": {"power_gpu_soc_mean_watts": 21.44, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.877, "gpu_utilization_percent_mean": 73.694, "power_watts_avg": 21.44, "energy_joules_est": 274.31, "duration_seconds": 12.794, "sample_count": 108}, "timestamp": "2026-01-26T13:27:19.346568"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8309.823, "latencies_ms": [8309.823], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A jockey is riding a brown horse with the number 8 on its saddle, racing on a track with banners for Magnum and Winstone in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21029.9, "ram_available_mb": 41810.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 20953.3, "ram_available_mb": 41887.6, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.676}, "power_stats": {"power_gpu_soc_mean_watts": 23.575, "power_cpu_cv_mean_watts": 1.398, "power_sys_5v0_mean_watts": 8.772, "gpu_utilization_percent_mean": 78.676, "power_watts_avg": 23.575, "energy_joules_est": 195.92, "duration_seconds": 8.311, "sample_count": 71}, "timestamp": "2026-01-26T13:27:29.675374"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9998.84, "latencies_ms": [9998.84], "images_per_second": 0.1, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image captures a moment of a horse race, with the horse and jockey in motion on a dirt track. The lighting is natural, suggesting it's daytime, and the weather appears to be clear with no signs of rain or adverse conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20953.3, "ram_available_mb": 41887.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 20959.4, "ram_available_mb": 41881.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.058}, "power_stats": {"power_gpu_soc_mean_watts": 22.512, "power_cpu_cv_mean_watts": 1.661, "power_sys_5v0_mean_watts": 8.859, "gpu_utilization_percent_mean": 74.058, "power_watts_avg": 22.512, "energy_joules_est": 225.11, "duration_seconds": 9.999, "sample_count": 86}, "timestamp": "2026-01-26T13:27:41.713435"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11590.075, "latencies_ms": [11590.075], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a brown dog is standing on a wooden deck, looking directly at the camera. The deck is surrounded by a lush green lawn and a wooden fence. The dog appears to be a medium-sized breed, possibly a Labrador Retriever or a similar type. The deck itself is made of wooden planks, and there is a tree nearby, adding", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20959.4, "ram_available_mb": 41881.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 11.4, "ram_used_mb": 21024.1, "ram_available_mb": 41816.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.838}, "power_stats": {"power_gpu_soc_mean_watts": 19.362, "power_cpu_cv_mean_watts": 2.6, "power_sys_5v0_mean_watts": 8.744, "gpu_utilization_percent_mean": 70.838, "power_watts_avg": 19.362, "energy_joules_est": 224.42, "duration_seconds": 11.591, "sample_count": 99}, "timestamp": "2026-01-26T13:27:55.328332"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7558.05, "latencies_ms": [7558.05], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "dog: 1, tree: 1, lemon: 3, fence: 1, bush: 1, step: 1, house: 1, shadow: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20970.6, "ram_available_mb": 41870.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 10.9, "ram_used_mb": 21029.9, "ram_available_mb": 41811.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.609}, "power_stats": {"power_gpu_soc_mean_watts": 21.488, "power_cpu_cv_mean_watts": 2.445, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 73.609, "power_watts_avg": 21.488, "energy_joules_est": 162.42, "duration_seconds": 7.559, "sample_count": 64}, "timestamp": "2026-01-26T13:28:04.908520"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8140.356, "latencies_ms": [8140.356], "images_per_second": 0.123, "prompt_tokens": 44, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The dog is standing on a ledge in the foreground of the image, with a garden and a fence in the background. The tree with green leaves is behind the dog, and there are yellow fruits hanging from it.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 21029.9, "ram_available_mb": 41811.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 10.9, "ram_used_mb": 21049.3, "ram_available_mb": 41791.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.609}, "power_stats": {"power_gpu_soc_mean_watts": 20.623, "power_cpu_cv_mean_watts": 2.436, "power_sys_5v0_mean_watts": 8.723, "gpu_utilization_percent_mean": 71.609, "power_watts_avg": 20.623, "energy_joules_est": 167.89, "duration_seconds": 8.141, "sample_count": 69}, "timestamp": "2026-01-26T13:28:15.075344"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6993.456, "latencies_ms": [6993.456], "images_per_second": 0.143, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A dog is standing on a concrete ledge in a backyard, with a wooden fence and trees in the background. The dog appears to be looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20976.6, "ram_available_mb": 41864.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 11.4, "ram_used_mb": 21034.8, "ram_available_mb": 41806.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.153}, "power_stats": {"power_gpu_soc_mean_watts": 21.892, "power_cpu_cv_mean_watts": 2.557, "power_sys_5v0_mean_watts": 8.706, "gpu_utilization_percent_mean": 75.153, "power_watts_avg": 21.892, "energy_joules_est": 153.11, "duration_seconds": 6.994, "sample_count": 59}, "timestamp": "2026-01-26T13:28:24.111226"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6683.383, "latencies_ms": [6683.383], "images_per_second": 0.15, "prompt_tokens": 36, "response_tokens_est": 37, "n_tiles": 16, "output_text": "The dog is a light brown color with a white chest and paws. It is standing on a concrete ledge with a wooden fence and green trees in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20981.4, "ram_available_mb": 41859.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 13.2, "ram_used_mb": 21036.6, "ram_available_mb": 41804.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.86}, "power_stats": {"power_gpu_soc_mean_watts": 21.971, "power_cpu_cv_mean_watts": 3.11, "power_sys_5v0_mean_watts": 8.809, "gpu_utilization_percent_mean": 75.86, "power_watts_avg": 21.971, "energy_joules_est": 146.85, "duration_seconds": 6.684, "sample_count": 57}, "timestamp": "2026-01-26T13:28:32.811793"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11592.386, "latencies_ms": [11592.386], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is standing on a wooden bench. The bench is positioned on a brick sidewalk, and the person is wearing red pants and blue shoes. The person's feet are placed on the bench, and they are standing upright. \n\nA blue sign is placed on the bench, and it contains a message that reads", "error": null, "sys_before": {"cpu_percent": 13.0, "ram_used_mb": 21036.6, "ram_available_mb": 41804.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 21052.1, "ram_available_mb": 41788.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.561}, "power_stats": {"power_gpu_soc_mean_watts": 19.429, "power_cpu_cv_mean_watts": 2.303, "power_sys_5v0_mean_watts": 8.725, "gpu_utilization_percent_mean": 69.561, "power_watts_avg": 19.429, "energy_joules_est": 225.24, "duration_seconds": 11.593, "sample_count": 98}, "timestamp": "2026-01-26T13:28:46.476917"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8124.798, "latencies_ms": [8124.798], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "bench: 1, person: 1, brick: 100, wall: 1, paper: 1, screw: 4, screwdriver: 1, shoes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21052.1, "ram_available_mb": 41788.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21084.1, "ram_available_mb": 41756.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.362}, "power_stats": {"power_gpu_soc_mean_watts": 20.941, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 73.362, "power_watts_avg": 20.941, "energy_joules_est": 170.15, "duration_seconds": 8.125, "sample_count": 69}, "timestamp": "2026-01-26T13:28:56.623927"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9747.663, "latencies_ms": [9747.663], "images_per_second": 0.103, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "A person is standing on a wooden bench that is positioned against a concrete wall. The bench is in the foreground of the image, while the concrete wall serves as the background. The person's feet are near the edge of the bench, indicating they are close to the bench.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21084.1, "ram_available_mb": 41756.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21084.4, "ram_available_mb": 41756.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.869}, "power_stats": {"power_gpu_soc_mean_watts": 19.766, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 8.683, "gpu_utilization_percent_mean": 71.869, "power_watts_avg": 19.766, "energy_joules_est": 192.69, "duration_seconds": 9.748, "sample_count": 84}, "timestamp": "2026-01-26T13:29:08.424345"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9355.853, "latencies_ms": [9355.853], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "A person is standing on a wooden bench with a handwritten note that says \"Wetener you got to be sitting on me.\" The bench is located on a brick pavement, and the person is wearing red pants and blue shoes with yellow soles.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21084.4, "ram_available_mb": 41756.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21084.6, "ram_available_mb": 41756.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.737}, "power_stats": {"power_gpu_soc_mean_watts": 20.394, "power_cpu_cv_mean_watts": 1.721, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 71.737, "power_watts_avg": 20.394, "energy_joules_est": 190.82, "duration_seconds": 9.356, "sample_count": 80}, "timestamp": "2026-01-26T13:29:19.798911"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6311.61, "latencies_ms": [6311.61], "images_per_second": 0.158, "prompt_tokens": 36, "response_tokens_est": 33, "n_tiles": 16, "output_text": "A person is standing on a wooden bench with red pants and blue shoes. The bench has a sign attached to it with blue tape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21084.6, "ram_available_mb": 41756.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21038.6, "ram_available_mb": 41802.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.396}, "power_stats": {"power_gpu_soc_mean_watts": 21.945, "power_cpu_cv_mean_watts": 1.457, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 74.396, "power_watts_avg": 21.945, "energy_joules_est": 138.52, "duration_seconds": 6.312, "sample_count": 53}, "timestamp": "2026-01-26T13:29:28.139198"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11647.541, "latencies_ms": [11647.541], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a well-decorated living room with a red couch positioned on the left side of the room. A dining table is set up in the middle of the room, surrounded by chairs. The table is covered with a white tablecloth, and there are various items on it, including a vase, a bowl, and a cup. \n\nThe", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20976.8, "ram_available_mb": 41864.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21026.3, "ram_available_mb": 41814.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.83}, "power_stats": {"power_gpu_soc_mean_watts": 19.289, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 69.83, "power_watts_avg": 19.289, "energy_joules_est": 224.68, "duration_seconds": 11.648, "sample_count": 100}, "timestamp": "2026-01-26T13:29:41.838679"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9050.004, "latencies_ms": [9050.004], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Couch: 2\n- Table: 2\n- Chair: 2\n- Lamp: 2\n- Picture frame: 1\n- Sofa: 1\n- Carpet: 1\n- Tablecloth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20970.9, "ram_available_mb": 41870.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21031.0, "ram_available_mb": 41809.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.234}, "power_stats": {"power_gpu_soc_mean_watts": 20.45, "power_cpu_cv_mean_watts": 1.71, "power_sys_5v0_mean_watts": 8.606, "gpu_utilization_percent_mean": 71.234, "power_watts_avg": 20.45, "energy_joules_est": 185.09, "duration_seconds": 9.051, "sample_count": 77}, "timestamp": "2026-01-26T13:29:52.918124"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11481.462, "latencies_ms": [11481.462], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The living room features a red sofa in the foreground, positioned near a wooden side table with a lamp and a vase of flowers. In the background, there is a dining area with a white tablecloth, chairs, and a TV mounted on the wall. The room is well-lit with natural light coming through the window with brown curtains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21031.0, "ram_available_mb": 41809.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20981.8, "ram_available_mb": 41859.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.082}, "power_stats": {"power_gpu_soc_mean_watts": 19.216, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.677, "gpu_utilization_percent_mean": 70.082, "power_watts_avg": 19.216, "energy_joules_est": 220.64, "duration_seconds": 11.482, "sample_count": 98}, "timestamp": "2026-01-26T13:30:06.442983"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10646.226, "latencies_ms": [10646.226], "images_per_second": 0.094, "prompt_tokens": 37, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image depicts a well-lit and cozy living room with a red sofa, a small round table with a white tablecloth, and a TV mounted on the wall. The room is decorated with a painting on the wall and a vase with flowers on the table, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20981.8, "ram_available_mb": 41859.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20984.5, "ram_available_mb": 41856.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.033}, "power_stats": {"power_gpu_soc_mean_watts": 19.828, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.61, "gpu_utilization_percent_mean": 71.033, "power_watts_avg": 19.828, "energy_joules_est": 211.11, "duration_seconds": 10.647, "sample_count": 91}, "timestamp": "2026-01-26T13:30:19.130968"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10327.273, "latencies_ms": [10327.273], "images_per_second": 0.097, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The room is well-lit with natural light coming through the sheer curtains, and the furniture is made of wood, giving it a warm and cozy feel. The color scheme consists of neutral tones like beige and brown, with pops of color from the orange couch and the white tablecloth.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20984.5, "ram_available_mb": 41856.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21027.4, "ram_available_mb": 41813.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.727}, "power_stats": {"power_gpu_soc_mean_watts": 19.498, "power_cpu_cv_mean_watts": 1.837, "power_sys_5v0_mean_watts": 8.666, "gpu_utilization_percent_mean": 70.727, "power_watts_avg": 19.498, "energy_joules_est": 201.37, "duration_seconds": 10.328, "sample_count": 88}, "timestamp": "2026-01-26T13:30:31.519788"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11641.014, "latencies_ms": [11641.014], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a close-up view of a cooking pan on a stove containing a stir-fry dish. The dish consists of various vegetables and meat. The vegetables include broccoli, carrots, and possibly some diced tomatoes, all of which are cut into bite-sized pieces. The meat appears to be diced and could be", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 21027.4, "ram_available_mb": 41813.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21050.6, "ram_available_mb": 41790.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.313}, "power_stats": {"power_gpu_soc_mean_watts": 19.144, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 71.313, "power_watts_avg": 19.144, "energy_joules_est": 222.87, "duration_seconds": 11.642, "sample_count": 99}, "timestamp": "2026-01-26T13:30:45.219844"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8586.184, "latencies_ms": [8586.184], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "broccoli: 12, carrots: 5, meat: 8, tomatoes: 3, onions: 1, garlic: 1, pepper: 1, olive oil: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20970.9, "ram_available_mb": 41870.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21021.1, "ram_available_mb": 41819.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.205}, "power_stats": {"power_gpu_soc_mean_watts": 20.77, "power_cpu_cv_mean_watts": 1.655, "power_sys_5v0_mean_watts": 8.61, "gpu_utilization_percent_mean": 73.205, "power_watts_avg": 20.77, "energy_joules_est": 178.35, "duration_seconds": 8.587, "sample_count": 73}, "timestamp": "2026-01-26T13:30:55.844332"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11570.368, "latencies_ms": [11570.368], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a skillet containing a mix of diced vegetables and meat, with the broccoli occupying the most space and being the most prominent object. The carrots and tomatoes are scattered around the broccoli, with the carrots being more towards the left side and the tomatoes more towards the right. The meat is interspersed", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21021.1, "ram_available_mb": 41819.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21014.6, "ram_available_mb": 41826.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.867}, "power_stats": {"power_gpu_soc_mean_watts": 19.388, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.681, "gpu_utilization_percent_mean": 69.867, "power_watts_avg": 19.388, "energy_joules_est": 224.34, "duration_seconds": 11.571, "sample_count": 98}, "timestamp": "2026-01-26T13:31:09.446202"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10050.866, "latencies_ms": [10050.866], "images_per_second": 0.099, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image shows a skillet on a stove containing a mix of diced vegetables and chunks of meat, likely being cooked. The vegetables include broccoli, carrots, and possibly onions, while the meat appears to be diced ham or a similar type of pork.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21014.6, "ram_available_mb": 41826.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21020.7, "ram_available_mb": 41820.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.207}, "power_stats": {"power_gpu_soc_mean_watts": 20.036, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 8.673, "gpu_utilization_percent_mean": 72.207, "power_watts_avg": 20.036, "energy_joules_est": 201.39, "duration_seconds": 10.052, "sample_count": 87}, "timestamp": "2026-01-26T13:31:21.519993"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8358.607, "latencies_ms": [8358.607], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a skillet on a stove containing a mix of cooked vegetables and meat. The vegetables appear to be broccoli and carrots, and the meat looks like it could be diced ham or sausage.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21020.7, "ram_available_mb": 41820.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21024.4, "ram_available_mb": 41816.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.493}, "power_stats": {"power_gpu_soc_mean_watts": 20.362, "power_cpu_cv_mean_watts": 1.696, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 71.493, "power_watts_avg": 20.362, "energy_joules_est": 170.21, "duration_seconds": 8.359, "sample_count": 71}, "timestamp": "2026-01-26T13:31:31.905688"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11608.698, "latencies_ms": [11608.698], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a delightful scene of a meal in progress. At the center of the frame, a brown plate holds four hot dogs, each nestled in a soft, light brown bun. The hot dogs are generously topped with bright yellow mustard, adding a pop of color to the scene. The plate is placed on a dark green countertop, providing a stark contrast", "error": null, "sys_before": {"cpu_percent": 10.7, "ram_used_mb": 20968.3, "ram_available_mb": 41872.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21022.0, "ram_available_mb": 41818.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.162}, "power_stats": {"power_gpu_soc_mean_watts": 19.152, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.662, "gpu_utilization_percent_mean": 70.162, "power_watts_avg": 19.152, "energy_joules_est": 222.34, "duration_seconds": 11.609, "sample_count": 99}, "timestamp": "2026-01-26T13:31:45.557455"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4848.595, "latencies_ms": [4848.595], "images_per_second": 0.206, "prompt_tokens": 39, "response_tokens_est": 18, "n_tiles": 16, "output_text": "hot dog: 4, bun: 4, mustard: 4", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21022.0, "ram_available_mb": 41818.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 21039.2, "ram_available_mb": 41801.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.195}, "power_stats": {"power_gpu_soc_mean_watts": 24.239, "power_cpu_cv_mean_watts": 1.074, "power_sys_5v0_mean_watts": 8.492, "gpu_utilization_percent_mean": 78.195, "power_watts_avg": 24.239, "energy_joules_est": 117.54, "duration_seconds": 4.849, "sample_count": 41}, "timestamp": "2026-01-26T13:31:52.428346"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10397.23, "latencies_ms": [10397.23], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The hot dogs are arranged in a row, with one in the foreground on the left and two slightly behind it to the right. They are placed on a dark plate which is on a surface that appears to be a kitchen counter. In the background, there is a partially visible box with the letters 'SL' visible on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21039.2, "ram_available_mb": 41801.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20961.1, "ram_available_mb": 41879.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.375}, "power_stats": {"power_gpu_soc_mean_watts": 19.836, "power_cpu_cv_mean_watts": 1.814, "power_sys_5v0_mean_watts": 8.685, "gpu_utilization_percent_mean": 71.375, "power_watts_avg": 19.836, "energy_joules_est": 206.25, "duration_seconds": 10.398, "sample_count": 88}, "timestamp": "2026-01-26T13:32:04.879011"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9201.085, "latencies_ms": [9201.085], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a plate with four hot dogs, each with a bun and a generous amount of yellow mustard on top. The plate is placed on a granite countertop, and there is a magazine with the title 'S' partially visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.1, "ram_available_mb": 41879.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20959.3, "ram_available_mb": 41881.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.139}, "power_stats": {"power_gpu_soc_mean_watts": 20.373, "power_cpu_cv_mean_watts": 1.727, "power_sys_5v0_mean_watts": 8.613, "gpu_utilization_percent_mean": 73.139, "power_watts_avg": 20.373, "energy_joules_est": 187.47, "duration_seconds": 9.202, "sample_count": 79}, "timestamp": "2026-01-26T13:32:16.105055"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9016.945, "latencies_ms": [9016.945], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows three hot dogs with bright yellow mustard on a brown plate, which is placed on a granite countertop. The lighting in the image is bright, highlighting the glossy texture of the mustard and the softness of the buns.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20959.3, "ram_available_mb": 41881.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21017.9, "ram_available_mb": 41823.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.895}, "power_stats": {"power_gpu_soc_mean_watts": 20.425, "power_cpu_cv_mean_watts": 1.722, "power_sys_5v0_mean_watts": 8.663, "gpu_utilization_percent_mean": 71.895, "power_watts_avg": 20.425, "energy_joules_est": 184.18, "duration_seconds": 9.018, "sample_count": 76}, "timestamp": "2026-01-26T13:32:27.150162"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12420.26, "latencies_ms": [12420.26], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene beach scene. A green umbrella, adorned with a floral pattern, stands tall on the sandy beach, providing a vibrant contrast to the surrounding environment. Two blue and one pink beach chair are positioned under the umbrella, offering a spot of shade for those who prefer to relax there. The beach is not crow", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 21017.9, "ram_available_mb": 41823.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 8.3, "ram_used_mb": 21042.8, "ram_available_mb": 41798.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.906}, "power_stats": {"power_gpu_soc_mean_watts": 21.589, "power_cpu_cv_mean_watts": 2.035, "power_sys_5v0_mean_watts": 8.945, "gpu_utilization_percent_mean": 72.906, "power_watts_avg": 21.589, "energy_joules_est": 268.15, "duration_seconds": 12.421, "sample_count": 106}, "timestamp": "2026-01-26T13:32:41.616959"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8642.528, "latencies_ms": [8642.528], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "umbrella: 1, chair: 2, person: 4, beach: 1, water: 1, sand: 1, sky: 1, wave: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20964.3, "ram_available_mb": 41876.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 10.0, "ram_used_mb": 21032.1, "ram_available_mb": 41808.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.493}, "power_stats": {"power_gpu_soc_mean_watts": 23.407, "power_cpu_cv_mean_watts": 2.138, "power_sys_5v0_mean_watts": 8.81, "gpu_utilization_percent_mean": 78.493, "power_watts_avg": 23.407, "energy_joules_est": 202.31, "duration_seconds": 8.643, "sample_count": 73}, "timestamp": "2026-01-26T13:32:52.281308"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12718.534, "latencies_ms": [12718.534], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two beach chairs, one pink and one blue, positioned close to each other under a green umbrella. The umbrella is on the right side of the image, providing shade over the chairs. In the background, there are four individuals in the water, with one person closer to the shore and the others further out. The water", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21032.1, "ram_available_mb": 41808.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 11.3, "ram_used_mb": 21036.3, "ram_available_mb": 41804.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.376}, "power_stats": {"power_gpu_soc_mean_watts": 21.682, "power_cpu_cv_mean_watts": 2.559, "power_sys_5v0_mean_watts": 8.941, "gpu_utilization_percent_mean": 73.376, "power_watts_avg": 21.682, "energy_joules_est": 275.78, "duration_seconds": 12.719, "sample_count": 109}, "timestamp": "2026-01-26T13:33:07.014975"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9543.99, "latencies_ms": [9543.99], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A group of people are swimming in the ocean near a beach, with a green umbrella and two folding chairs set up on the sand. The beach appears to be a popular spot for relaxation and leisure activities.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21036.3, "ram_available_mb": 41804.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 10.9, "ram_used_mb": 21004.2, "ram_available_mb": 41836.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.753}, "power_stats": {"power_gpu_soc_mean_watts": 23.101, "power_cpu_cv_mean_watts": 2.401, "power_sys_5v0_mean_watts": 8.899, "gpu_utilization_percent_mean": 76.753, "power_watts_avg": 23.101, "energy_joules_est": 220.49, "duration_seconds": 9.544, "sample_count": 81}, "timestamp": "2026-01-26T13:33:18.575630"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7940.892, "latencies_ms": [7940.892], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A bright green umbrella is open on the sandy beach, providing shade. The beach is populated with people swimming in the water and sitting on colorful beach chairs.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21004.2, "ram_available_mb": 41836.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 11.5, "ram_used_mb": 21045.5, "ram_available_mb": 41795.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.441}, "power_stats": {"power_gpu_soc_mean_watts": 23.763, "power_cpu_cv_mean_watts": 2.76, "power_sys_5v0_mean_watts": 9.027, "gpu_utilization_percent_mean": 78.441, "power_watts_avg": 23.763, "energy_joules_est": 188.71, "duration_seconds": 7.941, "sample_count": 68}, "timestamp": "2026-01-26T13:33:28.533931"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11614.943, "latencies_ms": [11614.943], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a vintage kitchen with a green and white color scheme. The kitchen is filled with various appliances and furniture, including a sink, a refrigerator, a stove, and a dining table. The table is surrounded by chairs, and there are several bowls placed on it. \n\nIn addition to the main appliances, there", "error": null, "sys_before": {"cpu_percent": 14.8, "ram_used_mb": 20983.4, "ram_available_mb": 41857.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.9, "ram_used_mb": 21043.2, "ram_available_mb": 41797.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.83}, "power_stats": {"power_gpu_soc_mean_watts": 19.417, "power_cpu_cv_mean_watts": 2.87, "power_sys_5v0_mean_watts": 8.761, "gpu_utilization_percent_mean": 69.83, "power_watts_avg": 19.417, "energy_joules_est": 225.54, "duration_seconds": 11.616, "sample_count": 100}, "timestamp": "2026-01-26T13:33:42.189758"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8835.302, "latencies_ms": [8835.302], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "- Table: 1\n- Chair: 1\n- Cabinet: 1\n- Stove: 1\n- Countertop: 1\n- Jar: 1\n- Pot: 1\n- Basket: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20982.3, "ram_available_mb": 41858.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21024.6, "ram_available_mb": 41816.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.373}, "power_stats": {"power_gpu_soc_mean_watts": 20.6, "power_cpu_cv_mean_watts": 1.686, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 72.373, "power_watts_avg": 20.6, "energy_joules_est": 182.02, "duration_seconds": 8.836, "sample_count": 75}, "timestamp": "2026-01-26T13:33:53.045691"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11585.515, "latencies_ms": [11585.515], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden chair positioned near the center of the image, with a table in front of it. The table has various items on it, including a bowl, a jar, and some books. In the background, there is a green refrigerator on the right side of the image, and a sink with a faucet on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21024.6, "ram_available_mb": 41816.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21047.4, "ram_available_mb": 41793.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.364}, "power_stats": {"power_gpu_soc_mean_watts": 19.301, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 70.364, "power_watts_avg": 19.301, "energy_joules_est": 223.62, "duration_seconds": 11.586, "sample_count": 99}, "timestamp": "2026-01-26T13:34:06.667593"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9381.58, "latencies_ms": [9381.58], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image depicts a vintage kitchen with green walls and a patterned wallpaper. The kitchen is furnished with a white refrigerator, a sink, a stove, and a table with various items on it, including a book and a bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21047.4, "ram_available_mb": 41793.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21060.9, "ram_available_mb": 41780.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.713}, "power_stats": {"power_gpu_soc_mean_watts": 20.404, "power_cpu_cv_mean_watts": 1.726, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 71.713, "power_watts_avg": 20.404, "energy_joules_est": 191.43, "duration_seconds": 9.382, "sample_count": 80}, "timestamp": "2026-01-26T13:34:18.093074"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8688.371, "latencies_ms": [8688.371], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The kitchen is filled with vibrant green cabinets and a white sink, creating a contrast of colors. The lighting is warm and natural, coming from the windows with curtains, and the wooden furniture adds a rustic touch to the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20964.6, "ram_available_mb": 41876.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21030.0, "ram_available_mb": 41810.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.216}, "power_stats": {"power_gpu_soc_mean_watts": 20.43, "power_cpu_cv_mean_watts": 1.736, "power_sys_5v0_mean_watts": 8.698, "gpu_utilization_percent_mean": 71.216, "power_watts_avg": 20.43, "energy_joules_est": 177.52, "duration_seconds": 8.689, "sample_count": 74}, "timestamp": "2026-01-26T13:34:28.811942"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11622.573, "latencies_ms": [11622.573], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a serene woodland, a black and white dog is captured in the midst of an energetic play. The dog, adorned with a blue collar, is holding a red frisbee in its mouth, ready to embark on a fun-filled chase. The frisbee, vibrant against the dog's black and white", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21030.0, "ram_available_mb": 41810.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20987.6, "ram_available_mb": 41853.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.273}, "power_stats": {"power_gpu_soc_mean_watts": 19.359, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.666, "gpu_utilization_percent_mean": 70.273, "power_watts_avg": 19.359, "energy_joules_est": 225.01, "duration_seconds": 11.623, "sample_count": 99}, "timestamp": "2026-01-26T13:34:42.507560"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7804.529, "latencies_ms": [7804.529], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "dog: 1, frisbee: 1, tree: 1, leaves: numerous, ground: 1, shadows: 2, sunlight: 1, branches: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20987.6, "ram_available_mb": 41853.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21032.1, "ram_available_mb": 41808.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.303}, "power_stats": {"power_gpu_soc_mean_watts": 21.202, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.576, "gpu_utilization_percent_mean": 73.303, "power_watts_avg": 21.202, "energy_joules_est": 165.49, "duration_seconds": 7.805, "sample_count": 66}, "timestamp": "2026-01-26T13:34:52.325030"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11632.39, "latencies_ms": [11632.39], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a black and white dog is positioned near the center of the image, holding a frisbee in its mouth. The dog is standing on a ground covered with brown leaves, which suggests it is in a park or a wooded area. To the right of the dog, there is a large green tree trunk, indicating that the dog is relatively close to the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20975.4, "ram_available_mb": 41865.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20972.4, "ram_available_mb": 41868.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.242}, "power_stats": {"power_gpu_soc_mean_watts": 19.173, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 70.242, "power_watts_avg": 19.173, "energy_joules_est": 223.04, "duration_seconds": 11.633, "sample_count": 99}, "timestamp": "2026-01-26T13:35:06.005107"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7320.396, "latencies_ms": [7320.396], "images_per_second": 0.137, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A black and white dog is playing with a frisbee in a wooded area with a large green tree. The ground is covered with fallen leaves, indicating that it is autumn.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20972.4, "ram_available_mb": 41868.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21019.9, "ram_available_mb": 41821.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.565}, "power_stats": {"power_gpu_soc_mean_watts": 21.679, "power_cpu_cv_mean_watts": 1.523, "power_sys_5v0_mean_watts": 8.579, "gpu_utilization_percent_mean": 73.565, "power_watts_avg": 21.679, "energy_joules_est": 158.71, "duration_seconds": 7.321, "sample_count": 62}, "timestamp": "2026-01-26T13:35:15.369480"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8609.486, "latencies_ms": [8609.486], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image captures a moment in a forest with a dog in mid-action, surrounded by the earthy tones of brown and green. The lighting is soft and diffused, suggesting an overcast day or a shaded area within the forest.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20966.4, "ram_available_mb": 41874.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21032.0, "ram_available_mb": 41808.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.877}, "power_stats": {"power_gpu_soc_mean_watts": 20.278, "power_cpu_cv_mean_watts": 1.71, "power_sys_5v0_mean_watts": 8.662, "gpu_utilization_percent_mean": 71.877, "power_watts_avg": 20.278, "energy_joules_est": 174.6, "duration_seconds": 8.61, "sample_count": 73}, "timestamp": "2026-01-26T13:35:26.003777"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11591.692, "latencies_ms": [11591.692], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is cross-country skiing on a snowy trail. The skier is wearing a white hat, a red backpack, and green boots. They are holding ski poles and appear to be in motion. The trail is surrounded by trees, and the sky is clear and blue. The skier's posture suggests they are in the middle of", "error": null, "sys_before": {"cpu_percent": 9.4, "ram_used_mb": 20977.0, "ram_available_mb": 41863.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21024.9, "ram_available_mb": 41816.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.194}, "power_stats": {"power_gpu_soc_mean_watts": 19.308, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.686, "gpu_utilization_percent_mean": 70.194, "power_watts_avg": 19.308, "energy_joules_est": 223.83, "duration_seconds": 11.592, "sample_count": 98}, "timestamp": "2026-01-26T13:35:39.628320"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7713.769, "latencies_ms": [7713.769], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "person: 1, ski: 2, ski pole: 2, backpack: 1, ski boot: 2, ski trail: 1, tree: many, sky: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21024.9, "ram_available_mb": 41816.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20981.2, "ram_available_mb": 41859.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.031}, "power_stats": {"power_gpu_soc_mean_watts": 21.308, "power_cpu_cv_mean_watts": 1.558, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 74.031, "power_watts_avg": 21.308, "energy_joules_est": 164.38, "duration_seconds": 7.714, "sample_count": 65}, "timestamp": "2026-01-26T13:35:49.396385"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11553.509, "latencies_ms": [11553.509], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "In the foreground, a skier is positioned on the left side of the image, moving towards the right along a snow-covered slope. The skier is closer to the viewer than the trees in the background, which line the horizon. The skier's shadow is cast on the snow, indicating a light source from the upper left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20981.2, "ram_available_mb": 41859.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21025.4, "ram_available_mb": 41815.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.929}, "power_stats": {"power_gpu_soc_mean_watts": 19.446, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.69, "gpu_utilization_percent_mean": 70.929, "power_watts_avg": 19.446, "energy_joules_est": 224.68, "duration_seconds": 11.554, "sample_count": 98}, "timestamp": "2026-01-26T13:36:02.991733"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7021.914, "latencies_ms": [7021.914], "images_per_second": 0.142, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A person is cross-country skiing on a snowy trail in a forested area. They are wearing a backpack and using ski poles to navigate the terrain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20970.4, "ram_available_mb": 41870.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21023.6, "ram_available_mb": 41817.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.831}, "power_stats": {"power_gpu_soc_mean_watts": 21.827, "power_cpu_cv_mean_watts": 1.486, "power_sys_5v0_mean_watts": 8.584, "gpu_utilization_percent_mean": 73.831, "power_watts_avg": 21.827, "energy_joules_est": 153.28, "duration_seconds": 7.022, "sample_count": 59}, "timestamp": "2026-01-26T13:36:12.035690"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9108.821, "latencies_ms": [9108.821], "images_per_second": 0.11, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The skier is wearing a white helmet, black pants, and bright green boots, with a red backpack. The snow is pristine and untouched except for the ski tracks, indicating a fresh snowfall or a well-preserved trail.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21023.6, "ram_available_mb": 41817.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21056.7, "ram_available_mb": 41784.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.416}, "power_stats": {"power_gpu_soc_mean_watts": 20.399, "power_cpu_cv_mean_watts": 1.736, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 72.416, "power_watts_avg": 20.399, "energy_joules_est": 185.82, "duration_seconds": 9.109, "sample_count": 77}, "timestamp": "2026-01-26T13:36:23.161503"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12356.09, "latencies_ms": [12356.09], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a serene landscape, a BNSF Railway train, painted in vibrant hues of orange and black, is captured in motion. The locomotive, bearing the number 6309, is pulling a series of freight cars behind it. The train is traveling on a track that cuts through a tranquil wooded area, with trees standing", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21056.7, "ram_available_mb": 41784.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21042.3, "ram_available_mb": 41798.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.867}, "power_stats": {"power_gpu_soc_mean_watts": 21.382, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.868, "gpu_utilization_percent_mean": 72.867, "power_watts_avg": 21.382, "energy_joules_est": 264.21, "duration_seconds": 12.357, "sample_count": 105}, "timestamp": "2026-01-26T13:36:37.551844"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9871.531, "latencies_ms": [9871.531], "images_per_second": 0.101, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "Train: 1\nCar: 1\nTrain car: 1\nTrain tracks: 1\nTrees: 1\nBNSF logo: 1\nNumber: 6309: 1\nFence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21042.3, "ram_available_mb": 41798.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21060.3, "ram_available_mb": 41780.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.488}, "power_stats": {"power_gpu_soc_mean_watts": 22.346, "power_cpu_cv_mean_watts": 1.596, "power_sys_5v0_mean_watts": 8.777, "gpu_utilization_percent_mean": 75.488, "power_watts_avg": 22.346, "energy_joules_est": 220.6, "duration_seconds": 9.872, "sample_count": 84}, "timestamp": "2026-01-26T13:36:49.484324"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 6996.317, "latencies_ms": [6996.317], "images_per_second": 0.143, "prompt_tokens": 44, "response_tokens_est": 34, "n_tiles": 16, "output_text": "The train is in the foreground of the image, moving from left to right. The trees in the background appear to be far away and are behind the train.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.0, "ram_available_mb": 41875.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21041.5, "ram_available_mb": 41799.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.847}, "power_stats": {"power_gpu_soc_mean_watts": 23.714, "power_cpu_cv_mean_watts": 1.391, "power_sys_5v0_mean_watts": 8.882, "gpu_utilization_percent_mean": 76.847, "power_watts_avg": 23.714, "energy_joules_est": 165.93, "duration_seconds": 6.997, "sample_count": 59}, "timestamp": "2026-01-26T13:36:58.531550"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9429.417, "latencies_ms": [9429.417], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A BNSF train, numbered 6309, is traveling on a railway track with a clear blue sky in the background. The train is surrounded by trees with no leaves, indicating that it might be autumn or winter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.2, "ram_available_mb": 41879.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20966.2, "ram_available_mb": 41874.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.278}, "power_stats": {"power_gpu_soc_mean_watts": 22.593, "power_cpu_cv_mean_watts": 1.545, "power_sys_5v0_mean_watts": 8.77, "gpu_utilization_percent_mean": 76.278, "power_watts_avg": 22.593, "energy_joules_est": 213.05, "duration_seconds": 9.43, "sample_count": 79}, "timestamp": "2026-01-26T13:37:10.000148"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6194.758, "latencies_ms": [6194.758], "images_per_second": 0.161, "prompt_tokens": 36, "response_tokens_est": 27, "n_tiles": 16, "output_text": "The train is predominantly orange with black and yellow accents. The sky is clear and blue, indicating good weather conditions.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20966.2, "ram_available_mb": 41874.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 21047.4, "ram_available_mb": 41793.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.115}, "power_stats": {"power_gpu_soc_mean_watts": 24.232, "power_cpu_cv_mean_watts": 1.239, "power_sys_5v0_mean_watts": 8.847, "gpu_utilization_percent_mean": 78.115, "power_watts_avg": 24.232, "energy_joules_est": 150.13, "duration_seconds": 6.195, "sample_count": 52}, "timestamp": "2026-01-26T13:37:18.238673"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11563.217, "latencies_ms": [11563.217], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a dining table with a plate of food on it. The plate contains a variety of food items, including a bowl of broccoli, a piece of bread with guacamole, and a slice of bread with cream cheese. The broccoli is placed in a bowl, while the other food items are spread out on the plate. The arrangement of", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20967.1, "ram_available_mb": 41873.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20966.7, "ram_available_mb": 41874.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.133}, "power_stats": {"power_gpu_soc_mean_watts": 19.464, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.697, "gpu_utilization_percent_mean": 70.133, "power_watts_avg": 19.464, "energy_joules_est": 225.08, "duration_seconds": 11.564, "sample_count": 98}, "timestamp": "2026-01-26T13:37:31.837939"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8384.881, "latencies_ms": [8384.881], "images_per_second": 0.119, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "plate: 2, bowl: 1, piece of bread: 1, avocado spread: 1, broccoli: 1, wooden table: 1, image: 1, website: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20966.7, "ram_available_mb": 41874.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 10.3, "ram_used_mb": 21031.0, "ram_available_mb": 41809.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.292}, "power_stats": {"power_gpu_soc_mean_watts": 20.909, "power_cpu_cv_mean_watts": 2.363, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 74.292, "power_watts_avg": 20.909, "energy_joules_est": 175.33, "duration_seconds": 8.386, "sample_count": 72}, "timestamp": "2026-01-26T13:37:42.251464"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10788.605, "latencies_ms": [10788.605], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground, there is a plate with a piece of bread topped with a green spread, which is positioned to the left of a bowl containing a green vegetable dish. The bowl is placed on the right side of the plate, and both are situated on a wooden surface that appears to be the background of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21031.0, "ram_available_mb": 41809.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21059.5, "ram_available_mb": 41781.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.326}, "power_stats": {"power_gpu_soc_mean_watts": 19.572, "power_cpu_cv_mean_watts": 1.858, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 70.326, "power_watts_avg": 19.572, "energy_joules_est": 211.17, "duration_seconds": 10.789, "sample_count": 92}, "timestamp": "2026-01-26T13:37:55.054446"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9972.153, "latencies_ms": [9972.153], "images_per_second": 0.1, "prompt_tokens": 37, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image shows a meal consisting of a slice of bread with avocado spread, a bowl of broccoli, and a small bowl of cream cheese, all placed on a wooden table. The food is arranged on a light blue plate, creating a visually appealing presentation.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21059.5, "ram_available_mb": 41781.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21042.0, "ram_available_mb": 41798.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.702}, "power_stats": {"power_gpu_soc_mean_watts": 20.186, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 8.629, "gpu_utilization_percent_mean": 71.702, "power_watts_avg": 20.186, "energy_joules_est": 201.31, "duration_seconds": 9.973, "sample_count": 84}, "timestamp": "2026-01-26T13:38:07.051977"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11557.907, "latencies_ms": [11557.907], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a meal on a wooden table with a blue plate. The plate contains a piece of bread with a green spread, a bowl of broccoli, and a dollop of cream cheese on another piece of bread. The lighting is warm and the colors are vibrant, with the green of the broccoli and avocado spread standing out against the", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21042.0, "ram_available_mb": 41798.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21031.7, "ram_available_mb": 41809.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.235}, "power_stats": {"power_gpu_soc_mean_watts": 19.325, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.696, "gpu_utilization_percent_mean": 70.235, "power_watts_avg": 19.325, "energy_joules_est": 223.37, "duration_seconds": 11.559, "sample_count": 98}, "timestamp": "2026-01-26T13:38:20.624057"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11598.234, "latencies_ms": [11598.234], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is seen sleeping on a wooden bench in a park. The person is wrapped in an orange sleeping bag, providing warmth and comfort. A blue backpack is placed on the bench next to the sleeping bag, suggesting the person might be traveling or on a journey. The bench is situated on a grassy area, with a wooden f", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21031.7, "ram_available_mb": 41809.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21052.1, "ram_available_mb": 41788.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.293}, "power_stats": {"power_gpu_soc_mean_watts": 19.369, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.697, "gpu_utilization_percent_mean": 70.293, "power_watts_avg": 19.369, "energy_joules_est": 224.66, "duration_seconds": 11.599, "sample_count": 99}, "timestamp": "2026-01-26T13:38:34.257246"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8105.274, "latencies_ms": [8105.274], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "bench: 1, sleeping bag: 1, backpack: 1, coat: 1, parking meter: 2, fence: 1, grass: 1, car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21052.1, "ram_available_mb": 41788.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21053.8, "ram_available_mb": 41787.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.5}, "power_stats": {"power_gpu_soc_mean_watts": 20.987, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 73.5, "power_watts_avg": 20.987, "energy_joules_est": 170.12, "duration_seconds": 8.106, "sample_count": 70}, "timestamp": "2026-01-26T13:38:44.394569"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11568.497, "latencies_ms": [11568.497], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden bench with a sleeping bag and an orange blanket on it, indicating a person might be resting there. Behind the bench, there are two parking meters, suggesting this is a public space, likely a park or a street with parking. The background features a wooden fence and some greenery, which could be part of", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 21053.8, "ram_available_mb": 41787.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20976.8, "ram_available_mb": 41864.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.051}, "power_stats": {"power_gpu_soc_mean_watts": 19.356, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 70.051, "power_watts_avg": 19.356, "energy_joules_est": 223.93, "duration_seconds": 11.569, "sample_count": 99}, "timestamp": "2026-01-26T13:38:57.997864"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10391.358, "latencies_ms": [10391.358], "images_per_second": 0.096, "prompt_tokens": 37, "response_tokens_est": 67, "n_tiles": 16, "output_text": "A person is sleeping on a park bench, covered with an orange sleeping bag, with a blue backpack and a red jacket visible. Behind the bench, there are two parking meters and a wooden fence, indicating that this scene is likely in a public park or a similar outdoor area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20976.8, "ram_available_mb": 41864.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21035.3, "ram_available_mb": 41805.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.899}, "power_stats": {"power_gpu_soc_mean_watts": 19.812, "power_cpu_cv_mean_watts": 1.798, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 70.899, "power_watts_avg": 19.812, "energy_joules_est": 205.89, "duration_seconds": 10.392, "sample_count": 89}, "timestamp": "2026-01-26T13:39:10.437887"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8243.468, "latencies_ms": [8243.468], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a person wrapped in an orange sleeping bag, lying on a wooden bench. The bench is situated in a grassy area with a wooden fence in the background and two parking meters are visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20974.3, "ram_available_mb": 41866.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20977.3, "ram_available_mb": 41863.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.4}, "power_stats": {"power_gpu_soc_mean_watts": 20.691, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 72.4, "power_watts_avg": 20.691, "energy_joules_est": 170.58, "duration_seconds": 8.244, "sample_count": 70}, "timestamp": "2026-01-26T13:39:20.718723"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11561.414, "latencies_ms": [11561.414], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a room with a white shelf displaying various vases and decorative items. The focal point is a large, colorful vase placed on the shelf, surrounded by smaller vases and other decorative objects. There are also several potted plants in the scene, adding a touch of greenery to the room.\n\nIn addition to the vases and plants,", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20977.3, "ram_available_mb": 41863.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20982.5, "ram_available_mb": 41858.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.919}, "power_stats": {"power_gpu_soc_mean_watts": 19.32, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.7, "gpu_utilization_percent_mean": 69.919, "power_watts_avg": 19.32, "energy_joules_est": 223.38, "duration_seconds": 11.562, "sample_count": 99}, "timestamp": "2026-01-26T13:39:34.313394"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8603.779, "latencies_ms": [8603.779], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "Vase: 1\nPlants: 1\nLeaves: 1\nFlowers: 1\nStems: 1\nVases: 2\nCeramic art: 1\nPaper: 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20982.5, "ram_available_mb": 41858.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20965.8, "ram_available_mb": 41875.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.315}, "power_stats": {"power_gpu_soc_mean_watts": 20.773, "power_cpu_cv_mean_watts": 1.672, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 73.315, "power_watts_avg": 20.773, "energy_joules_est": 178.74, "duration_seconds": 8.604, "sample_count": 73}, "timestamp": "2026-01-26T13:39:44.941884"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11652.254, "latencies_ms": [11652.254], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The vase with dried flowers is in the foreground, placed on a white pedestal. To the right, there is a decorative plate hanging on the wall, and further back, there are two jars, one on top of the other, on a shelf. The vase is positioned closer to the viewer than the plate and jars, creating a sense", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.8, "ram_available_mb": 41875.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20978.2, "ram_available_mb": 41862.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.03}, "power_stats": {"power_gpu_soc_mean_watts": 19.548, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.706, "gpu_utilization_percent_mean": 70.03, "power_watts_avg": 19.548, "energy_joules_est": 227.79, "duration_seconds": 11.653, "sample_count": 99}, "timestamp": "2026-01-26T13:39:58.604479"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7132.522, "latencies_ms": [7132.522], "images_per_second": 0.14, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image showcases a collection of vases and decorative items displayed on a shelf. The vases are of various shapes and sizes, with some featuring unique designs and patterns.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20978.2, "ram_available_mb": 41862.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21030.9, "ram_available_mb": 41810.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.317}, "power_stats": {"power_gpu_soc_mean_watts": 21.757, "power_cpu_cv_mean_watts": 1.494, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 74.317, "power_watts_avg": 21.757, "energy_joules_est": 155.2, "duration_seconds": 7.133, "sample_count": 60}, "timestamp": "2026-01-26T13:40:07.768367"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11656.08, "latencies_ms": [11656.08], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a vibrant and colorful display of flowers in a tall, slender vase with a marbled pattern of red, yellow, and blue hues. The flowers are arranged in a way that creates a sense of movement and depth, with some elements appearing to be suspended in mid-air. The lighting in the image is soft and natural, highlighting the intr", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21030.9, "ram_available_mb": 41810.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21044.4, "ram_available_mb": 41796.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.485}, "power_stats": {"power_gpu_soc_mean_watts": 19.365, "power_cpu_cv_mean_watts": 1.872, "power_sys_5v0_mean_watts": 8.662, "gpu_utilization_percent_mean": 70.485, "power_watts_avg": 19.365, "energy_joules_est": 225.73, "duration_seconds": 11.657, "sample_count": 99}, "timestamp": "2026-01-26T13:40:21.446500"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11554.276, "latencies_ms": [11554.276], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a skateboarder is performing a trick on a ramp at a skate park. The skateboarder is wearing a white shirt and blue jeans, and is skillfully riding his skateboard on the ramp. The ramp is designed for skateboarders to perform various tricks and maneuvers.\n\nThere are several", "error": null, "sys_before": {"cpu_percent": 13.0, "ram_used_mb": 21044.4, "ram_available_mb": 41796.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21045.1, "ram_available_mb": 41795.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.131}, "power_stats": {"power_gpu_soc_mean_watts": 19.404, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.699, "gpu_utilization_percent_mean": 71.131, "power_watts_avg": 19.404, "energy_joules_est": 224.21, "duration_seconds": 11.555, "sample_count": 99}, "timestamp": "2026-01-26T13:40:35.040408"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11566.289, "latencies_ms": [11566.289], "images_per_second": 0.086, "prompt_tokens": 39, "response_tokens_est": 77, "n_tiles": 16, "output_text": "- Skateboard: 1\n- Skateboarder: 1\n- Skate park: 1\n- Concrete ramp: 1\n- Skateboard wheels: 4\n- Skateboard trucks: 4\n- Skateboard deck: 1\n- Skateboarder's shoes: 2", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21045.1, "ram_available_mb": 41795.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20974.8, "ram_available_mb": 41866.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.633}, "power_stats": {"power_gpu_soc_mean_watts": 19.499, "power_cpu_cv_mean_watts": 1.846, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 70.633, "power_watts_avg": 19.499, "energy_joules_est": 225.54, "duration_seconds": 11.567, "sample_count": 98}, "timestamp": "2026-01-26T13:40:48.645950"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11545.564, "latencies_ms": [11545.564], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a skateboarder is captured in mid-air, performing a trick on a curved ramp. The skateboarder is positioned near the bottom of the ramp, which extends into the background of the image. There are other individuals in the background, likely spectators or other skateboarders, who are situated at various distances from the ramp", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20974.8, "ram_available_mb": 41866.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21029.7, "ram_available_mb": 41811.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.455}, "power_stats": {"power_gpu_soc_mean_watts": 19.363, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.692, "gpu_utilization_percent_mean": 70.455, "power_watts_avg": 19.363, "energy_joules_est": 223.57, "duration_seconds": 11.546, "sample_count": 99}, "timestamp": "2026-01-26T13:41:02.205494"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8563.956, "latencies_ms": [8563.956], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "A skateboarder is performing a trick on a curved ramp at a skatepark. The skateboarder is wearing a white t-shirt, blue jeans, and a helmet with a colorful design.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21029.7, "ram_available_mb": 41811.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21034.4, "ram_available_mb": 41806.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.644}, "power_stats": {"power_gpu_soc_mean_watts": 20.721, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 72.644, "power_watts_avg": 20.721, "energy_joules_est": 177.47, "duration_seconds": 8.565, "sample_count": 73}, "timestamp": "2026-01-26T13:41:12.807657"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8582.487, "latencies_ms": [8582.487], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The skateboarder is wearing a white t-shirt and blue jeans, and the skateboard is black with blue and white designs. The skate park is made of concrete and the lighting is natural, likely from the sun.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21034.4, "ram_available_mb": 41806.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20974.7, "ram_available_mb": 41866.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.014}, "power_stats": {"power_gpu_soc_mean_watts": 20.278, "power_cpu_cv_mean_watts": 1.721, "power_sys_5v0_mean_watts": 8.675, "gpu_utilization_percent_mean": 72.014, "power_watts_avg": 20.278, "energy_joules_est": 174.05, "duration_seconds": 8.583, "sample_count": 73}, "timestamp": "2026-01-26T13:41:23.414884"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11587.377, "latencies_ms": [11587.377], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a beautifully lit city square at night, featuring a large Christmas tree in the center. The tree is adorned with lights, creating a festive atmosphere. The square is surrounded by buildings, including a clock tower that stands out prominently in the scene.\n\nThere are several people walking around the square, enjoying the holiday ambiance. A few cars", "error": null, "sys_before": {"cpu_percent": 12.0, "ram_used_mb": 20974.7, "ram_available_mb": 41866.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20967.1, "ram_available_mb": 41873.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.684}, "power_stats": {"power_gpu_soc_mean_watts": 19.393, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 70.684, "power_watts_avg": 19.393, "energy_joules_est": 224.73, "duration_seconds": 11.588, "sample_count": 98}, "timestamp": "2026-01-26T13:41:37.028587"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9455.751, "latencies_ms": [9455.751], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Clock tower: 1\n- Christmas tree: 1\n- Lights: multiple strings of lights\n- People: multiple individuals\n- Buildings: multiple buildings\n- Cars: 1\n- Trash can: 1\n- Signs: multiple signs", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20967.1, "ram_available_mb": 41873.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 21020.1, "ram_available_mb": 41820.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.321}, "power_stats": {"power_gpu_soc_mean_watts": 20.342, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.663, "gpu_utilization_percent_mean": 71.321, "power_watts_avg": 20.342, "energy_joules_est": 192.36, "duration_seconds": 9.456, "sample_count": 81}, "timestamp": "2026-01-26T13:41:48.546219"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11234.054, "latencies_ms": [11234.054], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The large Christmas tree is situated in the foreground on the right side of the image, while the clock tower is in the background on the left side. The street lights and decorative lights are strung across the foreground, creating a festive atmosphere. The buildings in the background appear to be part of a shopping district, with various shops and stores visible.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21020.1, "ram_available_mb": 41820.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21021.3, "ram_available_mb": 41819.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.632}, "power_stats": {"power_gpu_soc_mean_watts": 19.474, "power_cpu_cv_mean_watts": 1.879, "power_sys_5v0_mean_watts": 8.688, "gpu_utilization_percent_mean": 70.632, "power_watts_avg": 19.474, "energy_joules_est": 218.79, "duration_seconds": 11.235, "sample_count": 95}, "timestamp": "2026-01-26T13:42:01.798622"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8937.605, "latencies_ms": [8937.605], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a festive city square at night, illuminated by string lights and adorned with a large Christmas tree. A prominent clock tower stands in the center, surrounded by buildings with lit windows, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20968.7, "ram_available_mb": 41872.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20974.6, "ram_available_mb": 41866.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.87}, "power_stats": {"power_gpu_soc_mean_watts": 20.414, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 8.601, "gpu_utilization_percent_mean": 72.87, "power_watts_avg": 20.414, "energy_joules_est": 182.46, "duration_seconds": 8.938, "sample_count": 77}, "timestamp": "2026-01-26T13:42:12.771763"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8918.808, "latencies_ms": [8918.808], "images_per_second": 0.112, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image features a large, ornate clock tower in the center, adorned with twinkling Christmas lights that create a festive atmosphere. The tower is surrounded by a large, snow-covered Christmas tree, adding to the holiday charm of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20974.6, "ram_available_mb": 41866.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21039.8, "ram_available_mb": 41801.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.68}, "power_stats": {"power_gpu_soc_mean_watts": 20.19, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.662, "gpu_utilization_percent_mean": 71.68, "power_watts_avg": 20.19, "energy_joules_est": 180.08, "duration_seconds": 8.919, "sample_count": 75}, "timestamp": "2026-01-26T13:42:23.723011"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11626.474, "latencies_ms": [11626.474], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is immersed in a game of tennis on a concrete court. He is dressed in a blue shirt and black shorts, and is in the midst of a powerful forehand swing. His yellow tennis racket is poised to strike the ball, which is captured in mid-air, frozen in time. The court is enclosed by a black", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21039.8, "ram_available_mb": 41801.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21040.8, "ram_available_mb": 41800.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.64}, "power_stats": {"power_gpu_soc_mean_watts": 19.229, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 70.64, "power_watts_avg": 19.229, "energy_joules_est": 223.58, "duration_seconds": 11.627, "sample_count": 100}, "timestamp": "2026-01-26T13:42:37.410764"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8007.338, "latencies_ms": [8007.338], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "boy: 1, tennis racket: 1, tennis ball: 1, tennis court: 1, fence: 1, trees: 1, sky: 1, shadows: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 21040.8, "ram_available_mb": 41800.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21041.8, "ram_available_mb": 41799.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.147}, "power_stats": {"power_gpu_soc_mean_watts": 21.121, "power_cpu_cv_mean_watts": 1.613, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 73.147, "power_watts_avg": 21.121, "energy_joules_est": 169.14, "duration_seconds": 8.008, "sample_count": 68}, "timestamp": "2026-01-26T13:42:47.476006"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11600.945, "latencies_ms": [11600.945], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is standing on a tennis court, holding a tennis racket and preparing to hit a tennis ball that is in the air to the right of the racket. The court lines are visible, indicating the boundaries of the playing area. In the background, there is a fence and trees, suggesting that the court is located in an outdoor area surrounded by veget", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20988.4, "ram_available_mb": 41852.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20983.3, "ram_available_mb": 41857.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.182}, "power_stats": {"power_gpu_soc_mean_watts": 19.282, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.662, "gpu_utilization_percent_mean": 69.182, "power_watts_avg": 19.282, "energy_joules_est": 223.7, "duration_seconds": 11.602, "sample_count": 99}, "timestamp": "2026-01-26T13:43:01.110660"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7091.507, "latencies_ms": [7091.507], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A person is playing tennis on a court with a fence and trees in the background. The player is in the middle of a forehand swing, preparing to hit the ball.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20983.3, "ram_available_mb": 41857.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20977.7, "ram_available_mb": 41863.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.067}, "power_stats": {"power_gpu_soc_mean_watts": 21.795, "power_cpu_cv_mean_watts": 1.507, "power_sys_5v0_mean_watts": 8.613, "gpu_utilization_percent_mean": 74.067, "power_watts_avg": 21.795, "energy_joules_est": 154.57, "duration_seconds": 7.092, "sample_count": 60}, "timestamp": "2026-01-26T13:43:10.217256"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7123.406, "latencies_ms": [7123.406], "images_per_second": 0.14, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image shows a person playing tennis on a court with a blue shirt and black shorts. The court is surrounded by a chain-link fence and there are trees in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20977.7, "ram_available_mb": 41863.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20967.4, "ram_available_mb": 41873.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.717}, "power_stats": {"power_gpu_soc_mean_watts": 21.194, "power_cpu_cv_mean_watts": 1.567, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 73.717, "power_watts_avg": 21.194, "energy_joules_est": 150.99, "duration_seconds": 7.124, "sample_count": 60}, "timestamp": "2026-01-26T13:43:19.358047"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11602.789, "latencies_ms": [11602.789], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a cozy living room with a fireplace as the focal point. The fireplace is surrounded by a white mantle, and there are two chairs placed in front of it. One chair is positioned on the left side of the room, while the other is on the right side. \n\nIn addition to the chairs, there are two potted plants in", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20967.4, "ram_available_mb": 41873.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21023.3, "ram_available_mb": 41817.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.444}, "power_stats": {"power_gpu_soc_mean_watts": 19.368, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.669, "gpu_utilization_percent_mean": 70.444, "power_watts_avg": 19.368, "energy_joules_est": 224.74, "duration_seconds": 11.603, "sample_count": 99}, "timestamp": "2026-01-26T13:43:32.989958"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9627.499, "latencies_ms": [9627.499], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- Bookshelf: 100+ books\n- Picture frame: 1\n- Fireplace: 1\n- Armchair: 1\n- End table: 2\n- Lamp: 2\n- Plant: 2\n- Chair: 1", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 20969.9, "ram_available_mb": 41871.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21023.3, "ram_available_mb": 41817.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.476}, "power_stats": {"power_gpu_soc_mean_watts": 20.223, "power_cpu_cv_mean_watts": 1.732, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 71.476, "power_watts_avg": 20.223, "energy_joules_est": 194.71, "duration_seconds": 9.628, "sample_count": 82}, "timestamp": "2026-01-26T13:43:44.665981"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11581.604, "latencies_ms": [11581.604], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a white armchair positioned near the center of the image, with a lamp on a small wooden table to its left. Behind the armchair, a fireplace with a mantelpiece above it serves as a central focal point. To the right of the fireplace, there is a bookshelf filled with books, and further to", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 21023.3, "ram_available_mb": 41817.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21036.8, "ram_available_mb": 41804.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.683}, "power_stats": {"power_gpu_soc_mean_watts": 19.146, "power_cpu_cv_mean_watts": 1.902, "power_sys_5v0_mean_watts": 8.689, "gpu_utilization_percent_mean": 69.683, "power_watts_avg": 19.146, "energy_joules_est": 221.75, "duration_seconds": 11.582, "sample_count": 101}, "timestamp": "2026-01-26T13:43:58.280224"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11287.628, "latencies_ms": [11287.628], "images_per_second": 0.089, "prompt_tokens": 37, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a fireplace, a chair, and a bookshelf. The fireplace is the focal point of the room, with a picture frame above it and a green marble mantle. The chair is placed in front of the fireplace, and the bookshelf is filled with books on both sides.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21036.8, "ram_available_mb": 41804.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20965.8, "ram_available_mb": 41875.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.125}, "power_stats": {"power_gpu_soc_mean_watts": 19.697, "power_cpu_cv_mean_watts": 1.838, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 71.125, "power_watts_avg": 19.697, "energy_joules_est": 222.34, "duration_seconds": 11.288, "sample_count": 96}, "timestamp": "2026-01-26T13:44:11.607489"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8692.609, "latencies_ms": [8692.609], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The room is well-lit with natural light, and the walls are painted in a light color, giving it a bright and airy feel. The furniture is primarily white, with a mix of wood and leather materials, creating a classic and elegant atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.8, "ram_available_mb": 41875.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20957.1, "ram_available_mb": 41883.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.96}, "power_stats": {"power_gpu_soc_mean_watts": 20.223, "power_cpu_cv_mean_watts": 1.729, "power_sys_5v0_mean_watts": 8.659, "gpu_utilization_percent_mean": 71.96, "power_watts_avg": 20.223, "energy_joules_est": 175.83, "duration_seconds": 8.695, "sample_count": 75}, "timestamp": "2026-01-26T13:44:22.352491"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11575.989, "latencies_ms": [11575.989], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a group of three zebras is grazing in a grassy field. They are standing close to each other, with one zebra on the left side of the frame, another in the middle, and the third one on the right side. The zebras are eating grass from the ground, and their distinctive black and white stripes are clearly visible.", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20957.1, "ram_available_mb": 41883.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20963.1, "ram_available_mb": 41877.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.343}, "power_stats": {"power_gpu_soc_mean_watts": 19.346, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.706, "gpu_utilization_percent_mean": 70.343, "power_watts_avg": 19.346, "energy_joules_est": 223.96, "duration_seconds": 11.577, "sample_count": 99}, "timestamp": "2026-01-26T13:44:35.983847"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7111.052, "latencies_ms": [7111.052], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 38, "n_tiles": 16, "output_text": "zebra: 3, grass: many, trees: sparse, sky: clear, ground: dry, sun: bright, shadows: visible, animals: zebras", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.1, "ram_available_mb": 41877.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20963.1, "ram_available_mb": 41877.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.083}, "power_stats": {"power_gpu_soc_mean_watts": 21.804, "power_cpu_cv_mean_watts": 1.494, "power_sys_5v0_mean_watts": 8.579, "gpu_utilization_percent_mean": 74.083, "power_watts_avg": 21.804, "energy_joules_est": 155.06, "duration_seconds": 7.112, "sample_count": 60}, "timestamp": "2026-01-26T13:44:45.127428"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11596.293, "latencies_ms": [11596.293], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a group of zebras with one partially obscured by the others, indicating they are close together. The zebra in the background is further away from the camera, making it appear smaller. The zebras are grazing on the grass, with one in the foreground on the left side of the image and the others spread out towards the right side", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.1, "ram_available_mb": 41877.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21035.2, "ram_available_mb": 41805.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.051}, "power_stats": {"power_gpu_soc_mean_watts": 19.198, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 70.051, "power_watts_avg": 19.198, "energy_joules_est": 222.64, "duration_seconds": 11.597, "sample_count": 98}, "timestamp": "2026-01-26T13:44:58.751699"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6967.022, "latencies_ms": [6967.022], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "Three zebras are grazing in a grassy field with dry grass and some bushes in the background. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20956.6, "ram_available_mb": 41884.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20967.4, "ram_available_mb": 41873.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.831}, "power_stats": {"power_gpu_soc_mean_watts": 21.947, "power_cpu_cv_mean_watts": 1.486, "power_sys_5v0_mean_watts": 8.565, "gpu_utilization_percent_mean": 73.831, "power_watts_avg": 21.947, "energy_joules_est": 152.92, "duration_seconds": 6.968, "sample_count": 59}, "timestamp": "2026-01-26T13:45:07.778294"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6902.477, "latencies_ms": [6902.477], "images_per_second": 0.145, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The zebras are grazing in a field with dry grass and sparse trees under a clear blue sky. The lighting is bright and natural, indicating it is likely daytime.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20967.4, "ram_available_mb": 41873.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21019.6, "ram_available_mb": 41821.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.707}, "power_stats": {"power_gpu_soc_mean_watts": 21.377, "power_cpu_cv_mean_watts": 1.539, "power_sys_5v0_mean_watts": 8.609, "gpu_utilization_percent_mean": 73.707, "power_watts_avg": 21.377, "energy_joules_est": 147.57, "duration_seconds": 6.903, "sample_count": 58}, "timestamp": "2026-01-26T13:45:16.742957"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11633.429, "latencies_ms": [11633.429], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people sitting around a dining table in a restaurant. There are at least 13 people in the scene, with some sitting at the table and others standing or sitting at nearby tables. The dining area is furnished with multiple chairs, and the tables are adorned with cups, bowls, and bottles.\n\nThe restaurant has a", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20957.8, "ram_available_mb": 41883.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20963.9, "ram_available_mb": 41877.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.354}, "power_stats": {"power_gpu_soc_mean_watts": 19.291, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 70.354, "power_watts_avg": 19.291, "energy_joules_est": 224.43, "duration_seconds": 11.634, "sample_count": 99}, "timestamp": "2026-01-26T13:45:30.422414"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8342.37, "latencies_ms": [8342.37], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "table: 1\nchairs: 4\npeople: 13\ncounters: 1\nclock: 1\nframed pictures: 4\nbottles: 10\nflags: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.9, "ram_available_mb": 41877.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21011.4, "ram_available_mb": 41829.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.113}, "power_stats": {"power_gpu_soc_mean_watts": 20.849, "power_cpu_cv_mean_watts": 1.646, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 73.113, "power_watts_avg": 20.849, "energy_joules_est": 173.94, "duration_seconds": 8.343, "sample_count": 71}, "timestamp": "2026-01-26T13:45:40.808362"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11602.87, "latencies_ms": [11602.87], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden table with several people seated around it, engaged in conversation. The table is surrounded by wooden chairs, and there are various items on the table such as cups, bottles, and menus. In the background, there are more people seated at other tables, and the room has a brick wall with a clock hanging on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20958.1, "ram_available_mb": 41882.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 21023.3, "ram_available_mb": 41817.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.293}, "power_stats": {"power_gpu_soc_mean_watts": 19.295, "power_cpu_cv_mean_watts": 2.009, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 70.293, "power_watts_avg": 19.295, "energy_joules_est": 223.89, "duration_seconds": 11.603, "sample_count": 99}, "timestamp": "2026-01-26T13:45:54.458193"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9174.99, "latencies_ms": [9174.99], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image depicts a group of people, including two men in military uniforms, sitting around a wooden table in a restaurant. They appear to be engaged in a conversation, with various items such as cups, bottles, and a menu visible on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21023.3, "ram_available_mb": 41817.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20999.2, "ram_available_mb": 41841.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.192}, "power_stats": {"power_gpu_soc_mean_watts": 20.296, "power_cpu_cv_mean_watts": 1.724, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 72.192, "power_watts_avg": 20.296, "energy_joules_est": 186.23, "duration_seconds": 9.176, "sample_count": 78}, "timestamp": "2026-01-26T13:46:05.645074"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7675.227, "latencies_ms": [7675.227], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows an indoor setting with warm lighting that highlights the wooden floors and furniture. The walls are adorned with brickwork, giving the space a rustic and cozy ambiance.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20999.2, "ram_available_mb": 41841.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21068.9, "ram_available_mb": 41772.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.273}, "power_stats": {"power_gpu_soc_mean_watts": 20.979, "power_cpu_cv_mean_watts": 1.637, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 73.273, "power_watts_avg": 20.979, "energy_joules_est": 161.03, "duration_seconds": 7.676, "sample_count": 66}, "timestamp": "2026-01-26T13:46:15.350083"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11553.267, "latencies_ms": [11553.267], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of white swans swimming in a body of water, possibly a lake or a pond. There are at least 13 swans in the scene, with some swimming close to the shore and others further away. The swans are spread out across the water, creating a beautiful and serene atmosphere.\n\nIn the background, there are several boats floating", "error": null, "sys_before": {"cpu_percent": 6.5, "ram_used_mb": 20963.7, "ram_available_mb": 41877.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21024.8, "ram_available_mb": 41816.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.38}, "power_stats": {"power_gpu_soc_mean_watts": 19.358, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.687, "gpu_utilization_percent_mean": 70.38, "power_watts_avg": 19.358, "energy_joules_est": 223.66, "duration_seconds": 11.554, "sample_count": 100}, "timestamp": "2026-01-26T13:46:28.950263"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7275.081, "latencies_ms": [7275.081], "images_per_second": 0.137, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "swan: 14, boat: 15, water: many, sun: visible, marina: visible, birds: 14, water: many, sky: visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20962.6, "ram_available_mb": 41878.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20963.5, "ram_available_mb": 41877.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.532}, "power_stats": {"power_gpu_soc_mean_watts": 21.788, "power_cpu_cv_mean_watts": 1.569, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 75.532, "power_watts_avg": 21.788, "energy_joules_est": 158.53, "duration_seconds": 7.276, "sample_count": 62}, "timestamp": "2026-01-26T13:46:38.262066"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11597.74, "latencies_ms": [11597.74], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a group of white swans swimming in the water, with some closer to the viewer and others further away. In the background, there are several boats docked at a marina, with some closer to the viewer and others further away. The swans are in the water in front of the boats, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20963.5, "ram_available_mb": 41877.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20965.4, "ram_available_mb": 41875.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.333}, "power_stats": {"power_gpu_soc_mean_watts": 19.304, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.676, "gpu_utilization_percent_mean": 70.333, "power_watts_avg": 19.304, "energy_joules_est": 223.89, "duration_seconds": 11.598, "sample_count": 99}, "timestamp": "2026-01-26T13:46:51.884708"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8340.537, "latencies_ms": [8340.537], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A group of white swans are swimming in a body of water near a marina with several boats docked in the background. The scene is peaceful and serene, with the swans gracefully gliding through the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.4, "ram_available_mb": 41875.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20971.6, "ram_available_mb": 41869.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.944}, "power_stats": {"power_gpu_soc_mean_watts": 20.918, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.595, "gpu_utilization_percent_mean": 72.944, "power_watts_avg": 20.918, "energy_joules_est": 174.48, "duration_seconds": 8.341, "sample_count": 71}, "timestamp": "2026-01-26T13:47:02.259515"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9292.464, "latencies_ms": [9292.464], "images_per_second": 0.108, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image features a serene scene with a group of white swans swimming in a body of water that reflects a purple hue, likely due to the lighting conditions. The water is calm, and the sky is clear, suggesting a peaceful and sunny day.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20971.6, "ram_available_mb": 41869.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21009.8, "ram_available_mb": 41831.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.731}, "power_stats": {"power_gpu_soc_mean_watts": 20.132, "power_cpu_cv_mean_watts": 1.749, "power_sys_5v0_mean_watts": 8.686, "gpu_utilization_percent_mean": 71.731, "power_watts_avg": 20.132, "energy_joules_est": 187.09, "duration_seconds": 9.293, "sample_count": 78}, "timestamp": "2026-01-26T13:47:13.572837"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11593.358, "latencies_ms": [11593.358], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of joy and celebration, set against the backdrop of a beautifully decorated wedding venue. The room is adorned with a white canopy, from which a string of pink and green triangular flags hang, adding a touch of color to the otherwise monochrome setting. The floor, made of wood, adds a rustic charm to", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20954.3, "ram_available_mb": 41886.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20960.1, "ram_available_mb": 41880.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.031}, "power_stats": {"power_gpu_soc_mean_watts": 19.206, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 70.031, "power_watts_avg": 19.206, "energy_joules_est": 222.67, "duration_seconds": 11.594, "sample_count": 98}, "timestamp": "2026-01-26T13:47:27.204772"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8953.431, "latencies_ms": [8953.431], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "wedding cake: 1, guests: 2, triangular flags: 12, microphones: 2, musical instruments: 3, gift boxes: 2, tablecloths: 2, chairs: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20960.1, "ram_available_mb": 41880.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20972.3, "ram_available_mb": 41868.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.974}, "power_stats": {"power_gpu_soc_mean_watts": 20.523, "power_cpu_cv_mean_watts": 1.674, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 71.974, "power_watts_avg": 20.523, "energy_joules_est": 183.76, "duration_seconds": 8.954, "sample_count": 76}, "timestamp": "2026-01-26T13:47:38.200301"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11650.581, "latencies_ms": [11650.581], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a couple is standing close together, with the woman in a white dress and the man in a suit, positioned near the center of the image. They are surrounded by a table with a white cloth and various items on it, which is located slightly to their right. In the background, there is a stage with musical equipment and a speaker, which is further away from the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20972.3, "ram_available_mb": 41868.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20951.4, "ram_available_mb": 41889.5, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.657}, "power_stats": {"power_gpu_soc_mean_watts": 19.393, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.71, "gpu_utilization_percent_mean": 70.657, "power_watts_avg": 19.393, "energy_joules_est": 225.95, "duration_seconds": 11.651, "sample_count": 99}, "timestamp": "2026-01-26T13:47:51.877884"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8618.496, "latencies_ms": [8618.496], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image captures a joyous moment at a wedding reception held in a large, elegantly decorated tent. A bride and groom are sharing a dance, surrounded by festive decorations and a stage set up for entertainment.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20951.4, "ram_available_mb": 41889.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21020.9, "ram_available_mb": 41820.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.123}, "power_stats": {"power_gpu_soc_mean_watts": 20.754, "power_cpu_cv_mean_watts": 1.661, "power_sys_5v0_mean_watts": 8.602, "gpu_utilization_percent_mean": 72.123, "power_watts_avg": 20.754, "energy_joules_est": 178.88, "duration_seconds": 8.619, "sample_count": 73}, "timestamp": "2026-01-26T13:48:02.509535"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10633.095, "latencies_ms": [10633.095], "images_per_second": 0.094, "prompt_tokens": 36, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image shows an indoor wedding reception with a large white tent-like ceiling, adorned with pink and green triangular bunting. The lighting is warm and ambient, coming from multiple sources including table lamps and what appears to be candles on the tables, creating a cozy and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20966.5, "ram_available_mb": 41874.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20972.1, "ram_available_mb": 41868.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.433}, "power_stats": {"power_gpu_soc_mean_watts": 19.661, "power_cpu_cv_mean_watts": 1.832, "power_sys_5v0_mean_watts": 8.68, "gpu_utilization_percent_mean": 71.433, "power_watts_avg": 19.661, "energy_joules_est": 209.07, "duration_seconds": 10.634, "sample_count": 90}, "timestamp": "2026-01-26T13:48:15.177810"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11613.483, "latencies_ms": [11613.483], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a cozy living room bathed in soft light. Dominating the scene is a blue sofa, its vibrant color contrasting beautifully with the warm red walls. The sofa is adorned with a white throw pillow, adding a touch of elegance to the room. \n\nTo the left of the sofa, a wooden coffee table s", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20972.1, "ram_available_mb": 41868.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20961.9, "ram_available_mb": 41879.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.434}, "power_stats": {"power_gpu_soc_mean_watts": 19.306, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.673, "gpu_utilization_percent_mean": 70.434, "power_watts_avg": 19.306, "energy_joules_est": 224.23, "duration_seconds": 11.614, "sample_count": 99}, "timestamp": "2026-01-26T13:48:28.825792"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8615.54, "latencies_ms": [8615.54], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "- Couch: 1\n\n- Table: 2\n\n- Plant: 3\n\n- Picture: 2\n\n- Window: 2\n\n- Rug: 1\n\n- Lamp: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20961.9, "ram_available_mb": 41879.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20956.8, "ram_available_mb": 41884.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.865}, "power_stats": {"power_gpu_soc_mean_watts": 20.516, "power_cpu_cv_mean_watts": 1.671, "power_sys_5v0_mean_watts": 8.59, "gpu_utilization_percent_mean": 72.865, "power_watts_avg": 20.516, "energy_joules_est": 176.77, "duration_seconds": 8.616, "sample_count": 74}, "timestamp": "2026-01-26T13:48:39.482512"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11594.8, "latencies_ms": [11594.8], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a blue sofa positioned against the wall, with a small wooden side table to its left. On the right side of the sofa, there is a round glass coffee table. In the background, there are two windows with white curtains; the window on the left is closer to the viewer and has a potted plant on the windowsill,", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20956.8, "ram_available_mb": 41884.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21018.0, "ram_available_mb": 41822.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.54}, "power_stats": {"power_gpu_soc_mean_watts": 19.254, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.707, "gpu_utilization_percent_mean": 70.54, "power_watts_avg": 19.254, "energy_joules_est": 223.26, "duration_seconds": 11.595, "sample_count": 100}, "timestamp": "2026-01-26T13:48:53.127238"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9147.771, "latencies_ms": [9147.771], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a blue sofa, a small round table with a lamp, and a potted plant. The room has red walls and two windows with white curtains, and there is a painting hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21018.0, "ram_available_mb": 41822.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21032.2, "ram_available_mb": 41808.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.519}, "power_stats": {"power_gpu_soc_mean_watts": 20.534, "power_cpu_cv_mean_watts": 1.705, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 72.519, "power_watts_avg": 20.534, "energy_joules_est": 187.85, "duration_seconds": 9.148, "sample_count": 77}, "timestamp": "2026-01-26T13:49:04.296088"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8017.616, "latencies_ms": [8017.616], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The room has a warm and cozy atmosphere with red walls and a combination of natural and artificial lighting. The furniture includes a blue sofa, a wooden coffee table, and a round glass table with a plant on it.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21032.2, "ram_available_mb": 41808.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21010.8, "ram_available_mb": 41830.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.632}, "power_stats": {"power_gpu_soc_mean_watts": 20.774, "power_cpu_cv_mean_watts": 1.665, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 72.632, "power_watts_avg": 20.774, "energy_joules_est": 166.57, "duration_seconds": 8.018, "sample_count": 68}, "timestamp": "2026-01-26T13:49:14.348301"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12434.164, "latencies_ms": [12434.164], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a scene featuring a doll with long, flowing red hair and a white headband. The doll is positioned in the center of the image, with its head tilted slightly to the right. It is holding a book in its hands, which is open and resting on a wooden surface. The background of the image is a light blue color, providing a contrast to the", "error": null, "sys_before": {"cpu_percent": 6.5, "ram_used_mb": 20957.4, "ram_available_mb": 41883.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20972.1, "ram_available_mb": 41868.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.819}, "power_stats": {"power_gpu_soc_mean_watts": 21.585, "power_cpu_cv_mean_watts": 1.806, "power_sys_5v0_mean_watts": 8.925, "gpu_utilization_percent_mean": 72.819, "power_watts_avg": 21.585, "energy_joules_est": 268.41, "duration_seconds": 12.435, "sample_count": 105}, "timestamp": "2026-01-26T13:49:28.814413"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8548.474, "latencies_ms": [8548.474], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "doll: 1, clock: 2, book: 1, hair: 2, face: 1, eyes: 2, hands: 2, numbers: 12", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20972.1, "ram_available_mb": 41868.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21037.0, "ram_available_mb": 41803.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.069}, "power_stats": {"power_gpu_soc_mean_watts": 23.427, "power_cpu_cv_mean_watts": 1.389, "power_sys_5v0_mean_watts": 8.76, "gpu_utilization_percent_mean": 78.069, "power_watts_avg": 23.427, "energy_joules_est": 200.28, "duration_seconds": 8.549, "sample_count": 72}, "timestamp": "2026-01-26T13:49:39.398169"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12033.936, "latencies_ms": [12033.936], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The clock is positioned to the left of the image, partially overlapping the figure on the right. The figure appears to be in the foreground, with the clock slightly behind it, creating a sense of depth. The background is a plain, light-colored surface that provides contrast to the darker tones of the figure and the clock.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20950.4, "ram_available_mb": 41890.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 21027.1, "ram_available_mb": 41813.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.961}, "power_stats": {"power_gpu_soc_mean_watts": 21.854, "power_cpu_cv_mean_watts": 1.84, "power_sys_5v0_mean_watts": 8.893, "gpu_utilization_percent_mean": 73.961, "power_watts_avg": 21.854, "energy_joules_est": 263.01, "duration_seconds": 12.035, "sample_count": 102}, "timestamp": "2026-01-26T13:49:53.459206"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 12956.814, "latencies_ms": [12956.814], "images_per_second": 0.077, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a whimsical scene with a doll sitting on a wooden surface, surrounded by two large clocks that are positioned as if they are the doll's head. The doll has long, flowing red hair and is wearing a white headband, while the clocks have black numbers and hands, with one showing the time as 10:10.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 21027.1, "ram_available_mb": 41813.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21042.9, "ram_available_mb": 41798.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.791}, "power_stats": {"power_gpu_soc_mean_watts": 21.76, "power_cpu_cv_mean_watts": 1.852, "power_sys_5v0_mean_watts": 8.856, "gpu_utilization_percent_mean": 73.791, "power_watts_avg": 21.76, "energy_joules_est": 281.95, "duration_seconds": 12.957, "sample_count": 110}, "timestamp": "2026-01-26T13:50:08.472918"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9660.472, "latencies_ms": [9660.472], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image features a doll with a warm, orange-toned hair and a clock with a yellow face and black hands. The doll is placed on a wooden surface with a soft, diffused lighting that gives the scene a gentle and serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21042.9, "ram_available_mb": 41798.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21044.2, "ram_available_mb": 41796.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.134}, "power_stats": {"power_gpu_soc_mean_watts": 22.698, "power_cpu_cv_mean_watts": 1.596, "power_sys_5v0_mean_watts": 8.882, "gpu_utilization_percent_mean": 76.134, "power_watts_avg": 22.698, "energy_joules_est": 219.29, "duration_seconds": 9.661, "sample_count": 82}, "timestamp": "2026-01-26T13:50:20.171696"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12425.761, "latencies_ms": [12425.761], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is sitting on a motorcycle, wearing a helmet and a tan jacket. He is holding onto the handlebars of the motorcycle, which is parked on the side of the road. The man appears to be looking down, possibly checking something or preparing to ride.\n\nThere are several other people in the scene, with one person standing near", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20958.2, "ram_available_mb": 41882.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21042.8, "ram_available_mb": 41798.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11229.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.991}, "power_stats": {"power_gpu_soc_mean_watts": 21.377, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 8.828, "gpu_utilization_percent_mean": 71.991, "power_watts_avg": 21.377, "energy_joules_est": 265.64, "duration_seconds": 12.426, "sample_count": 106}, "timestamp": "2026-01-26T13:50:34.629119"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10592.438, "latencies_ms": [10592.438], "images_per_second": 0.094, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Helmet: 1\n\n- Motorcycle: 1\n\n- Mirror: 1\n\n- Man: 1\n\n- Pants: 1\n\n- Jacket: 1\n\n- Boot: 1\n\n- Seat: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20964.3, "ram_available_mb": 41876.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21041.4, "ram_available_mb": 41799.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11258.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.549}, "power_stats": {"power_gpu_soc_mean_watts": 22.111, "power_cpu_cv_mean_watts": 1.653, "power_sys_5v0_mean_watts": 8.787, "gpu_utilization_percent_mean": 74.549, "power_watts_avg": 22.111, "energy_joules_est": 234.22, "duration_seconds": 10.593, "sample_count": 91}, "timestamp": "2026-01-26T13:50:47.266105"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12195.056, "latencies_ms": [12195.056], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The person is seated on a motorcycle, which is positioned in the foreground of the image. In the background, there are other individuals, one of whom is wearing a red helmet, suggesting a public or crowded space. The motorcycle and its rider are the main focus and are located near the bottom of the image, while the background figures are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21041.4, "ram_available_mb": 41799.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21057.9, "ram_available_mb": 41783.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11268.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.029}, "power_stats": {"power_gpu_soc_mean_watts": 21.481, "power_cpu_cv_mean_watts": 1.833, "power_sys_5v0_mean_watts": 8.904, "gpu_utilization_percent_mean": 72.029, "power_watts_avg": 21.481, "energy_joules_est": 261.98, "duration_seconds": 12.196, "sample_count": 105}, "timestamp": "2026-01-26T13:51:01.514603"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8083.657, "latencies_ms": [8083.657], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A man is sitting on a motorcycle, wearing a helmet and holding onto the handlebars. He appears to be in a crowded area with other people and vehicles around him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20953.1, "ram_available_mb": 41887.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20965.3, "ram_available_mb": 41875.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11254.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.265}, "power_stats": {"power_gpu_soc_mean_watts": 23.319, "power_cpu_cv_mean_watts": 1.406, "power_sys_5v0_mean_watts": 8.722, "gpu_utilization_percent_mean": 77.265, "power_watts_avg": 23.319, "energy_joules_est": 188.52, "duration_seconds": 8.084, "sample_count": 68}, "timestamp": "2026-01-26T13:51:11.644534"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11873.77, "latencies_ms": [11873.77], "images_per_second": 0.084, "prompt_tokens": 36, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The image shows a person wearing a light beige shirt and beige pants, sitting on a motorcycle. The motorcycle is predominantly white with red accents. The person is wearing a black helmet with a clear visor. The background is blurred, but it appears to be an outdoor setting with other people and vehicles.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20965.3, "ram_available_mb": 41875.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20980.5, "ram_available_mb": 41860.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11252.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.644}, "power_stats": {"power_gpu_soc_mean_watts": 21.535, "power_cpu_cv_mean_watts": 1.815, "power_sys_5v0_mean_watts": 8.913, "gpu_utilization_percent_mean": 71.644, "power_watts_avg": 21.535, "energy_joules_est": 255.71, "duration_seconds": 11.874, "sample_count": 101}, "timestamp": "2026-01-26T13:51:25.570352"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 9410.55, "latencies_ms": [9410.55], "images_per_second": 0.106, "prompt_tokens": 24, "response_tokens_est": 60, "n_tiles": 16, "output_text": "In the image, a person is in the process of cutting a large pizza with a spatula. The pizza is placed on a wooden table, and the person is using the spatula to slice it. The scene appears to be set in a kitchen or a similar environment.", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 20980.5, "ram_available_mb": 41860.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20953.8, "ram_available_mb": 41887.1, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.85}, "power_stats": {"power_gpu_soc_mean_watts": 19.898, "power_cpu_cv_mean_watts": 1.781, "power_sys_5v0_mean_watts": 8.689, "gpu_utilization_percent_mean": 70.85, "power_watts_avg": 19.898, "energy_joules_est": 187.26, "duration_seconds": 9.411, "sample_count": 80}, "timestamp": "2026-01-26T13:51:37.025648"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8605.917, "latencies_ms": [8605.917], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "pizza: 1\npiece: 1\ncheese: 1\ntomato sauce: 1\nbasil: 1\ngarlic: 1\nolive oil: 1\ndough: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20953.8, "ram_available_mb": 41887.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21017.7, "ram_available_mb": 41823.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.452}, "power_stats": {"power_gpu_soc_mean_watts": 20.809, "power_cpu_cv_mean_watts": 1.672, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 72.452, "power_watts_avg": 20.809, "energy_joules_est": 179.09, "duration_seconds": 8.607, "sample_count": 73}, "timestamp": "2026-01-26T13:51:47.673231"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11329.822, "latencies_ms": [11329.822], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The pizza is positioned in the foreground of the image, occupying the majority of the space. It is placed on a wooden surface, which appears to be a table. In the background, there is a person's arm visible, suggesting they are behind the pizza, and various kitchen utensils and containers are scattered around, indicating a kitchen setting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21017.7, "ram_available_mb": 41823.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20973.8, "ram_available_mb": 41867.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.74}, "power_stats": {"power_gpu_soc_mean_watts": 19.48, "power_cpu_cv_mean_watts": 1.863, "power_sys_5v0_mean_watts": 8.689, "gpu_utilization_percent_mean": 70.74, "power_watts_avg": 19.48, "energy_joules_est": 220.72, "duration_seconds": 11.33, "sample_count": 96}, "timestamp": "2026-01-26T13:52:01.044820"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9991.914, "latencies_ms": [9991.914], "images_per_second": 0.1, "prompt_tokens": 37, "response_tokens_est": 63, "n_tiles": 16, "output_text": "A large pizza with a golden-brown crust and melted cheese is being served. The pizza is placed on a wooden table, and a person's hand is visible, holding a spatula, suggesting that they are in the process of serving or cutting the pizza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.8, "ram_available_mb": 41867.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20979.4, "ram_available_mb": 41861.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.43}, "power_stats": {"power_gpu_soc_mean_watts": 20.069, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 71.43, "power_watts_avg": 20.069, "energy_joules_est": 200.54, "duration_seconds": 9.993, "sample_count": 86}, "timestamp": "2026-01-26T13:52:13.049355"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7969.286, "latencies_ms": [7969.286], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The pizza is topped with green herbs and melted cheese, with a golden-brown crust. The lighting in the image is dim, highlighting the pizza's texture and colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20979.4, "ram_available_mb": 41861.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20966.2, "ram_available_mb": 41874.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.478}, "power_stats": {"power_gpu_soc_mean_watts": 21.023, "power_cpu_cv_mean_watts": 1.636, "power_sys_5v0_mean_watts": 8.668, "gpu_utilization_percent_mean": 73.478, "power_watts_avg": 21.023, "energy_joules_est": 167.55, "duration_seconds": 7.97, "sample_count": 67}, "timestamp": "2026-01-26T13:52:23.039878"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11593.101, "latencies_ms": [11593.101], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a female tennis player is captured in the midst of a powerful serve on a grass court. She is dressed in a white tennis outfit, complete with a white visor and white shoes, which contrasts with the green of the grass court. The player's right arm is extended upwards, holding a blue and white tennis racket, poised to strike the ball", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20966.2, "ram_available_mb": 41874.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20967.5, "ram_available_mb": 41873.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.01}, "power_stats": {"power_gpu_soc_mean_watts": 19.157, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 70.01, "power_watts_avg": 19.157, "energy_joules_est": 222.1, "duration_seconds": 11.594, "sample_count": 99}, "timestamp": "2026-01-26T13:52:36.702839"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8628.507, "latencies_ms": [8628.507], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "tennis player: 1\ntennis racket: 1\nnet: 1\ngrass court: 1\nwhite dress: 1\nwhite shoes: 1\nwater bottle: 1\nshadow: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20967.5, "ram_available_mb": 41873.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21028.2, "ram_available_mb": 41812.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.959}, "power_stats": {"power_gpu_soc_mean_watts": 20.699, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.618, "gpu_utilization_percent_mean": 71.959, "power_watts_avg": 20.699, "energy_joules_est": 178.61, "duration_seconds": 8.629, "sample_count": 73}, "timestamp": "2026-01-26T13:52:47.390487"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11685.794, "latencies_ms": [11685.794], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a tennis player is positioned on the court, holding a tennis racket in her right hand and serving the ball with her left hand raised high above her head. The net is visible in the background, stretching across the width of the court. The player appears to be in the process of serving the ball, with her body angled towards the net and the ball in", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21028.2, "ram_available_mb": 41812.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20974.4, "ram_available_mb": 41866.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.02}, "power_stats": {"power_gpu_soc_mean_watts": 19.332, "power_cpu_cv_mean_watts": 1.877, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 70.02, "power_watts_avg": 19.332, "energy_joules_est": 225.92, "duration_seconds": 11.686, "sample_count": 100}, "timestamp": "2026-01-26T13:53:01.119851"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7248.228, "latencies_ms": [7248.228], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A tennis player in a white dress is captured in a dynamic pose on a grass court, holding a tennis racket and wearing a visor, likely during a match or practice session.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20974.4, "ram_available_mb": 41866.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21018.6, "ram_available_mb": 41822.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.033}, "power_stats": {"power_gpu_soc_mean_watts": 21.627, "power_cpu_cv_mean_watts": 1.502, "power_sys_5v0_mean_watts": 8.579, "gpu_utilization_percent_mean": 74.033, "power_watts_avg": 21.627, "energy_joules_est": 156.77, "duration_seconds": 7.249, "sample_count": 61}, "timestamp": "2026-01-26T13:53:10.386421"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7069.953, "latencies_ms": [7069.953], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The tennis player is wearing a white dress and holding a blue and white tennis racket. The court appears to be well-maintained with a green surface and white boundary lines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.6, "ram_available_mb": 41877.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20963.5, "ram_available_mb": 41877.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.633}, "power_stats": {"power_gpu_soc_mean_watts": 21.561, "power_cpu_cv_mean_watts": 1.527, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 73.633, "power_watts_avg": 21.561, "energy_joules_est": 152.45, "duration_seconds": 7.071, "sample_count": 60}, "timestamp": "2026-01-26T13:53:19.474276"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11610.882, "latencies_ms": [11610.882], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and well-organized bathroom. Dominating the left side of the frame is a pristine white toilet, its lid closed, standing against a wall painted in a soothing shade of yellow. Above it, a black towel hangs neatly, ready for use. \n\nOn the right, a white shower cur", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 20963.5, "ram_available_mb": 41877.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21024.6, "ram_available_mb": 41816.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.202}, "power_stats": {"power_gpu_soc_mean_watts": 19.294, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.68, "gpu_utilization_percent_mean": 70.202, "power_watts_avg": 19.294, "energy_joules_est": 224.03, "duration_seconds": 11.612, "sample_count": 99}, "timestamp": "2026-01-26T13:53:33.113471"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8915.499, "latencies_ms": [8915.499], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "toilet: 1, towel: 2, shower curtain: 1, toilet paper: 1, shelves: 4, towels: 2, bed: 1, closet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20962.9, "ram_available_mb": 41878.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21016.9, "ram_available_mb": 41824.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.5}, "power_stats": {"power_gpu_soc_mean_watts": 20.56, "power_cpu_cv_mean_watts": 1.69, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 72.5, "power_watts_avg": 20.56, "energy_joules_est": 183.32, "duration_seconds": 8.916, "sample_count": 76}, "timestamp": "2026-01-26T13:53:44.047424"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11496.168, "latencies_ms": [11496.168], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The toilet is located in the foreground on the left side of the image, while the shelves are in the background on the right side. The shower curtain is hanging near the toilet, and the towels are placed on the shelves, indicating that the toilet is closer to the viewer than the shelves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.4, "ram_available_mb": 41877.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.0, "ram_used_mb": 20960.7, "ram_available_mb": 41880.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.184}, "power_stats": {"power_gpu_soc_mean_watts": 19.2, "power_cpu_cv_mean_watts": 1.923, "power_sys_5v0_mean_watts": 8.686, "gpu_utilization_percent_mean": 70.184, "power_watts_avg": 19.2, "energy_joules_est": 220.74, "duration_seconds": 11.497, "sample_count": 98}, "timestamp": "2026-01-26T13:53:57.601366"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8596.104, "latencies_ms": [8596.104], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a small, compact bathroom with a white toilet on the left side and a white shelving unit on the right side. The shelves are empty, and there is a towel hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20960.7, "ram_available_mb": 41880.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21020.2, "ram_available_mb": 41820.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.479}, "power_stats": {"power_gpu_soc_mean_watts": 20.66, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 8.602, "gpu_utilization_percent_mean": 73.479, "power_watts_avg": 20.66, "energy_joules_est": 177.61, "duration_seconds": 8.597, "sample_count": 73}, "timestamp": "2026-01-26T13:54:08.231527"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7201.99, "latencies_ms": [7201.99], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image shows a bathroom with a white toilet and a white door. The walls are painted in a light yellow color, and there is a brown towel hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21020.2, "ram_available_mb": 41820.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20994.0, "ram_available_mb": 41846.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.607}, "power_stats": {"power_gpu_soc_mean_watts": 21.378, "power_cpu_cv_mean_watts": 1.574, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 72.607, "power_watts_avg": 21.378, "energy_joules_est": 153.98, "duration_seconds": 7.203, "sample_count": 61}, "timestamp": "2026-01-26T13:54:17.453999"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11583.211, "latencies_ms": [11583.211], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man and a woman are standing close to each other, holding wine glasses and smiling. They appear to be enjoying a glass of wine together. The woman is wearing glasses, and they both seem to be in a happy and relaxed mood.\n\nThe scene also includes a dining table with a few chairs around it. There are additional", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 20994.0, "ram_available_mb": 41846.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21057.3, "ram_available_mb": 41783.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.869}, "power_stats": {"power_gpu_soc_mean_watts": 19.38, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.679, "gpu_utilization_percent_mean": 70.869, "power_watts_avg": 19.38, "energy_joules_est": 224.49, "duration_seconds": 11.584, "sample_count": 99}, "timestamp": "2026-01-26T13:54:31.077304"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9045.669, "latencies_ms": [9045.669], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- People: 3\n- Wine glasses: 2\n- Table: 1\n- Papers: 2\n- Chair: 1\n- Window: 1\n- Outdoor view: 1\n- Grill: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21057.3, "ram_available_mb": 41783.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21057.6, "ram_available_mb": 41783.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.539}, "power_stats": {"power_gpu_soc_mean_watts": 20.566, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 8.621, "gpu_utilization_percent_mean": 72.539, "power_watts_avg": 20.566, "energy_joules_est": 186.05, "duration_seconds": 9.046, "sample_count": 76}, "timestamp": "2026-01-26T13:54:42.159192"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11605.068, "latencies_ms": [11605.068], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two individuals holding up wine glasses, positioned close to the camera, suggesting they are the main subjects of the image. In the background, there is another individual seated at a table, slightly farther away from the camera, indicating they are not the primary focus. The setting appears to be indoors, with a window providing natural light in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20952.4, "ram_available_mb": 41888.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21013.2, "ram_available_mb": 41827.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.867}, "power_stats": {"power_gpu_soc_mean_watts": 19.267, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.697, "gpu_utilization_percent_mean": 69.867, "power_watts_avg": 19.267, "energy_joules_est": 223.61, "duration_seconds": 11.606, "sample_count": 98}, "timestamp": "2026-01-26T13:54:55.778875"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8322.795, "latencies_ms": [8322.795], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "In the image, a man and a woman are standing in a room with a window, holding up wine glasses and smiling. There are other people in the background, and a dining table is visible with a pot on it.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21013.2, "ram_available_mb": 41827.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21032.9, "ram_available_mb": 41808.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.714}, "power_stats": {"power_gpu_soc_mean_watts": 20.986, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 73.714, "power_watts_avg": 20.986, "energy_joules_est": 174.67, "duration_seconds": 8.323, "sample_count": 70}, "timestamp": "2026-01-26T13:55:06.127491"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9510.782, "latencies_ms": [9510.782], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image shows two individuals holding up wine glasses, with a bright and natural light coming from the window in the background. The room has a casual and relaxed atmosphere, with a green t-shirt and a black t-shirt visible, and a metal container on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21032.9, "ram_available_mb": 41808.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20990.9, "ram_available_mb": 41850.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.136}, "power_stats": {"power_gpu_soc_mean_watts": 19.988, "power_cpu_cv_mean_watts": 1.783, "power_sys_5v0_mean_watts": 8.661, "gpu_utilization_percent_mean": 71.136, "power_watts_avg": 19.988, "energy_joules_est": 190.12, "duration_seconds": 9.512, "sample_count": 81}, "timestamp": "2026-01-26T13:55:17.681721"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11598.79, "latencies_ms": [11598.79], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a surfer is skillfully riding a wave on a sunny day. The surfer is wearing a black wetsuit and is positioned on a white surfboard. The wave, a beautiful shade of green, is curling over the surfer, creating a tunnel-like effect. The surfer is crouched down on the surfboard", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 20990.9, "ram_available_mb": 41850.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21020.6, "ram_available_mb": 41820.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.224}, "power_stats": {"power_gpu_soc_mean_watts": 19.38, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.684, "gpu_utilization_percent_mean": 70.224, "power_watts_avg": 19.38, "energy_joules_est": 224.8, "duration_seconds": 11.599, "sample_count": 98}, "timestamp": "2026-01-26T13:55:31.327724"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8364.13, "latencies_ms": [8364.13], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "wave: 1\nsurfboard: 1\nsurfer: 1\nwater: 1\nsand: 1\nsky: 1\nbathing suits: 1\nbeach: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20958.8, "ram_available_mb": 41882.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21018.7, "ram_available_mb": 41822.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.085}, "power_stats": {"power_gpu_soc_mean_watts": 20.892, "power_cpu_cv_mean_watts": 1.646, "power_sys_5v0_mean_watts": 8.596, "gpu_utilization_percent_mean": 73.085, "power_watts_avg": 20.892, "energy_joules_est": 174.76, "duration_seconds": 8.365, "sample_count": 71}, "timestamp": "2026-01-26T13:55:41.728877"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11550.01, "latencies_ms": [11550.01], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is curling over towards the right side of the image. The wave is the main object, occupying the central and left portion of the image, while the surfer is near the bottom of the wave. In the background, there is a sandy beach and a clear sky, which are far from the surfer", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21018.7, "ram_available_mb": 41822.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21036.1, "ram_available_mb": 41804.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.959}, "power_stats": {"power_gpu_soc_mean_watts": 19.385, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.711, "gpu_utilization_percent_mean": 69.959, "power_watts_avg": 19.385, "energy_joules_est": 223.91, "duration_seconds": 11.551, "sample_count": 98}, "timestamp": "2026-01-26T13:55:55.330485"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7527.316, "latencies_ms": [7527.316], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A surfer is skillfully riding a large wave in the ocean, showcasing their expertise and balance. The wave is curling over the surfer, creating a beautiful and dynamic scene.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21036.1, "ram_available_mb": 41804.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21037.3, "ram_available_mb": 41803.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.984}, "power_stats": {"power_gpu_soc_mean_watts": 21.512, "power_cpu_cv_mean_watts": 1.563, "power_sys_5v0_mean_watts": 8.616, "gpu_utilization_percent_mean": 73.984, "power_watts_avg": 21.512, "energy_joules_est": 161.94, "duration_seconds": 7.528, "sample_count": 64}, "timestamp": "2026-01-26T13:56:04.900040"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6858.111, "latencies_ms": [6858.111], "images_per_second": 0.146, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The surfer is riding a wave with a clear blue sky in the background. The water is a beautiful shade of green and the wave is creating a tunnel-like effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21037.3, "ram_available_mb": 41803.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21038.3, "ram_available_mb": 41802.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.569}, "power_stats": {"power_gpu_soc_mean_watts": 21.571, "power_cpu_cv_mean_watts": 1.538, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 73.569, "power_watts_avg": 21.571, "energy_joules_est": 147.95, "duration_seconds": 6.859, "sample_count": 58}, "timestamp": "2026-01-26T13:56:13.783457"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11603.115, "latencies_ms": [11603.115], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a wooden table with several laptops placed on it. There are five laptops in total, with one laptop positioned towards the left side of the table, another one in the middle, and three more on the right side. A backpack is placed on the table, occupying a significant portion of the space. \n\nIn addition to the laptops and", "error": null, "sys_before": {"cpu_percent": 11.5, "ram_used_mb": 20976.3, "ram_available_mb": 41864.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21028.7, "ram_available_mb": 41812.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.323}, "power_stats": {"power_gpu_soc_mean_watts": 19.302, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.688, "gpu_utilization_percent_mean": 70.323, "power_watts_avg": 19.302, "energy_joules_est": 223.98, "duration_seconds": 11.604, "sample_count": 99}, "timestamp": "2026-01-26T13:56:27.417024"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9825.16, "latencies_ms": [9825.16], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "- Laptop: 4\n- Backpack: 1\n- Wires: 1\n- Laptop charger: 1\n- Laptop mouse: 1\n- Laptop keyboard: 1\n- Laptop screen: 1\n- Laptop battery: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20975.2, "ram_available_mb": 41865.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21033.8, "ram_available_mb": 41807.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.94}, "power_stats": {"power_gpu_soc_mean_watts": 20.155, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 8.624, "gpu_utilization_percent_mean": 71.94, "power_watts_avg": 20.155, "energy_joules_est": 198.04, "duration_seconds": 9.826, "sample_count": 84}, "timestamp": "2026-01-26T13:56:39.274283"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10785.865, "latencies_ms": [10785.865], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground, there is a red and black backpack placed on a wooden table. Behind the backpack, there are several laptops of different sizes and colors, with one being the largest and located in the background. The laptops are arranged in a scattered manner, with some closer to the backpack and others further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21033.8, "ram_available_mb": 41807.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21073.2, "ram_available_mb": 41767.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.688}, "power_stats": {"power_gpu_soc_mean_watts": 19.544, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 8.673, "gpu_utilization_percent_mean": 70.688, "power_watts_avg": 19.544, "energy_joules_est": 210.81, "duration_seconds": 10.787, "sample_count": 93}, "timestamp": "2026-01-26T13:56:52.101760"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10787.199, "latencies_ms": [10787.199], "images_per_second": 0.093, "prompt_tokens": 37, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image shows a collection of laptops and a backpack placed on a wooden table. The laptops are of different sizes and brands, and they are arranged in a disorganized manner. The backpack is placed in front of the laptops, and there are multiple cords and cables scattered around the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21073.2, "ram_available_mb": 41767.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21017.8, "ram_available_mb": 41823.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.742}, "power_stats": {"power_gpu_soc_mean_watts": 19.63, "power_cpu_cv_mean_watts": 1.82, "power_sys_5v0_mean_watts": 8.621, "gpu_utilization_percent_mean": 71.742, "power_watts_avg": 19.63, "energy_joules_est": 211.76, "duration_seconds": 10.788, "sample_count": 93}, "timestamp": "2026-01-26T13:57:04.928615"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8043.241, "latencies_ms": [8043.241], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image shows a collection of laptops on a wooden table, with one laptop open and displaying a green screen. There is a backpack with a red and black color scheme placed on the table next to the laptops.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21017.8, "ram_available_mb": 41823.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21010.5, "ram_available_mb": 41830.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.797}, "power_stats": {"power_gpu_soc_mean_watts": 20.472, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.639, "gpu_utilization_percent_mean": 72.797, "power_watts_avg": 20.472, "energy_joules_est": 164.67, "duration_seconds": 8.044, "sample_count": 69}, "timestamp": "2026-01-26T13:57:15.016187"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11579.771, "latencies_ms": [11579.771], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a skier is captured in mid-air, performing a daring jump. The skier, clad in a vibrant red jacket and black pants, is holding ski poles and is in the process of executing a flip. The skis, adorned with yellow and black designs, are clearly visible beneath the skier's feet. The", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20948.8, "ram_available_mb": 41892.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21018.6, "ram_available_mb": 41822.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.374}, "power_stats": {"power_gpu_soc_mean_watts": 19.367, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.698, "gpu_utilization_percent_mean": 70.374, "power_watts_avg": 19.367, "energy_joules_est": 224.28, "duration_seconds": 11.58, "sample_count": 99}, "timestamp": "2026-01-26T13:57:28.666552"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7433.25, "latencies_ms": [7433.25], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "person: 1, ski: 2, snow: 1, mountain: 1, sky: 1, snowboard: 1, snow: 1, person: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20965.2, "ram_available_mb": 41875.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21014.1, "ram_available_mb": 41826.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.537, "power_cpu_cv_mean_watts": 1.543, "power_sys_5v0_mean_watts": 8.586, "gpu_utilization_percent_mean": 74.0, "power_watts_avg": 21.537, "energy_joules_est": 160.1, "duration_seconds": 7.434, "sample_count": 63}, "timestamp": "2026-01-26T13:57:38.114034"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11575.565, "latencies_ms": [11575.565], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a skier in mid-air, performing a jump with their skis crossed. They are positioned above the snowy slope, indicating they have launched off a jump. In the background, another skier is seen on the slope, closer to the bottom of the image, and appears to be skiing downhill. The skier in the foreground is", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21014.1, "ram_available_mb": 41826.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21028.8, "ram_available_mb": 41812.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.918}, "power_stats": {"power_gpu_soc_mean_watts": 19.355, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.692, "gpu_utilization_percent_mean": 69.918, "power_watts_avg": 19.355, "energy_joules_est": 224.06, "duration_seconds": 11.576, "sample_count": 98}, "timestamp": "2026-01-26T13:57:51.735116"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6293.618, "latencies_ms": [6293.618], "images_per_second": 0.159, "prompt_tokens": 37, "response_tokens_est": 31, "n_tiles": 16, "output_text": "A skier in a bright orange suit is performing a jump in the air above a snowy slope, with another person skiing in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21028.8, "ram_available_mb": 41812.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21031.1, "ram_available_mb": 41809.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.283}, "power_stats": {"power_gpu_soc_mean_watts": 22.541, "power_cpu_cv_mean_watts": 1.382, "power_sys_5v0_mean_watts": 8.556, "gpu_utilization_percent_mean": 75.283, "power_watts_avg": 22.541, "energy_joules_est": 141.88, "duration_seconds": 6.294, "sample_count": 53}, "timestamp": "2026-01-26T13:58:00.044422"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7339.06, "latencies_ms": [7339.06], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The skier is wearing a bright orange suit and a green helmet, contrasting against the clear blue sky. The snow appears pristine and untouched, indicating fresh powder conditions.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 20951.0, "ram_available_mb": 41889.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 20968.8, "ram_available_mb": 41872.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.873}, "power_stats": {"power_gpu_soc_mean_watts": 21.218, "power_cpu_cv_mean_watts": 2.026, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 73.873, "power_watts_avg": 21.218, "energy_joules_est": 155.73, "duration_seconds": 7.34, "sample_count": 63}, "timestamp": "2026-01-26T13:58:09.414209"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10524.783, "latencies_ms": [10524.783], "images_per_second": 0.095, "prompt_tokens": 24, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the image, a small bird is perched on the edge of a window sill, looking out at a body of water. The bird appears to be observing the water, possibly searching for food or simply enjoying the view. The window sill is located in a room, and the bird is the main focus of the scene.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 20968.8, "ram_available_mb": 41872.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 21023.5, "ram_available_mb": 41817.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.506}, "power_stats": {"power_gpu_soc_mean_watts": 19.604, "power_cpu_cv_mean_watts": 1.92, "power_sys_5v0_mean_watts": 8.695, "gpu_utilization_percent_mean": 70.506, "power_watts_avg": 19.604, "energy_joules_est": 206.34, "duration_seconds": 10.525, "sample_count": 89}, "timestamp": "2026-01-26T13:58:22.001330"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9657.141, "latencies_ms": [9657.141], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "1. Window: 1\n2. Seat: 1\n3. Bird: 1\n4. Water: 1\n5. Trees: 1\n6. Snow: 1\n7. Ground: 1\n8. Sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20970.2, "ram_available_mb": 41870.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21026.8, "ram_available_mb": 41814.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.89}, "power_stats": {"power_gpu_soc_mean_watts": 20.203, "power_cpu_cv_mean_watts": 1.737, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 71.89, "power_watts_avg": 20.203, "energy_joules_est": 195.12, "duration_seconds": 9.658, "sample_count": 82}, "timestamp": "2026-01-26T13:58:33.715629"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11308.671, "latencies_ms": [11308.671], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground, there is a small bird perched on the edge of a window frame, looking out at the view. The window is positioned in the lower right corner of the image, with the bird appearing closer to the viewer than the background. The background consists of a body of water and a distant shoreline, which is visible through the window.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21026.8, "ram_available_mb": 41814.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21018.1, "ram_available_mb": 41822.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.255}, "power_stats": {"power_gpu_soc_mean_watts": 19.435, "power_cpu_cv_mean_watts": 1.87, "power_sys_5v0_mean_watts": 8.701, "gpu_utilization_percent_mean": 71.255, "power_watts_avg": 19.435, "energy_joules_est": 219.8, "duration_seconds": 11.309, "sample_count": 98}, "timestamp": "2026-01-26T13:58:47.086239"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7235.614, "latencies_ms": [7235.614], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A bird is perched on the edge of a window frame, looking out at a body of water. The window appears to be old and weathered, with peeling paint and rust.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20957.2, "ram_available_mb": 41883.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20970.4, "ram_available_mb": 41870.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.475}, "power_stats": {"power_gpu_soc_mean_watts": 21.834, "power_cpu_cv_mean_watts": 1.522, "power_sys_5v0_mean_watts": 8.602, "gpu_utilization_percent_mean": 74.475, "power_watts_avg": 21.834, "energy_joules_est": 158.0, "duration_seconds": 7.236, "sample_count": 61}, "timestamp": "2026-01-26T13:58:56.365430"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10024.826, "latencies_ms": [10024.826], "images_per_second": 0.1, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows a small, dark-colored bird perched on a window sill, looking out at a body of water that reflects a grey, overcast sky. The window frame is made of metal and shows signs of rust and age, suggesting it has been exposed to the elements for a considerable time.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20970.4, "ram_available_mb": 41870.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21015.1, "ram_available_mb": 41825.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.259}, "power_stats": {"power_gpu_soc_mean_watts": 20.092, "power_cpu_cv_mean_watts": 1.817, "power_sys_5v0_mean_watts": 8.69, "gpu_utilization_percent_mean": 71.259, "power_watts_avg": 20.092, "energy_joules_est": 201.43, "duration_seconds": 10.025, "sample_count": 85}, "timestamp": "2026-01-26T13:59:08.446967"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11572.061, "latencies_ms": [11572.061], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing in a bathroom, attempting to fix a toilet. He is wearing a red hat and is focused on the task at hand. The bathroom is equipped with a toilet, a trash can, and a sink. There are multiple bottles scattered around the room, possibly containing cleaning supplies or other household items. The man", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20955.2, "ram_available_mb": 41885.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21020.5, "ram_available_mb": 41820.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.141}, "power_stats": {"power_gpu_soc_mean_watts": 19.305, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 70.141, "power_watts_avg": 19.305, "energy_joules_est": 223.41, "duration_seconds": 11.573, "sample_count": 99}, "timestamp": "2026-01-26T13:59:22.048234"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8966.969, "latencies_ms": [8966.969], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "toilet: 1, trash bag: 1, paint can: 1, paintbrush: 1, paint roller: 1, paint tray: 1, paint bottle: 1, paint container: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20967.0, "ram_available_mb": 41873.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21022.4, "ram_available_mb": 41818.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.421}, "power_stats": {"power_gpu_soc_mean_watts": 20.716, "power_cpu_cv_mean_watts": 1.674, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 72.421, "power_watts_avg": 20.716, "energy_joules_est": 185.77, "duration_seconds": 8.968, "sample_count": 76}, "timestamp": "2026-01-26T13:59:33.031916"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10619.112, "latencies_ms": [10619.112], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there is a white toilet with a black trash bag next to it. In the background, there is a person wearing an orange hat and a white shirt, who appears to be in the process of cleaning or fixing something. The person is standing near a shelf with various cleaning supplies.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21022.4, "ram_available_mb": 41818.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21039.9, "ram_available_mb": 41801.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.689}, "power_stats": {"power_gpu_soc_mean_watts": 19.773, "power_cpu_cv_mean_watts": 1.818, "power_sys_5v0_mean_watts": 8.685, "gpu_utilization_percent_mean": 71.689, "power_watts_avg": 19.773, "energy_joules_est": 209.98, "duration_seconds": 10.62, "sample_count": 90}, "timestamp": "2026-01-26T13:59:45.679499"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7470.437, "latencies_ms": [7470.437], "images_per_second": 0.134, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "In a small, cluttered bathroom, a man is seen bending over a toilet, possibly fixing or cleaning it. Another man is standing nearby, observing the situation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21039.9, "ram_available_mb": 41801.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20961.7, "ram_available_mb": 41879.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.143}, "power_stats": {"power_gpu_soc_mean_watts": 21.48, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 8.603, "gpu_utilization_percent_mean": 74.143, "power_watts_avg": 21.48, "energy_joules_est": 160.48, "duration_seconds": 7.471, "sample_count": 63}, "timestamp": "2026-01-26T13:59:55.167038"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8466.274, "latencies_ms": [8466.274], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image is taken indoors with artificial lighting, and the colors are muted with a predominance of browns and whites. There is a black trash bag on the floor, and the walls are a light beige color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.7, "ram_available_mb": 41879.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21026.2, "ram_available_mb": 41814.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.431}, "power_stats": {"power_gpu_soc_mean_watts": 20.647, "power_cpu_cv_mean_watts": 1.673, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 72.431, "power_watts_avg": 20.647, "energy_joules_est": 174.82, "duration_seconds": 8.467, "sample_count": 72}, "timestamp": "2026-01-26T14:00:05.668858"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11582.12, "latencies_ms": [11582.12], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is standing in a hallway, holding an umbrella to protect themselves from the rain. The person is wearing a blue jacket and is positioned in the center of the scene. The hallway is narrow, with red walls on both sides. There are two doors visible in the background, one on the left and one on the right. The person appears", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 21026.2, "ram_available_mb": 41814.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21039.6, "ram_available_mb": 41801.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.031}, "power_stats": {"power_gpu_soc_mean_watts": 19.156, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.676, "gpu_utilization_percent_mean": 70.031, "power_watts_avg": 19.156, "energy_joules_est": 221.88, "duration_seconds": 11.583, "sample_count": 98}, "timestamp": "2026-01-26T14:00:19.302607"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10203.766, "latencies_ms": [10203.766], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. Umbrella: 1\n2. Person: 1\n3. Door: 2\n4. Wall: 2\n5. Rain: 1\n6. Light fixture: 1\n7. Picture frame: 1\n8. Radiator: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20961.1, "ram_available_mb": 41879.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21019.7, "ram_available_mb": 41821.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.701}, "power_stats": {"power_gpu_soc_mean_watts": 20.081, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 8.666, "gpu_utilization_percent_mean": 71.701, "power_watts_avg": 20.081, "energy_joules_est": 204.92, "duration_seconds": 10.204, "sample_count": 87}, "timestamp": "2026-01-26T14:00:31.536549"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11671.443, "latencies_ms": [11671.443], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The person is standing in the foreground, holding an umbrella above their head. The umbrella is positioned in the upper left corner of the image, providing cover from the rain. The background consists of a red wall with a framed picture hanging on it, and a door to the left of the person. The person appears to be standing in a doorway or hallway", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20957.9, "ram_available_mb": 41883.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21019.5, "ram_available_mb": 41821.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.616}, "power_stats": {"power_gpu_soc_mean_watts": 19.371, "power_cpu_cv_mean_watts": 1.875, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 70.616, "power_watts_avg": 19.371, "energy_joules_est": 226.1, "duration_seconds": 11.672, "sample_count": 99}, "timestamp": "2026-01-26T14:00:45.254405"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8939.915, "latencies_ms": [8939.915], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "A person is standing in a hallway, holding an umbrella with the words \"Capsa Mo\" written on it, suggesting that it is raining. The person is wearing a blue jacket and appears to be walking through the hallway.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20957.1, "ram_available_mb": 41883.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21017.4, "ram_available_mb": 41823.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.539}, "power_stats": {"power_gpu_soc_mean_watts": 20.695, "power_cpu_cv_mean_watts": 1.69, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 72.539, "power_watts_avg": 20.695, "energy_joules_est": 185.02, "duration_seconds": 8.941, "sample_count": 76}, "timestamp": "2026-01-26T14:00:56.229699"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11080.915, "latencies_ms": [11080.915], "images_per_second": 0.09, "prompt_tokens": 36, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image depicts a person standing under a black umbrella with the words \"Capsas Mod\" written on it, in a corridor with red walls and a reflective floor. The lighting is dim, with a mix of natural light coming from the top left corner, creating a contrast between the illuminated and shadowed areas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21017.4, "ram_available_mb": 41823.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21036.3, "ram_available_mb": 41804.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.326}, "power_stats": {"power_gpu_soc_mean_watts": 19.545, "power_cpu_cv_mean_watts": 1.853, "power_sys_5v0_mean_watts": 8.687, "gpu_utilization_percent_mean": 70.326, "power_watts_avg": 19.545, "energy_joules_est": 216.59, "duration_seconds": 11.082, "sample_count": 95}, "timestamp": "2026-01-26T14:01:09.364718"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11582.924, "latencies_ms": [11582.924], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two individuals in a lush, green forest setting. The person on the left, clad in a vibrant orange robe, is standing on a wooden bridge that arches over a small stream. The person on the right, dressed in a red shirt and blue jeans, is holding a walking stick and appears to be in a contemplative pose", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20965.4, "ram_available_mb": 41875.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21021.6, "ram_available_mb": 41819.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.76}, "power_stats": {"power_gpu_soc_mean_watts": 19.345, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.68, "gpu_utilization_percent_mean": 69.76, "power_watts_avg": 19.345, "energy_joules_est": 224.08, "duration_seconds": 11.584, "sample_count": 100}, "timestamp": "2026-01-26T14:01:22.999725"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7769.218, "latencies_ms": [7769.218], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "person: 2, rock: 10, sign: 1, staircase: 1, railing: 1, plant: 5, path: 1, step: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.3, "ram_available_mb": 41879.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21019.4, "ram_available_mb": 41821.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.284}, "power_stats": {"power_gpu_soc_mean_watts": 21.237, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 74.284, "power_watts_avg": 21.237, "energy_joules_est": 165.01, "duration_seconds": 7.77, "sample_count": 67}, "timestamp": "2026-01-26T14:01:32.803294"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10790.482, "latencies_ms": [10790.482], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground, there is a person with a backpack standing on a gravel path, looking upwards. Behind this individual, there is a wooden railing with a person in a yellow robe standing on a higher platform to the left. The background is filled with lush greenery and a signpost with multiple directional signs.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21019.4, "ram_available_mb": 41821.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20976.6, "ram_available_mb": 41864.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.674}, "power_stats": {"power_gpu_soc_mean_watts": 19.403, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.681, "gpu_utilization_percent_mean": 70.674, "power_watts_avg": 19.403, "energy_joules_est": 209.38, "duration_seconds": 10.791, "sample_count": 92}, "timestamp": "2026-01-26T14:01:45.644220"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10230.235, "latencies_ms": [10230.235], "images_per_second": 0.098, "prompt_tokens": 37, "response_tokens_est": 65, "n_tiles": 16, "output_text": "A man in a red shirt and blue jeans is standing on a gravel path with a walking stick, looking up at a signpost with multiple arrows pointing in different directions. In the background, there is a person dressed in a yellow robe, possibly a monk, standing on a wooden bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20976.6, "ram_available_mb": 41864.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20972.3, "ram_available_mb": 41868.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.102}, "power_stats": {"power_gpu_soc_mean_watts": 19.763, "power_cpu_cv_mean_watts": 1.778, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 72.102, "power_watts_avg": 19.763, "energy_joules_est": 202.19, "duration_seconds": 10.231, "sample_count": 88}, "timestamp": "2026-01-26T14:01:57.903021"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10530.396, "latencies_ms": [10530.396], "images_per_second": 0.095, "prompt_tokens": 36, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image depicts a scene with a person in a yellow robe standing on a wooden bridge with a railing, and another person in a red shirt with a backpack and trekking pole standing on the ground below. The environment is lush with greenery, and the lighting suggests it is taken during the day.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20972.3, "ram_available_mb": 41868.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20999.0, "ram_available_mb": 41841.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.685}, "power_stats": {"power_gpu_soc_mean_watts": 19.614, "power_cpu_cv_mean_watts": 1.848, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 70.685, "power_watts_avg": 19.614, "energy_joules_est": 206.55, "duration_seconds": 10.531, "sample_count": 89}, "timestamp": "2026-01-26T14:02:10.449902"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12353.097, "latencies_ms": [12353.097], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of camaraderie among four men in a room that seems to be a bar or a similar social setting. The man on the far left, clad in a blue and white checkered shirt, stands with a smile, his arm casually draped over the shoulder of the man next to him. This man, dressed in a white shirt and", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 20999.0, "ram_available_mb": 41841.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 20990.7, "ram_available_mb": 41850.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.292}, "power_stats": {"power_gpu_soc_mean_watts": 21.649, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.961, "gpu_utilization_percent_mean": 73.292, "power_watts_avg": 21.649, "energy_joules_est": 267.45, "duration_seconds": 12.354, "sample_count": 106}, "timestamp": "2026-01-26T14:02:24.847870"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11205.752, "latencies_ms": [11205.752], "images_per_second": 0.089, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "- Chair: 2\n\n- Table: 1\n\n- Bottle: 10\n\n- Glass: 5\n\n- Armchair: 1\n\n- Jacket: 1\n\n- Shirt: 4\n\n- Suit: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20990.7, "ram_available_mb": 41850.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21024.7, "ram_available_mb": 41816.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.76}, "power_stats": {"power_gpu_soc_mean_watts": 22.231, "power_cpu_cv_mean_watts": 1.659, "power_sys_5v0_mean_watts": 8.822, "gpu_utilization_percent_mean": 75.76, "power_watts_avg": 22.231, "energy_joules_est": 249.13, "duration_seconds": 11.206, "sample_count": 96}, "timestamp": "2026-01-26T14:02:38.089520"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10258.614, "latencies_ms": [10258.614], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 58, "n_tiles": 16, "output_text": "In the foreground, there are four individuals standing close together, with one person slightly ahead of the others, creating a sense of depth. The background features a bar setting with a counter and shelves stocked with bottles, providing context to the location of the gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.2, "ram_available_mb": 41879.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20975.4, "ram_available_mb": 41865.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.402}, "power_stats": {"power_gpu_soc_mean_watts": 22.461, "power_cpu_cv_mean_watts": 1.633, "power_sys_5v0_mean_watts": 8.871, "gpu_utilization_percent_mean": 74.402, "power_watts_avg": 22.461, "energy_joules_est": 230.44, "duration_seconds": 10.26, "sample_count": 87}, "timestamp": "2026-01-26T14:02:50.365207"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6692.395, "latencies_ms": [6692.395], "images_per_second": 0.149, "prompt_tokens": 37, "response_tokens_est": 25, "n_tiles": 16, "output_text": "Four men are standing together in a room with a bar in the background, possibly at a social event or gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20975.4, "ram_available_mb": 41865.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 20949.9, "ram_available_mb": 41891.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 81.482}, "power_stats": {"power_gpu_soc_mean_watts": 24.888, "power_cpu_cv_mean_watts": 1.115, "power_sys_5v0_mean_watts": 8.712, "gpu_utilization_percent_mean": 81.482, "power_watts_avg": 24.888, "energy_joules_est": 166.58, "duration_seconds": 6.693, "sample_count": 56}, "timestamp": "2026-01-26T14:02:59.094160"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8728.424, "latencies_ms": [8728.424], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows four individuals standing indoors with a warm and soft lighting that suggests an indoor setting. The walls are painted in a light color, and there is a red accent wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20949.9, "ram_available_mb": 41891.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20962.7, "ram_available_mb": 41878.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_gpu_soc_mean_watts": 23.182, "power_cpu_cv_mean_watts": 1.509, "power_sys_5v0_mean_watts": 8.86, "gpu_utilization_percent_mean": 77.0, "power_watts_avg": 23.182, "energy_joules_est": 202.36, "duration_seconds": 8.729, "sample_count": 74}, "timestamp": "2026-01-26T14:03:09.850707"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11578.984, "latencies_ms": [11578.984], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant street scene. Dominating the foreground is a **yellow traffic sign** with a **black border** and a **black arrow** pointing to the right. The sign is affixed to a **black pole** that stands on a **concrete base**. The pole is situated on the left side of the image, while the sign is positioned on", "error": null, "sys_before": {"cpu_percent": 11.5, "ram_used_mb": 20962.7, "ram_available_mb": 41878.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21009.1, "ram_available_mb": 41831.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.798}, "power_stats": {"power_gpu_soc_mean_watts": 19.144, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.691, "gpu_utilization_percent_mean": 69.798, "power_watts_avg": 19.144, "energy_joules_est": 221.68, "duration_seconds": 11.58, "sample_count": 99}, "timestamp": "2026-01-26T14:03:23.482708"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10216.718, "latencies_ms": [10216.718], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "- Buildings: 10\n\n- Cars: 4\n\n- Traffic lights: 1\n\n- Signs: 1\n\n- Pole: 1\n\n- Tree: 0\n\n- Pedestrians: 2\n\n- Bus: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20956.8, "ram_available_mb": 41884.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21026.4, "ram_available_mb": 41814.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.678}, "power_stats": {"power_gpu_soc_mean_watts": 19.94, "power_cpu_cv_mean_watts": 1.785, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 71.678, "power_watts_avg": 19.94, "energy_joules_est": 203.73, "duration_seconds": 10.217, "sample_count": 87}, "timestamp": "2026-01-26T14:03:35.743246"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11667.159, "latencies_ms": [11667.159], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a yellow sign mounted on a black pole, positioned on the right side of the image. The background features a street scene with buildings on the left, a red traffic light in the middle distance, and vehicles on the road. The sign is near the curb, and the pole appears to be damaged at the base, with a large crack running through it", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21026.4, "ram_available_mb": 41814.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21028.3, "ram_available_mb": 41812.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.465}, "power_stats": {"power_gpu_soc_mean_watts": 19.387, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.701, "gpu_utilization_percent_mean": 70.465, "power_watts_avg": 19.387, "energy_joules_est": 226.2, "duration_seconds": 11.668, "sample_count": 99}, "timestamp": "2026-01-26T14:03:49.466126"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9293.574, "latencies_ms": [9293.574], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image depicts a street scene with a yellow sign mounted on a pole, which appears to be damaged and leaning at an angle. The sign is located on the side of a road with multiple lanes, and there are various vehicles and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20956.4, "ram_available_mb": 41884.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21011.6, "ram_available_mb": 41829.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.321}, "power_stats": {"power_gpu_soc_mean_watts": 20.384, "power_cpu_cv_mean_watts": 1.708, "power_sys_5v0_mean_watts": 8.639, "gpu_utilization_percent_mean": 72.321, "power_watts_avg": 20.384, "energy_joules_est": 189.45, "duration_seconds": 9.294, "sample_count": 78}, "timestamp": "2026-01-26T14:04:00.777737"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7199.128, "latencies_ms": [7199.128], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image shows a street scene with a yellow sign mounted on a black pole. The weather appears to be overcast with a cloudy sky, and the street is wet, suggesting recent rain.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21011.6, "ram_available_mb": 41829.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21010.1, "ram_available_mb": 41830.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.066}, "power_stats": {"power_gpu_soc_mean_watts": 21.456, "power_cpu_cv_mean_watts": 1.548, "power_sys_5v0_mean_watts": 8.621, "gpu_utilization_percent_mean": 74.066, "power_watts_avg": 21.456, "energy_joules_est": 154.48, "duration_seconds": 7.2, "sample_count": 61}, "timestamp": "2026-01-26T14:04:10.039694"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11584.729, "latencies_ms": [11584.729], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is standing on a tennis court, holding a tennis racket and preparing to hit a tennis ball. He is wearing a white shirt and black shorts. The court is surrounded by a fence, and there are two red signs on the fence. One sign reads \"VOX SPORTS\" and the other reads \"Believe it,", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 21010.1, "ram_available_mb": 41830.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21028.8, "ram_available_mb": 41812.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.2}, "power_stats": {"power_gpu_soc_mean_watts": 19.385, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.702, "gpu_utilization_percent_mean": 69.2, "power_watts_avg": 19.385, "energy_joules_est": 224.59, "duration_seconds": 11.586, "sample_count": 100}, "timestamp": "2026-01-26T14:04:23.667012"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8138.869, "latencies_ms": [8138.869], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "fence: 4\nsign: 2\nbench: 1\ntennis court: 1\nplayer: 1\nracket: 1\nball: 0\nbottle: 0", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20950.4, "ram_available_mb": 41890.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21015.9, "ram_available_mb": 41825.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.271}, "power_stats": {"power_gpu_soc_mean_watts": 20.997, "power_cpu_cv_mean_watts": 1.623, "power_sys_5v0_mean_watts": 8.618, "gpu_utilization_percent_mean": 73.271, "power_watts_avg": 20.997, "energy_joules_est": 170.9, "duration_seconds": 8.139, "sample_count": 70}, "timestamp": "2026-01-26T14:04:33.849901"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11526.016, "latencies_ms": [11526.016], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is standing on a tennis court holding a racket, positioned near the center of the image. The court is surrounded by a fence in the background, and there are signs attached to the fence, one of which is clearly visible with the text 'VOX SPORTS prince rule the court.' The signs are located at a distance from the person,", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21015.9, "ram_available_mb": 41825.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21030.4, "ram_available_mb": 41810.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.143}, "power_stats": {"power_gpu_soc_mean_watts": 19.442, "power_cpu_cv_mean_watts": 1.907, "power_sys_5v0_mean_watts": 8.742, "gpu_utilization_percent_mean": 70.143, "power_watts_avg": 19.442, "energy_joules_est": 224.1, "duration_seconds": 11.527, "sample_count": 98}, "timestamp": "2026-01-26T14:04:47.401291"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5504.875, "latencies_ms": [5504.875], "images_per_second": 0.182, "prompt_tokens": 37, "response_tokens_est": 24, "n_tiles": 16, "output_text": "A person is playing tennis on a court at night, with a fence and advertisements in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20968.5, "ram_available_mb": 41872.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 20969.0, "ram_available_mb": 41871.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.957}, "power_stats": {"power_gpu_soc_mean_watts": 23.362, "power_cpu_cv_mean_watts": 1.244, "power_sys_5v0_mean_watts": 8.532, "gpu_utilization_percent_mean": 76.957, "power_watts_avg": 23.362, "energy_joules_est": 128.62, "duration_seconds": 5.506, "sample_count": 46}, "timestamp": "2026-01-26T14:04:54.934191"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6651.212, "latencies_ms": [6651.212], "images_per_second": 0.15, "prompt_tokens": 36, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The tennis court is surrounded by a green fence and there is a red banner with white text on it. The sky is dark, indicating that it is nighttime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20969.0, "ram_available_mb": 41871.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20956.6, "ram_available_mb": 41884.3, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.357}, "power_stats": {"power_gpu_soc_mean_watts": 21.573, "power_cpu_cv_mean_watts": 1.508, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 74.357, "power_watts_avg": 21.573, "energy_joules_est": 143.5, "duration_seconds": 6.652, "sample_count": 56}, "timestamp": "2026-01-26T14:05:03.603743"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12403.097, "latencies_ms": [12403.097], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are three people standing in the snow, all wearing ski gear and holding ski poles. They are positioned behind a blue fence, which is likely a safety barrier. The person on the left is wearing a black jacket and red gloves, while the person in the middle is wearing a white jacket and red gloves. The person", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 20956.6, "ram_available_mb": 41884.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21045.7, "ram_available_mb": 41795.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.61}, "power_stats": {"power_gpu_soc_mean_watts": 21.615, "power_cpu_cv_mean_watts": 1.81, "power_sys_5v0_mean_watts": 8.949, "gpu_utilization_percent_mean": 71.61, "power_watts_avg": 21.615, "energy_joules_est": 268.11, "duration_seconds": 12.404, "sample_count": 105}, "timestamp": "2026-01-26T14:05:18.049728"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10058.974, "latencies_ms": [10058.974], "images_per_second": 0.099, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Helmets: 3\n\n- Gloves: 3\n\n- Skis: 3\n\n- Fence: 1\n\n- Snow: 1\n\n- Ground: 1\n\n- People: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.6, "ram_available_mb": 41875.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20953.9, "ram_available_mb": 41887.0, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.976}, "power_stats": {"power_gpu_soc_mean_watts": 22.718, "power_cpu_cv_mean_watts": 1.558, "power_sys_5v0_mean_watts": 8.793, "gpu_utilization_percent_mean": 74.976, "power_watts_avg": 22.718, "energy_joules_est": 228.53, "duration_seconds": 10.06, "sample_count": 85}, "timestamp": "2026-01-26T14:05:30.145508"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12510.893, "latencies_ms": [12510.893], "images_per_second": 0.08, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, a person wearing a black jacket and red gloves is standing and gesturing towards another individual who is partially obscured by a blue net. The person behind the net appears to be wearing a white helmet and is facing the first person. In the background, there are other individuals and ski equipment, suggesting this is a skiing event.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20953.9, "ram_available_mb": 41887.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21040.1, "ram_available_mb": 41800.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.224}, "power_stats": {"power_gpu_soc_mean_watts": 21.641, "power_cpu_cv_mean_watts": 1.776, "power_sys_5v0_mean_watts": 8.879, "gpu_utilization_percent_mean": 73.224, "power_watts_avg": 21.641, "energy_joules_est": 270.76, "duration_seconds": 12.512, "sample_count": 107}, "timestamp": "2026-01-26T14:05:44.693242"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10028.591, "latencies_ms": [10028.591], "images_per_second": 0.1, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "In the image, a group of skiers is gathered around a blue net, possibly discussing something or taking a break from skiing. The setting appears to be a ski resort, with snow-covered slopes and other skiers in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21040.1, "ram_available_mb": 41800.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20959.4, "ram_available_mb": 41881.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.176}, "power_stats": {"power_gpu_soc_mean_watts": 22.749, "power_cpu_cv_mean_watts": 1.563, "power_sys_5v0_mean_watts": 8.789, "gpu_utilization_percent_mean": 75.176, "power_watts_avg": 22.749, "energy_joules_est": 228.16, "duration_seconds": 10.029, "sample_count": 85}, "timestamp": "2026-01-26T14:05:56.754012"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10464.453, "latencies_ms": [10464.453], "images_per_second": 0.096, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image shows a group of people on a snowy surface, likely engaged in a winter sport, with bright sunlight casting shadows on the ground. The weather appears to be clear and sunny, as indicated by the bright lighting and the absence of any snowfall or wet conditions.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20959.4, "ram_available_mb": 41881.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21026.5, "ram_available_mb": 41814.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.753}, "power_stats": {"power_gpu_soc_mean_watts": 22.376, "power_cpu_cv_mean_watts": 1.641, "power_sys_5v0_mean_watts": 8.886, "gpu_utilization_percent_mean": 75.753, "power_watts_avg": 22.376, "energy_joules_est": 234.17, "duration_seconds": 10.465, "sample_count": 89}, "timestamp": "2026-01-26T14:06:09.271793"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11613.221, "latencies_ms": [11613.221], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment on a wet, cloudy day in a residential neighborhood. A blue truck, adorned with the words \"HUISMANGROUP.COM\" on its side, is the main subject of the image. The truck is driving on the right side of the road, following the rules of the road in countries where driving on the right is the", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 20961.5, "ram_available_mb": 41879.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.8, "ram_used_mb": 21022.0, "ram_available_mb": 41818.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.889}, "power_stats": {"power_gpu_soc_mean_watts": 19.319, "power_cpu_cv_mean_watts": 2.074, "power_sys_5v0_mean_watts": 8.684, "gpu_utilization_percent_mean": 69.889, "power_watts_avg": 19.319, "energy_joules_est": 224.37, "duration_seconds": 11.614, "sample_count": 99}, "timestamp": "2026-01-26T14:06:22.926550"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7683.737, "latencies_ms": [7683.737], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "truck: 1, car: 3, house: 3, tree: 2, street light: 1, sidewalk: 1, grass: 1, cloud: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20960.3, "ram_available_mb": 41880.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21021.6, "ram_available_mb": 41819.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.846}, "power_stats": {"power_gpu_soc_mean_watts": 21.283, "power_cpu_cv_mean_watts": 1.576, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 73.846, "power_watts_avg": 21.283, "energy_joules_est": 163.55, "duration_seconds": 7.684, "sample_count": 65}, "timestamp": "2026-01-26T14:06:32.647971"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9391.653, "latencies_ms": [9391.653], "images_per_second": 0.106, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "A blue truck is in the foreground on the left side of the image, driving on the road. In the background, there are houses on the left and a row of parked cars on the right side of the road. The sky is visible above the houses and the truck.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21021.6, "ram_available_mb": 41819.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21034.1, "ram_available_mb": 41806.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.291}, "power_stats": {"power_gpu_soc_mean_watts": 20.171, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.679, "gpu_utilization_percent_mean": 71.291, "power_watts_avg": 20.171, "energy_joules_est": 189.45, "duration_seconds": 9.392, "sample_count": 79}, "timestamp": "2026-01-26T14:06:44.060268"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7307.812, "latencies_ms": [7307.812], "images_per_second": 0.137, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A blue Huisman Group truck is driving down a wet street in a residential area. The street is lined with houses and trees, and there are other vehicles on the road.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21034.1, "ram_available_mb": 41806.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21034.1, "ram_available_mb": 41806.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.984}, "power_stats": {"power_gpu_soc_mean_watts": 21.693, "power_cpu_cv_mean_watts": 1.517, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 73.984, "power_watts_avg": 21.693, "energy_joules_est": 158.54, "duration_seconds": 7.308, "sample_count": 62}, "timestamp": "2026-01-26T14:06:53.379631"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7450.434, "latencies_ms": [7450.434], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A blue Huisman Group truck is driving on a wet road, indicating recent rain. The sky is overcast, and the street is lined with houses and trees, creating a suburban atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21034.1, "ram_available_mb": 41806.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20957.0, "ram_available_mb": 41883.9, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.952}, "power_stats": {"power_gpu_soc_mean_watts": 20.967, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 73.952, "power_watts_avg": 20.967, "energy_joules_est": 156.23, "duration_seconds": 7.451, "sample_count": 63}, "timestamp": "2026-01-26T14:07:02.854156"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11576.588, "latencies_ms": [11576.588], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment at an airport, where a large commercial airplane is in the process of landing. The airplane, painted in a sleek gray color, is positioned in the center of the frame, its nose pointed downwards as it descends towards the runway. The runway itself is a solid gray, marked with white lines that guide the airplane's", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20957.0, "ram_available_mb": 41883.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21018.2, "ram_available_mb": 41822.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.75}, "power_stats": {"power_gpu_soc_mean_watts": 19.321, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.683, "gpu_utilization_percent_mean": 69.75, "power_watts_avg": 19.321, "energy_joules_est": 223.68, "duration_seconds": 11.577, "sample_count": 100}, "timestamp": "2026-01-26T14:07:16.481830"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8244.724, "latencies_ms": [8244.724], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "runway: 1\nairplane: 1\ntaxiway: 1\nfence: 1\ngrass: 1\nmountains: 1\ntrees: 1\nbuildings: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20955.6, "ram_available_mb": 41885.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21003.5, "ram_available_mb": 41837.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.9}, "power_stats": {"power_gpu_soc_mean_watts": 20.991, "power_cpu_cv_mean_watts": 1.623, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 72.9, "power_watts_avg": 20.991, "energy_joules_est": 173.08, "duration_seconds": 8.246, "sample_count": 70}, "timestamp": "2026-01-26T14:07:26.754535"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11588.237, "latencies_ms": [11588.237], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are several red circular objects mounted on poles, which appear to be part of an airport's ground equipment, possibly related to aircraft navigation or positioning. In the background, there is an airplane on the runway, positioned centrally and slightly to the right of the frame, indicating it is either preparing for takeoff or has just land", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21003.5, "ram_available_mb": 41837.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21026.2, "ram_available_mb": 41814.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.455}, "power_stats": {"power_gpu_soc_mean_watts": 19.265, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 70.455, "power_watts_avg": 19.265, "energy_joules_est": 223.26, "duration_seconds": 11.589, "sample_count": 99}, "timestamp": "2026-01-26T14:07:40.379382"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7427.7, "latencies_ms": [7427.7], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A large commercial airplane is taxiing on the runway, preparing for takeoff. The airport is surrounded by mountains and the sky is hazy, indicating possible poor air quality.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21026.2, "ram_available_mb": 41814.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21009.9, "ram_available_mb": 41831.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.297}, "power_stats": {"power_gpu_soc_mean_watts": 21.528, "power_cpu_cv_mean_watts": 1.563, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 74.297, "power_watts_avg": 21.528, "energy_joules_est": 159.92, "duration_seconds": 7.428, "sample_count": 64}, "timestamp": "2026-01-26T14:07:49.859391"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10758.854, "latencies_ms": [10758.854], "images_per_second": 0.093, "prompt_tokens": 36, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image features a foggy or hazy day with low visibility, which is evident from the obscured background and the overall grayish tone of the sky. The airplane is a large, commercial jet with a dark body and a white wing, and it is taxiing on the wet runway, which reflects the plane's lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21009.9, "ram_available_mb": 41831.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21026.8, "ram_available_mb": 41814.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.022}, "power_stats": {"power_gpu_soc_mean_watts": 19.523, "power_cpu_cv_mean_watts": 1.872, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 71.022, "power_watts_avg": 19.523, "energy_joules_est": 210.06, "duration_seconds": 10.759, "sample_count": 93}, "timestamp": "2026-01-26T14:08:02.652850"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11583.669, "latencies_ms": [11583.669], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people on a beach, with a woman holding a baseball bat in the foreground. There are several other people scattered across the scene, some closer to the water and others further back. A few cars are parked nearby, and a lifeguard tower can be seen in the background. The beach appears to be a popular spot for both relaxation and recreational activities", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20954.9, "ram_available_mb": 41885.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21013.4, "ram_available_mb": 41827.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.33}, "power_stats": {"power_gpu_soc_mean_watts": 19.361, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 70.33, "power_watts_avg": 19.361, "energy_joules_est": 224.28, "duration_seconds": 11.584, "sample_count": 100}, "timestamp": "2026-01-26T14:08:16.271791"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7536.205, "latencies_ms": [7536.205], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "people: 9, cars: 2, flags: 1, buildings: 1, palm trees: 1, cones: 1, sand: numerous, beach: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20957.3, "ram_available_mb": 41883.6, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21011.2, "ram_available_mb": 41829.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.344}, "power_stats": {"power_gpu_soc_mean_watts": 21.445, "power_cpu_cv_mean_watts": 1.563, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 73.344, "power_watts_avg": 21.445, "energy_joules_est": 161.63, "duration_seconds": 7.537, "sample_count": 64}, "timestamp": "2026-01-26T14:08:25.841777"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11596.702, "latencies_ms": [11596.702], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is holding a baseball bat up in the air, appearing to be in the middle of a game or practice. Behind them, several other individuals are scattered across the sandy field, some closer to the camera and others further away, indicating they are part of the same event. In the background, there is a lifeguard tower with an American flag, situated", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21011.2, "ram_available_mb": 41829.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21034.1, "ram_available_mb": 41806.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.768}, "power_stats": {"power_gpu_soc_mean_watts": 19.231, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.681, "gpu_utilization_percent_mean": 69.768, "power_watts_avg": 19.231, "energy_joules_est": 223.03, "duration_seconds": 11.597, "sample_count": 99}, "timestamp": "2026-01-26T14:08:39.495799"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8106.409, "latencies_ms": [8106.409], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A group of people are gathered on a sandy beach, with a lifeguard tower and palm trees in the background. One person is holding a baseball bat, and there are orange cones set up in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20955.8, "ram_available_mb": 41885.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21017.8, "ram_available_mb": 41823.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.014}, "power_stats": {"power_gpu_soc_mean_watts": 21.116, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.591, "gpu_utilization_percent_mean": 73.014, "power_watts_avg": 21.116, "energy_joules_est": 171.19, "duration_seconds": 8.107, "sample_count": 69}, "timestamp": "2026-01-26T14:08:49.660645"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8591.691, "latencies_ms": [8591.691], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a group of people on a sandy beach with a clear sky and bright sunlight casting shadows on the ground. The beach is equipped with a lifeguard tower and a flag, indicating it's a public recreational area.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 21017.8, "ram_available_mb": 41823.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21058.2, "ram_available_mb": 41782.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.658}, "power_stats": {"power_gpu_soc_mean_watts": 20.415, "power_cpu_cv_mean_watts": 1.721, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 71.658, "power_watts_avg": 20.415, "energy_joules_est": 175.41, "duration_seconds": 8.592, "sample_count": 73}, "timestamp": "2026-01-26T14:09:00.290513"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11542.777, "latencies_ms": [11542.777], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a tennis player is standing on a tennis court, holding a tennis racket in his right hand and a tennis ball in his left hand. He appears to be preparing to serve the ball, as he is holding the ball close to his face. The player is wearing a blue shirt and white shorts, and his posture suggests that he is focused on the game", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20961.4, "ram_available_mb": 41879.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21010.5, "ram_available_mb": 41830.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.224}, "power_stats": {"power_gpu_soc_mean_watts": 19.372, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.684, "gpu_utilization_percent_mean": 70.224, "power_watts_avg": 19.372, "energy_joules_est": 223.62, "duration_seconds": 11.544, "sample_count": 98}, "timestamp": "2026-01-26T14:09:13.884514"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7570.467, "latencies_ms": [7570.467], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "ball: 1, racket: 1, shoe: 2, word: 3, letter: 3, player: 1, court: 1, shadow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20950.4, "ram_available_mb": 41890.5, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21010.3, "ram_available_mb": 41830.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.337, "power_cpu_cv_mean_watts": 1.551, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 74.0, "power_watts_avg": 21.337, "energy_joules_est": 161.55, "duration_seconds": 7.571, "sample_count": 65}, "timestamp": "2026-01-26T14:09:23.513617"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11682.159, "latencies_ms": [11682.159], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The tennis player is positioned in the foreground on a tennis court, with the white lines of the court clearly visible in the background. The player is standing to the right side of the image, holding a tennis racket in his right hand and a tennis ball in his left hand. The shadows of the player and the racket are cast on the court, indicating that the light source is", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21010.3, "ram_available_mb": 41830.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21036.0, "ram_available_mb": 41804.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.98}, "power_stats": {"power_gpu_soc_mean_watts": 19.289, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 69.98, "power_watts_avg": 19.289, "energy_joules_est": 225.35, "duration_seconds": 11.683, "sample_count": 100}, "timestamp": "2026-01-26T14:09:37.255138"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7600.434, "latencies_ms": [7600.434], "images_per_second": 0.132, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A tennis player is standing on a tennis court, holding a tennis racket and preparing to serve the ball. The court is marked with the word \"Wimbledon\" in large white letters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20974.1, "ram_available_mb": 41866.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20969.0, "ram_available_mb": 41871.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.156}, "power_stats": {"power_gpu_soc_mean_watts": 21.212, "power_cpu_cv_mean_watts": 1.563, "power_sys_5v0_mean_watts": 8.59, "gpu_utilization_percent_mean": 75.156, "power_watts_avg": 21.212, "energy_joules_est": 161.23, "duration_seconds": 7.601, "sample_count": 64}, "timestamp": "2026-01-26T14:09:46.866895"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6625.568, "latencies_ms": [6625.568], "images_per_second": 0.151, "prompt_tokens": 36, "response_tokens_est": 35, "n_tiles": 16, "output_text": "The image features a tennis player on a court with a blue surface and white markings. The lighting is bright, casting a shadow of the player on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20969.0, "ram_available_mb": 41871.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20958.8, "ram_available_mb": 41882.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.821}, "power_stats": {"power_gpu_soc_mean_watts": 21.898, "power_cpu_cv_mean_watts": 1.458, "power_sys_5v0_mean_watts": 8.653, "gpu_utilization_percent_mean": 74.821, "power_watts_avg": 21.898, "energy_joules_est": 145.1, "duration_seconds": 6.626, "sample_count": 56}, "timestamp": "2026-01-26T14:09:55.509339"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11569.994, "latencies_ms": [11569.994], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing in a kitchen, leaning over a black stove top oven. She appears to be cooking or preparing food. The kitchen is equipped with a refrigerator, a sink, and a microwave. There are various kitchen utensils and items scattered around the room, including a knife, a spoon, a bow", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20958.8, "ram_available_mb": 41882.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21012.5, "ram_available_mb": 41828.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.879}, "power_stats": {"power_gpu_soc_mean_watts": 19.302, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.684, "gpu_utilization_percent_mean": 69.879, "power_watts_avg": 19.302, "energy_joules_est": 223.34, "duration_seconds": 11.571, "sample_count": 99}, "timestamp": "2026-01-26T14:10:09.114928"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8256.794, "latencies_ms": [8256.794], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "fan: 1, bowl: 2, pot: 2, ladder: 1, canister: 2, spoon: 1, kettle: 1, spatula: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21012.5, "ram_available_mb": 41828.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21031.7, "ram_available_mb": 41809.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.929}, "power_stats": {"power_gpu_soc_mean_watts": 21.044, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 72.929, "power_watts_avg": 21.044, "energy_joules_est": 173.77, "duration_seconds": 8.257, "sample_count": 70}, "timestamp": "2026-01-26T14:10:19.400820"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11669.654, "latencies_ms": [11669.654], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the foreground, a person is standing on the left side of a black stove, which is positioned centrally in the image. The stove is surrounded by various kitchen items such as pots and pans, which are scattered around the right side and the background. The person appears to be in the process of cooking, with their attention focused on the stove.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21031.7, "ram_available_mb": 41809.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 8.3, "ram_used_mb": 20984.8, "ram_available_mb": 41856.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.343}, "power_stats": {"power_gpu_soc_mean_watts": 19.385, "power_cpu_cv_mean_watts": 2.126, "power_sys_5v0_mean_watts": 8.69, "gpu_utilization_percent_mean": 70.343, "power_watts_avg": 19.385, "energy_joules_est": 226.23, "duration_seconds": 11.67, "sample_count": 99}, "timestamp": "2026-01-26T14:10:33.083814"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9185.192, "latencies_ms": [9185.192], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "A woman is standing in a kitchen, leaning over a black stove, and appears to be cooking or preparing food. The kitchen has a vintage feel with a brick oven, wooden flooring, and various kitchen items on the countertops.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20984.8, "ram_available_mb": 41856.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21026.0, "ram_available_mb": 41814.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.065}, "power_stats": {"power_gpu_soc_mean_watts": 20.488, "power_cpu_cv_mean_watts": 1.715, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 72.065, "power_watts_avg": 20.488, "energy_joules_est": 188.2, "duration_seconds": 9.186, "sample_count": 77}, "timestamp": "2026-01-26T14:10:44.286189"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8443.318, "latencies_ms": [8443.318], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a person in a kitchen with a black stove and a white ceiling fan with a light fixture. The lighting is dim, and the room has a vintage feel with wooden flooring and a brick fireplace.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21026.0, "ram_available_mb": 41814.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21054.7, "ram_available_mb": 41786.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.056}, "power_stats": {"power_gpu_soc_mean_watts": 20.624, "power_cpu_cv_mean_watts": 1.678, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 73.056, "power_watts_avg": 20.624, "energy_joules_est": 174.15, "duration_seconds": 8.444, "sample_count": 72}, "timestamp": "2026-01-26T14:10:54.756206"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12422.565, "latencies_ms": [12422.565], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene moment in a zoo enclosure, where two majestic giraffes stand tall against the backdrop of a large, beige building. The giraffes, with their distinctive brown and white spotted coats, are positioned on a rocky terrain, their long necks reaching towards the sky. The building behind them is a large,", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 20974.4, "ram_available_mb": 41866.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21056.5, "ram_available_mb": 41784.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.79}, "power_stats": {"power_gpu_soc_mean_watts": 21.582, "power_cpu_cv_mean_watts": 1.81, "power_sys_5v0_mean_watts": 8.942, "gpu_utilization_percent_mean": 71.79, "power_watts_avg": 21.582, "energy_joules_est": 268.12, "duration_seconds": 12.423, "sample_count": 105}, "timestamp": "2026-01-26T14:11:09.232700"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8442.963, "latencies_ms": [8442.963], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "giraffe: 2, rock: numerous, building: 1, trees: many, fence: visible, grass: patches, ground: rocky, sky: overcast", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20952.8, "ram_available_mb": 41888.1, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21026.6, "ram_available_mb": 41814.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.282}, "power_stats": {"power_gpu_soc_mean_watts": 23.461, "power_cpu_cv_mean_watts": 1.392, "power_sys_5v0_mean_watts": 8.769, "gpu_utilization_percent_mean": 77.282, "power_watts_avg": 23.461, "energy_joules_est": 198.1, "duration_seconds": 8.444, "sample_count": 71}, "timestamp": "2026-01-26T14:11:19.718791"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12723.481, "latencies_ms": [12723.481], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "Two giraffes are standing in the foreground of the image, with one slightly in front of the other, both facing the same direction. They are positioned near a rocky ground with a building in the background, which appears to be a part of their enclosure. The trees in the background are behind the building, suggesting that the building is closer to the viewer than the trees", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21026.6, "ram_available_mb": 41814.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20954.1, "ram_available_mb": 41886.8, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.827}, "power_stats": {"power_gpu_soc_mean_watts": 21.599, "power_cpu_cv_mean_watts": 1.793, "power_sys_5v0_mean_watts": 8.895, "gpu_utilization_percent_mean": 72.827, "power_watts_avg": 21.599, "energy_joules_est": 274.83, "duration_seconds": 12.724, "sample_count": 110}, "timestamp": "2026-01-26T14:11:34.454398"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6669.961, "latencies_ms": [6669.961], "images_per_second": 0.15, "prompt_tokens": 37, "response_tokens_est": 25, "n_tiles": 16, "output_text": "Two giraffes are standing in front of a building with a stone wall, surrounded by greenery and trees.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 20954.1, "ram_available_mb": 41886.8, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 21014.3, "ram_available_mb": 41826.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 82.07}, "power_stats": {"power_gpu_soc_mean_watts": 24.801, "power_cpu_cv_mean_watts": 1.151, "power_sys_5v0_mean_watts": 8.714, "gpu_utilization_percent_mean": 82.07, "power_watts_avg": 24.801, "energy_joules_est": 165.44, "duration_seconds": 6.671, "sample_count": 57}, "timestamp": "2026-01-26T14:11:43.154890"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11385.971, "latencies_ms": [11385.971], "images_per_second": 0.088, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image features two giraffes with a pattern of dark brown patches separated by lighter lines, standing in front of a beige building with a sloped roof. The lighting is soft and diffused, suggesting an overcast day, and the ground is covered with a mix of rocks and patches of grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20949.9, "ram_available_mb": 41891.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21035.6, "ram_available_mb": 41805.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.567}, "power_stats": {"power_gpu_soc_mean_watts": 22.092, "power_cpu_cv_mean_watts": 1.728, "power_sys_5v0_mean_watts": 8.887, "gpu_utilization_percent_mean": 74.567, "power_watts_avg": 22.092, "energy_joules_est": 251.55, "duration_seconds": 11.387, "sample_count": 97}, "timestamp": "2026-01-26T14:11:56.603158"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11608.249, "latencies_ms": [11608.249], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a sunlit field, a young baseball player is captured in the midst of a powerful swing. Dressed in a vibrant green and yellow uniform, the player's helmet gleams under the sunlight, reflecting his determination and focus. His stance is firm, his grip on the bat firm yet relaxed, ready to send the ball flying", "error": null, "sys_before": {"cpu_percent": 3.2, "ram_used_mb": 20973.6, "ram_available_mb": 41867.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21033.7, "ram_available_mb": 41807.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.515}, "power_stats": {"power_gpu_soc_mean_watts": 19.378, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 70.515, "power_watts_avg": 19.378, "energy_joules_est": 224.96, "duration_seconds": 11.609, "sample_count": 99}, "timestamp": "2026-01-26T14:12:10.269908"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8924.733, "latencies_ms": [8924.733], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Fence: 1\n- Baseball bat: 1\n- Baseball: 1\n- Pants: 1\n- Socks: 1\n- Helmet: 1\n- Glove: 1\n- Car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20951.6, "ram_available_mb": 41889.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21008.4, "ram_available_mb": 41832.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.133}, "power_stats": {"power_gpu_soc_mean_watts": 20.595, "power_cpu_cv_mean_watts": 1.686, "power_sys_5v0_mean_watts": 8.601, "gpu_utilization_percent_mean": 73.133, "power_watts_avg": 20.595, "energy_joules_est": 183.82, "duration_seconds": 8.925, "sample_count": 75}, "timestamp": "2026-01-26T14:12:21.218670"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11579.796, "latencies_ms": [11579.796], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The main object, a young baseball player, is positioned in the foreground on the left side of the image, swinging a bat at a baseball that is in the air to the right. The background features a chain-link fence and some greenery, with a car partially visible behind the fence. The player is closer to the camera than the fence and the car, indicating", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21008.4, "ram_available_mb": 41832.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20973.4, "ram_available_mb": 41867.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.394}, "power_stats": {"power_gpu_soc_mean_watts": 19.321, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.686, "gpu_utilization_percent_mean": 70.394, "power_watts_avg": 19.321, "energy_joules_est": 223.74, "duration_seconds": 11.58, "sample_count": 99}, "timestamp": "2026-01-26T14:12:34.818452"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8427.204, "latencies_ms": [8427.204], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A young baseball player is in the middle of a swing, attempting to hit a baseball that is in mid-air. The player is standing on a dirt field, with a chain-link fence and some greenery in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.4, "ram_available_mb": 41867.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20967.9, "ram_available_mb": 41873.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.278}, "power_stats": {"power_gpu_soc_mean_watts": 20.957, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 73.278, "power_watts_avg": 20.957, "energy_joules_est": 176.62, "duration_seconds": 8.428, "sample_count": 72}, "timestamp": "2026-01-26T14:12:45.262158"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10059.788, "latencies_ms": [10059.788], "images_per_second": 0.099, "prompt_tokens": 36, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image shows a young baseball player in a green and yellow uniform swinging a blue bat. The player is wearing a black helmet with a red stripe and green socks. The background features a chain-link fence and a grassy area with some trees and a car parked behind the fence.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20967.9, "ram_available_mb": 41873.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20956.6, "ram_available_mb": 41884.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.128}, "power_stats": {"power_gpu_soc_mean_watts": 19.84, "power_cpu_cv_mean_watts": 1.824, "power_sys_5v0_mean_watts": 8.698, "gpu_utilization_percent_mean": 70.128, "power_watts_avg": 19.84, "energy_joules_est": 199.6, "duration_seconds": 10.06, "sample_count": 86}, "timestamp": "2026-01-26T14:12:57.369153"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11665.645, "latencies_ms": [11665.645], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a cobblestone street with a variety of vehicles parked along the side. There are several motorcycles, including a black motorcycle and a vintage car, as well as a bus parked on the street. A truck is also visible in the scene. \n\nThere are multiple people walking around the area, with some closer to the vehicles and others", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20956.6, "ram_available_mb": 41884.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21009.3, "ram_available_mb": 41831.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.46}, "power_stats": {"power_gpu_soc_mean_watts": 19.276, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.688, "gpu_utilization_percent_mean": 70.46, "power_watts_avg": 19.276, "energy_joules_est": 224.88, "duration_seconds": 11.666, "sample_count": 100}, "timestamp": "2026-01-26T14:13:11.094737"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9163.406, "latencies_ms": [9163.406], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Car: 2\n- Motorcycle: 3\n- Bus: 1\n- Truck: 1\n- Bike: 1\n- People: 5\n- Trees: 4\n- Scaffolding: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20956.0, "ram_available_mb": 41884.9, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21012.2, "ram_available_mb": 41828.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.208}, "power_stats": {"power_gpu_soc_mean_watts": 20.478, "power_cpu_cv_mean_watts": 1.699, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 72.208, "power_watts_avg": 20.478, "energy_joules_est": 187.66, "duration_seconds": 9.164, "sample_count": 77}, "timestamp": "2026-01-26T14:13:22.287967"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11577.378, "latencies_ms": [11577.378], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a black vintage car on the left side of the image, and a black vintage car on the right side of the image. In the background, there are several motorcycles parked in a row, with a red bus on the right side of the image and a red tripod on the left side of the image. There are also several", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21012.2, "ram_available_mb": 41828.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21031.4, "ram_available_mb": 41809.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.96}, "power_stats": {"power_gpu_soc_mean_watts": 19.311, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.676, "gpu_utilization_percent_mean": 69.96, "power_watts_avg": 19.311, "energy_joules_est": 223.58, "duration_seconds": 11.578, "sample_count": 100}, "timestamp": "2026-01-26T14:13:35.877527"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9701.426, "latencies_ms": [9701.426], "images_per_second": 0.103, "prompt_tokens": 37, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image depicts a cobblestone street lined with vintage cars and motorcycles, with a red double-decker bus and a building under construction in the background. People can be seen walking around and admiring the vehicles, suggesting a car show or gathering.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20976.2, "ram_available_mb": 41864.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20972.2, "ram_available_mb": 41868.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.89}, "power_stats": {"power_gpu_soc_mean_watts": 20.305, "power_cpu_cv_mean_watts": 1.747, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 71.89, "power_watts_avg": 20.305, "energy_joules_est": 197.0, "duration_seconds": 9.702, "sample_count": 82}, "timestamp": "2026-01-26T14:13:47.601326"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9254.012, "latencies_ms": [9254.012], "images_per_second": 0.108, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image features a cobblestone street with a mix of vintage and modern vehicles, including a classic black car in the foreground and a red double-decker bus in the background. The weather appears to be clear and sunny, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20972.2, "ram_available_mb": 41868.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21026.7, "ram_available_mb": 41814.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.295}, "power_stats": {"power_gpu_soc_mean_watts": 20.168, "power_cpu_cv_mean_watts": 1.77, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 71.295, "power_watts_avg": 20.168, "energy_joules_est": 186.65, "duration_seconds": 9.255, "sample_count": 78}, "timestamp": "2026-01-26T14:13:58.883612"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11614.952, "latencies_ms": [11614.952], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene scene of a parking lot during sunset. The sun is setting in the background, casting a warm, golden glow over the entire scene. The parking lot is filled with cars, their details slightly blurred due to the shallow depth of field. The focus is on the parking meters in the foreground, which are clearly visible and have", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21026.7, "ram_available_mb": 41814.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20986.9, "ram_available_mb": 41854.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_gpu_soc_mean_watts": 19.295, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 71.0, "power_watts_avg": 19.295, "energy_joules_est": 224.12, "duration_seconds": 11.616, "sample_count": 99}, "timestamp": "2026-01-26T14:14:12.518804"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11784.843, "latencies_ms": [11784.843], "images_per_second": 0.085, "prompt_tokens": 39, "response_tokens_est": 79, "n_tiles": 16, "output_text": "object: parking meter, count: 2\nobject: sun, count: 1\nobject: sky, count: 1\nobject: buildings, count: 1\nobject: shadows, count: 1\nobject: light, count: 1\nobject: parking lot, count: 1\nobject: parking meter, count: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20986.9, "ram_available_mb": 41854.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21027.8, "ram_available_mb": 41813.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.02}, "power_stats": {"power_gpu_soc_mean_watts": 19.47, "power_cpu_cv_mean_watts": 1.857, "power_sys_5v0_mean_watts": 8.639, "gpu_utilization_percent_mean": 70.02, "power_watts_avg": 19.47, "energy_joules_est": 229.46, "duration_seconds": 11.786, "sample_count": 100}, "timestamp": "2026-01-26T14:14:26.338690"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11576.884, "latencies_ms": [11576.884], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The parking meters are positioned in the foreground of the image, appearing closer to the viewer, while the background is dominated by the warm glow of the sun setting behind the buildings, creating a sense of depth. The parking meters are on the left side of the frame, and there is a clear spatial separation between them and the background, emphasizing their prominence in", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20966.0, "ram_available_mb": 41874.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 20969.9, "ram_available_mb": 41871.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.394}, "power_stats": {"power_gpu_soc_mean_watts": 19.213, "power_cpu_cv_mean_watts": 1.96, "power_sys_5v0_mean_watts": 8.71, "gpu_utilization_percent_mean": 70.394, "power_watts_avg": 19.213, "energy_joules_est": 222.44, "duration_seconds": 11.577, "sample_count": 99}, "timestamp": "2026-01-26T14:14:39.954245"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9842.067, "latencies_ms": [9842.067], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image captures a serene sunset view from the perspective of a parked car, with the focus on the car's side mirrors reflecting the warm hues of the setting sun. The background is softly blurred, highlighting the car and its immediate surroundings.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20969.9, "ram_available_mb": 41871.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21024.5, "ram_available_mb": 41816.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.687}, "power_stats": {"power_gpu_soc_mean_watts": 20.061, "power_cpu_cv_mean_watts": 1.75, "power_sys_5v0_mean_watts": 8.627, "gpu_utilization_percent_mean": 71.687, "power_watts_avg": 20.061, "energy_joules_est": 197.45, "duration_seconds": 9.843, "sample_count": 83}, "timestamp": "2026-01-26T14:14:51.819164"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9945.44, "latencies_ms": [9945.44], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image features a warm, golden hue with the sun low on the horizon, casting a soft light that creates a serene atmosphere. The focus is on a pair of round, metallic parking meters with a blurred background of a cityscape, suggesting a calm evening in an urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21024.5, "ram_available_mb": 41816.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21069.5, "ram_available_mb": 41771.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.329}, "power_stats": {"power_gpu_soc_mean_watts": 19.816, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 70.329, "power_watts_avg": 19.816, "energy_joules_est": 197.09, "duration_seconds": 9.946, "sample_count": 85}, "timestamp": "2026-01-26T14:15:03.784807"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11562.109, "latencies_ms": [11562.109], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two people standing next to a large suitcase. The suitcase is brown and has stickers on it, indicating that it has traveled to various countries such as China, India, and Bangladesh. The man and woman are smiling and posing for the picture, with the man wearing a yellow shirt and the woman wearing a blue sh", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 21069.5, "ram_available_mb": 41771.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21069.7, "ram_available_mb": 41771.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.43}, "power_stats": {"power_gpu_soc_mean_watts": 19.385, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.695, "gpu_utilization_percent_mean": 70.43, "power_watts_avg": 19.385, "energy_joules_est": 224.14, "duration_seconds": 11.563, "sample_count": 100}, "timestamp": "2026-01-26T14:15:17.373458"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10627.984, "latencies_ms": [10627.984], "images_per_second": 0.094, "prompt_tokens": 39, "response_tokens_est": 69, "n_tiles": 16, "output_text": "- Suitcase: 1\n- Sticker: 4\n- Woman: 1\n- Man: 1\n- Sculpture: 1\n- Sculpture base: 1\n- Sculpture plaque: 1\n- Sculpture plaque text: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.7, "ram_available_mb": 41877.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21023.9, "ram_available_mb": 41817.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.598}, "power_stats": {"power_gpu_soc_mean_watts": 19.822, "power_cpu_cv_mean_watts": 1.818, "power_sys_5v0_mean_watts": 8.683, "gpu_utilization_percent_mean": 71.598, "power_watts_avg": 19.822, "energy_joules_est": 210.68, "duration_seconds": 10.629, "sample_count": 92}, "timestamp": "2026-01-26T14:15:30.048577"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11594.537, "latencies_ms": [11594.537], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large, brown suitcase with various stickers on it, including \"TH\" for Thailand, \"IND\" for India, \"DHAKA\" for Bangladesh, and \"TYW\" for Tyler. To the right of the suitcase, there are two people standing close to it, one wearing a blue jacket and the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21023.9, "ram_available_mb": 41817.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21024.1, "ram_available_mb": 41816.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.04}, "power_stats": {"power_gpu_soc_mean_watts": 19.177, "power_cpu_cv_mean_watts": 1.902, "power_sys_5v0_mean_watts": 8.664, "gpu_utilization_percent_mean": 70.04, "power_watts_avg": 19.177, "energy_joules_est": 222.36, "duration_seconds": 11.595, "sample_count": 101}, "timestamp": "2026-01-26T14:15:43.675061"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9008.642, "latencies_ms": [9008.642], "images_per_second": 0.111, "prompt_tokens": 37, "response_tokens_est": 55, "n_tiles": 16, "output_text": "A large, decorated suitcase is placed on a platform outside a building with a sign that reads \"Fidelity Investments.\" Two people, a man and a woman, are standing next to the suitcase, smiling and posing for the photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21024.1, "ram_available_mb": 41816.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20960.3, "ram_available_mb": 41880.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.714}, "power_stats": {"power_gpu_soc_mean_watts": 20.596, "power_cpu_cv_mean_watts": 1.705, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 71.714, "power_watts_avg": 20.596, "energy_joules_est": 185.55, "duration_seconds": 9.009, "sample_count": 77}, "timestamp": "2026-01-26T14:15:54.722111"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10624.362, "latencies_ms": [10624.362], "images_per_second": 0.094, "prompt_tokens": 36, "response_tokens_est": 71, "n_tiles": 16, "output_text": "A large, brown suitcase with various stickers on it is placed on a platform. The suitcase has a lock and a handle, and the stickers are in different shapes and sizes, including a circle with a red border and a yellow star, and a rectangle with the text \"INDIA\" and \"BANGLADESH.\"", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20960.3, "ram_available_mb": 41880.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21024.0, "ram_available_mb": 41816.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.478}, "power_stats": {"power_gpu_soc_mean_watts": 19.644, "power_cpu_cv_mean_watts": 1.845, "power_sys_5v0_mean_watts": 8.702, "gpu_utilization_percent_mean": 70.478, "power_watts_avg": 19.644, "energy_joules_est": 208.72, "duration_seconds": 10.625, "sample_count": 90}, "timestamp": "2026-01-26T14:16:07.386954"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11586.29, "latencies_ms": [11586.29], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a close-up view of a dish consisting of a piece of fish, likely a white fish such as cod or halibut, accompanied by a generous serving of mushrooms and broccoli. The fish is cooked to a golden brown color, indicating it has been pan-fried or grilled. The mushrooms are sliced and appear to be", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21024.0, "ram_available_mb": 41816.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21055.2, "ram_available_mb": 41785.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.111}, "power_stats": {"power_gpu_soc_mean_watts": 19.365, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.683, "gpu_utilization_percent_mean": 70.111, "power_watts_avg": 19.365, "energy_joules_est": 224.38, "duration_seconds": 11.587, "sample_count": 99}, "timestamp": "2026-01-26T14:16:21.007235"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5961.509, "latencies_ms": [5961.509], "images_per_second": 0.168, "prompt_tokens": 39, "response_tokens_est": 28, "n_tiles": 16, "output_text": "fish: 1, mushrooms: 12, broccoli: 10, parsley: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21055.2, "ram_available_mb": 41785.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21072.6, "ram_available_mb": 41768.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.8}, "power_stats": {"power_gpu_soc_mean_watts": 22.965, "power_cpu_cv_mean_watts": 1.296, "power_sys_5v0_mean_watts": 8.571, "gpu_utilization_percent_mean": 76.8, "power_watts_avg": 22.965, "energy_joules_est": 136.92, "duration_seconds": 5.962, "sample_count": 50}, "timestamp": "2026-01-26T14:16:28.988955"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11491.843, "latencies_ms": [11491.843], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "In the foreground, there is a plate of food with a piece of fish on the left side, which is near the center of the plate. The fish is surrounded by mushrooms and garnished with parsley. In the background, there are pieces of broccoli that appear to be further away from the plate, giving a sense of depth to the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21072.6, "ram_available_mb": 41768.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21015.0, "ram_available_mb": 41825.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.52}, "power_stats": {"power_gpu_soc_mean_watts": 19.327, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.659, "gpu_utilization_percent_mean": 70.52, "power_watts_avg": 19.327, "energy_joules_est": 222.11, "duration_seconds": 11.492, "sample_count": 98}, "timestamp": "2026-01-26T14:16:42.516743"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9488.801, "latencies_ms": [9488.801], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a close-up of a dish consisting of a piece of fish, sliced mushrooms, and broccoli, garnished with parsley. The food is cooked and presented on a plate, suggesting a meal ready to be served.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21015.0, "ram_available_mb": 41825.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20962.3, "ram_available_mb": 41878.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.728}, "power_stats": {"power_gpu_soc_mean_watts": 20.31, "power_cpu_cv_mean_watts": 1.729, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 71.728, "power_watts_avg": 20.31, "energy_joules_est": 192.73, "duration_seconds": 9.489, "sample_count": 81}, "timestamp": "2026-01-26T14:16:54.040917"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9268.739, "latencies_ms": [9268.739], "images_per_second": 0.108, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image features a plate of food with a focus on a piece of fish topped with herbs. The lighting in the image highlights the glistening surface of the fish and the fresh green of the parsley, creating a vibrant and appetizing appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20962.3, "ram_available_mb": 41878.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21012.2, "ram_available_mb": 41828.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.538}, "power_stats": {"power_gpu_soc_mean_watts": 20.177, "power_cpu_cv_mean_watts": 1.755, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 71.538, "power_watts_avg": 20.177, "energy_joules_est": 187.03, "duration_seconds": 9.269, "sample_count": 78}, "timestamp": "2026-01-26T14:17:05.322974"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11589.62, "latencies_ms": [11589.62], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a display of various vegetables, including a large pile of carrots in a basket, which is the main focus of the scene. The carrots are arranged in a way that they are the most prominent vegetable in the image. There are also several other vegetables, such as broccoli and lettuce, placed around the carrots, creating a color", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 21012.2, "ram_available_mb": 41828.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21034.4, "ram_available_mb": 41806.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.51}, "power_stats": {"power_gpu_soc_mean_watts": 19.382, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.679, "gpu_utilization_percent_mean": 69.51, "power_watts_avg": 19.382, "energy_joules_est": 224.64, "duration_seconds": 11.59, "sample_count": 98}, "timestamp": "2026-01-26T14:17:18.936952"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9034.463, "latencies_ms": [9034.463], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "carrot: 20, broccoli: 15, lettuce: 10, cabbage: 5, artichoke: 3, zucchini: 2, onion: 1, garlic: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21034.4, "ram_available_mb": 41806.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21064.2, "ram_available_mb": 41776.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.671}, "power_stats": {"power_gpu_soc_mean_watts": 20.581, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 8.666, "gpu_utilization_percent_mean": 72.671, "power_watts_avg": 20.581, "energy_joules_est": 185.95, "duration_seconds": 9.035, "sample_count": 76}, "timestamp": "2026-01-26T14:17:29.985758"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11573.811, "latencies_ms": [11573.811], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a basket filled with orange carrots, positioned centrally in the image. Behind the carrots, there are various leafy greens, including broccoli and lettuce, displayed on shelves that recede into the background. To the right, there is a wooden crate with the word 'FOSIL' on", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20977.2, "ram_available_mb": 41863.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20972.8, "ram_available_mb": 41868.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.82}, "power_stats": {"power_gpu_soc_mean_watts": 19.321, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.705, "gpu_utilization_percent_mean": 69.82, "power_watts_avg": 19.321, "energy_joules_est": 223.63, "duration_seconds": 11.575, "sample_count": 100}, "timestamp": "2026-01-26T14:17:43.570803"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9361.154, "latencies_ms": [9361.154], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image showcases a variety of fresh vegetables, including carrots, broccoli, and cabbage, displayed in baskets on a table. The arrangement suggests that this could be a farmer's market or a produce section of a grocery store.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20972.8, "ram_available_mb": 41868.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20973.4, "ram_available_mb": 41867.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.241}, "power_stats": {"power_gpu_soc_mean_watts": 20.448, "power_cpu_cv_mean_watts": 1.727, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 72.241, "power_watts_avg": 20.448, "energy_joules_est": 191.43, "duration_seconds": 9.362, "sample_count": 79}, "timestamp": "2026-01-26T14:17:54.958756"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9946.723, "latencies_ms": [9946.723], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image showcases a variety of vegetables, including vibrant green broccoli and orange carrots, displayed in a natural light setting that highlights their freshness and colors. The vegetables are arranged in baskets and on wooden shelves, creating a rustic and organic presentation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.4, "ram_available_mb": 41867.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21010.4, "ram_available_mb": 41830.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.624}, "power_stats": {"power_gpu_soc_mean_watts": 19.863, "power_cpu_cv_mean_watts": 1.817, "power_sys_5v0_mean_watts": 8.701, "gpu_utilization_percent_mean": 70.624, "power_watts_avg": 19.863, "energy_joules_est": 197.58, "duration_seconds": 9.947, "sample_count": 85}, "timestamp": "2026-01-26T14:18:06.941416"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11612.208, "latencies_ms": [11612.208], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a donut shop with a conveyor belt system for making donuts. There are several donuts on the conveyor belt, and a person is working on the machine, possibly preparing the donuts. The shop is bustling with activity, as there are multiple people present in the scene.\n\nIn addition to the donut-making process, there are", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 20956.9, "ram_available_mb": 41884.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21016.3, "ram_available_mb": 41824.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.162}, "power_stats": {"power_gpu_soc_mean_watts": 19.357, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.702, "gpu_utilization_percent_mean": 70.162, "power_watts_avg": 19.357, "energy_joules_est": 224.79, "duration_seconds": 11.613, "sample_count": 99}, "timestamp": "2026-01-26T14:18:20.600083"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8048.384, "latencies_ms": [8048.384], "images_per_second": 0.124, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "donut: 12\nmachine: 1\nlabel: 1\ntag: 1\nwarning sign: 1\ndonuts: 12\npeople: 5\nbags: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21016.3, "ram_available_mb": 41824.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21015.2, "ram_available_mb": 41825.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.232}, "power_stats": {"power_gpu_soc_mean_watts": 20.813, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 73.232, "power_watts_avg": 20.813, "energy_joules_est": 167.53, "duration_seconds": 8.049, "sample_count": 69}, "timestamp": "2026-01-26T14:18:30.664820"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11367.829, "latencies_ms": [11367.829], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a conveyor belt with donuts moving along it, indicating a production line. In the background, there are people standing at various workstations, suggesting a busy kitchen or bakery environment. The donuts in the foreground appear to be near the 'HOT' sign, indicating they are freshly cooked and hot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20954.9, "ram_available_mb": 41886.0, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 9.5, "ram_used_mb": 21028.2, "ram_available_mb": 41812.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.351}, "power_stats": {"power_gpu_soc_mean_watts": 19.377, "power_cpu_cv_mean_watts": 2.175, "power_sys_5v0_mean_watts": 8.697, "gpu_utilization_percent_mean": 69.351, "power_watts_avg": 19.377, "energy_joules_est": 220.29, "duration_seconds": 11.368, "sample_count": 97}, "timestamp": "2026-01-26T14:18:44.093398"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8110.071, "latencies_ms": [8110.071], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a bustling donut shop with a conveyor belt system for making and displaying freshly baked donuts. People are seen working and customers are browsing the selection of donuts on the display.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21028.2, "ram_available_mb": 41812.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 21053.5, "ram_available_mb": 41787.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.217}, "power_stats": {"power_gpu_soc_mean_watts": 21.034, "power_cpu_cv_mean_watts": 1.821, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 73.217, "power_watts_avg": 21.034, "energy_joules_est": 170.6, "duration_seconds": 8.111, "sample_count": 69}, "timestamp": "2026-01-26T14:18:54.223746"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6865.176, "latencies_ms": [6865.176], "images_per_second": 0.146, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image shows a bustling donut shop with a conveyor belt system. The machines and surfaces are predominantly metallic, reflecting the bright overhead lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20974.0, "ram_available_mb": 41866.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21022.1, "ram_available_mb": 41818.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.649}, "power_stats": {"power_gpu_soc_mean_watts": 21.57, "power_cpu_cv_mean_watts": 1.565, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 74.649, "power_watts_avg": 21.57, "energy_joules_est": 148.1, "duration_seconds": 6.866, "sample_count": 57}, "timestamp": "2026-01-26T14:19:03.102842"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10573.257, "latencies_ms": [10573.257], "images_per_second": 0.095, "prompt_tokens": 24, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the image, a person is standing in a wooded area, holding a frisbee in their hands. They are wearing a green jacket and white shoes, and appear to be preparing to throw the frisbee. The person is surrounded by trees, and the ground is covered with leaves and dirt.", "error": null, "sys_before": {"cpu_percent": 3.6, "ram_used_mb": 20968.8, "ram_available_mb": 41872.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21030.7, "ram_available_mb": 41810.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.286}, "power_stats": {"power_gpu_soc_mean_watts": 19.653, "power_cpu_cv_mean_watts": 1.856, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 71.286, "power_watts_avg": 19.653, "energy_joules_est": 207.81, "duration_seconds": 10.574, "sample_count": 91}, "timestamp": "2026-01-26T14:19:15.725995"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8590.596, "latencies_ms": [8590.596], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "tree: numerous\nman: 1\nfrisbee: 1\nleaves: scattered\nground: uneven\ndirt: visible\nbranches: many\ngreen jacket: 1\nwhite socks: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21030.7, "ram_available_mb": 41810.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21036.0, "ram_available_mb": 41804.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.479}, "power_stats": {"power_gpu_soc_mean_watts": 20.764, "power_cpu_cv_mean_watts": 1.661, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 72.479, "power_watts_avg": 20.764, "energy_joules_est": 178.39, "duration_seconds": 8.591, "sample_count": 73}, "timestamp": "2026-01-26T14:19:26.346667"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11597.61, "latencies_ms": [11597.61], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person in a green jacket and white shoes is bending over to pick up an orange frisbee. The frisbee is closer to the camera than the person, indicating it is nearer to the viewer. The person is standing on a dirt ground with a large fallen tree trunk to their left and numerous trees in the background,", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.9, "ram_available_mb": 41877.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20966.8, "ram_available_mb": 41874.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.222}, "power_stats": {"power_gpu_soc_mean_watts": 19.301, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.681, "gpu_utilization_percent_mean": 70.222, "power_watts_avg": 19.301, "energy_joules_est": 223.86, "duration_seconds": 11.598, "sample_count": 99}, "timestamp": "2026-01-26T14:19:39.968409"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8709.794, "latencies_ms": [8709.794], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A person in a green jacket and white shoes is playing frisbee in a wooded area with a fallen tree trunk in the foreground. The ground is covered with fallen leaves and twigs, indicating it might be autumn.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20966.8, "ram_available_mb": 41874.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21023.0, "ram_available_mb": 41817.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.257}, "power_stats": {"power_gpu_soc_mean_watts": 20.716, "power_cpu_cv_mean_watts": 1.665, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 72.257, "power_watts_avg": 20.716, "energy_joules_est": 180.45, "duration_seconds": 8.71, "sample_count": 74}, "timestamp": "2026-01-26T14:19:50.721776"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8160.753, "latencies_ms": [8160.753], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image shows a person in a green jacket and white shoes playing with a frisbee in a forest. The ground is covered with dry leaves and the trees have bare branches, suggesting it might be autumn or winter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20969.7, "ram_available_mb": 41871.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21021.7, "ram_available_mb": 41819.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.362}, "power_stats": {"power_gpu_soc_mean_watts": 20.54, "power_cpu_cv_mean_watts": 1.659, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 71.362, "power_watts_avg": 20.54, "energy_joules_est": 167.63, "duration_seconds": 8.161, "sample_count": 69}, "timestamp": "2026-01-26T14:20:00.897636"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11544.975, "latencies_ms": [11544.975], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and well-organized bathroom. The dominant color scheme is a soothing beige, complemented by a pristine white toilet and sink. The sink, positioned on the left side of the image, is adorned with a silver faucet and a matching silver mirror. \n\nAdjacent to the sink, a", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21021.7, "ram_available_mb": 41819.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21044.2, "ram_available_mb": 41796.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.01}, "power_stats": {"power_gpu_soc_mean_watts": 19.225, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.69, "gpu_utilization_percent_mean": 70.01, "power_watts_avg": 19.225, "energy_joules_est": 221.98, "duration_seconds": 11.546, "sample_count": 99}, "timestamp": "2026-01-26T14:20:14.484970"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11669.725, "latencies_ms": [11669.725], "images_per_second": 0.086, "prompt_tokens": 39, "response_tokens_est": 78, "n_tiles": 16, "output_text": "1. Toilet: 1\n2. Sink: 1\n3. Shower curtain: 1\n4. Shower head: 1\n5. Hair dryer: 1\n6. Toilet paper: 1\n7. Toilet brush: 1\n8. Toiletries: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.4, "ram_available_mb": 41875.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21022.0, "ram_available_mb": 41818.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.614}, "power_stats": {"power_gpu_soc_mean_watts": 19.574, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 70.614, "power_watts_avg": 19.574, "energy_joules_est": 228.44, "duration_seconds": 11.67, "sample_count": 101}, "timestamp": "2026-01-26T14:20:28.182100"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11570.622, "latencies_ms": [11570.622], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "In the foreground, there is a white sink with a silver faucet on the left side of the image. In the background, there is a white toilet and a bathtub with striped curtains. The showerhead is located above the bathtub, and there are two cans of AJA on the countertop near the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21022.0, "ram_available_mb": 41818.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20966.5, "ram_available_mb": 41874.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.98}, "power_stats": {"power_gpu_soc_mean_watts": 19.258, "power_cpu_cv_mean_watts": 1.868, "power_sys_5v0_mean_watts": 8.696, "gpu_utilization_percent_mean": 70.98, "power_watts_avg": 19.258, "energy_joules_est": 222.84, "duration_seconds": 11.571, "sample_count": 99}, "timestamp": "2026-01-26T14:20:41.813956"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9481.54, "latencies_ms": [9481.54], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image depicts a small, well-lit bathroom with a sink, toilet, and bathtub. There are two cans of AJ-1 on the countertop, and a striped shower curtain is hanging in the bathtub.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20966.5, "ram_available_mb": 41874.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21012.1, "ram_available_mb": 41828.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.438}, "power_stats": {"power_gpu_soc_mean_watts": 20.395, "power_cpu_cv_mean_watts": 1.736, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 72.438, "power_watts_avg": 20.395, "energy_joules_est": 193.39, "duration_seconds": 9.482, "sample_count": 80}, "timestamp": "2026-01-26T14:20:53.317742"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6722.971, "latencies_ms": [6722.971], "images_per_second": 0.149, "prompt_tokens": 36, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The bathroom has a beige color scheme with a tiled floor and walls. There is a white sink with a silver faucet and a white toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20958.7, "ram_available_mb": 41882.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21021.8, "ram_available_mb": 41819.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.368}, "power_stats": {"power_gpu_soc_mean_watts": 21.916, "power_cpu_cv_mean_watts": 1.509, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 74.368, "power_watts_avg": 21.916, "energy_joules_est": 147.35, "duration_seconds": 6.724, "sample_count": 57}, "timestamp": "2026-01-26T14:21:02.079439"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11628.901, "latencies_ms": [11628.901], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a kitchen with a large island in the center, which has a sink and a faucet. The island is made of wood and has a black countertop. There are two chairs placed around the island, providing seating for people. The kitchen also has a dining table with chairs, and a refrigerator is visible in the background.\n\nVarious", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20968.5, "ram_available_mb": 41872.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20952.5, "ram_available_mb": 41888.4, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.684}, "power_stats": {"power_gpu_soc_mean_watts": 19.336, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.676, "gpu_utilization_percent_mean": 70.684, "power_watts_avg": 19.336, "energy_joules_est": 224.87, "duration_seconds": 11.63, "sample_count": 98}, "timestamp": "2026-01-26T14:21:15.735585"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9945.802, "latencies_ms": [9945.802], "images_per_second": 0.101, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "- Sink: 1\n- Countertop: 1\n- Faucet: 1\n- Coffee maker: 1\n- Coffee pot: 1\n- Bottles: 2\n- Chairs: 2\n- Window: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20952.5, "ram_available_mb": 41888.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21021.7, "ram_available_mb": 41819.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.706}, "power_stats": {"power_gpu_soc_mean_watts": 20.094, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 8.616, "gpu_utilization_percent_mean": 70.706, "power_watts_avg": 20.094, "energy_joules_est": 199.86, "duration_seconds": 9.946, "sample_count": 85}, "timestamp": "2026-01-26T14:21:27.729190"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9502.587, "latencies_ms": [9502.587], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The kitchen sink is located in the foreground on the left side of the image, while the dining table is in the background on the right side. There are two chairs placed near the dining table, and a window is visible in the background, allowing natural light to enter the room.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20961.5, "ram_available_mb": 41879.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21017.2, "ram_available_mb": 41823.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.037}, "power_stats": {"power_gpu_soc_mean_watts": 20.108, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.677, "gpu_utilization_percent_mean": 71.037, "power_watts_avg": 20.108, "energy_joules_est": 191.09, "duration_seconds": 9.503, "sample_count": 80}, "timestamp": "2026-01-26T14:21:39.252277"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8238.028, "latencies_ms": [8238.028], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image depicts a modern kitchen with a wooden island in the center, featuring a sink and a faucet. There are two chairs placed around the island, and a window with blinds is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.8, "ram_available_mb": 41877.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20966.4, "ram_available_mb": 41874.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.914}, "power_stats": {"power_gpu_soc_mean_watts": 20.991, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 72.914, "power_watts_avg": 20.991, "energy_joules_est": 172.94, "duration_seconds": 8.239, "sample_count": 70}, "timestamp": "2026-01-26T14:21:49.537245"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8160.125, "latencies_ms": [8160.125], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The kitchen features a wooden countertop with a black granite countertop, and the cabinets are made of wood with silver handles. The lighting in the room is natural, coming from the windows that show a view of trees outside.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20966.4, "ram_available_mb": 41874.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21007.4, "ram_available_mb": 41833.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.171}, "power_stats": {"power_gpu_soc_mean_watts": 20.641, "power_cpu_cv_mean_watts": 1.692, "power_sys_5v0_mean_watts": 8.669, "gpu_utilization_percent_mean": 72.171, "power_watts_avg": 20.641, "energy_joules_est": 168.45, "duration_seconds": 8.161, "sample_count": 70}, "timestamp": "2026-01-26T14:21:59.733200"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11543.718, "latencies_ms": [11543.718], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene indoor scene. Dominating the frame is a bed, neatly made with a dark blue comforter adorned with a white daisy pattern. The bed is positioned against a wall, and a window punctuates the right side of the frame, allowing a glimpse of the world outside. The room is bathed in soft, natural", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 21007.4, "ram_available_mb": 41833.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21042.1, "ram_available_mb": 41798.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.918}, "power_stats": {"power_gpu_soc_mean_watts": 19.287, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.696, "gpu_utilization_percent_mean": 69.918, "power_watts_avg": 19.287, "energy_joules_est": 222.66, "duration_seconds": 11.544, "sample_count": 98}, "timestamp": "2026-01-26T14:22:13.349264"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8487.145, "latencies_ms": [8487.145], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "bed: 1\ncomforter: 1\nflowers: 10\nbedspread: 1\npillow: 2\nblanket: 1\nsheets: 1\ndoll: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.5, "ram_available_mb": 41879.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21014.6, "ram_available_mb": 41826.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.681}, "power_stats": {"power_gpu_soc_mean_watts": 20.797, "power_cpu_cv_mean_watts": 1.639, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 72.681, "power_watts_avg": 20.797, "energy_joules_est": 176.52, "duration_seconds": 8.488, "sample_count": 72}, "timestamp": "2026-01-26T14:22:23.897674"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11644.632, "latencies_ms": [11644.632], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The bed with a floral patterned blanket is positioned in the foreground of the image, occupying the central space. The pillow is placed on the right side of the bed, near the edge. The left side of the image is darker, suggesting it is further away from the light source, possibly a window, which is not directly visible but inferred from the lighting", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.2, "ram_available_mb": 41879.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20962.2, "ram_available_mb": 41878.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.43}, "power_stats": {"power_gpu_soc_mean_watts": 19.385, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.661, "gpu_utilization_percent_mean": 70.43, "power_watts_avg": 19.385, "energy_joules_est": 225.74, "duration_seconds": 11.645, "sample_count": 100}, "timestamp": "2026-01-26T14:22:37.560418"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10185.612, "latencies_ms": [10185.612], "images_per_second": 0.098, "prompt_tokens": 37, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image shows a person lying in bed under a dark-colored blanket with a floral pattern, possibly asleep or resting. The room appears to be dimly lit, with a visible window on the right side, suggesting it might be nighttime or the room's lighting is turned off.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20962.2, "ram_available_mb": 41878.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 21024.3, "ram_available_mb": 41816.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.701}, "power_stats": {"power_gpu_soc_mean_watts": 20.193, "power_cpu_cv_mean_watts": 1.78, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 71.701, "power_watts_avg": 20.193, "energy_joules_est": 205.69, "duration_seconds": 10.186, "sample_count": 87}, "timestamp": "2026-01-26T14:22:49.762073"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10409.759, "latencies_ms": [10409.759], "images_per_second": 0.096, "prompt_tokens": 36, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image shows a bed with a dark-colored blanket that has a floral pattern, possibly daisies, in a lighter color. The lighting in the room is dim, with the primary light source coming from the window on the right, casting a soft glow on the bed and creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21024.3, "ram_available_mb": 41816.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 21036.8, "ram_available_mb": 41804.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.921}, "power_stats": {"power_gpu_soc_mean_watts": 19.888, "power_cpu_cv_mean_watts": 1.992, "power_sys_5v0_mean_watts": 8.731, "gpu_utilization_percent_mean": 71.921, "power_watts_avg": 19.888, "energy_joules_est": 207.04, "duration_seconds": 10.41, "sample_count": 89}, "timestamp": "2026-01-26T14:23:02.196150"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11566.266, "latencies_ms": [11566.266], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photo, a person is skillfully skateboarding on a concrete ledge. The skateboarder, dressed in a white t-shirt and black shorts, is in the process of performing a trick. The skateboard, with its white wheels and black grip tape, is positioned under the skateboarder's feet.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 21036.8, "ram_available_mb": 41804.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20969.7, "ram_available_mb": 41871.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.929}, "power_stats": {"power_gpu_soc_mean_watts": 19.289, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.687, "gpu_utilization_percent_mean": 68.929, "power_watts_avg": 19.289, "energy_joules_est": 223.11, "duration_seconds": 11.567, "sample_count": 99}, "timestamp": "2026-01-26T14:23:15.789869"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7556.04, "latencies_ms": [7556.04], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "person: 1, skateboard: 1, building: 1, sky: 1, shadow: 1, concrete: 1, ground: 1, photo: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20969.7, "ram_available_mb": 41871.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21020.3, "ram_available_mb": 41820.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.484}, "power_stats": {"power_gpu_soc_mean_watts": 21.594, "power_cpu_cv_mean_watts": 1.557, "power_sys_5v0_mean_watts": 8.608, "gpu_utilization_percent_mean": 73.484, "power_watts_avg": 21.594, "energy_joules_est": 163.18, "duration_seconds": 7.557, "sample_count": 64}, "timestamp": "2026-01-26T14:23:25.397266"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11633.189, "latencies_ms": [11633.189], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The skateboarder is in the foreground, performing a trick on a skateboard, with a clear shadow cast on the ground behind them. In the background, there is another person standing on the edge of a concrete structure, possibly a skatepark ramp, with buildings visible in the distance. The skateboarder is closer to the camera than the person in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21020.3, "ram_available_mb": 41820.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20973.6, "ram_available_mb": 41867.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.08}, "power_stats": {"power_gpu_soc_mean_watts": 19.533, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.695, "gpu_utilization_percent_mean": 71.08, "power_watts_avg": 19.533, "energy_joules_est": 227.25, "duration_seconds": 11.634, "sample_count": 100}, "timestamp": "2026-01-26T14:23:39.062152"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8826.917, "latencies_ms": [8826.917], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "A person is skateboarding on a concrete surface, possibly in an urban area with buildings in the background. The skateboarder is wearing a white t-shirt and dark shorts, and is in the process of performing a trick.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20973.6, "ram_available_mb": 41867.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20975.1, "ram_available_mb": 41865.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.527}, "power_stats": {"power_gpu_soc_mean_watts": 20.82, "power_cpu_cv_mean_watts": 1.687, "power_sys_5v0_mean_watts": 8.629, "gpu_utilization_percent_mean": 72.527, "power_watts_avg": 20.82, "energy_joules_est": 183.79, "duration_seconds": 8.828, "sample_count": 74}, "timestamp": "2026-01-26T14:23:49.900297"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7288.642, "latencies_ms": [7288.642], "images_per_second": 0.137, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image is in black and white, featuring a person skateboarding on a concrete surface. The lighting is bright and appears to be natural sunlight, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20975.1, "ram_available_mb": 41865.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20966.1, "ram_available_mb": 41874.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.967}, "power_stats": {"power_gpu_soc_mean_watts": 21.455, "power_cpu_cv_mean_watts": 1.574, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 74.967, "power_watts_avg": 21.455, "energy_joules_est": 156.39, "duration_seconds": 7.289, "sample_count": 61}, "timestamp": "2026-01-26T14:23:59.202733"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11587.474, "latencies_ms": [11587.474], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a man lying on the floor, surrounded by various items. He is resting his head on his hand, and the scene includes a laptop, a cell phone, a book, a tennis racket, and a backpack. The laptop is open and placed on the floor, while the cell phone is located nearby. The book is positioned to the right of the man, and", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20966.1, "ram_available_mb": 41874.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21015.8, "ram_available_mb": 41825.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.53}, "power_stats": {"power_gpu_soc_mean_watts": 19.377, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.687, "gpu_utilization_percent_mean": 70.53, "power_watts_avg": 19.377, "energy_joules_est": 224.54, "duration_seconds": 11.588, "sample_count": 100}, "timestamp": "2026-01-26T14:24:12.826435"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9160.059, "latencies_ms": [9160.059], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Laptop: 1\n- Cell phone: 1\n- Camera: 1\n- Book: 1\n- Tennis racket: 1\n- Keychain: 1\n- Water bottle: 1\n- Backpack: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20962.5, "ram_available_mb": 41878.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20960.5, "ram_available_mb": 41880.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.308}, "power_stats": {"power_gpu_soc_mean_watts": 20.423, "power_cpu_cv_mean_watts": 1.719, "power_sys_5v0_mean_watts": 8.668, "gpu_utilization_percent_mean": 72.308, "power_watts_avg": 20.423, "energy_joules_est": 187.09, "duration_seconds": 9.161, "sample_count": 78}, "timestamp": "2026-01-26T14:24:24.028874"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11588.783, "latencies_ms": [11588.783], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a tennis racket lying horizontally across the image, with its head resting on a book that has a brown cover. To the right of the racket, there is a black camera lens cap and a smartphone with a colorful screen. In the background, there is a laptop with a green chili pepper image on its screen, a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20960.5, "ram_available_mb": 41880.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21018.5, "ram_available_mb": 41822.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.939}, "power_stats": {"power_gpu_soc_mean_watts": 19.24, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.698, "gpu_utilization_percent_mean": 69.939, "power_watts_avg": 19.24, "energy_joules_est": 222.98, "duration_seconds": 11.589, "sample_count": 99}, "timestamp": "2026-01-26T14:24:37.636235"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11792.996, "latencies_ms": [11792.996], "images_per_second": 0.085, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a cluttered workspace on a carpeted floor with various items scattered around, including a laptop displaying an image of green chili peppers, a camera, a smartphone, a book titled \"The Men Who Built India\" by Philip Mason, a water bottle, a black backpack, a notebook, a pen, and a pair of earphones", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 21018.5, "ram_available_mb": 41822.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21046.7, "ram_available_mb": 41794.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.43}, "power_stats": {"power_gpu_soc_mean_watts": 19.505, "power_cpu_cv_mean_watts": 1.861, "power_sys_5v0_mean_watts": 8.653, "gpu_utilization_percent_mean": 70.43, "power_watts_avg": 19.505, "energy_joules_est": 230.04, "duration_seconds": 11.794, "sample_count": 100}, "timestamp": "2026-01-26T14:24:51.462967"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11625.151, "latencies_ms": [11625.151], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a variety of objects on a carpeted floor, including a black laptop with a green chili pepper image on its screen, a black camera, a black phone, a red water bottle, a black backpack, a black notebook, a black pen, a black book titled \"The Men Who Built India\" by Philip Mason, a black tablet, a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21046.7, "ram_available_mb": 41794.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20993.4, "ram_available_mb": 41847.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.15}, "power_stats": {"power_gpu_soc_mean_watts": 19.148, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 70.15, "power_watts_avg": 19.148, "energy_joules_est": 222.61, "duration_seconds": 11.626, "sample_count": 100}, "timestamp": "2026-01-26T14:25:05.134689"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11599.293, "latencies_ms": [11599.293], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a kitchen countertop, bathed in the soft glow of ambient light. Dominating the scene is a black stove top, its surface gleaming under the light. The stove top is adorned with four burners, each a vibrant shade of blue, red, and green, standing out against the black. \n\nTo", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20993.4, "ram_available_mb": 41847.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21033.1, "ram_available_mb": 41807.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.74}, "power_stats": {"power_gpu_soc_mean_watts": 19.273, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.701, "gpu_utilization_percent_mean": 69.74, "power_watts_avg": 19.273, "energy_joules_est": 223.57, "duration_seconds": 11.6, "sample_count": 100}, "timestamp": "2026-01-26T14:25:18.764904"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11767.414, "latencies_ms": [11767.414], "images_per_second": 0.085, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "- Oven: 1\n\n- Burner: 1\n\n- Burner knobs: 6\n\n- Burner knobs (red): 3\n\n- Burner knobs (blue): 3\n\n- Burner knobs (green): 1\n\n- Burner knobs (orange): 1\n\n- Burner knobs", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20955.5, "ram_available_mb": 41885.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21019.7, "ram_available_mb": 41821.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.347}, "power_stats": {"power_gpu_soc_mean_watts": 19.486, "power_cpu_cv_mean_watts": 1.866, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 69.347, "power_watts_avg": 19.486, "energy_joules_est": 229.31, "duration_seconds": 11.768, "sample_count": 101}, "timestamp": "2026-01-26T14:25:32.552595"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11608.164, "latencies_ms": [11608.164], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a black stovetop with four burners, positioned on the left side of the image. Behind the stovetop, on the right side, there is a wall with a marble-patterned backsplash. Above the stovetop, a black vent is mounted on the wall, and to the right of the st", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21019.7, "ram_available_mb": 41821.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21038.0, "ram_available_mb": 41802.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.439}, "power_stats": {"power_gpu_soc_mean_watts": 19.201, "power_cpu_cv_mean_watts": 1.915, "power_sys_5v0_mean_watts": 8.702, "gpu_utilization_percent_mean": 70.439, "power_watts_avg": 19.201, "energy_joules_est": 222.9, "duration_seconds": 11.609, "sample_count": 98}, "timestamp": "2026-01-26T14:25:46.177127"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9902.527, "latencies_ms": [9902.527], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image shows a kitchen counter with a black stovetop and a black microwave oven mounted above it. The counter is made of a dark material, possibly granite or marble, and there is a set of colorful spice bottles on the left side of the counter.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21038.0, "ram_available_mb": 41802.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20968.9, "ram_available_mb": 41872.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.714}, "power_stats": {"power_gpu_soc_mean_watts": 19.936, "power_cpu_cv_mean_watts": 1.753, "power_sys_5v0_mean_watts": 8.598, "gpu_utilization_percent_mean": 71.714, "power_watts_avg": 19.936, "energy_joules_est": 197.43, "duration_seconds": 9.903, "sample_count": 84}, "timestamp": "2026-01-26T14:25:58.098107"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10869.374, "latencies_ms": [10869.374], "images_per_second": 0.092, "prompt_tokens": 36, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The kitchen features a black stovetop with a modern design, set against a backdrop of marble tiles in shades of gold, white, and gray. Above the stove, there is a black vent hood, and to the right, a towel hangs from a hook, adding a homey touch to the space.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20968.9, "ram_available_mb": 41872.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21018.9, "ram_available_mb": 41822.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.978}, "power_stats": {"power_gpu_soc_mean_watts": 19.477, "power_cpu_cv_mean_watts": 1.866, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 70.978, "power_watts_avg": 19.477, "energy_joules_est": 211.72, "duration_seconds": 10.87, "sample_count": 92}, "timestamp": "2026-01-26T14:26:10.988708"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11588.586, "latencies_ms": [11588.586], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is sitting at a dining table, enjoying a meal consisting of donuts and coffee. He is holding a donut in his hand, and there are several other donuts on the table, including one with sprinkles. A cup of coffee is also present on the table, along with a tray containing more donuts.\n\nThe d", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 20965.4, "ram_available_mb": 41875.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20970.0, "ram_available_mb": 41870.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.133}, "power_stats": {"power_gpu_soc_mean_watts": 19.171, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 70.133, "power_watts_avg": 19.171, "energy_joules_est": 222.18, "duration_seconds": 11.589, "sample_count": 98}, "timestamp": "2026-01-26T14:26:24.614657"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8622.219, "latencies_ms": [8622.219], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "tray: 1\ndonut: 3\ndonuts: 3\nperson: 1\ndonut hole: 1\nchocolate: 1\nsprinkles: 1\ncoffee: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20970.0, "ram_available_mb": 41870.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20969.4, "ram_available_mb": 41871.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.425}, "power_stats": {"power_gpu_soc_mean_watts": 20.769, "power_cpu_cv_mean_watts": 1.661, "power_sys_5v0_mean_watts": 8.618, "gpu_utilization_percent_mean": 73.425, "power_watts_avg": 20.769, "energy_joules_est": 179.09, "duration_seconds": 8.623, "sample_count": 73}, "timestamp": "2026-01-26T14:26:35.294337"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11689.999, "latencies_ms": [11689.999], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a tray with a chocolate-covered donut topped with colorful sprinkles, placed near the center. To the right of the tray, there is another donut, plain and light brown, on a white plate. In the background, there is a blurred figure of a person sitting at a table with a cup of", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20969.4, "ram_available_mb": 41871.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20965.3, "ram_available_mb": 41875.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.98}, "power_stats": {"power_gpu_soc_mean_watts": 19.338, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 69.98, "power_watts_avg": 19.338, "energy_joules_est": 226.07, "duration_seconds": 11.691, "sample_count": 99}, "timestamp": "2026-01-26T14:26:49.042382"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8764.761, "latencies_ms": [8764.761], "images_per_second": 0.114, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A person is sitting at a table in a cafe, holding a donut and looking at the camera. On the table, there are two donuts, one with chocolate frosting and sprinkles, and another plain donut.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20965.3, "ram_available_mb": 41875.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 21015.3, "ram_available_mb": 41825.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.27}, "power_stats": {"power_gpu_soc_mean_watts": 20.619, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 72.27, "power_watts_avg": 20.619, "energy_joules_est": 180.73, "duration_seconds": 8.765, "sample_count": 74}, "timestamp": "2026-01-26T14:26:59.819428"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11678.004, "latencies_ms": [11678.004], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a tray with a variety of donuts, including a chocolate one with sprinkles and a plain glazed one. The tray is placed on a table with a cup of coffee and a glass of soda. The lighting in the image is warm and cozy, creating a comfortable atmosphere. The materials used in the image include wood for the table and t", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21015.3, "ram_available_mb": 41825.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20968.2, "ram_available_mb": 41872.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.212}, "power_stats": {"power_gpu_soc_mean_watts": 19.33, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 70.212, "power_watts_avg": 19.33, "energy_joules_est": 225.75, "duration_seconds": 11.679, "sample_count": 99}, "timestamp": "2026-01-26T14:27:13.541788"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11615.26, "latencies_ms": [11615.26], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a bathroom, bathed in soft light. Dominating the scene is a pristine white sink, its surface gleaming under the light. The sink is mounted on a wall, its position slightly off-center, adding a touch of asymmetry to the otherwise orderly scene. \n\nAbove the sink, a blue and white checkered", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 20968.2, "ram_available_mb": 41872.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21025.8, "ram_available_mb": 41815.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.357}, "power_stats": {"power_gpu_soc_mean_watts": 19.27, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 70.357, "power_watts_avg": 19.27, "energy_joules_est": 223.84, "duration_seconds": 11.616, "sample_count": 98}, "timestamp": "2026-01-26T14:27:27.179973"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10928.216, "latencies_ms": [10928.216], "images_per_second": 0.092, "prompt_tokens": 39, "response_tokens_est": 71, "n_tiles": 16, "output_text": "1. Toilet: 1\n2. Bathtub: 1\n3. Shelf: 1\n4. Tiles: 1\n5. Bowl: 1\n6. Plate: 1\n7. Toilet paper: 1\n8. Toilet brush: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21025.8, "ram_available_mb": 41815.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20985.3, "ram_available_mb": 41855.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.043}, "power_stats": {"power_gpu_soc_mean_watts": 19.613, "power_cpu_cv_mean_watts": 1.82, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 71.043, "power_watts_avg": 19.613, "energy_joules_est": 214.35, "duration_seconds": 10.929, "sample_count": 93}, "timestamp": "2026-01-26T14:27:40.138501"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8702.961, "latencies_ms": [8702.961], "images_per_second": 0.115, "prompt_tokens": 44, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The bathtub is positioned in the background, behind the toilet which is in the foreground. The toilet is closer to the viewer, while the bathtub is further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20985.3, "ram_available_mb": 41855.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21024.5, "ram_available_mb": 41816.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.919}, "power_stats": {"power_gpu_soc_mean_watts": 20.484, "power_cpu_cv_mean_watts": 1.698, "power_sys_5v0_mean_watts": 8.663, "gpu_utilization_percent_mean": 71.919, "power_watts_avg": 20.484, "energy_joules_est": 178.28, "duration_seconds": 8.704, "sample_count": 74}, "timestamp": "2026-01-26T14:27:50.862262"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9171.244, "latencies_ms": [9171.244], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a modern bathroom with a white sink and a toilet. The sink is mounted on a glass shelf above a black and white checkered floor, and there is a blue and white patterned toilet seat cover on the toilet.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 20969.6, "ram_available_mb": 41871.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21036.2, "ram_available_mb": 41804.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.718}, "power_stats": {"power_gpu_soc_mean_watts": 20.469, "power_cpu_cv_mean_watts": 1.713, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 72.718, "power_watts_avg": 20.469, "energy_joules_est": 187.74, "duration_seconds": 9.172, "sample_count": 78}, "timestamp": "2026-01-26T14:28:02.069071"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9346.031, "latencies_ms": [9346.031], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image shows a bathroom with a checkered black and white floor, a white sink with a blue and white towel, and a toilet with a cow print seat cover. The lighting is bright and the materials appear to be a mix of ceramic and glass.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20974.0, "ram_available_mb": 41866.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21026.9, "ram_available_mb": 41814.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.101}, "power_stats": {"power_gpu_soc_mean_watts": 20.276, "power_cpu_cv_mean_watts": 1.742, "power_sys_5v0_mean_watts": 8.687, "gpu_utilization_percent_mean": 72.101, "power_watts_avg": 20.276, "energy_joules_est": 189.51, "duration_seconds": 9.347, "sample_count": 79}, "timestamp": "2026-01-26T14:28:13.456504"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11561.723, "latencies_ms": [11561.723], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a statue of a woman holding two teddy bears. The statue is positioned on a brick base and is surrounded by a grassy area. The woman is depicted in a brown dress, and the teddy bears are white. The statue is located in front of a white fence and a tree. There is a sign next to the statue that reads", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 20965.0, "ram_available_mb": 41875.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21021.4, "ram_available_mb": 41819.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.01}, "power_stats": {"power_gpu_soc_mean_watts": 19.331, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 70.01, "power_watts_avg": 19.331, "energy_joules_est": 223.51, "duration_seconds": 11.562, "sample_count": 98}, "timestamp": "2026-01-26T14:28:27.052024"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9781.423, "latencies_ms": [9781.423], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Stuffed bear: 2\n- Stuffed animal: 2\n- Wooden sign: 1\n- Brick border: 1\n- Grass: 1\n- Dirt: 1\n- Plants: 1\n- Post: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20958.2, "ram_available_mb": 41882.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21027.2, "ram_available_mb": 41813.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.542}, "power_stats": {"power_gpu_soc_mean_watts": 20.128, "power_cpu_cv_mean_watts": 1.755, "power_sys_5v0_mean_watts": 8.63, "gpu_utilization_percent_mean": 72.542, "power_watts_avg": 20.128, "energy_joules_est": 196.89, "duration_seconds": 9.782, "sample_count": 83}, "timestamp": "2026-01-26T14:28:38.850941"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11098.315, "latencies_ms": [11098.315], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "In the foreground, there is a statue of a figure holding two teddy bears, positioned on a pedestal. To the left of the statue, there is a wooden sign with names and dates, and to the right, there is a white pillar. In the background, there are green plants and a pathway leading to a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21027.2, "ram_available_mb": 41813.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20988.6, "ram_available_mb": 41852.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.234}, "power_stats": {"power_gpu_soc_mean_watts": 19.575, "power_cpu_cv_mean_watts": 1.848, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 71.234, "power_watts_avg": 19.575, "energy_joules_est": 217.26, "duration_seconds": 11.099, "sample_count": 94}, "timestamp": "2026-01-26T14:28:51.982429"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7809.941, "latencies_ms": [7809.941], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image shows a statue of a woman holding two teddy bears, with a signpost in front of her. The setting appears to be a garden or park with plants and a pathway in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20988.6, "ram_available_mb": 41852.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20990.5, "ram_available_mb": 41850.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.379}, "power_stats": {"power_gpu_soc_mean_watts": 21.419, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.608, "gpu_utilization_percent_mean": 73.379, "power_watts_avg": 21.419, "energy_joules_est": 167.3, "duration_seconds": 7.811, "sample_count": 66}, "timestamp": "2026-01-26T14:29:01.812556"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10168.758, "latencies_ms": [10168.758], "images_per_second": 0.098, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image features a weathered, light-colored statue of a woman holding a teddy bear, standing on a pedestal with a sign that reads \"JUSEPH PANIS MISTLE MAIL\". The statue is surrounded by greenery and a brick pathway, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20990.5, "ram_available_mb": 41850.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21019.7, "ram_available_mb": 41821.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.302}, "power_stats": {"power_gpu_soc_mean_watts": 19.864, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 71.302, "power_watts_avg": 19.864, "energy_joules_est": 202.01, "duration_seconds": 10.169, "sample_count": 86}, "timestamp": "2026-01-26T14:29:14.000361"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11613.232, "latencies_ms": [11613.232], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a large group of people gathered in a restaurant, sitting at various dining tables and enjoying their meals. The restaurant has a spacious and open layout, with a large clock hanging on the wall, adding a unique touch to the ambiance. There are numerous chairs and dining tables scattered throughout the space, accommodating the numerous guests.\n\n", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 20957.9, "ram_available_mb": 41883.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21022.6, "ram_available_mb": 41818.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.04}, "power_stats": {"power_gpu_soc_mean_watts": 19.177, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.689, "gpu_utilization_percent_mean": 71.04, "power_watts_avg": 19.177, "energy_joules_est": 222.72, "duration_seconds": 11.614, "sample_count": 100}, "timestamp": "2026-01-26T14:29:27.661024"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8697.655, "latencies_ms": [8697.655], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "people: 20+\nchairs: numerous\ntables: multiple\nglasses: 1\nclock: 1\nbar: 1\nlight fixtures: 2\nwalls: 2\nceilings: 2", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21022.6, "ram_available_mb": 41818.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21037.8, "ram_available_mb": 41803.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.64}, "power_stats": {"power_gpu_soc_mean_watts": 20.546, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 72.64, "power_watts_avg": 20.546, "energy_joules_est": 178.71, "duration_seconds": 8.698, "sample_count": 75}, "timestamp": "2026-01-26T14:29:38.413963"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11601.661, "latencies_ms": [11601.661], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large glass bowl placed on a table, which is near the bottom left corner of the image. The background features a large, ornate clock mounted on the wall, which is situated towards the upper right quadrant of the image. The people are seated in the middle to the right of the image, with some standing in the background, creating a sense", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21037.8, "ram_available_mb": 41803.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21038.0, "ram_available_mb": 41802.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.388}, "power_stats": {"power_gpu_soc_mean_watts": 19.278, "power_cpu_cv_mean_watts": 1.903, "power_sys_5v0_mean_watts": 8.672, "gpu_utilization_percent_mean": 70.388, "power_watts_avg": 19.278, "energy_joules_est": 223.67, "duration_seconds": 11.602, "sample_count": 98}, "timestamp": "2026-01-26T14:29:52.042797"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8826.529, "latencies_ms": [8826.529], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image depicts a bustling restaurant with a large, ornate clock as a focal point on the wall. People are seated at tables, enjoying their meals and drinks, while others are standing or walking around the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21038.0, "ram_available_mb": 41802.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21055.0, "ram_available_mb": 41785.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.947}, "power_stats": {"power_gpu_soc_mean_watts": 20.611, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 72.947, "power_watts_avg": 20.611, "energy_joules_est": 181.94, "duration_seconds": 8.827, "sample_count": 75}, "timestamp": "2026-01-26T14:30:02.898182"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8695.685, "latencies_ms": [8695.685], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts an indoor setting with warm lighting that casts a cozy ambiance. The ceiling is made of dark wood beams, and there is a large, ornate clock with a black and white design dominating the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21055.0, "ram_available_mb": 41785.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21002.5, "ram_available_mb": 41838.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.108}, "power_stats": {"power_gpu_soc_mean_watts": 20.403, "power_cpu_cv_mean_watts": 1.725, "power_sys_5v0_mean_watts": 8.687, "gpu_utilization_percent_mean": 71.108, "power_watts_avg": 20.403, "energy_joules_est": 177.43, "duration_seconds": 8.696, "sample_count": 74}, "timestamp": "2026-01-26T14:30:13.606980"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11451.62, "latencies_ms": [11451.62], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 78, "n_tiles": 16, "output_text": "In the image, a man and a child are skiing down a snowy mountain. The man is standing in the foreground, while the child is skiing behind him. Both of them are wearing skis and appear to be enjoying their time on the slopes. The scene is set in a beautiful, snowy environment, with trees visible in the background.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 21002.5, "ram_available_mb": 41838.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21032.5, "ram_available_mb": 41808.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.072}, "power_stats": {"power_gpu_soc_mean_watts": 19.217, "power_cpu_cv_mean_watts": 1.894, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 70.072, "power_watts_avg": 19.217, "energy_joules_est": 220.08, "duration_seconds": 11.452, "sample_count": 97}, "timestamp": "2026-01-26T14:30:27.104003"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7230.708, "latencies_ms": [7230.708], "images_per_second": 0.138, "prompt_tokens": 39, "response_tokens_est": 39, "n_tiles": 16, "output_text": "person: 2, ski: 2, tree: 4, rock: 3, snow: numerous, shadow: 1, sun: 1, mountain: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20969.2, "ram_available_mb": 41871.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21026.3, "ram_available_mb": 41814.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.459}, "power_stats": {"power_gpu_soc_mean_watts": 21.665, "power_cpu_cv_mean_watts": 1.515, "power_sys_5v0_mean_watts": 8.619, "gpu_utilization_percent_mean": 74.459, "power_watts_avg": 21.665, "energy_joules_est": 156.67, "duration_seconds": 7.231, "sample_count": 61}, "timestamp": "2026-01-26T14:30:36.355719"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10610.897, "latencies_ms": [10610.897], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there is a person standing on skis, positioned near the center of the image. To the left and slightly behind the person, there is a child also on skis, appearing smaller due to the distance. In the background, there are trees and more snow-covered ground, indicating a mountainous terrain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21026.3, "ram_available_mb": 41814.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20979.8, "ram_available_mb": 41861.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.6}, "power_stats": {"power_gpu_soc_mean_watts": 19.754, "power_cpu_cv_mean_watts": 1.81, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 71.6, "power_watts_avg": 19.754, "energy_joules_est": 209.62, "duration_seconds": 10.612, "sample_count": 90}, "timestamp": "2026-01-26T14:30:48.996628"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6890.356, "latencies_ms": [6890.356], "images_per_second": 0.145, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "In the image, a man and a child are skiing on a snowy mountain. The man is standing upright while the child is skiing down the slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20979.8, "ram_available_mb": 41861.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20986.4, "ram_available_mb": 41854.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.741}, "power_stats": {"power_gpu_soc_mean_watts": 21.999, "power_cpu_cv_mean_watts": 1.456, "power_sys_5v0_mean_watts": 8.572, "gpu_utilization_percent_mean": 74.741, "power_watts_avg": 21.999, "energy_joules_est": 151.59, "duration_seconds": 6.891, "sample_count": 58}, "timestamp": "2026-01-26T14:30:57.908127"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7656.496, "latencies_ms": [7656.496], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image captures a bright, sunny day on a snowy mountain slope with clear blue skies. The snow appears fresh and powdery, indicating recent snowfall or good weather conditions for skiing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20986.4, "ram_available_mb": 41854.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.4, "ram_used_mb": 21023.3, "ram_available_mb": 41817.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.831}, "power_stats": {"power_gpu_soc_mean_watts": 21.115, "power_cpu_cv_mean_watts": 1.853, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 73.831, "power_watts_avg": 21.115, "energy_joules_est": 161.68, "duration_seconds": 7.657, "sample_count": 65}, "timestamp": "2026-01-26T14:31:07.599928"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11623.083, "latencies_ms": [11623.083], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a pair of feet wearing black sandals, standing on a wooden floor. The feet are positioned near a collection of cell phones, which are scattered around the area. There are three cell phones in total, with one located to the left of the feet and two others on the right side. The arrangement of the phones and the feet creates an interesting scene, possibly", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20962.4, "ram_available_mb": 41878.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21014.9, "ram_available_mb": 41826.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.899}, "power_stats": {"power_gpu_soc_mean_watts": 19.305, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.664, "gpu_utilization_percent_mean": 69.899, "power_watts_avg": 19.305, "energy_joules_est": 224.4, "duration_seconds": 11.624, "sample_count": 99}, "timestamp": "2026-01-26T14:31:21.256213"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9951.259, "latencies_ms": [9951.259], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "- Cell phone: 3\n- Battery: 1\n- Remote control: 1\n- Flip phone: 1\n- Footwear: 2\n- Toes: 10\n- Toes: 10\n- Toes: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.4, "ram_available_mb": 41879.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21019.4, "ram_available_mb": 41821.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.179}, "power_stats": {"power_gpu_soc_mean_watts": 20.163, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 71.179, "power_watts_avg": 20.163, "energy_joules_est": 200.66, "duration_seconds": 9.952, "sample_count": 84}, "timestamp": "2026-01-26T14:31:33.231795"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11598.19, "latencies_ms": [11598.19], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two feet wearing black sandals with white nail polish. Behind the feet, on the wooden surface, there are three cell phones. The smallest phone is to the left, the middle one is in the center, and the largest one is to the right. Additionally, there is a black phone case and a disassembled phone with its", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21019.4, "ram_available_mb": 41821.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21041.3, "ram_available_mb": 41799.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.98}, "power_stats": {"power_gpu_soc_mean_watts": 19.317, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.677, "gpu_utilization_percent_mean": 69.98, "power_watts_avg": 19.317, "energy_joules_est": 224.05, "duration_seconds": 11.599, "sample_count": 99}, "timestamp": "2026-01-26T14:31:46.867058"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8930.191, "latencies_ms": [8930.191], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a wooden table with two feet wearing black sandals, and three cell phones scattered across the surface. The person's feet are positioned on either side of the table, with one foot on the left and the other on the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21041.3, "ram_available_mb": 41799.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21042.3, "ram_available_mb": 41798.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.539}, "power_stats": {"power_gpu_soc_mean_watts": 20.574, "power_cpu_cv_mean_watts": 1.695, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 72.539, "power_watts_avg": 20.574, "energy_joules_est": 183.74, "duration_seconds": 8.931, "sample_count": 76}, "timestamp": "2026-01-26T14:31:57.827876"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7107.088, "latencies_ms": [7107.088], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image features a wooden surface with a pair of black sandals, a broken cell phone, and a small black case. The lighting is natural and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20988.9, "ram_available_mb": 41852.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21075.0, "ram_available_mb": 41765.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.183}, "power_stats": {"power_gpu_soc_mean_watts": 21.326, "power_cpu_cv_mean_watts": 1.587, "power_sys_5v0_mean_watts": 8.668, "gpu_utilization_percent_mean": 73.183, "power_watts_avg": 21.326, "energy_joules_est": 151.58, "duration_seconds": 7.108, "sample_count": 60}, "timestamp": "2026-01-26T14:32:06.998637"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12336.738, "latencies_ms": [12336.738], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a breathtaking view of the Houses of Parliament and the Elizabeth Tower, also known as Big Ben, in London, England. The iconic clock tower stands tall in the background, its golden facade gleaming under the soft light of the setting sun. The Elizabeth Tower, a symbol of British history and culture, is a prominent feature of the city's sky", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 20962.8, "ram_available_mb": 41878.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21035.3, "ram_available_mb": 41805.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.736}, "power_stats": {"power_gpu_soc_mean_watts": 21.384, "power_cpu_cv_mean_watts": 1.82, "power_sys_5v0_mean_watts": 8.863, "gpu_utilization_percent_mean": 72.736, "power_watts_avg": 21.384, "energy_joules_est": 263.82, "duration_seconds": 12.337, "sample_count": 106}, "timestamp": "2026-01-26T14:32:21.381438"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 12628.328, "latencies_ms": [12628.328], "images_per_second": 0.079, "prompt_tokens": 39, "response_tokens_est": 80, "n_tiles": 16, "output_text": "- Boats: 3\n\n- Buildings: 2\n\n- Windows: Several, exact count not discernible\n\n- Trees: Several, exact count not discernible\n\n- Sky: Overcast, no specific count\n\n- Water: Several, exact count not discernible\n\n- Life preservers: 2 visible on the boat", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20956.5, "ram_available_mb": 41884.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21035.1, "ram_available_mb": 41805.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.196}, "power_stats": {"power_gpu_soc_mean_watts": 21.502, "power_cpu_cv_mean_watts": 1.769, "power_sys_5v0_mean_watts": 8.797, "gpu_utilization_percent_mean": 73.196, "power_watts_avg": 21.502, "energy_joules_est": 271.55, "duration_seconds": 12.629, "sample_count": 107}, "timestamp": "2026-01-26T14:32:36.026796"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12176.716, "latencies_ms": [12176.716], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a boat named 'PACE' on the left side of the image, closer to the viewer than the main buildings. The large, ornate building known as the Houses of Parliament is in the background, with the clock tower Big Ben visible on the right side. The river Thames separates the boats in the foreground from the grand architecture of the", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21035.1, "ram_available_mb": 41805.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21054.3, "ram_available_mb": 41786.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.904}, "power_stats": {"power_gpu_soc_mean_watts": 21.394, "power_cpu_cv_mean_watts": 1.828, "power_sys_5v0_mean_watts": 8.91, "gpu_utilization_percent_mean": 71.904, "power_watts_avg": 21.394, "energy_joules_est": 260.52, "duration_seconds": 12.177, "sample_count": 104}, "timestamp": "2026-01-26T14:32:50.265274"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 12629.755, "latencies_ms": [12629.755], "images_per_second": 0.079, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene evening view of the Palace of Westminster, the official residence of the British Prime Minister, located on the banks of the River Thames in London. The iconic clock tower Big Ben is illuminated, casting a warm glow on the historic building, while the surrounding area is bustling with boats, including a ferry and a barge,", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21054.3, "ram_available_mb": 41786.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20953.2, "ram_available_mb": 41887.7, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.509}, "power_stats": {"power_gpu_soc_mean_watts": 21.475, "power_cpu_cv_mean_watts": 1.778, "power_sys_5v0_mean_watts": 8.805, "gpu_utilization_percent_mean": 72.509, "power_watts_avg": 21.475, "energy_joules_est": 271.24, "duration_seconds": 12.63, "sample_count": 108}, "timestamp": "2026-01-26T14:33:04.955718"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8617.034, "latencies_ms": [8617.034], "images_per_second": 0.116, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The sky is overcast with a mix of blue and gray hues, suggesting a cloudy day. The buildings are illuminated with warm yellow and orange lights, creating a contrast against the cooler tones of the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20953.2, "ram_available_mb": 41887.7, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20961.7, "ram_available_mb": 41879.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.986}, "power_stats": {"power_gpu_soc_mean_watts": 22.602, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.873, "gpu_utilization_percent_mean": 74.986, "power_watts_avg": 22.602, "energy_joules_est": 194.78, "duration_seconds": 8.618, "sample_count": 72}, "timestamp": "2026-01-26T14:33:15.614330"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11606.553, "latencies_ms": [11606.553], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a spacious living room with a variety of furniture and decorations. There are two couches, one on the left side of the room and another on the right side. A TV is placed in the center of the room, and a fan is located above it. The room features a large window, providing ample natural light.\n\nSeveral", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20961.7, "ram_available_mb": 41879.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21020.2, "ram_available_mb": 41820.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.04}, "power_stats": {"power_gpu_soc_mean_watts": 19.353, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 70.04, "power_watts_avg": 19.353, "energy_joules_est": 224.63, "duration_seconds": 11.607, "sample_count": 100}, "timestamp": "2026-01-26T14:33:29.243410"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8697.141, "latencies_ms": [8697.141], "images_per_second": 0.115, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "- Chair: 2\n- Couch: 2\n- Sofa: 2\n- Table: 3\n- Plant: 4\n- Television: 2\n- Fan: 1\n- Mirror: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21020.2, "ram_available_mb": 41820.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21034.6, "ram_available_mb": 41806.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.973}, "power_stats": {"power_gpu_soc_mean_watts": 20.658, "power_cpu_cv_mean_watts": 1.671, "power_sys_5v0_mean_watts": 8.616, "gpu_utilization_percent_mean": 72.973, "power_watts_avg": 20.658, "energy_joules_est": 179.68, "duration_seconds": 8.698, "sample_count": 74}, "timestamp": "2026-01-26T14:33:39.955719"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11599.962, "latencies_ms": [11599.962], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a living room with a variety of furniture, including a red armchair, a green couch, and a black leather couch. The furniture is arranged in a way that creates a cozy and inviting space. In the background, there are large windows that let in natural light and provide a view of the outside. The", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20954.4, "ram_available_mb": 41886.4, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21014.6, "ram_available_mb": 41826.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.041}, "power_stats": {"power_gpu_soc_mean_watts": 19.329, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.709, "gpu_utilization_percent_mean": 70.041, "power_watts_avg": 19.329, "energy_joules_est": 224.23, "duration_seconds": 11.601, "sample_count": 98}, "timestamp": "2026-01-26T14:33:53.570941"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8455.246, "latencies_ms": [8455.246], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a cozy living room with a variety of furniture, including a couch, chairs, and a coffee table. There are multiple potted plants on the windowsill, adding a touch of greenery to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21014.6, "ram_available_mb": 41826.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21040.6, "ram_available_mb": 41800.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.931}, "power_stats": {"power_gpu_soc_mean_watts": 20.901, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.603, "gpu_utilization_percent_mean": 71.931, "power_watts_avg": 20.901, "energy_joules_est": 176.74, "duration_seconds": 8.456, "sample_count": 72}, "timestamp": "2026-01-26T14:34:04.055244"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10651.558, "latencies_ms": [10651.558], "images_per_second": 0.094, "prompt_tokens": 36, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The room is well-lit with natural light coming through the large windows, and the wooden flooring adds a warm tone to the space. There are several pieces of furniture, including a red leather chair, a green couch, and a black leather couch, as well as a ceiling fan and a flat-screen TV.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20960.3, "ram_available_mb": 41880.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21020.4, "ram_available_mb": 41820.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.078}, "power_stats": {"power_gpu_soc_mean_watts": 19.47, "power_cpu_cv_mean_watts": 1.85, "power_sys_5v0_mean_watts": 8.687, "gpu_utilization_percent_mean": 71.078, "power_watts_avg": 19.47, "energy_joules_est": 207.4, "duration_seconds": 10.652, "sample_count": 90}, "timestamp": "2026-01-26T14:34:16.740595"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11558.463, "latencies_ms": [11558.463], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene on a city street. Dominating the foreground is a red pole, which supports two black parking meters. The pole is positioned on the left side of the image, standing upright and casting a shadow on the ground. \n\nBehind the pole, a brick building with large windows can be seen. The building's facade is ad", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 20958.4, "ram_available_mb": 41882.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21014.5, "ram_available_mb": 41826.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.01}, "power_stats": {"power_gpu_soc_mean_watts": 19.334, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 70.01, "power_watts_avg": 19.334, "energy_joules_est": 223.48, "duration_seconds": 11.559, "sample_count": 99}, "timestamp": "2026-01-26T14:34:30.325970"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7814.431, "latencies_ms": [7814.431], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "parking meter: 2, red pole: 1, building: 1, window: 4, sign: 1, bush: 1, leaf: 1, bench: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21014.5, "ram_available_mb": 41826.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21033.2, "ram_available_mb": 41807.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.167}, "power_stats": {"power_gpu_soc_mean_watts": 21.182, "power_cpu_cv_mean_watts": 1.576, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 74.167, "power_watts_avg": 21.182, "energy_joules_est": 165.54, "duration_seconds": 7.815, "sample_count": 66}, "timestamp": "2026-01-26T14:34:40.156421"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11666.327, "latencies_ms": [11666.327], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a red pole with a circular object attached to it, which is positioned in front of two parking meters. The parking meters are located behind the red pole, closer to the background, and are situated in front of a building with a large window displaying the text '40 YEARS OF SAVING LIVES'. The ground in the for", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21033.2, "ram_available_mb": 41807.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20991.8, "ram_available_mb": 41849.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.02}, "power_stats": {"power_gpu_soc_mean_watts": 19.357, "power_cpu_cv_mean_watts": 1.877, "power_sys_5v0_mean_watts": 8.669, "gpu_utilization_percent_mean": 71.02, "power_watts_avg": 19.357, "energy_joules_est": 225.84, "duration_seconds": 11.667, "sample_count": 100}, "timestamp": "2026-01-26T14:34:53.842509"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8034.165, "latencies_ms": [8034.165], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a red parking meter on a sidewalk in front of a building with a sign that reads \"40 years of saving lives.\" The parking meter is empty and there are no cars parked nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20991.8, "ram_available_mb": 41849.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20980.6, "ram_available_mb": 41860.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.147}, "power_stats": {"power_gpu_soc_mean_watts": 21.007, "power_cpu_cv_mean_watts": 1.612, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 74.147, "power_watts_avg": 21.007, "energy_joules_est": 168.79, "duration_seconds": 8.035, "sample_count": 68}, "timestamp": "2026-01-26T14:35:03.924554"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8310.954, "latencies_ms": [8310.954], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image features a red metal pole with a circular design at the top, supporting two black parking meters. The background shows a building with large windows and a sign that reads \"40 YEARS OF SAVING LIVES.\"", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20980.6, "ram_available_mb": 41860.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 21018.2, "ram_available_mb": 41822.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.817}, "power_stats": {"power_gpu_soc_mean_watts": 20.751, "power_cpu_cv_mean_watts": 1.837, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 73.817, "power_watts_avg": 20.751, "energy_joules_est": 172.47, "duration_seconds": 8.312, "sample_count": 71}, "timestamp": "2026-01-26T14:35:14.258296"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11591.266, "latencies_ms": [11591.266], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a man and a woman sitting on a couch in a living room, watching television. They are surrounded by a cozy atmosphere, with a dining table nearby. On the table, there are several cups, a cell phone, and a bowl. A vase is also present on the table, adding to the ambiance.\n\nIn the room, there are", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20964.9, "ram_available_mb": 41876.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21022.8, "ram_available_mb": 41818.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.6}, "power_stats": {"power_gpu_soc_mean_watts": 19.352, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.692, "gpu_utilization_percent_mean": 68.6, "power_watts_avg": 19.352, "energy_joules_est": 224.33, "duration_seconds": 11.592, "sample_count": 100}, "timestamp": "2026-01-26T14:35:27.897618"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9127.503, "latencies_ms": [9127.503], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Television: 1\n- Couch: 1\n- Cup: 2\n- Snacks: 1\n- Candle: 1\n- Cell phone: 1\n- Glass: 2\n- Poinsettia: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21022.8, "ram_available_mb": 41818.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21024.3, "ram_available_mb": 41816.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.104}, "power_stats": {"power_gpu_soc_mean_watts": 20.513, "power_cpu_cv_mean_watts": 1.705, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 72.104, "power_watts_avg": 20.513, "energy_joules_est": 187.25, "duration_seconds": 9.128, "sample_count": 77}, "timestamp": "2026-01-26T14:35:39.050213"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11589.046, "latencies_ms": [11589.046], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a coffee table with various items on it, including a red poinsettia plant, a jar of candy, and a plate of snacks. Behind the coffee table, there is a couple sitting on a couch, with the man on the right side and the woman on the left. In the background, there is a wooden entertainment center with", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20969.4, "ram_available_mb": 41871.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21022.7, "ram_available_mb": 41818.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.43}, "power_stats": {"power_gpu_soc_mean_watts": 19.312, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.706, "gpu_utilization_percent_mean": 70.43, "power_watts_avg": 19.312, "energy_joules_est": 223.82, "duration_seconds": 11.59, "sample_count": 100}, "timestamp": "2026-01-26T14:35:52.661521"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8687.692, "latencies_ms": [8687.692], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A couple is sitting on a couch in a cozy living room, watching television and enjoying a snack of chips. The room is decorated with a Christmas tree and other festive decorations, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21022.7, "ram_available_mb": 41818.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21056.9, "ram_available_mb": 41784.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.712}, "power_stats": {"power_gpu_soc_mean_watts": 20.781, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 71.712, "power_watts_avg": 20.781, "energy_joules_est": 180.55, "duration_seconds": 8.688, "sample_count": 73}, "timestamp": "2026-01-26T14:36:03.396954"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8257.754, "latencies_ms": [8257.754], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a cozy indoor setting with warm lighting, primarily from the television screen and the ambient lighting. The colors are muted with earth tones, and the materials visible include wood furniture and fabric couches.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21056.9, "ram_available_mb": 41784.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21060.1, "ram_available_mb": 41780.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.521}, "power_stats": {"power_gpu_soc_mean_watts": 20.406, "power_cpu_cv_mean_watts": 1.679, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 72.521, "power_watts_avg": 20.406, "energy_joules_est": 168.52, "duration_seconds": 8.258, "sample_count": 71}, "timestamp": "2026-01-26T14:36:13.710104"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11583.679, "latencies_ms": [11583.679], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person's hand is holding a white remote control, which is positioned in front of a white toilet. The remote control has several buttons, including a power button, a volume control, and a few other buttons. The toilet is situated in a bathroom, and the person appears to be using the remote control to operate the toilet. The", "error": null, "sys_before": {"cpu_percent": 29.2, "ram_used_mb": 21062.3, "ram_available_mb": 41778.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21060.1, "ram_available_mb": 41780.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.879}, "power_stats": {"power_gpu_soc_mean_watts": 19.306, "power_cpu_cv_mean_watts": 1.92, "power_sys_5v0_mean_watts": 8.687, "gpu_utilization_percent_mean": 68.879, "power_watts_avg": 19.306, "energy_joules_est": 223.65, "duration_seconds": 11.584, "sample_count": 99}, "timestamp": "2026-01-26T14:36:27.354022"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7917.963, "latencies_ms": [7917.963], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "toilet: 1, remote control: 1, floor: 1, wall: 1, person: 1, hand: 1, bathroom: 1, control panel: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20962.8, "ram_available_mb": 41878.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21022.6, "ram_available_mb": 41818.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.836}, "power_stats": {"power_gpu_soc_mean_watts": 21.088, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 73.836, "power_watts_avg": 21.088, "energy_joules_est": 166.99, "duration_seconds": 7.919, "sample_count": 67}, "timestamp": "2026-01-26T14:36:37.301429"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11585.467, "latencies_ms": [11585.467], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the foreground, a person's hand is holding a white device with multiple buttons, positioned near the center of the image. Behind this, there is a white toilet with a control panel on its side, and further back, a white wall and a white floor can be seen. The person appears to be interacting with the toilet's control panel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21022.6, "ram_available_mb": 41818.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21044.4, "ram_available_mb": 41796.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.596}, "power_stats": {"power_gpu_soc_mean_watts": 19.369, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 70.596, "power_watts_avg": 19.369, "energy_joules_est": 224.41, "duration_seconds": 11.586, "sample_count": 99}, "timestamp": "2026-01-26T14:36:50.911366"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8360.054, "latencies_ms": [8360.054], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A person is holding a remote control in front of a toilet, which is located in a small, enclosed space with white walls. The remote control has multiple buttons, and the person appears to be adjusting or testing it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21044.4, "ram_available_mb": 41796.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21045.8, "ram_available_mb": 41795.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.181}, "power_stats": {"power_gpu_soc_mean_watts": 20.88, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 8.598, "gpu_utilization_percent_mean": 73.181, "power_watts_avg": 20.88, "energy_joules_est": 174.57, "duration_seconds": 8.361, "sample_count": 72}, "timestamp": "2026-01-26T14:37:01.304867"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8488.205, "latencies_ms": [8488.205], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a person's hand holding a white electronic device with multiple buttons, which is being inserted into a toilet. The toilet is white and has a modern design with a closed lid and a visible control panel on the side.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21045.8, "ram_available_mb": 41795.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21067.0, "ram_available_mb": 41773.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.11}, "power_stats": {"power_gpu_soc_mean_watts": 20.294, "power_cpu_cv_mean_watts": 1.716, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 72.11, "power_watts_avg": 20.294, "energy_joules_est": 172.27, "duration_seconds": 8.489, "sample_count": 73}, "timestamp": "2026-01-26T14:37:11.818669"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11587.033, "latencies_ms": [11587.033], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a thrilling moment of a snowboarder performing a jump on a snowy hill. The snowboarder is in mid-air, skillfully maneuvering their snowboard as they soar above the snow. \n\nThere are several people watching the snowboarder's performance from various positions on the hill. Some of them are closer to the snowboard", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20971.7, "ram_available_mb": 41869.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21023.8, "ram_available_mb": 41817.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.337}, "power_stats": {"power_gpu_soc_mean_watts": 19.47, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 70.337, "power_watts_avg": 19.47, "energy_joules_est": 225.61, "duration_seconds": 11.588, "sample_count": 98}, "timestamp": "2026-01-26T14:37:25.434798"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8117.739, "latencies_ms": [8117.739], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "people: 10, snowboarder: 1, snowboard: 1, snow: 1, hill: 1, red flag: 2, yellow flag: 1, blue sky: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21023.8, "ram_available_mb": 41817.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21040.6, "ram_available_mb": 41800.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.043}, "power_stats": {"power_gpu_soc_mean_watts": 21.04, "power_cpu_cv_mean_watts": 1.63, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 73.043, "power_watts_avg": 21.04, "energy_joules_est": 170.81, "duration_seconds": 8.118, "sample_count": 69}, "timestamp": "2026-01-26T14:37:35.582429"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11576.865, "latencies_ms": [11576.865], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a snow-covered slope with a ramp and a group of people standing on the left side. In the background, there is a clear blue sky. In the center of the image, a snowboarder is performing a trick in the air, with another snowboarder in the background also performing a trick. The snowboarder in the air is closer", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20961.2, "ram_available_mb": 41879.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21022.0, "ram_available_mb": 41818.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.96}, "power_stats": {"power_gpu_soc_mean_watts": 19.269, "power_cpu_cv_mean_watts": 1.906, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 69.96, "power_watts_avg": 19.269, "energy_joules_est": 223.09, "duration_seconds": 11.577, "sample_count": 101}, "timestamp": "2026-01-26T14:37:49.198660"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8234.348, "latencies_ms": [8234.348], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image captures a thrilling moment at a snowboarding event where a snowboarder is performing a daring trick in mid-air. Spectators are gathered on the snowy slopes, watching the action unfold.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 21022.0, "ram_available_mb": 41818.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21001.1, "ram_available_mb": 41839.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.3}, "power_stats": {"power_gpu_soc_mean_watts": 21.004, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 72.3, "power_watts_avg": 21.004, "energy_joules_est": 172.97, "duration_seconds": 8.235, "sample_count": 70}, "timestamp": "2026-01-26T14:37:59.469646"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10636.175, "latencies_ms": [10636.175], "images_per_second": 0.094, "prompt_tokens": 36, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The image captures a snowboarder in mid-air performing a trick against a clear blue sky. The snowboarder is wearing a red and white outfit, and the snowboard is white with red and black stripes. The snow-covered ground and the bright sunlight create a vibrant contrast with the blue sky.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21001.1, "ram_available_mb": 41839.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21066.0, "ram_available_mb": 41774.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.978}, "power_stats": {"power_gpu_soc_mean_watts": 19.632, "power_cpu_cv_mean_watts": 1.86, "power_sys_5v0_mean_watts": 8.675, "gpu_utilization_percent_mean": 70.978, "power_watts_avg": 19.632, "energy_joules_est": 208.82, "duration_seconds": 10.637, "sample_count": 91}, "timestamp": "2026-01-26T14:38:12.152713"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11606.178, "latencies_ms": [11606.178], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a small, well-lit room with a desk and a computer chair. The desk is equipped with a laptop, a keyboard, and a mouse, creating a comfortable workspace. A lamp is placed on the desk, providing additional lighting. The room also has a potted plant, adding a touch of greenery to the space.\n\nIn the room", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20959.6, "ram_available_mb": 41881.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21009.0, "ram_available_mb": 41831.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.737}, "power_stats": {"power_gpu_soc_mean_watts": 19.326, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.662, "gpu_utilization_percent_mean": 69.737, "power_watts_avg": 19.326, "energy_joules_est": 224.31, "duration_seconds": 11.607, "sample_count": 99}, "timestamp": "2026-01-26T14:38:25.798128"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9059.076, "latencies_ms": [9059.076], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Desk: 1\n- Chair: 1\n- Laptop: 1\n- Keyboard: 1\n- Computer mouse: 1\n- Plant: 1\n- Cable: 1\n- Floor lamp: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20956.6, "ram_available_mb": 41884.3, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21013.5, "ram_available_mb": 41827.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.961}, "power_stats": {"power_gpu_soc_mean_watts": 20.477, "power_cpu_cv_mean_watts": 1.71, "power_sys_5v0_mean_watts": 8.614, "gpu_utilization_percent_mean": 71.961, "power_watts_avg": 20.477, "energy_joules_est": 185.51, "duration_seconds": 9.06, "sample_count": 77}, "timestamp": "2026-01-26T14:38:36.909825"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10560.72, "latencies_ms": [10560.72], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there is a black office chair positioned in front of a desk with a laptop, a monitor, a keyboard, and a mouse. The desk is situated against a wall with a bookshelf in the background. To the right of the desk, there is a potted plant on a small table.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20960.0, "ram_available_mb": 41880.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20964.4, "ram_available_mb": 41876.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.056}, "power_stats": {"power_gpu_soc_mean_watts": 19.612, "power_cpu_cv_mean_watts": 1.841, "power_sys_5v0_mean_watts": 8.699, "gpu_utilization_percent_mean": 71.056, "power_watts_avg": 19.612, "energy_joules_est": 207.13, "duration_seconds": 10.561, "sample_count": 90}, "timestamp": "2026-01-26T14:38:49.493244"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9176.173, "latencies_ms": [9176.173], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image depicts a well-lit room with a desk and chair set up for work or study. On the desk, there is a laptop, a monitor, a keyboard, and a mouse, along with some books and a lamp providing additional lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20964.4, "ram_available_mb": 41876.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21020.3, "ram_available_mb": 41820.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.205}, "power_stats": {"power_gpu_soc_mean_watts": 20.32, "power_cpu_cv_mean_watts": 1.708, "power_sys_5v0_mean_watts": 8.609, "gpu_utilization_percent_mean": 72.205, "power_watts_avg": 20.32, "energy_joules_est": 186.47, "duration_seconds": 9.177, "sample_count": 78}, "timestamp": "2026-01-26T14:39:00.726358"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9147.172, "latencies_ms": [9147.172], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The room is lit by a warm yellow lamp with a black shade, casting a soft glow on the wooden desk and the black office chair. The carpeted floor has a beige color, and there is a white sofa on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 21020.3, "ram_available_mb": 41820.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 21038.4, "ram_available_mb": 41802.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.558}, "power_stats": {"power_gpu_soc_mean_watts": 20.268, "power_cpu_cv_mean_watts": 1.85, "power_sys_5v0_mean_watts": 8.673, "gpu_utilization_percent_mean": 70.558, "power_watts_avg": 20.268, "energy_joules_est": 185.41, "duration_seconds": 9.148, "sample_count": 77}, "timestamp": "2026-01-26T14:39:11.926338"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11582.119, "latencies_ms": [11582.119], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is riding a motorcycle on a gravel road that winds through a mountainous landscape. The rider is wearing a black helmet and is positioned in the center of the image, with the motorcycle slightly to the left. The road is surrounded by green hills and trees, creating a scenic backdrop. The sky above is blue with a", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20983.1, "ram_available_mb": 41857.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.6, "ram_used_mb": 21045.1, "ram_available_mb": 41795.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.899}, "power_stats": {"power_gpu_soc_mean_watts": 19.415, "power_cpu_cv_mean_watts": 2.236, "power_sys_5v0_mean_watts": 8.724, "gpu_utilization_percent_mean": 68.899, "power_watts_avg": 19.415, "energy_joules_est": 224.88, "duration_seconds": 11.583, "sample_count": 99}, "timestamp": "2026-01-26T14:39:25.551240"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8954.92, "latencies_ms": [8954.92], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "motorcycle: 2\n\nmountain: 1\n\ncloud: 10\n\npath: 1\n\nrider: 1\n\nbackpack: 1\n\ntire: 1\n\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.0, "ram_available_mb": 41867.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21034.3, "ram_available_mb": 41806.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.987}, "power_stats": {"power_gpu_soc_mean_watts": 20.352, "power_cpu_cv_mean_watts": 1.699, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 72.987, "power_watts_avg": 20.352, "energy_joules_est": 182.26, "duration_seconds": 8.956, "sample_count": 77}, "timestamp": "2026-01-26T14:39:36.534388"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11398.82, "latencies_ms": [11398.82], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a motorcycle with a rider on the left side of the image, positioned on a gravel path that leads into the distance. The background features a vast landscape of rolling hills and a clear blue sky with scattered clouds. The motorcycle and rider are near the viewer, while the hills and sky are far in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21034.3, "ram_available_mb": 41806.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20965.1, "ram_available_mb": 41875.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.092}, "power_stats": {"power_gpu_soc_mean_watts": 19.152, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 70.092, "power_watts_avg": 19.152, "energy_joules_est": 218.32, "duration_seconds": 11.399, "sample_count": 98}, "timestamp": "2026-01-26T14:39:49.973436"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7465.762, "latencies_ms": [7465.762], "images_per_second": 0.134, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A person is riding a motorcycle on a gravel road that winds through a hilly landscape. The sky is blue with some clouds, and the surrounding hills are covered in greenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20965.1, "ram_available_mb": 41875.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20976.7, "ram_available_mb": 41864.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.476}, "power_stats": {"power_gpu_soc_mean_watts": 21.266, "power_cpu_cv_mean_watts": 1.55, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 74.476, "power_watts_avg": 21.266, "energy_joules_est": 158.78, "duration_seconds": 7.466, "sample_count": 63}, "timestamp": "2026-01-26T14:39:59.455281"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8147.09, "latencies_ms": [8147.09], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a clear blue sky with scattered white clouds, and the lighting suggests it is a sunny day. The terrain is a mix of green grassy hills and a gravel path, indicating a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20976.7, "ram_available_mb": 41864.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21034.9, "ram_available_mb": 41806.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.826}, "power_stats": {"power_gpu_soc_mean_watts": 20.518, "power_cpu_cv_mean_watts": 1.664, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 72.826, "power_watts_avg": 20.518, "energy_joules_est": 167.17, "duration_seconds": 8.148, "sample_count": 69}, "timestamp": "2026-01-26T14:40:09.657982"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 8478.085, "latencies_ms": [8478.085], "images_per_second": 0.118, "prompt_tokens": 24, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a well-lit kitchen with wooden cabinets and white appliances. There's a dining table with a bowl of oranges and bananas on it. The kitchen has a modern design with a clean and organized appearance.", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 21034.9, "ram_available_mb": 41806.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21035.4, "ram_available_mb": 41805.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.197}, "power_stats": {"power_gpu_soc_mean_watts": 20.702, "power_cpu_cv_mean_watts": 1.702, "power_sys_5v0_mean_watts": 8.68, "gpu_utilization_percent_mean": 72.197, "power_watts_avg": 20.702, "energy_joules_est": 175.53, "duration_seconds": 8.479, "sample_count": 71}, "timestamp": "2026-01-26T14:40:20.171562"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8255.666, "latencies_ms": [8255.666], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "table: 1\noranges: 3\nbananas: 2\nfridge: 1\ndoor: 2\nwindow: 1\nlight fixture: 1\ncabinet: 1\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.8, "ram_available_mb": 41867.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20973.9, "ram_available_mb": 41867.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.743}, "power_stats": {"power_gpu_soc_mean_watts": 20.924, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 8.591, "gpu_utilization_percent_mean": 72.743, "power_watts_avg": 20.924, "energy_joules_est": 172.76, "duration_seconds": 8.256, "sample_count": 70}, "timestamp": "2026-01-26T14:40:30.460456"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11606.774, "latencies_ms": [11606.774], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a wooden dining table with a bowl of oranges and bananas placed on it, indicating it is near the viewer. The kitchen area is in the background, with wooden cabinets and a white refrigerator, suggesting it is further away from the viewer. The living area is to the left, with a white door and a beige", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.9, "ram_available_mb": 41867.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20970.8, "ram_available_mb": 41870.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.57}, "power_stats": {"power_gpu_soc_mean_watts": 19.155, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.707, "gpu_utilization_percent_mean": 70.57, "power_watts_avg": 19.155, "energy_joules_est": 222.34, "duration_seconds": 11.607, "sample_count": 100}, "timestamp": "2026-01-26T14:40:44.089821"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8228.507, "latencies_ms": [8228.507], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image depicts a well-lit, modern kitchen with wooden cabinets and white appliances. A bowl of oranges and bananas is placed on the kitchen island, suggesting a focus on healthy eating.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20970.8, "ram_available_mb": 41870.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21022.8, "ram_available_mb": 41818.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.629}, "power_stats": {"power_gpu_soc_mean_watts": 21.025, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 8.587, "gpu_utilization_percent_mean": 72.629, "power_watts_avg": 21.025, "energy_joules_est": 173.02, "duration_seconds": 8.229, "sample_count": 70}, "timestamp": "2026-01-26T14:40:54.344769"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7892.931, "latencies_ms": [7892.931], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The kitchen features wooden cabinets and white appliances, creating a warm and modern aesthetic. The lighting is bright and natural, coming from the ceiling lights and the sunlight streaming in through the window.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21022.8, "ram_available_mb": 41818.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21045.0, "ram_available_mb": 41795.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.254}, "power_stats": {"power_gpu_soc_mean_watts": 20.868, "power_cpu_cv_mean_watts": 1.648, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 72.254, "power_watts_avg": 20.868, "energy_joules_est": 164.72, "duration_seconds": 7.894, "sample_count": 67}, "timestamp": "2026-01-26T14:41:04.296551"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11648.309, "latencies_ms": [11648.309], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a female tennis player is in the midst of a powerful swing, her body poised to strike the ball. She is dressed in a white tank top and a black skirt, her white shoes contrasting with the green of the tennis court. The court itself is a vibrant green, marked with crisp white lines that define the boundaries of the game. \n", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 21045.0, "ram_available_mb": 41795.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21001.4, "ram_available_mb": 41839.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.46}, "power_stats": {"power_gpu_soc_mean_watts": 19.209, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.693, "gpu_utilization_percent_mean": 70.46, "power_watts_avg": 19.209, "energy_joules_est": 223.76, "duration_seconds": 11.649, "sample_count": 100}, "timestamp": "2026-01-26T14:41:18.006379"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8594.134, "latencies_ms": [8594.134], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "ball: 1, tennis racket: 1, tennis player: 1, tennis outfit: 1, tennis court: 1, tennis ball: 1, tennis racket: 1, tennis outfit: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21001.4, "ram_available_mb": 41839.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21019.1, "ram_available_mb": 41821.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.74}, "power_stats": {"power_gpu_soc_mean_watts": 20.726, "power_cpu_cv_mean_watts": 1.655, "power_sys_5v0_mean_watts": 8.592, "gpu_utilization_percent_mean": 71.74, "power_watts_avg": 20.726, "energy_joules_est": 178.14, "duration_seconds": 8.595, "sample_count": 73}, "timestamp": "2026-01-26T14:41:28.643917"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11578.674, "latencies_ms": [11578.674], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a tennis player is positioned on the right side of the image, reaching up to hit a tennis ball that is near the top left corner of the image. The player is standing on a tennis court with a blue 'POLO' logo visible in the background on the left side. Another person can be seen in the background, standing behind the player, near the edge of", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21019.1, "ram_available_mb": 41821.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20986.0, "ram_available_mb": 41854.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.98}, "power_stats": {"power_gpu_soc_mean_watts": 19.367, "power_cpu_cv_mean_watts": 1.907, "power_sys_5v0_mean_watts": 8.727, "gpu_utilization_percent_mean": 69.98, "power_watts_avg": 19.367, "energy_joules_est": 224.26, "duration_seconds": 11.579, "sample_count": 98}, "timestamp": "2026-01-26T14:41:42.246282"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6754.437, "latencies_ms": [6754.437], "images_per_second": 0.148, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A female tennis player is in the middle of a serve on a tennis court, with a ball in the air and a \"POLO\" sign visible in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20986.0, "ram_available_mb": 41854.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20979.7, "ram_available_mb": 41861.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.491}, "power_stats": {"power_gpu_soc_mean_watts": 22.15, "power_cpu_cv_mean_watts": 1.467, "power_sys_5v0_mean_watts": 8.56, "gpu_utilization_percent_mean": 74.491, "power_watts_avg": 22.15, "energy_joules_est": 149.63, "duration_seconds": 6.755, "sample_count": 57}, "timestamp": "2026-01-26T14:41:51.034392"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7922.857, "latencies_ms": [7922.857], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The tennis player is wearing a white top and black skirt, and is in the process of serving the ball. The court is green with white lines, and there is a blue \"POLO\" sign on the ground.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20979.7, "ram_available_mb": 41861.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21026.6, "ram_available_mb": 41814.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.448}, "power_stats": {"power_gpu_soc_mean_watts": 20.604, "power_cpu_cv_mean_watts": 1.648, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 72.448, "power_watts_avg": 20.604, "energy_joules_est": 163.26, "duration_seconds": 7.924, "sample_count": 67}, "timestamp": "2026-01-26T14:42:01.004112"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12432.236, "latencies_ms": [12432.236], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene on a sunny day. Dominating the foreground is a red fire hydrant, its white cap gleaming under the sunlight. The hydrant is situated on a sidewalk, which extends into the distance, inviting pedestrians to take a leisurely stroll. \n\nAbove the hydrant, a yellow", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 21026.6, "ram_available_mb": 41814.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20960.8, "ram_available_mb": 41880.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11229.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.39, "power_cpu_cv_mean_watts": 1.795, "power_sys_5v0_mean_watts": 8.846, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 21.39, "energy_joules_est": 265.94, "duration_seconds": 12.433, "sample_count": 105}, "timestamp": "2026-01-26T14:42:15.467565"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8760.496, "latencies_ms": [8760.496], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "sign: 1, pedestrian: 2, tree: 1, building: 1, fire hydrant: 1, sidewalk: 1, grass: 1, car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20960.8, "ram_available_mb": 41880.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21007.0, "ram_available_mb": 41833.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11258.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.04}, "power_stats": {"power_gpu_soc_mean_watts": 22.899, "power_cpu_cv_mean_watts": 1.505, "power_sys_5v0_mean_watts": 8.777, "gpu_utilization_percent_mean": 77.04, "power_watts_avg": 22.899, "energy_joules_est": 200.62, "duration_seconds": 8.761, "sample_count": 75}, "timestamp": "2026-01-26T14:42:26.246560"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12237.266, "latencies_ms": [12237.266], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a red fire hydrant on the left side of the image, positioned on a sidewalk. Behind the hydrant, a pedestrian crossing sign is mounted on a pole to the right, indicating a crossing area for pedestrians. The background features a residential area with houses and trees, suggesting that the hydrant and sign are located in", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21007.0, "ram_available_mb": 41833.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20981.8, "ram_available_mb": 41859.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11268.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.792}, "power_stats": {"power_gpu_soc_mean_watts": 21.334, "power_cpu_cv_mean_watts": 1.831, "power_sys_5v0_mean_watts": 8.905, "gpu_utilization_percent_mean": 71.792, "power_watts_avg": 21.334, "energy_joules_est": 261.08, "duration_seconds": 12.238, "sample_count": 106}, "timestamp": "2026-01-26T14:42:40.544838"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8615.187, "latencies_ms": [8615.187], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A bright yellow pedestrian crossing sign is mounted on a pole next to a red fire hydrant on a sidewalk. A person is sitting on a stroller on the grassy area beside the sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20981.8, "ram_available_mb": 41859.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20980.6, "ram_available_mb": 41860.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11254.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.243}, "power_stats": {"power_gpu_soc_mean_watts": 23.016, "power_cpu_cv_mean_watts": 1.487, "power_sys_5v0_mean_watts": 8.768, "gpu_utilization_percent_mean": 77.243, "power_watts_avg": 23.016, "energy_joules_est": 198.3, "duration_seconds": 8.616, "sample_count": 74}, "timestamp": "2026-01-26T14:42:51.186221"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7589.989, "latencies_ms": [7589.989], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A bright yellow pedestrian crossing sign is mounted on a pole, with a red fire hydrant in the foreground. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20980.6, "ram_available_mb": 41860.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20985.8, "ram_available_mb": 41855.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11252.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.688}, "power_stats": {"power_gpu_soc_mean_watts": 23.264, "power_cpu_cv_mean_watts": 1.476, "power_sys_5v0_mean_watts": 8.837, "gpu_utilization_percent_mean": 76.688, "power_watts_avg": 23.264, "energy_joules_est": 176.59, "duration_seconds": 7.591, "sample_count": 64}, "timestamp": "2026-01-26T14:43:00.820397"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11599.774, "latencies_ms": [11599.774], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a small, compact bathroom bathed in a soft, natural light. Dominating the scene is a pristine white toilet, its lid closed, standing against a stark white wall. The toilet is positioned centrally in the image, drawing the viewer's attention. \n\nTo the left of the toilet, a white", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 20985.8, "ram_available_mb": 41855.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21049.0, "ram_available_mb": 41791.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.04}, "power_stats": {"power_gpu_soc_mean_watts": 19.335, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.685, "gpu_utilization_percent_mean": 70.04, "power_watts_avg": 19.335, "energy_joules_est": 224.29, "duration_seconds": 11.6, "sample_count": 100}, "timestamp": "2026-01-26T14:43:14.448210"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7998.468, "latencies_ms": [7998.468], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "toilet: 1, tank: 1, flush handle: 1, pipe: 1, wall: 1, floor: 1, trash can: 1, pedal: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21049.0, "ram_available_mb": 41791.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21040.8, "ram_available_mb": 41800.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.441}, "power_stats": {"power_gpu_soc_mean_watts": 21.2, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.606, "gpu_utilization_percent_mean": 73.441, "power_watts_avg": 21.2, "energy_joules_est": 169.58, "duration_seconds": 7.999, "sample_count": 68}, "timestamp": "2026-01-26T14:43:24.478787"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10294.11, "latencies_ms": [10294.11], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The toilet is located in the foreground of the image, positioned towards the right side. It is adjacent to a white tank on top and a green container to its left. In the background, there is a white wall that extends to the top of the image, and a white floor that is visible at the bottom.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21040.8, "ram_available_mb": 41800.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.0, "ram_used_mb": 21044.7, "ram_available_mb": 41796.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.506}, "power_stats": {"power_gpu_soc_mean_watts": 19.815, "power_cpu_cv_mean_watts": 1.946, "power_sys_5v0_mean_watts": 8.697, "gpu_utilization_percent_mean": 70.506, "power_watts_avg": 19.815, "energy_joules_est": 203.99, "duration_seconds": 10.295, "sample_count": 87}, "timestamp": "2026-01-26T14:43:36.796577"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10161.041, "latencies_ms": [10161.041], "images_per_second": 0.098, "prompt_tokens": 37, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image depicts a small, cramped bathroom with a toilet and a trash can. The toilet is white and appears to be in a poor condition, with a broken seat and a missing lid. The trash can is green and is placed next to the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21044.7, "ram_available_mb": 41796.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21045.4, "ram_available_mb": 41795.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.36}, "power_stats": {"power_gpu_soc_mean_watts": 20.179, "power_cpu_cv_mean_watts": 1.782, "power_sys_5v0_mean_watts": 8.66, "gpu_utilization_percent_mean": 71.36, "power_watts_avg": 20.179, "energy_joules_est": 205.05, "duration_seconds": 10.162, "sample_count": 86}, "timestamp": "2026-01-26T14:43:48.974797"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7227.491, "latencies_ms": [7227.491], "images_per_second": 0.138, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image shows a small, compact bathroom with a white toilet and a white tank. The walls are painted in a light color, and there is a brown carpet on the floor.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20965.8, "ram_available_mb": 41875.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20961.1, "ram_available_mb": 41879.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.016}, "power_stats": {"power_gpu_soc_mean_watts": 21.303, "power_cpu_cv_mean_watts": 1.574, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 73.016, "power_watts_avg": 21.303, "energy_joules_est": 153.98, "duration_seconds": 7.228, "sample_count": 61}, "timestamp": "2026-01-26T14:43:58.235028"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11624.438, "latencies_ms": [11624.438], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is skiing down a snowy mountain slope. The skier is wearing a red jacket and is in the process of making a turn on the slope. The mountain slope is covered in snow, and the skier is navigating through the snowy terrain. The skier is also wearing skis, which are essential for skiing down the mountain.", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 20961.1, "ram_available_mb": 41879.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20954.7, "ram_available_mb": 41886.2, "ram_percent": 33.3}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.768}, "power_stats": {"power_gpu_soc_mean_watts": 19.314, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 70.768, "power_watts_avg": 19.314, "energy_joules_est": 224.53, "duration_seconds": 11.625, "sample_count": 99}, "timestamp": "2026-01-26T14:44:11.890244"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8744.267, "latencies_ms": [8744.267], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "mountain: 1, ski tracks: multiple, skier: 1, red jacket: 1, black pants: 1, ski poles: 2, snow: multiple, blue sky: 1, trees: 0", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 20954.7, "ram_available_mb": 41886.2, "ram_percent": 33.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21015.3, "ram_available_mb": 41825.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.311}, "power_stats": {"power_gpu_soc_mean_watts": 20.652, "power_cpu_cv_mean_watts": 1.649, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 72.311, "power_watts_avg": 20.652, "energy_joules_est": 180.6, "duration_seconds": 8.745, "sample_count": 74}, "timestamp": "2026-01-26T14:44:22.678365"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9856.955, "latencies_ms": [9856.955], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "In the foreground, there is a person skiing on the left side of the image, moving towards the right. The background features a snowy mountain with a clear blue sky above it. The mountain appears to be quite large and is located behind the skier, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21015.3, "ram_available_mb": 41825.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20978.3, "ram_available_mb": 41862.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.141}, "power_stats": {"power_gpu_soc_mean_watts": 19.891, "power_cpu_cv_mean_watts": 1.808, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 71.141, "power_watts_avg": 19.891, "energy_joules_est": 196.08, "duration_seconds": 9.858, "sample_count": 85}, "timestamp": "2026-01-26T14:44:34.553938"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6751.772, "latencies_ms": [6751.772], "images_per_second": 0.148, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A person in a red jacket is skiing down a snowy mountain slope. The sky is clear and blue, and the mountain appears to be quite steep.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20978.3, "ram_available_mb": 41862.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 20970.5, "ram_available_mb": 41870.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.895}, "power_stats": {"power_gpu_soc_mean_watts": 22.239, "power_cpu_cv_mean_watts": 1.446, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 75.895, "power_watts_avg": 22.239, "energy_joules_est": 150.17, "duration_seconds": 6.752, "sample_count": 57}, "timestamp": "2026-01-26T14:44:43.327066"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7548.658, "latencies_ms": [7548.658], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The skier is wearing a bright red jacket and black pants, and is skiing on a snow-covered mountain. The sky is clear and blue, indicating good weather conditions for skiing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20970.5, "ram_available_mb": 41870.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20961.9, "ram_available_mb": 41878.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.762}, "power_stats": {"power_gpu_soc_mean_watts": 21.035, "power_cpu_cv_mean_watts": 1.626, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 73.762, "power_watts_avg": 21.035, "energy_joules_est": 158.8, "duration_seconds": 7.549, "sample_count": 63}, "timestamp": "2026-01-26T14:44:52.903321"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11588.362, "latencies_ms": [11588.362], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is skiing down a snowy hill, wearing a red and white jacket and black pants. He is holding two ski poles and appears to be enjoying the winter activity. There are other people in the background, also skiing and snowboarding, adding to the lively atmosphere of the scene.\n\nThe snow-covered hill is", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20961.9, "ram_available_mb": 41878.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21018.4, "ram_available_mb": 41822.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.172}, "power_stats": {"power_gpu_soc_mean_watts": 19.298, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 69.172, "power_watts_avg": 19.298, "energy_joules_est": 223.64, "duration_seconds": 11.589, "sample_count": 99}, "timestamp": "2026-01-26T14:45:06.555306"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10013.578, "latencies_ms": [10013.578], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Skier: 1\n2. Trees: numerous\n3. Mountain: 1\n4. Snow: covering ground and trees\n5. Skis: 2\n6. Pole: 1\n7. Jacket: 1\n8. Hat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20964.1, "ram_available_mb": 41876.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21026.5, "ram_available_mb": 41814.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.721}, "power_stats": {"power_gpu_soc_mean_watts": 19.967, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 8.63, "gpu_utilization_percent_mean": 71.721, "power_watts_avg": 19.967, "energy_joules_est": 199.96, "duration_seconds": 10.014, "sample_count": 86}, "timestamp": "2026-01-26T14:45:18.628242"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10634.049, "latencies_ms": [10634.049], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, a skier wearing a red jacket with the number 30 is skiing on a snowy slope. In the background, there are more trees covered in snow and another person can be seen skiing further down the slope. The skier is closer to the camera than the other person and the trees.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 20964.6, "ram_available_mb": 41876.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21018.4, "ram_available_mb": 41822.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.433}, "power_stats": {"power_gpu_soc_mean_watts": 19.711, "power_cpu_cv_mean_watts": 1.818, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 70.433, "power_watts_avg": 19.711, "energy_joules_est": 209.62, "duration_seconds": 10.635, "sample_count": 90}, "timestamp": "2026-01-26T14:45:31.313378"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9314.247, "latencies_ms": [9314.247], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "A person is cross-country skiing on a snowy trail surrounded by a forest of snow-covered trees. The skier is wearing a red and white bib with the number 30 on it, indicating they may be participating in a race or event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21018.4, "ram_available_mb": 41822.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21028.8, "ram_available_mb": 41812.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.165}, "power_stats": {"power_gpu_soc_mean_watts": 20.418, "power_cpu_cv_mean_watts": 1.697, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 72.165, "power_watts_avg": 20.418, "energy_joules_est": 190.19, "duration_seconds": 9.315, "sample_count": 79}, "timestamp": "2026-01-26T14:45:42.656105"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8786.559, "latencies_ms": [8786.559], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a snowy landscape with a person skiing. The skier is wearing a red jacket, black pants, and a gray beanie. The trees in the background are covered in snow, and the sky is overcast.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 21028.8, "ram_available_mb": 41812.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21029.3, "ram_available_mb": 41811.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.197}, "power_stats": {"power_gpu_soc_mean_watts": 20.417, "power_cpu_cv_mean_watts": 1.711, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 71.197, "power_watts_avg": 20.417, "energy_joules_est": 179.41, "duration_seconds": 8.787, "sample_count": 76}, "timestamp": "2026-01-26T14:45:53.481041"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11538.12, "latencies_ms": [11538.12], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility in a workspace. Dominating the scene is a desktop computer, its screen alive with the vibrant colors of a PowerPoint presentation. The presentation, titled \"WORKPLACE\", is set against a stark black background, creating a striking contrast. The slide showcases a sleek, modern office chair, its design hinting at comfort", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21029.3, "ram_available_mb": 41811.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21029.8, "ram_available_mb": 41811.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.816}, "power_stats": {"power_gpu_soc_mean_watts": 19.335, "power_cpu_cv_mean_watts": 1.911, "power_sys_5v0_mean_watts": 8.685, "gpu_utilization_percent_mean": 69.816, "power_watts_avg": 19.335, "energy_joules_est": 223.1, "duration_seconds": 11.539, "sample_count": 98}, "timestamp": "2026-01-26T14:46:07.053270"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7444.648, "latencies_ms": [7444.648], "images_per_second": 0.134, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "monitor: 1, keyboard: 1, mouse: 1, desk: 1, wall: 1, screen: 1, image: 1, text: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20957.9, "ram_available_mb": 41883.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20957.7, "ram_available_mb": 41883.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.429}, "power_stats": {"power_gpu_soc_mean_watts": 21.633, "power_cpu_cv_mean_watts": 1.562, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 73.429, "power_watts_avg": 21.633, "energy_joules_est": 161.06, "duration_seconds": 7.445, "sample_count": 63}, "timestamp": "2026-01-26T14:46:16.559729"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11208.828, "latencies_ms": [11208.828], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The computer monitor is positioned on the left side of the image, displaying a workspace on its screen. In the foreground, there is a black keyboard and mouse placed on a desk. The background is a plain wall, and the word 'WORKPLACE' is written in the upper right corner, suggesting the setting is a professional or office environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20957.7, "ram_available_mb": 41883.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21018.4, "ram_available_mb": 41822.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.6}, "power_stats": {"power_gpu_soc_mean_watts": 19.5, "power_cpu_cv_mean_watts": 1.87, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 70.6, "power_watts_avg": 19.5, "energy_joules_est": 218.58, "duration_seconds": 11.209, "sample_count": 95}, "timestamp": "2026-01-26T14:46:29.812693"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8627.777, "latencies_ms": [8627.777], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a workspace with a desktop computer, a keyboard, and a mouse placed on a desk. The computer screen displays a black and white image of a workspace with a keyboard and mouse, creating a visual representation of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21018.4, "ram_available_mb": 41822.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21027.4, "ram_available_mb": 41813.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.534}, "power_stats": {"power_gpu_soc_mean_watts": 20.639, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 72.534, "power_watts_avg": 20.639, "energy_joules_est": 178.08, "duration_seconds": 8.628, "sample_count": 73}, "timestamp": "2026-01-26T14:46:40.476079"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7634.378, "latencies_ms": [7634.378], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image is in black and white, featuring a workplace setting with a computer monitor displaying a desktop wallpaper. The monitor is placed on a white desk with a black keyboard and mouse in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21027.4, "ram_available_mb": 41813.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21038.3, "ram_available_mb": 41802.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.625}, "power_stats": {"power_gpu_soc_mean_watts": 21.257, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 73.625, "power_watts_avg": 21.257, "energy_joules_est": 162.3, "duration_seconds": 7.635, "sample_count": 64}, "timestamp": "2026-01-26T14:46:50.146867"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11585.03, "latencies_ms": [11585.03], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is sitting at a table in a restaurant, holding a bagel in her hand. She is smiling and appears to be enjoying her meal. The table is set with a cup, a bowl, and a sandwich. There are other people in the background, possibly waiting for their orders or enjoying their meals as well. The restaurant has a", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 20984.9, "ram_available_mb": 41856.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20986.2, "ram_available_mb": 41854.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.485}, "power_stats": {"power_gpu_soc_mean_watts": 19.294, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.675, "gpu_utilization_percent_mean": 70.485, "power_watts_avg": 19.294, "energy_joules_est": 223.53, "duration_seconds": 11.586, "sample_count": 99}, "timestamp": "2026-01-26T14:47:03.780033"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9977.368, "latencies_ms": [9977.368], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "- Woman: 1\n\n- Bagel: 1\n\n- Coffee cup: 1\n\n- Table: 1\n\n- Window: 1\n\n- Train car: 1\n\n- Passengers: 1\n\n- Glasses: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20986.2, "ram_available_mb": 41854.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21045.3, "ram_available_mb": 41795.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.918}, "power_stats": {"power_gpu_soc_mean_watts": 20.069, "power_cpu_cv_mean_watts": 1.756, "power_sys_5v0_mean_watts": 8.614, "gpu_utilization_percent_mean": 71.918, "power_watts_avg": 20.069, "energy_joules_est": 200.25, "duration_seconds": 9.978, "sample_count": 85}, "timestamp": "2026-01-26T14:47:15.775633"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11658.944, "latencies_ms": [11658.944], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person is holding a bagel in their left hand, with a cup on the table in front of them. In the background, there is a window showing a view of greenery outside, and another person is standing near the window. The person holding the bagel is seated at the table, and there is a cardboard box on the table to their right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21045.3, "ram_available_mb": 41795.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 10.3, "ram_used_mb": 21333.9, "ram_available_mb": 41507.0, "ram_percent": 33.9}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.222}, "power_stats": {"power_gpu_soc_mean_watts": 19.593, "power_cpu_cv_mean_watts": 1.981, "power_sys_5v0_mean_watts": 8.7, "gpu_utilization_percent_mean": 70.222, "power_watts_avg": 19.593, "energy_joules_est": 228.45, "duration_seconds": 11.66, "sample_count": 99}, "timestamp": "2026-01-26T14:47:29.495895"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6856.37, "latencies_ms": [6856.37], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "A woman is sitting at a table in a train, holding a bagel in her hand. She is smiling and appears to be enjoying her meal.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 21270.1, "ram_available_mb": 41570.8, "ram_percent": 33.8}, "sys_after": {"cpu_percent": 34.1, "ram_used_mb": 21984.6, "ram_available_mb": 40856.3, "ram_percent": 35.0}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.293}, "power_stats": {"power_gpu_soc_mean_watts": 21.234, "power_cpu_cv_mean_watts": 5.576, "power_sys_5v0_mean_watts": 8.777, "gpu_utilization_percent_mean": 76.293, "power_watts_avg": 21.234, "energy_joules_est": 145.6, "duration_seconds": 6.857, "sample_count": 58}, "timestamp": "2026-01-26T14:47:38.364405"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11471.917, "latencies_ms": [11471.917], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The image shows a person holding a bagel with a light-colored interior, possibly cream cheese, in a setting with warm indoor lighting. The person is wearing a purple striped shirt, and there is a window in the background showing a blurred view of greenery and a building, suggesting it might be a sunny day outside.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 21984.6, "ram_available_mb": 40856.3, "ram_percent": 35.0}, "sys_after": {"cpu_percent": 16.8, "ram_used_mb": 21109.2, "ram_available_mb": 41731.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.141}, "power_stats": {"power_gpu_soc_mean_watts": 19.605, "power_cpu_cv_mean_watts": 4.225, "power_sys_5v0_mean_watts": 8.819, "gpu_utilization_percent_mean": 70.141, "power_watts_avg": 19.605, "energy_joules_est": 224.93, "duration_seconds": 11.473, "sample_count": 99}, "timestamp": "2026-01-26T14:47:51.869313"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11611.839, "latencies_ms": [11611.839], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two zebras standing in a grassy field. They are grazing on the grass, with one zebra on the left and the other on the right. The zebras are facing each other, and their heads are close together as they eat. The field is filled with green grass, and the zebras appear to be enjoying their meal", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 21109.2, "ram_available_mb": 41731.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21162.5, "ram_available_mb": 41678.4, "ram_percent": 33.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.131}, "power_stats": {"power_gpu_soc_mean_watts": 19.366, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 70.131, "power_watts_avg": 19.366, "energy_joules_est": 224.89, "duration_seconds": 11.612, "sample_count": 99}, "timestamp": "2026-01-26T14:48:05.548072"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7568.874, "latencies_ms": [7568.874], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "zebra: 3, grass: many, dirt: patches, flowers: few, trees: none visible, fence: none visible, sky: not visible, water: not visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21106.9, "ram_available_mb": 41734.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21164.5, "ram_available_mb": 41676.4, "ram_percent": 33.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.438}, "power_stats": {"power_gpu_soc_mean_watts": 21.432, "power_cpu_cv_mean_watts": 1.551, "power_sys_5v0_mean_watts": 8.576, "gpu_utilization_percent_mean": 73.438, "power_watts_avg": 21.432, "energy_joules_est": 162.23, "duration_seconds": 7.569, "sample_count": 64}, "timestamp": "2026-01-26T14:48:15.156858"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9527.272, "latencies_ms": [9527.272], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "In the foreground, there are two zebras standing close to each other, with one slightly in front of the other. They are both facing the same direction, grazing on the grass. In the background, there is another zebra partially visible, further away from the main subjects.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21164.5, "ram_available_mb": 41676.4, "ram_percent": 33.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21184.4, "ram_available_mb": 41656.5, "ram_percent": 33.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.634}, "power_stats": {"power_gpu_soc_mean_watts": 19.955, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.7, "gpu_utilization_percent_mean": 71.634, "power_watts_avg": 19.955, "energy_joules_est": 190.13, "duration_seconds": 9.528, "sample_count": 82}, "timestamp": "2026-01-26T14:48:26.701661"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7334.824, "latencies_ms": [7334.824], "images_per_second": 0.136, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "Two zebras are grazing in a grassy field with other zebras in the background. The zebras are standing close together and appear to be enjoying their meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21184.4, "ram_available_mb": 41656.5, "ram_percent": 33.7}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21183.0, "ram_available_mb": 41657.9, "ram_percent": 33.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_gpu_soc_mean_watts": 21.64, "power_cpu_cv_mean_watts": 1.53, "power_sys_5v0_mean_watts": 8.587, "gpu_utilization_percent_mean": 74.0, "power_watts_avg": 21.64, "energy_joules_est": 158.74, "duration_seconds": 7.335, "sample_count": 62}, "timestamp": "2026-01-26T14:48:36.055307"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6099.345, "latencies_ms": [6099.345], "images_per_second": 0.164, "prompt_tokens": 36, "response_tokens_est": 31, "n_tiles": 16, "output_text": "The zebras are grazing in a field with lush green grass. The lighting is natural and bright, indicating it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21183.0, "ram_available_mb": 41657.9, "ram_percent": 33.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21183.2, "ram_available_mb": 41657.7, "ram_percent": 33.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.02}, "power_stats": {"power_gpu_soc_mean_watts": 22.036, "power_cpu_cv_mean_watts": 1.404, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 74.02, "power_watts_avg": 22.036, "energy_joules_est": 134.42, "duration_seconds": 6.1, "sample_count": 51}, "timestamp": "2026-01-26T14:48:44.169037"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11628.572, "latencies_ms": [11628.572], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a busy street scene with several people riding bicycles and motorcycles. There are two young men riding bicycles, one in the foreground and another further back. A man is riding a motorcycle, and another person is riding a scooter. \n\nIn addition to the riders, there are two more people on the", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 21119.7, "ram_available_mb": 41721.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21170.8, "ram_available_mb": 41670.1, "ram_percent": 33.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.03}, "power_stats": {"power_gpu_soc_mean_watts": 19.282, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.686, "gpu_utilization_percent_mean": 70.03, "power_watts_avg": 19.282, "energy_joules_est": 224.23, "duration_seconds": 11.629, "sample_count": 99}, "timestamp": "2026-01-26T14:48:57.852873"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9594.587, "latencies_ms": [9594.587], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "- Bicycle: 1\n\n- Motorcycle: 2\n\n- Plant: 1\n\n- Storefront: 1\n\n- Sign: 1\n\n- Helmet: 1\n\n- Chair: 1\n\n- Cart: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 21170.8, "ram_available_mb": 41670.1, "ram_percent": 33.7}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21099.2, "ram_available_mb": 41741.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.235}, "power_stats": {"power_gpu_soc_mean_watts": 20.306, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 72.235, "power_watts_avg": 20.306, "energy_joules_est": 194.84, "duration_seconds": 9.595, "sample_count": 81}, "timestamp": "2026-01-26T14:49:09.470028"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11584.247, "latencies_ms": [11584.247], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a green bicycle with a person riding it, positioned to the left of the frame. Behind the bicycle, there is a red motorcycle parked on the side of the street. In the background, there is a blue motorcycle with a person riding it, and further back, there is a person walking on the sidewalk", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21099.2, "ram_available_mb": 41741.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21158.0, "ram_available_mb": 41682.9, "ram_percent": 33.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.255}, "power_stats": {"power_gpu_soc_mean_watts": 19.254, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.688, "gpu_utilization_percent_mean": 70.255, "power_watts_avg": 19.254, "energy_joules_est": 223.06, "duration_seconds": 11.585, "sample_count": 98}, "timestamp": "2026-01-26T14:49:23.084605"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11352.173, "latencies_ms": [11352.173], "images_per_second": 0.088, "prompt_tokens": 37, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The image depicts a busy street scene with multiple modes of transportation. There are two men riding bicycles, one in the foreground and one in the background, and a woman riding a motorcycle in the middle of the scene. Additionally, there are other people on motorcycles and a Coca-Cola store in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21158.0, "ram_available_mb": 41682.9, "ram_percent": 33.7}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21094.5, "ram_available_mb": 41746.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.552}, "power_stats": {"power_gpu_soc_mean_watts": 19.606, "power_cpu_cv_mean_watts": 1.842, "power_sys_5v0_mean_watts": 8.632, "gpu_utilization_percent_mean": 70.552, "power_watts_avg": 19.606, "energy_joules_est": 222.58, "duration_seconds": 11.353, "sample_count": 96}, "timestamp": "2026-01-26T14:49:36.454628"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9049.495, "latencies_ms": [9049.495], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image features a vibrant green bicycle with a basket in the foreground, set against a backdrop of a busy street scene. The lighting suggests it's daytime with natural light casting shadows on the ground, indicating it might be late afternoon.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21094.5, "ram_available_mb": 41746.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21092.1, "ram_available_mb": 41748.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.846}, "power_stats": {"power_gpu_soc_mean_watts": 20.049, "power_cpu_cv_mean_watts": 1.76, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 71.846, "power_watts_avg": 20.049, "energy_joules_est": 181.45, "duration_seconds": 9.05, "sample_count": 78}, "timestamp": "2026-01-26T14:49:47.526889"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11578.674, "latencies_ms": [11578.674], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a tennis match in progress on a grass court. Two tennis players are actively engaged in the game, each holding a tennis racket and standing on opposite sides of the court. The court is surrounded by a crowd of spectators, who are watching the match intently. \n\nThere are several chairs placed around the court, likely for the players to rest or for", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 21092.1, "ram_available_mb": 41748.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21148.8, "ram_available_mb": 41692.1, "ram_percent": 33.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.592}, "power_stats": {"power_gpu_soc_mean_watts": 19.397, "power_cpu_cv_mean_watts": 1.907, "power_sys_5v0_mean_watts": 8.721, "gpu_utilization_percent_mean": 69.592, "power_watts_avg": 19.397, "energy_joules_est": 224.61, "duration_seconds": 11.579, "sample_count": 98}, "timestamp": "2026-01-26T14:50:01.166591"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8482.857, "latencies_ms": [8482.857], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "audience: numerous\nball: 1\nnet: 1\nracket: 2\nplayer: 2\nchairs: 2\nbags: 2\nbottles: 0\ncamera operators: 2", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21148.8, "ram_available_mb": 41692.1, "ram_percent": 33.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21163.3, "ram_available_mb": 41677.6, "ram_percent": 33.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.5}, "power_stats": {"power_gpu_soc_mean_watts": 20.819, "power_cpu_cv_mean_watts": 1.656, "power_sys_5v0_mean_watts": 8.612, "gpu_utilization_percent_mean": 72.5, "power_watts_avg": 20.819, "energy_joules_est": 176.62, "duration_seconds": 8.483, "sample_count": 72}, "timestamp": "2026-01-26T14:50:11.683910"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11579.099, "latencies_ms": [11579.099], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a tennis player dressed in white is positioned near the baseline, holding a tennis racket and preparing to hit the ball. The ball is in the air, slightly above and to the right of the player, indicating it is being served. In the background, there is a crowd of spectators seated in the stands, watching the game. The stands are filled", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21081.6, "ram_available_mb": 41759.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21143.1, "ram_available_mb": 41697.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.133}, "power_stats": {"power_gpu_soc_mean_watts": 19.377, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.685, "gpu_utilization_percent_mean": 70.133, "power_watts_avg": 19.377, "energy_joules_est": 224.38, "duration_seconds": 11.58, "sample_count": 98}, "timestamp": "2026-01-26T14:50:25.282822"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9251.613, "latencies_ms": [9251.613], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image captures a tennis match in progress on a grass court, with a player in white attire actively engaged in the game, holding a tennis racket and preparing to hit the ball. Spectators are seated in the stands, watching the match intently.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21143.1, "ram_available_mb": 41697.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21163.1, "ram_available_mb": 41677.8, "ram_percent": 33.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.103}, "power_stats": {"power_gpu_soc_mean_watts": 20.372, "power_cpu_cv_mean_watts": 1.708, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 72.103, "power_watts_avg": 20.372, "energy_joules_est": 188.49, "duration_seconds": 9.252, "sample_count": 78}, "timestamp": "2026-01-26T14:50:36.555378"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7935.729, "latencies_ms": [7935.729], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The tennis court is surrounded by green seats, and the players are wearing white and red attire. The lighting appears to be natural, likely from the sun, as there are no visible artificial light sources on the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21163.1, "ram_available_mb": 41677.8, "ram_percent": 33.7}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21161.9, "ram_available_mb": 41679.0, "ram_percent": 33.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.91}, "power_stats": {"power_gpu_soc_mean_watts": 20.604, "power_cpu_cv_mean_watts": 1.642, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 71.91, "power_watts_avg": 20.604, "energy_joules_est": 163.52, "duration_seconds": 7.936, "sample_count": 67}, "timestamp": "2026-01-26T14:50:46.549941"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11598.79, "latencies_ms": [11598.79], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a cozy living room with a couch positioned near a large window. The room is well-lit, thanks to the natural light streaming in through the window. On the couch, there are two pillows, providing a comfortable seating area. \n\nIn the room, there are several potted plants, adding a touch of greenery and life to the space", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21161.9, "ram_available_mb": 41679.0, "ram_percent": 33.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21162.6, "ram_available_mb": 41678.3, "ram_percent": 33.7}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.343}, "power_stats": {"power_gpu_soc_mean_watts": 19.355, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.691, "gpu_utilization_percent_mean": 70.343, "power_watts_avg": 19.355, "energy_joules_est": 224.51, "duration_seconds": 11.599, "sample_count": 99}, "timestamp": "2026-01-26T14:51:00.191221"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9168.476, "latencies_ms": [9168.476], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Couch: 1\n- Plant: 3\n- Television: 1\n- Table: 1\n- Remote control: 1\n- Curtains: 2\n- Rug: 1\n- Pillow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.4, "ram_available_mb": 41758.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21129.6, "ram_available_mb": 41711.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.282}, "power_stats": {"power_gpu_soc_mean_watts": 20.425, "power_cpu_cv_mean_watts": 1.713, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 72.282, "power_watts_avg": 20.425, "energy_joules_est": 187.28, "duration_seconds": 9.169, "sample_count": 78}, "timestamp": "2026-01-26T14:51:11.382311"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11243.038, "latencies_ms": [11243.038], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground, there is a brown sofa positioned against the wall on the right side of the image. In the background, there is a large window with white curtains, and a potted plant is placed on the windowsill. The sofa is located near the window, providing a comfortable seating area with a view of the outdoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21067.9, "ram_available_mb": 41773.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21135.4, "ram_available_mb": 41705.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.379}, "power_stats": {"power_gpu_soc_mean_watts": 19.461, "power_cpu_cv_mean_watts": 1.866, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 70.379, "power_watts_avg": 19.461, "energy_joules_est": 218.81, "duration_seconds": 11.244, "sample_count": 95}, "timestamp": "2026-01-26T14:51:24.652787"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8802.454, "latencies_ms": [8802.454], "images_per_second": 0.114, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a brown couch, a wooden coffee table, and a flat-screen TV. There are several potted plants on the table and a window with curtains, providing natural light to the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21079.8, "ram_available_mb": 41761.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 21140.4, "ram_available_mb": 41700.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.8}, "power_stats": {"power_gpu_soc_mean_watts": 20.61, "power_cpu_cv_mean_watts": 1.707, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 73.8, "power_watts_avg": 20.61, "energy_joules_est": 181.43, "duration_seconds": 8.803, "sample_count": 75}, "timestamp": "2026-01-26T14:51:35.478160"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8843.578, "latencies_ms": [8843.578], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The room is brightly lit with natural light coming in through the large window with sheer curtains. The furniture includes a brown sofa, a wooden entertainment center with a flat-screen TV, and a wooden coffee table with various decorative items.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21078.3, "ram_available_mb": 41762.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 21138.6, "ram_available_mb": 41702.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.446}, "power_stats": {"power_gpu_soc_mean_watts": 20.111, "power_cpu_cv_mean_watts": 1.914, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 72.446, "power_watts_avg": 20.111, "energy_joules_est": 177.86, "duration_seconds": 8.844, "sample_count": 74}, "timestamp": "2026-01-26T14:51:46.351435"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11613.061, "latencies_ms": [11613.061], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing on a tennis court, holding a tennis racket in her right hand. She is wearing a red dress and a white visor, which suggests she is a tennis player. The court appears to be a clay court, as it has a reddish-brown color.\n\nThere are several other people in the scene, some of whom", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 21074.7, "ram_available_mb": 41766.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21131.7, "ram_available_mb": 41709.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.283}, "power_stats": {"power_gpu_soc_mean_watts": 19.201, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.688, "gpu_utilization_percent_mean": 70.283, "power_watts_avg": 19.201, "energy_joules_est": 223.0, "duration_seconds": 11.614, "sample_count": 99}, "timestamp": "2026-01-26T14:52:00.014690"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10177.681, "latencies_ms": [10177.681], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "- Player: 1\n\n- Racket: 1\n\n- Dress: 1\n\n- Tennis court: 1\n\n- White lines: 2\n\n- Sandals: 2\n\n- Towel: 1\n\n- Crocodile logo: 1", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 21069.3, "ram_available_mb": 41771.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21123.8, "ram_available_mb": 41717.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.488}, "power_stats": {"power_gpu_soc_mean_watts": 20.018, "power_cpu_cv_mean_watts": 1.777, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 71.488, "power_watts_avg": 20.018, "energy_joules_est": 203.75, "duration_seconds": 10.178, "sample_count": 86}, "timestamp": "2026-01-26T14:52:12.208805"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11580.29, "latencies_ms": [11580.29], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a woman dressed in a red tennis dress and white visor is holding a tennis racket, positioned near the center of the image. She appears to be on a tennis court, with the red clay surface extending into the background. There are white lines marking the boundaries of the court, and a green mat with a crocodile logo is visible in the upper", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21123.8, "ram_available_mb": 41717.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21068.3, "ram_available_mb": 41772.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.776}, "power_stats": {"power_gpu_soc_mean_watts": 19.164, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.679, "gpu_utilization_percent_mean": 69.776, "power_watts_avg": 19.164, "energy_joules_est": 221.94, "duration_seconds": 11.581, "sample_count": 98}, "timestamp": "2026-01-26T14:52:25.822331"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7786.737, "latencies_ms": [7786.737], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A woman in a red dress is standing on a clay tennis court, holding a tennis racket and looking towards the right side of the image. She appears to be preparing to play a game of tennis.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21068.3, "ram_available_mb": 41772.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21070.4, "ram_available_mb": 41770.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.364}, "power_stats": {"power_gpu_soc_mean_watts": 21.251, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 73.364, "power_watts_avg": 21.251, "energy_joules_est": 165.49, "duration_seconds": 7.787, "sample_count": 66}, "timestamp": "2026-01-26T14:52:35.643987"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7554.417, "latencies_ms": [7554.417], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The tennis player is wearing a bright orange dress and a white visor, standing on a clay court with white boundary lines. The court appears to be dry and the lighting suggests it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21070.4, "ram_available_mb": 41770.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21129.5, "ram_available_mb": 41711.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.547}, "power_stats": {"power_gpu_soc_mean_watts": 21.076, "power_cpu_cv_mean_watts": 1.626, "power_sys_5v0_mean_watts": 8.636, "gpu_utilization_percent_mean": 72.547, "power_watts_avg": 21.076, "energy_joules_est": 159.23, "duration_seconds": 7.555, "sample_count": 64}, "timestamp": "2026-01-26T14:52:45.212323"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11575.735, "latencies_ms": [11575.735], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a busy city street filled with numerous cars and people. There are at least 14 cars visible, parked or driving along the street, and several pedestrians are walking or standing on the sidewalks. The street is lined with various buildings, including a restaurant and a bar, which contribute to the bustling atmosphere.\n\nIn addition to", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 21129.5, "ram_available_mb": 41711.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 20996.5, "ram_available_mb": 41844.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.704}, "power_stats": {"power_gpu_soc_mean_watts": 19.408, "power_cpu_cv_mean_watts": 1.919, "power_sys_5v0_mean_watts": 8.713, "gpu_utilization_percent_mean": 69.704, "power_watts_avg": 19.408, "energy_joules_est": 224.67, "duration_seconds": 11.576, "sample_count": 98}, "timestamp": "2026-01-26T14:52:58.834149"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7669.488, "latencies_ms": [7669.488], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "car: 10\nbuilding: 5\nsign: 2\nperson: 5\nwindow: 15\nstreet: 1\nairplane: 1\ntree: 0", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20996.5, "ram_available_mb": 41844.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20976.3, "ram_available_mb": 41864.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.631}, "power_stats": {"power_gpu_soc_mean_watts": 21.374, "power_cpu_cv_mean_watts": 1.551, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 73.631, "power_watts_avg": 21.374, "energy_joules_est": 163.94, "duration_seconds": 7.67, "sample_count": 65}, "timestamp": "2026-01-26T14:53:08.525112"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11568.596, "latencies_ms": [11568.596], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are several cars parked along the street, with one white car in the left foreground and others in the background. The buildings in the background are taller and appear to be further away, with the tallest building on the right side of the image. The street sign is located in the upper left corner of the image, indicating its position relative to the other objects", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20976.3, "ram_available_mb": 41864.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21035.1, "ram_available_mb": 41805.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.918}, "power_stats": {"power_gpu_soc_mean_watts": 19.409, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.695, "gpu_utilization_percent_mean": 69.918, "power_watts_avg": 19.409, "energy_joules_est": 224.55, "duration_seconds": 11.569, "sample_count": 98}, "timestamp": "2026-01-26T14:53:22.141660"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10304.834, "latencies_ms": [10304.834], "images_per_second": 0.097, "prompt_tokens": 37, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image depicts a busy street scene in a city with multiple cars parked and driving along the road. There are several buildings with storefronts, including a restaurant and a bar, and people are sitting at tables outside. The sky is overcast, and an airplane can be seen flying in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20971.6, "ram_available_mb": 41869.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21039.3, "ram_available_mb": 41801.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.943}, "power_stats": {"power_gpu_soc_mean_watts": 19.816, "power_cpu_cv_mean_watts": 1.782, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 71.943, "power_watts_avg": 19.816, "energy_joules_est": 204.21, "duration_seconds": 10.305, "sample_count": 88}, "timestamp": "2026-01-26T14:53:34.510244"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8568.939, "latencies_ms": [8568.939], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a cloudy day with overcast skies, casting a soft light over the scene. The buildings are primarily made of brick and have a variety of colors, including red, brown, and green, with some featuring large billboards and signs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20985.1, "ram_available_mb": 41855.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21032.1, "ram_available_mb": 41808.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.111}, "power_stats": {"power_gpu_soc_mean_watts": 20.528, "power_cpu_cv_mean_watts": 1.717, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 72.111, "power_watts_avg": 20.528, "energy_joules_est": 175.92, "duration_seconds": 8.57, "sample_count": 72}, "timestamp": "2026-01-26T14:53:45.097350"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11592.504, "latencies_ms": [11592.504], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is playing tennis on a court. She is wearing a pink outfit and is in the process of hitting a tennis ball with her racket. The tennis ball is located near the right side of the image, and the woman is focused on making contact with it. The court appears to be a blue surface, which is typical for tennis courts. The woman is", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 20978.8, "ram_available_mb": 41862.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21034.1, "ram_available_mb": 41806.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.657}, "power_stats": {"power_gpu_soc_mean_watts": 19.346, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 70.657, "power_watts_avg": 19.346, "energy_joules_est": 224.28, "duration_seconds": 11.593, "sample_count": 99}, "timestamp": "2026-01-26T14:53:58.745614"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9637.299, "latencies_ms": [9637.299], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "tennis player: 1\ntennis racket: 1\ntennis ball: 1\ngreen tennis court: 1\nwhite tennis court line: 1\nred tennis outfit: 1\nwhite tennis shoes: 1\nwhite tennis visor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20979.1, "ram_available_mb": 41861.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21042.8, "ram_available_mb": 41798.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.41}, "power_stats": {"power_gpu_soc_mean_watts": 20.172, "power_cpu_cv_mean_watts": 1.75, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 72.41, "power_watts_avg": 20.172, "energy_joules_est": 194.42, "duration_seconds": 9.638, "sample_count": 83}, "timestamp": "2026-01-26T14:54:10.398189"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9527.86, "latencies_ms": [9527.86], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The tennis player is positioned in the foreground on the left side of the image, preparing to hit the tennis ball that is near the far right side of the image. The tennis court has a blue surface with white boundary lines, and the player is wearing a pink outfit.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21042.8, "ram_available_mb": 41798.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21045.5, "ram_available_mb": 41795.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.494}, "power_stats": {"power_gpu_soc_mean_watts": 19.866, "power_cpu_cv_mean_watts": 1.788, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 70.494, "power_watts_avg": 19.866, "energy_joules_est": 189.29, "duration_seconds": 9.529, "sample_count": 81}, "timestamp": "2026-01-26T14:54:21.984192"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9454.26, "latencies_ms": [9454.26], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "A female tennis player is in the middle of a forehand swing, wearing a pink outfit and white visor, on a tennis court with a green surface and a blue boundary line. A tennis ball is visible in the air, indicating that she is about to hit it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20967.0, "ram_available_mb": 41873.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21018.4, "ram_available_mb": 41822.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.5}, "power_stats": {"power_gpu_soc_mean_watts": 20.462, "power_cpu_cv_mean_watts": 1.751, "power_sys_5v0_mean_watts": 8.669, "gpu_utilization_percent_mean": 72.5, "power_watts_avg": 20.462, "energy_joules_est": 193.47, "duration_seconds": 9.455, "sample_count": 80}, "timestamp": "2026-01-26T14:54:33.470652"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7549.529, "latencies_ms": [7549.529], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The tennis player is wearing a pink outfit and white shoes, and is holding a yellow tennis racket. The court is blue and green, and there is a yellow tennis ball in the air.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20963.4, "ram_available_mb": 41877.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21031.3, "ram_available_mb": 41809.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.523}, "power_stats": {"power_gpu_soc_mean_watts": 21.091, "power_cpu_cv_mean_watts": 1.638, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 73.523, "power_watts_avg": 21.091, "energy_joules_est": 159.24, "duration_seconds": 7.55, "sample_count": 65}, "timestamp": "2026-01-26T14:54:43.082731"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12391.385, "latencies_ms": [12391.385], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility in a lush, green landscape. Dominating the scene is a train, painted in hues of blue and orange, with a white roof. The train is stationary, resting on a track that cuts through the verdant greenery. The track itself is nestled amidst a field of tall grass and bushes, adding a", "error": null, "sys_before": {"cpu_percent": 8.1, "ram_used_mb": 21031.3, "ram_available_mb": 41809.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21047.1, "ram_available_mb": 41793.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.364}, "power_stats": {"power_gpu_soc_mean_watts": 21.599, "power_cpu_cv_mean_watts": 1.825, "power_sys_5v0_mean_watts": 8.95, "gpu_utilization_percent_mean": 72.364, "power_watts_avg": 21.599, "energy_joules_est": 267.66, "duration_seconds": 12.392, "sample_count": 107}, "timestamp": "2026-01-26T14:54:57.522324"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9578.807, "latencies_ms": [9578.807], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "Train: 1\nTrack: 1\nRailroad ties: 1\nGrass: 1\nWeeds: 1\nLeaves: 1\nTrees: 1\nBushes: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20966.9, "ram_available_mb": 41874.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21029.6, "ram_available_mb": 41811.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.915}, "power_stats": {"power_gpu_soc_mean_watts": 22.907, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 8.805, "gpu_utilization_percent_mean": 76.915, "power_watts_avg": 22.907, "energy_joules_est": 219.44, "duration_seconds": 9.579, "sample_count": 82}, "timestamp": "2026-01-26T14:55:09.144370"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12118.614, "latencies_ms": [12118.614], "images_per_second": 0.083, "prompt_tokens": 44, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The train is positioned in the background of the image, traveling along a curved track that is surrounded by vegetation. The train appears to be moving away from the viewer, as it is on the opposite side of the track from the perspective of the camera. The vegetation is in the foreground, with the train and track situated behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20967.8, "ram_available_mb": 41873.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21030.8, "ram_available_mb": 41810.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.505}, "power_stats": {"power_gpu_soc_mean_watts": 21.824, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.903, "gpu_utilization_percent_mean": 73.505, "power_watts_avg": 21.824, "energy_joules_est": 264.49, "duration_seconds": 12.119, "sample_count": 105}, "timestamp": "2026-01-26T14:55:23.319412"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9444.715, "latencies_ms": [9444.715], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A train is traveling through a rural area with lush greenery on both sides of the tracks. The train appears to be a passenger train with multiple cars, and it is moving along the tracks amidst the natural surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21030.8, "ram_available_mb": 41810.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21047.0, "ram_available_mb": 41793.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.926}, "power_stats": {"power_gpu_soc_mean_watts": 22.969, "power_cpu_cv_mean_watts": 1.522, "power_sys_5v0_mean_watts": 8.799, "gpu_utilization_percent_mean": 75.926, "power_watts_avg": 22.969, "energy_joules_est": 216.95, "duration_seconds": 9.445, "sample_count": 81}, "timestamp": "2026-01-26T14:55:34.791796"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8370.73, "latencies_ms": [8370.73], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The train in the image is predominantly blue and orange with a white front. It is traveling on a track that is surrounded by greenery and appears to be in a rural or natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20968.0, "ram_available_mb": 41872.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 21034.9, "ram_available_mb": 41806.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.577}, "power_stats": {"power_gpu_soc_mean_watts": 23.35, "power_cpu_cv_mean_watts": 1.612, "power_sys_5v0_mean_watts": 8.89, "gpu_utilization_percent_mean": 77.577, "power_watts_avg": 23.35, "energy_joules_est": 195.47, "duration_seconds": 8.371, "sample_count": 71}, "timestamp": "2026-01-26T14:55:45.203218"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11597.156, "latencies_ms": [11597.156], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two cats lying on a pink couch. One cat is sleeping on the left side of the couch, while the other cat is sleeping on the right side. Both cats are curled up and appear to be in a relaxed state. \n\nThere are two remotes placed on the couch, one on the left side and", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 21034.9, "ram_available_mb": 41806.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21066.6, "ram_available_mb": 41774.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.869}, "power_stats": {"power_gpu_soc_mean_watts": 19.368, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.698, "gpu_utilization_percent_mean": 70.869, "power_watts_avg": 19.368, "energy_joules_est": 224.63, "duration_seconds": 11.598, "sample_count": 99}, "timestamp": "2026-01-26T14:55:58.838148"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4691.072, "latencies_ms": [4691.072], "images_per_second": 0.213, "prompt_tokens": 39, "response_tokens_est": 17, "n_tiles": 16, "output_text": "couch: 1\ncat: 2\nremote control: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20987.8, "ram_available_mb": 41853.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 21073.0, "ram_available_mb": 41767.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 80.077}, "power_stats": {"power_gpu_soc_mean_watts": 24.609, "power_cpu_cv_mean_watts": 1.036, "power_sys_5v0_mean_watts": 8.508, "gpu_utilization_percent_mean": 80.077, "power_watts_avg": 24.609, "energy_joules_est": 115.46, "duration_seconds": 4.692, "sample_count": 39}, "timestamp": "2026-01-26T14:56:05.574998"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11604.806, "latencies_ms": [11604.806], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The two cats are lying on a pink couch with their bodies parallel to each other, occupying the central space of the image. The remote controls are placed on the couch, with one remote to the left and slightly behind the other cat, and the second remote is to the right and slightly in front of the other cat. The couch itself is the main object in the for", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20994.5, "ram_available_mb": 41846.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20997.1, "ram_available_mb": 41843.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.778}, "power_stats": {"power_gpu_soc_mean_watts": 19.225, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.699, "gpu_utilization_percent_mean": 70.778, "power_watts_avg": 19.225, "energy_joules_est": 223.11, "duration_seconds": 11.605, "sample_count": 99}, "timestamp": "2026-01-26T14:56:19.224024"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7635.937, "latencies_ms": [7635.937], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "Two cats are sleeping on a pink couch with remote controls nearby. The couch appears to be in a living room, and the cats seem to be enjoying a peaceful nap.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20997.1, "ram_available_mb": 41843.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 20972.6, "ram_available_mb": 41868.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.031}, "power_stats": {"power_gpu_soc_mean_watts": 21.495, "power_cpu_cv_mean_watts": 1.563, "power_sys_5v0_mean_watts": 8.614, "gpu_utilization_percent_mean": 74.031, "power_watts_avg": 21.495, "energy_joules_est": 164.15, "duration_seconds": 7.637, "sample_count": 64}, "timestamp": "2026-01-26T14:56:28.898596"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7122.207, "latencies_ms": [7122.207], "images_per_second": 0.14, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "Two cats are lying on a pink couch with two remote controls nearby. The couch has a soft, plush texture and the cats appear to be sleeping comfortably.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20972.6, "ram_available_mb": 41868.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21020.8, "ram_available_mb": 41820.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.917}, "power_stats": {"power_gpu_soc_mean_watts": 21.14, "power_cpu_cv_mean_watts": 1.561, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 72.917, "power_watts_avg": 21.14, "energy_joules_est": 150.58, "duration_seconds": 7.123, "sample_count": 60}, "timestamp": "2026-01-26T14:56:38.071858"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11604.566, "latencies_ms": [11604.566], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seen surfing on a wave in a river. He is wearing a black wetsuit and is skillfully riding the wave with a blue surfboard. The river is surrounded by trees, creating a serene and natural environment. There are also a few benches visible in the background, providing a place for people to sit and enjoy the view", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20967.4, "ram_available_mb": 41873.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21026.3, "ram_available_mb": 41814.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.163}, "power_stats": {"power_gpu_soc_mean_watts": 19.376, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 70.163, "power_watts_avg": 19.376, "energy_joules_est": 224.87, "duration_seconds": 11.606, "sample_count": 98}, "timestamp": "2026-01-26T14:56:51.714089"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9847.635, "latencies_ms": [9847.635], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "1. Surfboard: 1\n2. Man: 1\n3. Water: 1\n4. Bridge: 1\n5. Trees: 1\n6. Bench: 1\n7. Leaves: 1\n8. Log: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20964.7, "ram_available_mb": 41876.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21019.8, "ram_available_mb": 41821.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.393}, "power_stats": {"power_gpu_soc_mean_watts": 20.014, "power_cpu_cv_mean_watts": 1.758, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 71.393, "power_watts_avg": 20.014, "energy_joules_est": 197.1, "duration_seconds": 9.848, "sample_count": 84}, "timestamp": "2026-01-26T14:57:03.618083"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10657.838, "latencies_ms": [10657.838], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "In the foreground, a person in a black wetsuit is surfing on a wave in the water. To the left, there is a concrete structure, possibly a bridge or a dam, partially submerged in the water. In the background, there are trees and a blue kayak on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21019.8, "ram_available_mb": 41821.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20958.0, "ram_available_mb": 41882.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.233}, "power_stats": {"power_gpu_soc_mean_watts": 19.684, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 70.233, "power_watts_avg": 19.684, "energy_joules_est": 209.8, "duration_seconds": 10.658, "sample_count": 90}, "timestamp": "2026-01-26T14:57:16.294263"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7301.663, "latencies_ms": [7301.663], "images_per_second": 0.137, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A person in a black wetsuit is surfing a wave in a river with a concrete bridge in the background. There is another person with a blue surfboard in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20958.0, "ram_available_mb": 41882.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20967.6, "ram_available_mb": 41873.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.871}, "power_stats": {"power_gpu_soc_mean_watts": 21.704, "power_cpu_cv_mean_watts": 1.549, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 73.871, "power_watts_avg": 21.704, "energy_joules_est": 158.49, "duration_seconds": 7.302, "sample_count": 62}, "timestamp": "2026-01-26T14:57:25.642999"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11589.949, "latencies_ms": [11589.949], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a person surfing on a wave in a river with a bridge in the background. The surfer is wearing a black wetsuit and the water is white and foamy. The bridge is made of stone and has a statue on top. The trees surrounding the river are green and lush. The sky is overcast and the overall mood of the image is peace", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20967.6, "ram_available_mb": 41873.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21024.3, "ram_available_mb": 41816.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.806}, "power_stats": {"power_gpu_soc_mean_watts": 19.201, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.686, "gpu_utilization_percent_mean": 69.806, "power_watts_avg": 19.201, "energy_joules_est": 222.55, "duration_seconds": 11.591, "sample_count": 98}, "timestamp": "2026-01-26T14:57:39.276480"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12441.823, "latencies_ms": [12441.823], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman and a young child are standing in a grassy field, flying a kite together. The woman is wearing a black jacket and jeans, while the child is dressed in a pink shirt and jeans. They are both holding onto the kite strings, with the woman's arms spread wide, and the child's arms also extended.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 21024.3, "ram_available_mb": 41816.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21046.2, "ram_available_mb": 41794.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.726}, "power_stats": {"power_gpu_soc_mean_watts": 21.528, "power_cpu_cv_mean_watts": 1.804, "power_sys_5v0_mean_watts": 8.933, "gpu_utilization_percent_mean": 72.726, "power_watts_avg": 21.528, "energy_joules_est": 267.86, "duration_seconds": 12.442, "sample_count": 106}, "timestamp": "2026-01-26T14:57:53.789978"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9582.25, "latencies_ms": [9582.25], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "person: 2, kite: 1, kite string: 2, kite: 1, kite string: 2, kite: 1, kite string: 2, kite: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21046.2, "ram_available_mb": 41794.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20993.6, "ram_available_mb": 41847.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.346}, "power_stats": {"power_gpu_soc_mean_watts": 22.998, "power_cpu_cv_mean_watts": 1.517, "power_sys_5v0_mean_watts": 8.764, "gpu_utilization_percent_mean": 75.346, "power_watts_avg": 22.998, "energy_joules_est": 220.39, "duration_seconds": 9.583, "sample_count": 81}, "timestamp": "2026-01-26T14:58:05.386378"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10954.029, "latencies_ms": [10954.029], "images_per_second": 0.091, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "In the foreground, there is a person standing with their back to the camera, holding a kite that is flying in the air to their right. The person is standing on a grassy field with trees in the background. There are other people in the distance, some of whom are also flying kites.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20993.6, "ram_available_mb": 41847.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21045.5, "ram_available_mb": 41795.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.424}, "power_stats": {"power_gpu_soc_mean_watts": 22.328, "power_cpu_cv_mean_watts": 1.683, "power_sys_5v0_mean_watts": 8.884, "gpu_utilization_percent_mean": 75.424, "power_watts_avg": 22.328, "energy_joules_est": 244.6, "duration_seconds": 10.955, "sample_count": 92}, "timestamp": "2026-01-26T14:58:18.360951"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8076.492, "latencies_ms": [8076.492], "images_per_second": 0.124, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "In a park, a person is flying a colorful kite while standing on a grassy field. Another child is also present in the scene, watching the kite flying.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20965.4, "ram_available_mb": 41875.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 21030.0, "ram_available_mb": 41810.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 79.559}, "power_stats": {"power_gpu_soc_mean_watts": 23.769, "power_cpu_cv_mean_watts": 1.353, "power_sys_5v0_mean_watts": 8.758, "gpu_utilization_percent_mean": 79.559, "power_watts_avg": 23.769, "energy_joules_est": 191.98, "duration_seconds": 8.077, "sample_count": 68}, "timestamp": "2026-01-26T14:58:28.483299"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12716.442, "latencies_ms": [12716.442], "images_per_second": 0.079, "prompt_tokens": 36, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The image shows a person in a black leather jacket with purple and white accents, standing in a grassy field with arms outstretched, holding onto a colorful kite with a design that includes orange, pink, and blue. The weather appears to be clear and sunny, as indicated by the bright lighting and shadows cast on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21030.0, "ram_available_mb": 41810.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21054.5, "ram_available_mb": 41786.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.704}, "power_stats": {"power_gpu_soc_mean_watts": 21.662, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 8.902, "gpu_utilization_percent_mean": 73.704, "power_watts_avg": 21.662, "energy_joules_est": 275.48, "duration_seconds": 12.717, "sample_count": 108}, "timestamp": "2026-01-26T14:58:43.223437"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11626.302, "latencies_ms": [11626.302], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a young man is captured in the midst of a powerful swing. He's dressed in a crisp white shirt and black shorts, his red cap adding a pop of color to his ensemble. His right hand grips a yellow tennis racket, poised to strike the yellow tennis ball that hovers near the net. The green tennis court beneath", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 20965.9, "ram_available_mb": 41874.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21018.3, "ram_available_mb": 41822.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.859}, "power_stats": {"power_gpu_soc_mean_watts": 19.299, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 68.859, "power_watts_avg": 19.299, "energy_joules_est": 224.39, "duration_seconds": 11.627, "sample_count": 99}, "timestamp": "2026-01-26T14:58:56.880412"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9038.547, "latencies_ms": [9038.547], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Boy: 1\n- Racket: 1\n- Tennis ball: 1\n- Net: 1\n- Tennis court: 1\n- Green tarp: 2\n- Sign: 1\n- Fence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.9, "ram_available_mb": 41877.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20958.8, "ram_available_mb": 41882.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.342}, "power_stats": {"power_gpu_soc_mean_watts": 20.641, "power_cpu_cv_mean_watts": 1.711, "power_sys_5v0_mean_watts": 8.633, "gpu_utilization_percent_mean": 71.342, "power_watts_avg": 20.641, "energy_joules_est": 186.58, "duration_seconds": 9.039, "sample_count": 76}, "timestamp": "2026-01-26T14:59:07.936523"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11596.173, "latencies_ms": [11596.173], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, a tennis player is positioned near the baseline, preparing to hit a tennis ball that is near the net. The background features a green tarp with a banner that reads \"Are you next? National Masters.\" The player is closer to the viewer than the tarp, indicating that the action is taking place in the foreground of the image.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20958.8, "ram_available_mb": 41882.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21014.4, "ram_available_mb": 41826.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.204}, "power_stats": {"power_gpu_soc_mean_watts": 19.282, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 70.204, "power_watts_avg": 19.282, "energy_joules_est": 223.61, "duration_seconds": 11.597, "sample_count": 98}, "timestamp": "2026-01-26T14:59:21.562931"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9384.819, "latencies_ms": [9384.819], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "A tennis player is captured in the middle of a backhand swing, wearing a white shirt, black shorts, and a red cap. The setting appears to be a tennis court with a green backdrop and a banner that reads \"Are you next? National Masters.\"", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20959.4, "ram_available_mb": 41881.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20967.3, "ram_available_mb": 41873.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.287}, "power_stats": {"power_gpu_soc_mean_watts": 20.423, "power_cpu_cv_mean_watts": 1.721, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 72.287, "power_watts_avg": 20.423, "energy_joules_est": 191.68, "duration_seconds": 9.385, "sample_count": 80}, "timestamp": "2026-01-26T14:59:32.966985"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7346.267, "latencies_ms": [7346.267], "images_per_second": 0.136, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image shows a tennis player on a court with a greenish hue, indicating it might be an artificial surface. The lighting appears to be natural daylight, casting shadows on the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20967.3, "ram_available_mb": 41873.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21023.5, "ram_available_mb": 41817.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.726}, "power_stats": {"power_gpu_soc_mean_watts": 21.085, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 73.726, "power_watts_avg": 21.085, "energy_joules_est": 154.91, "duration_seconds": 7.347, "sample_count": 62}, "timestamp": "2026-01-26T14:59:42.330467"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11592.649, "latencies_ms": [11592.649], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a room that appears to be in the process of being packed or unpacked. There is a bed with a brown comforter, and a chair is placed near the bed. The room is filled with various items, including a backpack, a handbag, and several boxes. Some of the boxes are placed on the floor, while others are stacked on", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 20960.1, "ram_available_mb": 41880.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.6, "ram_used_mb": 20961.2, "ram_available_mb": 41879.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.212}, "power_stats": {"power_gpu_soc_mean_watts": 19.35, "power_cpu_cv_mean_watts": 2.224, "power_sys_5v0_mean_watts": 8.706, "gpu_utilization_percent_mean": 70.212, "power_watts_avg": 19.35, "energy_joules_est": 224.33, "duration_seconds": 11.593, "sample_count": 99}, "timestamp": "2026-01-26T14:59:55.946041"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9176.334, "latencies_ms": [9176.334], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Bed: 1\n- Chair: 1\n- Backpack: 2\n- Luggage: 1\n- Box: 5\n- Pillow: 1\n- Blanket: 1\n- Lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.2, "ram_available_mb": 41879.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 10.1, "ram_used_mb": 21034.5, "ram_available_mb": 41806.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.13}, "power_stats": {"power_gpu_soc_mean_watts": 20.465, "power_cpu_cv_mean_watts": 2.084, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 72.13, "power_watts_avg": 20.465, "energy_joules_est": 187.81, "duration_seconds": 9.177, "sample_count": 77}, "timestamp": "2026-01-26T15:00:07.143239"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11567.631, "latencies_ms": [11567.631], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a bed with a brown blanket and pillows, and a pile of clothes and bags on the floor. Behind the bed, there are cardboard boxes and a ladder leaning against the wall. To the right, there is a chair and a table with a basket on it. The room appears to be in the process of being moved into", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21034.5, "ram_available_mb": 41806.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 10.7, "ram_used_mb": 20976.4, "ram_available_mb": 41864.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.143}, "power_stats": {"power_gpu_soc_mean_watts": 19.34, "power_cpu_cv_mean_watts": 2.385, "power_sys_5v0_mean_watts": 8.745, "gpu_utilization_percent_mean": 70.143, "power_watts_avg": 19.34, "energy_joules_est": 223.73, "duration_seconds": 11.568, "sample_count": 98}, "timestamp": "2026-01-26T15:00:20.725153"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11236.708, "latencies_ms": [11236.708], "images_per_second": 0.089, "prompt_tokens": 37, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image depicts a room that appears to be in the process of being moved into or out of, with a bed covered in a brown blanket, a chair, and various items scattered around the room. There are cardboard boxes, a backpack, and a suitcase, suggesting that someone is either packing or unpacking their belongings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20976.4, "ram_available_mb": 41864.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.4, "ram_used_mb": 21054.1, "ram_available_mb": 41786.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.677}, "power_stats": {"power_gpu_soc_mean_watts": 19.616, "power_cpu_cv_mean_watts": 2.03, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 69.677, "power_watts_avg": 19.616, "energy_joules_est": 220.43, "duration_seconds": 11.237, "sample_count": 96}, "timestamp": "2026-01-26T15:00:33.983160"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8472.533, "latencies_ms": [8472.533], "images_per_second": 0.118, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The room is dimly lit with a mix of natural and artificial light, creating a cozy atmosphere. The walls are made of brick, and there is a bed covered with a brown blanket, a chair, and various items scattered around the room.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21054.1, "ram_available_mb": 41786.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 9.4, "ram_used_mb": 20996.2, "ram_available_mb": 41844.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.361}, "power_stats": {"power_gpu_soc_mean_watts": 20.419, "power_cpu_cv_mean_watts": 1.94, "power_sys_5v0_mean_watts": 8.69, "gpu_utilization_percent_mean": 71.361, "power_watts_avg": 20.419, "energy_joules_est": 173.01, "duration_seconds": 8.473, "sample_count": 72}, "timestamp": "2026-01-26T15:00:44.474387"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11590.107, "latencies_ms": [11590.107], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is riding a brown horse in an equestrian competition. The rider is wearing a red and green jacket, white pants, and a green helmet. The horse is equipped with a saddle and bridle, and is jumping over a wooden obstacle. The rider is holding a whip in their right hand. The horse", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 20996.2, "ram_available_mb": 41844.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 21049.3, "ram_available_mb": 41791.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.25}, "power_stats": {"power_gpu_soc_mean_watts": 19.409, "power_cpu_cv_mean_watts": 2.205, "power_sys_5v0_mean_watts": 8.719, "gpu_utilization_percent_mean": 70.25, "power_watts_avg": 19.409, "energy_joules_est": 224.97, "duration_seconds": 11.591, "sample_count": 100}, "timestamp": "2026-01-26T15:00:58.085471"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8160.604, "latencies_ms": [8160.604], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "horse: 1, rider: 1, helmet: 1, number 76: 1, flower: 1, wooden fence: 1, grass: 1, tree: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21049.3, "ram_available_mb": 41791.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 20991.8, "ram_available_mb": 41849.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.971}, "power_stats": {"power_gpu_soc_mean_watts": 20.814, "power_cpu_cv_mean_watts": 1.623, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 72.971, "power_watts_avg": 20.814, "energy_joules_est": 169.87, "duration_seconds": 8.161, "sample_count": 70}, "timestamp": "2026-01-26T15:01:08.278713"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11628.796, "latencies_ms": [11628.796], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The horse and rider are in the foreground, jumping over a wooden obstacle. In the background, there are trees and more open grassy areas, indicating the setting is likely an outdoor equestrian course. The rider is positioned on the horse's left side, and they are both centered over the jump, suggesting they are in the middle of the course.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20991.8, "ram_available_mb": 41849.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20989.6, "ram_available_mb": 41851.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.788}, "power_stats": {"power_gpu_soc_mean_watts": 19.139, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.669, "gpu_utilization_percent_mean": 70.788, "power_watts_avg": 19.139, "energy_joules_est": 222.57, "duration_seconds": 11.629, "sample_count": 99}, "timestamp": "2026-01-26T15:01:21.933334"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8854.906, "latencies_ms": [8854.906], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "A rider in a red and green outfit is seen jumping a horse over a wooden obstacle in an outdoor setting with trees in the background. The horse is wearing a white saddle pad with the word \"LISTON\" on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20989.6, "ram_available_mb": 41851.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21046.3, "ram_available_mb": 41794.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.855}, "power_stats": {"power_gpu_soc_mean_watts": 20.403, "power_cpu_cv_mean_watts": 1.69, "power_sys_5v0_mean_watts": 8.613, "gpu_utilization_percent_mean": 72.855, "power_watts_avg": 20.403, "energy_joules_est": 180.69, "duration_seconds": 8.856, "sample_count": 76}, "timestamp": "2026-01-26T15:01:32.807815"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10783.525, "latencies_ms": [10783.525], "images_per_second": 0.093, "prompt_tokens": 36, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image features a horse and rider in mid-air, with the horse's coat a rich chestnut color and the rider wearing a vibrant red and green jacket. The lighting is bright and natural, suggesting the photo was taken on a sunny day, and the background is filled with lush green trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20982.5, "ram_available_mb": 41858.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21041.1, "ram_available_mb": 41799.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.604}, "power_stats": {"power_gpu_soc_mean_watts": 19.512, "power_cpu_cv_mean_watts": 1.856, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 69.604, "power_watts_avg": 19.512, "energy_joules_est": 210.42, "duration_seconds": 10.784, "sample_count": 91}, "timestamp": "2026-01-26T15:01:45.619365"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11588.84, "latencies_ms": [11588.84], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features two men sitting under a large umbrella on a sidewalk, possibly in a cafe or a restaurant. They are surrounded by a few chairs, with one of the men sitting on a chair and the other on the ground. There are also a couple of benches nearby. \n\nIn the background, there are two cars parked on the street, and a", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20989.3, "ram_available_mb": 41851.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21043.6, "ram_available_mb": 41797.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.071}, "power_stats": {"power_gpu_soc_mean_watts": 19.347, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.691, "gpu_utilization_percent_mean": 70.071, "power_watts_avg": 19.347, "energy_joules_est": 224.22, "duration_seconds": 11.589, "sample_count": 99}, "timestamp": "2026-01-26T15:01:59.231088"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9989.974, "latencies_ms": [9989.974], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Chair: 1\n2. Table: 1\n3. Umbrella: 1\n4. Cars: 2\n5. Bicycle: 1\n6. Men: 2\n7. Sign: 1\n8. Chair leg: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20981.0, "ram_available_mb": 41859.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20970.1, "ram_available_mb": 41870.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.977}, "power_stats": {"power_gpu_soc_mean_watts": 20.064, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 70.977, "power_watts_avg": 20.064, "energy_joules_est": 200.45, "duration_seconds": 9.991, "sample_count": 86}, "timestamp": "2026-01-26T15:02:11.244287"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11240.636, "latencies_ms": [11240.636], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground, there is a table with various objects on it, and two men are seated on chairs on either side of the table. The man on the left is seated closer to the camera, while the man on the right is seated further back. In the background, there are cars parked on the street and buildings lining the street.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20970.1, "ram_available_mb": 41870.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21033.5, "ram_available_mb": 41807.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.135}, "power_stats": {"power_gpu_soc_mean_watts": 19.445, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.697, "gpu_utilization_percent_mean": 70.135, "power_watts_avg": 19.445, "energy_joules_est": 218.59, "duration_seconds": 11.241, "sample_count": 96}, "timestamp": "2026-01-26T15:02:24.502566"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8229.083, "latencies_ms": [8229.083], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "Two men are sitting under an umbrella on a cobblestone street, with a car and a bicycle parked nearby. The setting appears to be a busy urban area with shops and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21033.5, "ram_available_mb": 41807.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20986.1, "ram_available_mb": 41854.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.761}, "power_stats": {"power_gpu_soc_mean_watts": 20.966, "power_cpu_cv_mean_watts": 1.64, "power_sys_5v0_mean_watts": 8.595, "gpu_utilization_percent_mean": 73.761, "power_watts_avg": 20.966, "energy_joules_est": 172.54, "duration_seconds": 8.23, "sample_count": 71}, "timestamp": "2026-01-26T15:02:34.780126"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7784.977, "latencies_ms": [7784.977], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image is in black and white, with a focus on two men sitting under a large umbrella. The weather appears to be overcast, as the umbrella is open and the lighting is diffused.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20986.1, "ram_available_mb": 41854.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21035.3, "ram_available_mb": 41805.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.806}, "power_stats": {"power_gpu_soc_mean_watts": 20.874, "power_cpu_cv_mean_watts": 1.648, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 72.806, "power_watts_avg": 20.874, "energy_joules_est": 162.52, "duration_seconds": 7.786, "sample_count": 67}, "timestamp": "2026-01-26T15:02:44.609700"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11580.435, "latencies_ms": [11580.435], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a small, clean, and well-organized kitchen with white cabinets and appliances. The kitchen is equipped with a refrigerator, a stove, and a dishwasher. There are several bottles placed on the countertop, and a bowl can be seen nearby. A ceiling fan is installed above the kitchen, providing ventilation", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 20981.6, "ram_available_mb": 41859.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 20973.2, "ram_available_mb": 41867.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.56}, "power_stats": {"power_gpu_soc_mean_watts": 19.278, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.674, "gpu_utilization_percent_mean": 70.56, "power_watts_avg": 19.278, "energy_joules_est": 223.26, "duration_seconds": 11.581, "sample_count": 100}, "timestamp": "2026-01-26T15:02:58.248372"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11804.063, "latencies_ms": [11804.063], "images_per_second": 0.085, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "fan: 1\nceiling: 1\nlight fixture: 1\nblinds: 2\nwindows: 1\nkitchen cabinets: 6\nfridge: 1\ndishwasher: 1\nstove: 1\noven: 1\nmicrowave: 1\ntoaster: 1\nkitchen utens", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.2, "ram_available_mb": 41867.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21028.9, "ram_available_mb": 41812.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.262}, "power_stats": {"power_gpu_soc_mean_watts": 19.539, "power_cpu_cv_mean_watts": 1.876, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 70.262, "power_watts_avg": 19.539, "energy_joules_est": 230.65, "duration_seconds": 11.805, "sample_count": 103}, "timestamp": "2026-01-26T15:03:12.081187"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11635.947, "latencies_ms": [11635.947], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The kitchen is well-lit with natural light coming from the window, which is located on the right side of the image. The refrigerator, which is a large appliance, is positioned on the right side of the image, near the back wall. The stove and oven are on the left side of the image, with the stove being closer to the foreground and", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21028.9, "ram_available_mb": 41812.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20982.4, "ram_available_mb": 41858.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.158}, "power_stats": {"power_gpu_soc_mean_watts": 19.359, "power_cpu_cv_mean_watts": 1.89, "power_sys_5v0_mean_watts": 8.713, "gpu_utilization_percent_mean": 70.158, "power_watts_avg": 19.359, "energy_joules_est": 225.27, "duration_seconds": 11.637, "sample_count": 101}, "timestamp": "2026-01-26T15:03:25.734555"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8598.775, "latencies_ms": [8598.775], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image depicts a well-lit kitchen with white cabinets and a black countertop. A ceiling fan with a light fixture hangs above the kitchen, and various kitchen utensils and items are scattered on the countertop.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20982.4, "ram_available_mb": 41858.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20973.5, "ram_available_mb": 41867.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.351}, "power_stats": {"power_gpu_soc_mean_watts": 20.7, "power_cpu_cv_mean_watts": 1.665, "power_sys_5v0_mean_watts": 8.599, "gpu_utilization_percent_mean": 73.351, "power_watts_avg": 20.7, "energy_joules_est": 178.01, "duration_seconds": 8.599, "sample_count": 74}, "timestamp": "2026-01-26T15:03:36.355158"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6375.826, "latencies_ms": [6375.826], "images_per_second": 0.157, "prompt_tokens": 36, "response_tokens_est": 33, "n_tiles": 16, "output_text": "The kitchen has a dark ceiling with a ceiling fan that has multiple lights. The cabinets are white and there is a window with white blinds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20973.5, "ram_available_mb": 41867.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20976.0, "ram_available_mb": 41864.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.119, "power_cpu_cv_mean_watts": 1.504, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 76.0, "power_watts_avg": 22.119, "energy_joules_est": 141.04, "duration_seconds": 6.376, "sample_count": 54}, "timestamp": "2026-01-26T15:03:44.745688"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11557.341, "latencies_ms": [11557.341], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a baby is peacefully sleeping on a bed. The baby is lying on its side, with its head resting on a pillow. The bed is covered with a blanket that has a pattern of trains on it. The room is dimly lit, with a lamp providing the primary source of light. The walls of the room are painted in a dark color, creating", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 20976.0, "ram_available_mb": 41864.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 20981.4, "ram_available_mb": 41859.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.449}, "power_stats": {"power_gpu_soc_mean_watts": 19.163, "power_cpu_cv_mean_watts": 1.903, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 70.449, "power_watts_avg": 19.163, "energy_joules_est": 221.49, "duration_seconds": 11.558, "sample_count": 98}, "timestamp": "2026-01-26T15:03:58.354562"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9522.032, "latencies_ms": [9522.032], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Trains: 10\n- Flowers: 4\n- Stuffed animal: 1\n- Pillow: 1\n- Bed: 1\n- Sheet: 1\n- Nightstand: 1\n- Lamp: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 20981.4, "ram_available_mb": 41859.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20970.9, "ram_available_mb": 41870.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.938}, "power_stats": {"power_gpu_soc_mean_watts": 20.319, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.601, "gpu_utilization_percent_mean": 71.938, "power_watts_avg": 20.319, "energy_joules_est": 193.49, "duration_seconds": 9.523, "sample_count": 81}, "timestamp": "2026-01-26T15:04:09.918555"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10401.017, "latencies_ms": [10401.017], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The baby is lying on the left side of the bed, closer to the foreground, while the lamp is positioned in the background, casting a light that illuminates the baby's face. The bed has a patterned blanket with trains on it, and the baby is wearing a sleeveless top.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20970.9, "ram_available_mb": 41870.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 20968.4, "ram_available_mb": 41872.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.478}, "power_stats": {"power_gpu_soc_mean_watts": 19.719, "power_cpu_cv_mean_watts": 1.832, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 71.478, "power_watts_avg": 19.719, "energy_joules_est": 205.11, "duration_seconds": 10.402, "sample_count": 90}, "timestamp": "2026-01-26T15:04:22.340116"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8151.006, "latencies_ms": [8151.006], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A baby is sleeping peacefully on a bed with a blue blanket that has white daisies and a pattern of trains. The room is dimly lit, with a lamp providing a soft glow in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20968.4, "ram_available_mb": 41872.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21017.7, "ram_available_mb": 41823.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.843}, "power_stats": {"power_gpu_soc_mean_watts": 20.997, "power_cpu_cv_mean_watts": 1.629, "power_sys_5v0_mean_watts": 8.61, "gpu_utilization_percent_mean": 72.843, "power_watts_avg": 20.997, "energy_joules_est": 171.16, "duration_seconds": 8.152, "sample_count": 70}, "timestamp": "2026-01-26T15:04:32.540610"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11680.35, "latencies_ms": [11680.35], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a child sleeping on a bed with a dark blue blanket that has a floral pattern with white and yellow flowers. The child is wearing a white tank top and is under a white pillow with a blue and white checkered pattern. The room is dimly lit, with a single lamp providing the main light source, casting a soft glow on the child and the", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 21017.7, "ram_available_mb": 41823.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 21029.1, "ram_available_mb": 41811.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.119}, "power_stats": {"power_gpu_soc_mean_watts": 19.272, "power_cpu_cv_mean_watts": 1.937, "power_sys_5v0_mean_watts": 8.68, "gpu_utilization_percent_mean": 70.119, "power_watts_avg": 19.272, "energy_joules_est": 225.12, "duration_seconds": 11.681, "sample_count": 101}, "timestamp": "2026-01-26T15:04:46.246718"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12368.338, "latencies_ms": [12368.338], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene on a highway, where a green highway sign stands prominently. The sign, which is rectangular in shape, is affixed to a metal pole. The sign is adorned with white text that reads \"Queens Bronx\" and \"EAST 278\", indicating the direction and route of the highway. Above the sign, there'", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21029.1, "ram_available_mb": 41811.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 10.2, "ram_used_mb": 21017.3, "ram_available_mb": 41823.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.755}, "power_stats": {"power_gpu_soc_mean_watts": 21.387, "power_cpu_cv_mean_watts": 2.239, "power_sys_5v0_mean_watts": 8.88, "gpu_utilization_percent_mean": 72.755, "power_watts_avg": 21.387, "energy_joules_est": 264.54, "duration_seconds": 12.369, "sample_count": 106}, "timestamp": "2026-01-26T15:05:00.638634"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8623.378, "latencies_ms": [8623.378], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "sign: 2, truck: 0, highway: 1, graffiti: 2, letter: 10, word: 3, city: 2, direction: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21017.3, "ram_available_mb": 41823.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.6, "ram_used_mb": 21106.6, "ram_available_mb": 41734.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.63}, "power_stats": {"power_gpu_soc_mean_watts": 22.953, "power_cpu_cv_mean_watts": 1.941, "power_sys_5v0_mean_watts": 8.809, "gpu_utilization_percent_mean": 76.63, "power_watts_avg": 22.953, "energy_joules_est": 197.95, "duration_seconds": 8.624, "sample_count": 73}, "timestamp": "2026-01-26T15:05:11.328399"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12177.802, "latencies_ms": [12177.802], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The 'NO TRUCKS' sign is positioned in the upper left corner, above the main highway sign. The highway sign is in the foreground, indicating directions to Queens and the Bronx, and is located in the left foreground of the image. The 'Interstate 278' logo is centrally placed on the highway sign, and the 'Dept.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21003.2, "ram_available_mb": 41837.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 20994.0, "ram_available_mb": 41846.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.654}, "power_stats": {"power_gpu_soc_mean_watts": 21.427, "power_cpu_cv_mean_watts": 1.974, "power_sys_5v0_mean_watts": 8.926, "gpu_utilization_percent_mean": 69.654, "power_watts_avg": 21.427, "energy_joules_est": 260.95, "duration_seconds": 12.178, "sample_count": 104}, "timestamp": "2026-01-26T15:05:25.559686"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9999.727, "latencies_ms": [9999.727], "images_per_second": 0.1, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a green highway sign with white text that reads \"NO TRUCKS\" at the top and \"EAST Interstate 278 Queens Bronx\" at the bottom. The sign is mounted on a metal structure with a bridge in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20994.0, "ram_available_mb": 41846.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 20983.0, "ram_available_mb": 41857.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.141}, "power_stats": {"power_gpu_soc_mean_watts": 22.306, "power_cpu_cv_mean_watts": 1.605, "power_sys_5v0_mean_watts": 8.757, "gpu_utilization_percent_mean": 75.141, "power_watts_avg": 22.306, "energy_joules_est": 223.07, "duration_seconds": 10.0, "sample_count": 85}, "timestamp": "2026-01-26T15:05:37.577372"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7448.548, "latencies_ms": [7448.548], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The sign is predominantly green with white lettering, indicating it is likely a directional highway sign. It is mounted on a metal structure with a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20983.0, "ram_available_mb": 41857.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21065.7, "ram_available_mb": 41775.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.806}, "power_stats": {"power_gpu_soc_mean_watts": 23.498, "power_cpu_cv_mean_watts": 1.465, "power_sys_5v0_mean_watts": 8.894, "gpu_utilization_percent_mean": 76.806, "power_watts_avg": 23.498, "energy_joules_est": 175.04, "duration_seconds": 7.449, "sample_count": 62}, "timestamp": "2026-01-26T15:05:47.038013"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11574.46, "latencies_ms": [11574.46], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a vintage red Chevrolet truck parked in a parking lot. The truck is positioned in the center of the frame, with its rear end facing the viewer. The parking lot is filled with other cars, some of which are parked behind the red truck. The scene is set under a cloudy sky, giving the image a", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 21065.7, "ram_available_mb": 41775.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21064.2, "ram_available_mb": 41776.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.717}, "power_stats": {"power_gpu_soc_mean_watts": 19.358, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.718, "gpu_utilization_percent_mean": 69.717, "power_watts_avg": 19.358, "energy_joules_est": 224.07, "duration_seconds": 11.575, "sample_count": 99}, "timestamp": "2026-01-26T15:06:00.676067"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8951.328, "latencies_ms": [8951.328], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "- Car: 1\n- Truck: 1\n- Street light: 1\n- Clouds: 1\n- Sky: 1\n- Tire: 1\n- Grill: 1\n- Tent: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21064.2, "ram_available_mb": 41776.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21079.7, "ram_available_mb": 41761.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.724}, "power_stats": {"power_gpu_soc_mean_watts": 20.372, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 8.635, "gpu_utilization_percent_mean": 72.724, "power_watts_avg": 20.372, "energy_joules_est": 182.37, "duration_seconds": 8.952, "sample_count": 76}, "timestamp": "2026-01-26T15:06:11.671914"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10329.353, "latencies_ms": [10329.353], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The red Chevrolet truck is positioned in the foreground of the image, appearing large and prominent. It is located on the left side of the frame, with other cars visible in the background. The truck is parked on a gravel surface, and there is a street lamp to the right of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21012.4, "ram_available_mb": 41828.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21002.9, "ram_available_mb": 41838.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.159}, "power_stats": {"power_gpu_soc_mean_watts": 19.56, "power_cpu_cv_mean_watts": 1.828, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 70.159, "power_watts_avg": 19.56, "energy_joules_est": 202.05, "duration_seconds": 10.33, "sample_count": 88}, "timestamp": "2026-01-26T15:06:24.038071"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9389.634, "latencies_ms": [9389.634], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image features a vintage red Chevrolet truck parked on a gravel surface, likely at a car show or gathering. The truck is the main focus of the image, with a clear blue sky and a few other cars visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21002.9, "ram_available_mb": 41838.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21056.0, "ram_available_mb": 41784.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.812}, "power_stats": {"power_gpu_soc_mean_watts": 20.379, "power_cpu_cv_mean_watts": 1.721, "power_sys_5v0_mean_watts": 8.606, "gpu_utilization_percent_mean": 71.812, "power_watts_avg": 20.379, "energy_joules_est": 191.36, "duration_seconds": 9.39, "sample_count": 80}, "timestamp": "2026-01-26T15:06:35.455660"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10613.049, "latencies_ms": [10613.049], "images_per_second": 0.094, "prompt_tokens": 36, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The image features a vibrant red Chevrolet truck with a shiny, reflective surface that catches the light beautifully. The weather appears to be clear with a few clouds in the sky, and the lighting suggests it might be a sunny day, enhancing the truck's glossy finish.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21056.0, "ram_available_mb": 41784.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21014.5, "ram_available_mb": 41826.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.256}, "power_stats": {"power_gpu_soc_mean_watts": 19.627, "power_cpu_cv_mean_watts": 1.841, "power_sys_5v0_mean_watts": 8.708, "gpu_utilization_percent_mean": 70.256, "power_watts_avg": 19.627, "energy_joules_est": 208.32, "duration_seconds": 10.614, "sample_count": 90}, "timestamp": "2026-01-26T15:06:48.109802"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11606.529, "latencies_ms": [11606.529], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a black and white photograph of three cows standing behind a barbed wire fence. The cows are looking directly at the camera, giving the impression that they are posing for the picture. The fence is made of barbed wire, which is a common type of fencing used to keep livestock contained. The cows appear to be in a rural setting,", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 21014.5, "ram_available_mb": 41826.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21034.3, "ram_available_mb": 41806.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.535}, "power_stats": {"power_gpu_soc_mean_watts": 19.275, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.686, "gpu_utilization_percent_mean": 70.535, "power_watts_avg": 19.275, "energy_joules_est": 223.73, "duration_seconds": 11.607, "sample_count": 99}, "timestamp": "2026-01-26T15:07:01.759231"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7796.182, "latencies_ms": [7796.182], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "cow: 3\nbarbed wire: 1\nwire: 1\ngrass: 1\nfield: 1\nsky: 1\nclouds: 1\nland: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20972.5, "ram_available_mb": 41868.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 20981.9, "ram_available_mb": 41859.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.818}, "power_stats": {"power_gpu_soc_mean_watts": 21.203, "power_cpu_cv_mean_watts": 1.576, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 73.818, "power_watts_avg": 21.203, "energy_joules_est": 165.32, "duration_seconds": 7.797, "sample_count": 66}, "timestamp": "2026-01-26T15:07:11.579039"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11573.138, "latencies_ms": [11573.138], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are three cows standing close to each other, with one facing the camera and the other two partially visible behind it. The cows are positioned near the bottom of the image, with the grassy field extending upwards towards the sky. The barbed wire fence runs horizontally across the image, separating the cows from the background which consists of", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20981.9, "ram_available_mb": 41859.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21025.8, "ram_available_mb": 41815.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.469}, "power_stats": {"power_gpu_soc_mean_watts": 19.373, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.667, "gpu_utilization_percent_mean": 70.469, "power_watts_avg": 19.373, "energy_joules_est": 224.22, "duration_seconds": 11.574, "sample_count": 98}, "timestamp": "2026-01-26T15:07:25.177206"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6969.924, "latencies_ms": [6969.924], "images_per_second": 0.143, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "Three cows are standing behind a barbed wire fence, looking directly at the camera. The image is in black and white, giving it a timeless and classic feel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20969.8, "ram_available_mb": 41871.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 20963.6, "ram_available_mb": 41877.3, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.627}, "power_stats": {"power_gpu_soc_mean_watts": 21.967, "power_cpu_cv_mean_watts": 1.499, "power_sys_5v0_mean_watts": 8.588, "gpu_utilization_percent_mean": 74.627, "power_watts_avg": 21.967, "energy_joules_est": 153.12, "duration_seconds": 6.971, "sample_count": 59}, "timestamp": "2026-01-26T15:07:34.183054"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11540.752, "latencies_ms": [11540.752], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a black and white photograph, which gives it a timeless and classic feel. The lighting is natural and soft, suggesting that the photo was taken during the day. The cows are standing behind a barbed wire fence, and the background shows a cloudy sky and a hilly landscape. The grass is tall and the cows are looking directly at the camera, giving", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20963.6, "ram_available_mb": 41877.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21036.6, "ram_available_mb": 41804.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.717}, "power_stats": {"power_gpu_soc_mean_watts": 19.342, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.692, "gpu_utilization_percent_mean": 69.717, "power_watts_avg": 19.342, "energy_joules_est": 223.23, "duration_seconds": 11.541, "sample_count": 99}, "timestamp": "2026-01-26T15:07:47.781834"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11568.99, "latencies_ms": [11568.99], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and cozy bedroom, bathed in soft light. Dominating the scene is a large bed, dressed in a pristine white comforter that contrasts beautifully with the warm tones of the wooden headboard. The bed is flanked by two nightstands, each adorned with a lamp, casting a gentle glow that", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21036.6, "ram_available_mb": 41804.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 20974.4, "ram_available_mb": 41866.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.475}, "power_stats": {"power_gpu_soc_mean_watts": 19.383, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.698, "gpu_utilization_percent_mean": 71.475, "power_watts_avg": 19.383, "energy_joules_est": 224.25, "duration_seconds": 11.57, "sample_count": 99}, "timestamp": "2026-01-26T15:08:01.374336"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7899.011, "latencies_ms": [7899.011], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "bed: 1, chair: 1, armchair: 1, fireplace: 1, clock: 1, lamp: 1, pillow: 2, blanket: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20974.4, "ram_available_mb": 41866.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21023.5, "ram_available_mb": 41817.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.379}, "power_stats": {"power_gpu_soc_mean_watts": 21.167, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 73.379, "power_watts_avg": 21.167, "energy_joules_est": 167.21, "duration_seconds": 7.9, "sample_count": 66}, "timestamp": "2026-01-26T15:08:11.296115"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8266.583, "latencies_ms": [8266.583], "images_per_second": 0.121, "prompt_tokens": 44, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The bed is positioned in the foreground on the left side of the image, while the fireplace is in the background on the right side. The chair is placed near the fireplace, closer to the viewer than the fireplace.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21023.5, "ram_available_mb": 41817.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 20974.1, "ram_available_mb": 41866.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.914}, "power_stats": {"power_gpu_soc_mean_watts": 20.705, "power_cpu_cv_mean_watts": 1.703, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 71.914, "power_watts_avg": 20.705, "energy_joules_est": 171.17, "duration_seconds": 8.267, "sample_count": 70}, "timestamp": "2026-01-26T15:08:21.619561"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7212.316, "latencies_ms": [7212.316], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image depicts a cozy bedroom with a large bed, a chair, and a fireplace. The room is dimly lit, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20974.1, "ram_available_mb": 41866.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21023.2, "ram_available_mb": 41817.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.902}, "power_stats": {"power_gpu_soc_mean_watts": 21.714, "power_cpu_cv_mean_watts": 1.509, "power_sys_5v0_mean_watts": 8.571, "gpu_utilization_percent_mean": 74.902, "power_watts_avg": 21.714, "energy_joules_est": 156.62, "duration_seconds": 7.213, "sample_count": 61}, "timestamp": "2026-01-26T15:08:30.857328"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9168.864, "latencies_ms": [9168.864], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The room is warmly lit with a soft glow, featuring a wooden ceiling with recessed lighting that casts a warm ambiance. The walls are adorned with a clock and a fireplace, adding a cozy and inviting atmosphere to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20961.5, "ram_available_mb": 41879.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21017.4, "ram_available_mb": 41823.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.09}, "power_stats": {"power_gpu_soc_mean_watts": 20.1, "power_cpu_cv_mean_watts": 1.749, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 71.09, "power_watts_avg": 20.1, "energy_joules_est": 184.31, "duration_seconds": 9.169, "sample_count": 78}, "timestamp": "2026-01-26T15:08:42.043905"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11623.342, "latencies_ms": [11623.342], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are three guinea fowls walking across a dry grass field. The birds are spread out, with one in the foreground, one in the middle, and one in the background. They are all walking in the same direction, and the field appears to be dry and barren. The sky is visible in the background, and the overall scene is quite peaceful.", "error": null, "sys_before": {"cpu_percent": 8.8, "ram_used_mb": 21017.4, "ram_available_mb": 41823.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21035.9, "ram_available_mb": 41805.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.333}, "power_stats": {"power_gpu_soc_mean_watts": 19.287, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.681, "gpu_utilization_percent_mean": 70.333, "power_watts_avg": 19.287, "energy_joules_est": 224.19, "duration_seconds": 11.624, "sample_count": 99}, "timestamp": "2026-01-26T15:08:55.734511"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9613.905, "latencies_ms": [9613.905], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "1. Birds: 3\n2. Grass: 1\n3. Hill: 1\n4. Bush: 1\n5. Sky: 1\n6. Ground: 1\n7. Water: 1\n8. Tree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21035.9, "ram_available_mb": 41805.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21067.3, "ram_available_mb": 41773.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.086}, "power_stats": {"power_gpu_soc_mean_watts": 20.231, "power_cpu_cv_mean_watts": 1.729, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 72.086, "power_watts_avg": 20.231, "energy_joules_est": 194.52, "duration_seconds": 9.615, "sample_count": 81}, "timestamp": "2026-01-26T15:09:07.374408"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9768.99, "latencies_ms": [9768.99], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "In the foreground, there are three guinea fowls on a dry grassy hill. The largest bird is positioned slightly behind the other two, which are closer to the camera. In the background, there are some shrubs and a body of water, which appear smaller due to the distance.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21067.3, "ram_available_mb": 41773.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 21019.1, "ram_available_mb": 41821.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.084}, "power_stats": {"power_gpu_soc_mean_watts": 19.966, "power_cpu_cv_mean_watts": 1.852, "power_sys_5v0_mean_watts": 8.653, "gpu_utilization_percent_mean": 71.084, "power_watts_avg": 19.966, "energy_joules_est": 195.06, "duration_seconds": 9.77, "sample_count": 83}, "timestamp": "2026-01-26T15:09:19.174593"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7096.396, "latencies_ms": [7096.396], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "Three guinea fowls are walking across a dry grassy field with sparse vegetation. The sky is overcast, and there are some bushes and trees in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 21019.1, "ram_available_mb": 41821.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 9.2, "ram_used_mb": 21026.8, "ram_available_mb": 41814.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.617}, "power_stats": {"power_gpu_soc_mean_watts": 21.841, "power_cpu_cv_mean_watts": 2.001, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 74.617, "power_watts_avg": 21.841, "energy_joules_est": 155.01, "duration_seconds": 7.097, "sample_count": 60}, "timestamp": "2026-01-26T15:09:28.313921"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9637.623, "latencies_ms": [9637.623], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image features three guinea fowls with speckled black feathers, standing on a dry, grassy hillside. The lighting is soft and diffused, suggesting an overcast day, and the weather appears to be calm without any visible signs of rain or harsh sunlight.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21026.8, "ram_available_mb": 41814.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21046.8, "ram_available_mb": 41794.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.268}, "power_stats": {"power_gpu_soc_mean_watts": 19.785, "power_cpu_cv_mean_watts": 1.806, "power_sys_5v0_mean_watts": 8.673, "gpu_utilization_percent_mean": 71.268, "power_watts_avg": 19.785, "energy_joules_est": 190.69, "duration_seconds": 9.638, "sample_count": 82}, "timestamp": "2026-01-26T15:09:39.966555"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11617.261, "latencies_ms": [11617.261], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, three people are standing in the snow, each holding ski poles and wearing skis. They appear to be posing for a picture, possibly for a skiing trip or a group photo. The skis are visible on the ground, and the snowy environment suggests that they are in a winter setting.\n\nThe three people are standing close to each other, with", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21046.8, "ram_available_mb": 41794.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21047.7, "ram_available_mb": 41793.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.881}, "power_stats": {"power_gpu_soc_mean_watts": 19.254, "power_cpu_cv_mean_watts": 1.898, "power_sys_5v0_mean_watts": 8.656, "gpu_utilization_percent_mean": 69.881, "power_watts_avg": 19.254, "energy_joules_est": 223.69, "duration_seconds": 11.618, "sample_count": 101}, "timestamp": "2026-01-26T15:09:53.614641"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8802.83, "latencies_ms": [8802.83], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "- Skier: 3\n- Skis: 3\n- Trees: numerous\n- Snow: covering ground\n- Snowboard: 1\n- Snowboarder: 1\n- Snow: covering ground\n- Snow: covering ground", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 20977.4, "ram_available_mb": 41863.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20971.8, "ram_available_mb": 41869.1, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.813}, "power_stats": {"power_gpu_soc_mean_watts": 20.643, "power_cpu_cv_mean_watts": 1.686, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 72.813, "power_watts_avg": 20.643, "energy_joules_est": 181.73, "duration_seconds": 8.803, "sample_count": 75}, "timestamp": "2026-01-26T15:10:04.445494"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10335.66, "latencies_ms": [10335.66], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, there are three individuals engaged in cross-country skiing. The person on the left is positioned slightly behind the other two, moving towards the right side of the image. The background features a snowy landscape with trees covered in snow, indicating that the skiing is taking place in a wooded area.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 20971.8, "ram_available_mb": 41869.1, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21030.2, "ram_available_mb": 41810.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.739}, "power_stats": {"power_gpu_soc_mean_watts": 19.718, "power_cpu_cv_mean_watts": 1.819, "power_sys_5v0_mean_watts": 8.666, "gpu_utilization_percent_mean": 70.739, "power_watts_avg": 19.718, "energy_joules_est": 203.81, "duration_seconds": 10.336, "sample_count": 88}, "timestamp": "2026-01-26T15:10:16.811866"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6306.184, "latencies_ms": [6306.184], "images_per_second": 0.159, "prompt_tokens": 37, "response_tokens_est": 31, "n_tiles": 16, "output_text": "Three people are skiing in a snowy forest on a sunny day. They are all wearing winter gear and holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21030.2, "ram_available_mb": 41810.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21074.6, "ram_available_mb": 41766.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.434}, "power_stats": {"power_gpu_soc_mean_watts": 22.36, "power_cpu_cv_mean_watts": 1.374, "power_sys_5v0_mean_watts": 8.545, "gpu_utilization_percent_mean": 76.434, "power_watts_avg": 22.36, "energy_joules_est": 141.02, "duration_seconds": 6.307, "sample_count": 53}, "timestamp": "2026-01-26T15:10:25.182014"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11521.638, "latencies_ms": [11521.638], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The image shows three individuals cross-country skiing on a snowy landscape with trees in the background. They are wearing winter clothing, including jackets and hats, and are equipped with ski poles and skis. The weather appears to be cold and snowy, as evidenced by the snow-covered trees and the skiers' attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20962.6, "ram_available_mb": 41878.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21030.8, "ram_available_mb": 41810.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.01}, "power_stats": {"power_gpu_soc_mean_watts": 19.156, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 69.01, "power_watts_avg": 19.156, "energy_joules_est": 220.73, "duration_seconds": 11.523, "sample_count": 98}, "timestamp": "2026-01-26T15:10:38.735800"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12418.886, "latencies_ms": [12418.886], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment on a city street, where a white bus with a blue stripe running along its side is in motion. The bus is adorned with a digital display on the front, proudly announcing \"51 CrossTown\" in bold, black letters against a white background. The bus number, \"211\", is also prominently displayed in red", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21030.8, "ram_available_mb": 41810.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 20971.0, "ram_available_mb": 41869.9, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.443}, "power_stats": {"power_gpu_soc_mean_watts": 21.509, "power_cpu_cv_mean_watts": 1.823, "power_sys_5v0_mean_watts": 8.923, "gpu_utilization_percent_mean": 72.443, "power_watts_avg": 21.509, "energy_joules_est": 267.13, "duration_seconds": 12.42, "sample_count": 106}, "timestamp": "2026-01-26T15:10:53.182741"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9354.489, "latencies_ms": [9354.489], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "bus: 1, window: multiple, license plate: 1, destination sign: 1, front grille: 1, headlight: 2, front bumper: 1, front wheel: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20971.0, "ram_available_mb": 41869.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 20966.3, "ram_available_mb": 41874.6, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.278}, "power_stats": {"power_gpu_soc_mean_watts": 22.982, "power_cpu_cv_mean_watts": 1.499, "power_sys_5v0_mean_watts": 8.786, "gpu_utilization_percent_mean": 75.278, "power_watts_avg": 22.982, "energy_joules_est": 215.0, "duration_seconds": 9.355, "sample_count": 79}, "timestamp": "2026-01-26T15:11:04.554892"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10591.37, "latencies_ms": [10591.37], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The bus is in the foreground of the image, with the front facing the viewer. It is positioned on the right side of the image, with the road visible in the background. The building with the number 51 is in the background, slightly to the left of the bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20966.3, "ram_available_mb": 41874.6, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 20960.9, "ram_available_mb": 41880.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.533}, "power_stats": {"power_gpu_soc_mean_watts": 22.385, "power_cpu_cv_mean_watts": 1.663, "power_sys_5v0_mean_watts": 8.901, "gpu_utilization_percent_mean": 74.533, "power_watts_avg": 22.385, "energy_joules_est": 237.1, "duration_seconds": 10.592, "sample_count": 90}, "timestamp": "2026-01-26T15:11:17.161896"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9599.238, "latencies_ms": [9599.238], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A bus with the number 51 and the destination \"Crosstown\" displayed on its front is driving on a city street. The bus is white with blue and green accents and has the number 61 on its side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20960.9, "ram_available_mb": 41880.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21029.6, "ram_available_mb": 41811.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.12}, "power_stats": {"power_gpu_soc_mean_watts": 22.691, "power_cpu_cv_mean_watts": 1.533, "power_sys_5v0_mean_watts": 8.794, "gpu_utilization_percent_mean": 76.12, "power_watts_avg": 22.691, "energy_joules_est": 217.83, "duration_seconds": 9.6, "sample_count": 83}, "timestamp": "2026-01-26T15:11:28.799170"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8284.439, "latencies_ms": [8284.439], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The bus is predominantly white with blue and green accents, and the number 51 is displayed in red on the front. The sky is partly cloudy, suggesting variable weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21029.6, "ram_available_mb": 41811.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 20972.1, "ram_available_mb": 41868.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.643}, "power_stats": {"power_gpu_soc_mean_watts": 23.25, "power_cpu_cv_mean_watts": 1.435, "power_sys_5v0_mean_watts": 8.828, "gpu_utilization_percent_mean": 78.643, "power_watts_avg": 23.25, "energy_joules_est": 192.63, "duration_seconds": 8.285, "sample_count": 70}, "timestamp": "2026-01-26T15:11:39.129581"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11322.559, "latencies_ms": [11322.559], "images_per_second": 0.088, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing in front of a white wall, exuding an air of confidence. He is dressed in a navy blue blazer, which is adorned with a white crest on the left lapel, adding a touch of elegance to his attire. The blazer is paired with a white shirt, which is complemented by a blue", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 20972.1, "ram_available_mb": 41868.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 20967.4, "ram_available_mb": 41873.5, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 9953.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.417}, "power_stats": {"power_gpu_soc_mean_watts": 17.75, "power_cpu_cv_mean_watts": 1.922, "power_sys_5v0_mean_watts": 8.548, "gpu_utilization_percent_mean": 68.417, "power_watts_avg": 17.75, "energy_joules_est": 200.99, "duration_seconds": 11.323, "sample_count": 96}, "timestamp": "2026-01-26T15:11:52.496943"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9382.328, "latencies_ms": [9382.328], "images_per_second": 0.107, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Man: 1\n\n- Jacket: 1\n\n- Shirt: 1\n\n- Tie: 1\n\n- Skirt: 1\n\n- Pants: 1\n\n- Pocket: 1\n\n- Bag: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 20967.4, "ram_available_mb": 41873.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21000.1, "ram_available_mb": 41840.8, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 9976.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.861}, "power_stats": {"power_gpu_soc_mean_watts": 18.696, "power_cpu_cv_mean_watts": 1.788, "power_sys_5v0_mean_watts": 8.531, "gpu_utilization_percent_mean": 69.861, "power_watts_avg": 18.696, "energy_joules_est": 175.42, "duration_seconds": 9.383, "sample_count": 79}, "timestamp": "2026-01-26T15:12:03.903493"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9321.212, "latencies_ms": [9321.212], "images_per_second": 0.107, "prompt_tokens": 44, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The man is standing in the foreground with a bag in his left hand and a cane in his right hand. The background consists of a white wall and a red barrier. The man is positioned near the center of the image, with the bag and cane placed close to his body.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21000.1, "ram_available_mb": 41840.8, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 21001.7, "ram_available_mb": 41839.2, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 9983.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.291}, "power_stats": {"power_gpu_soc_mean_watts": 18.342, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 8.551, "gpu_utilization_percent_mean": 69.291, "power_watts_avg": 18.342, "energy_joules_est": 170.98, "duration_seconds": 9.322, "sample_count": 79}, "timestamp": "2026-01-26T15:12:15.252491"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7530.65, "latencies_ms": [7530.65], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A man is standing in a room with a red and white striped background, wearing a blue blazer, white shirt, and a striped tie. He is holding a black bag in his left hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21001.7, "ram_available_mb": 41839.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21025.2, "ram_available_mb": 41815.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 9973.0, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.677}, "power_stats": {"power_gpu_soc_mean_watts": 19.68, "power_cpu_cv_mean_watts": 1.662, "power_sys_5v0_mean_watts": 8.498, "gpu_utilization_percent_mean": 71.677, "power_watts_avg": 19.68, "energy_joules_est": 148.21, "duration_seconds": 7.531, "sample_count": 65}, "timestamp": "2026-01-26T15:12:24.810163"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7139.924, "latencies_ms": [7139.924], "images_per_second": 0.14, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image shows a person in a formal outfit with a blue blazer, white shirt, and a striped tie. The person is standing indoors with a red and white striped background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 20972.6, "ram_available_mb": 41868.3, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21030.0, "ram_available_mb": 41810.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 9971.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.467}, "power_stats": {"power_gpu_soc_mean_watts": 19.66, "power_cpu_cv_mean_watts": 1.654, "power_sys_5v0_mean_watts": 8.532, "gpu_utilization_percent_mean": 71.467, "power_watts_avg": 19.66, "energy_joules_est": 140.38, "duration_seconds": 7.141, "sample_count": 60}, "timestamp": "2026-01-26T15:12:33.987181"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11642.873, "latencies_ms": [11642.873], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a power plant, bathed in a sepia-toned haze. The plant is a complex network of metal structures, with a series of large cylindrical tanks and pipes that stretch across the frame. These tanks and pipes are arranged in a grid-like pattern, creating a sense of order and precision. The perspective of the image", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 20976.7, "ram_available_mb": 41864.2, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21028.3, "ram_available_mb": 41812.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.556}, "power_stats": {"power_gpu_soc_mean_watts": 19.278, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 70.556, "power_watts_avg": 19.278, "energy_joules_est": 224.46, "duration_seconds": 11.644, "sample_count": 99}, "timestamp": "2026-01-26T15:12:47.677841"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11092.665, "latencies_ms": [11092.665], "images_per_second": 0.09, "prompt_tokens": 39, "response_tokens_est": 73, "n_tiles": 16, "output_text": "1. Transformer: 10\n2. Transformer: 10\n3. Transformer: 10\n4. Transformer: 10\n5. Transformer: 10\n6. Transformer: 10\n7. Transformer: 10\n8. Transformer: 10", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21028.3, "ram_available_mb": 41812.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21042.0, "ram_available_mb": 41798.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.135}, "power_stats": {"power_gpu_soc_mean_watts": 19.618, "power_cpu_cv_mean_watts": 1.838, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 71.135, "power_watts_avg": 19.618, "energy_joules_est": 217.63, "duration_seconds": 11.093, "sample_count": 96}, "timestamp": "2026-01-26T15:13:00.810314"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11580.135, "latencies_ms": [11580.135], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are multiple cylindrical objects that appear to be storage tanks or silos, arranged in a row and spaced evenly apart. They are positioned near the bottom of the image, creating a sense of depth as they recede into the background. The background features a series of tall, slender poles that are connected by a network of wires", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21042.0, "ram_available_mb": 41798.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21014.9, "ram_available_mb": 41826.0, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.697}, "power_stats": {"power_gpu_soc_mean_watts": 19.246, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.695, "gpu_utilization_percent_mean": 70.697, "power_watts_avg": 19.246, "energy_joules_est": 222.88, "duration_seconds": 11.581, "sample_count": 99}, "timestamp": "2026-01-26T15:13:14.413868"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9639.95, "latencies_ms": [9639.95], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image depicts a large industrial facility with multiple cylindrical storage tanks arranged in rows, connected by a network of pipes and cables. The facility appears to be a power plant or a chemical processing plant, with the tanks likely containing various fluids or chemicals.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21014.9, "ram_available_mb": 41826.0, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21085.6, "ram_available_mb": 41755.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.12}, "power_stats": {"power_gpu_soc_mean_watts": 20.163, "power_cpu_cv_mean_watts": 1.755, "power_sys_5v0_mean_watts": 8.608, "gpu_utilization_percent_mean": 72.12, "power_watts_avg": 20.163, "energy_joules_est": 194.38, "duration_seconds": 9.641, "sample_count": 83}, "timestamp": "2026-01-26T15:13:26.081876"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11587.956, "latencies_ms": [11587.956], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a series of cylindrical objects that appear to be storage tanks, possibly for oil or chemicals, arranged in a row. The tanks are metallic and have a reflective surface, indicating they are likely made of steel or a similar material. The lighting in the image is dim, with the tanks casting shadows, and the overall color tone", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21085.6, "ram_available_mb": 41755.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 21030.1, "ram_available_mb": 41810.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.848}, "power_stats": {"power_gpu_soc_mean_watts": 19.29, "power_cpu_cv_mean_watts": 2.098, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 69.848, "power_watts_avg": 19.29, "energy_joules_est": 223.54, "duration_seconds": 11.589, "sample_count": 99}, "timestamp": "2026-01-26T15:13:39.692773"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11562.217, "latencies_ms": [11562.217], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a bathroom with a white toilet positioned in the center. The floor is covered with various items, including a pile of shoes, a backpack, and a handbag. There are also multiple bottles scattered around the room, some of which are placed near the toilet. A potted plant can be seen in the background, adding a", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 21030.1, "ram_available_mb": 41810.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21062.8, "ram_available_mb": 41778.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.646}, "power_stats": {"power_gpu_soc_mean_watts": 19.185, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.682, "gpu_utilization_percent_mean": 70.646, "power_watts_avg": 19.185, "energy_joules_est": 221.83, "duration_seconds": 11.563, "sample_count": 99}, "timestamp": "2026-01-26T15:13:53.287433"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8487.905, "latencies_ms": [8487.905], "images_per_second": 0.118, "prompt_tokens": 39, "response_tokens_est": 50, "n_tiles": 16, "output_text": "shoes: 12, backpacks: 2, bottles: 4, toilet: 1, plants: 2, towels: 3, books: 1, bags: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21062.8, "ram_available_mb": 41778.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21063.6, "ram_available_mb": 41777.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.123}, "power_stats": {"power_gpu_soc_mean_watts": 20.722, "power_cpu_cv_mean_watts": 1.655, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 73.123, "power_watts_avg": 20.722, "energy_joules_est": 175.9, "duration_seconds": 8.489, "sample_count": 73}, "timestamp": "2026-01-26T15:14:03.796257"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11636.642, "latencies_ms": [11636.642], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a pile of shoes and a backpack on the floor, indicating a cluttered space. The toilet is positioned in the background, slightly to the left, and there is a plant on a stand to the right of the toilet. The shoes are scattered around the floor, with some closer to the viewer", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21063.6, "ram_available_mb": 41777.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21063.6, "ram_available_mb": 41777.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.87}, "power_stats": {"power_gpu_soc_mean_watts": 19.381, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.684, "gpu_utilization_percent_mean": 70.87, "power_watts_avg": 19.381, "energy_joules_est": 225.54, "duration_seconds": 11.637, "sample_count": 100}, "timestamp": "2026-01-26T15:14:17.487532"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10321.391, "latencies_ms": [10321.391], "images_per_second": 0.097, "prompt_tokens": 37, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image depicts a bathroom with a white toilet and a pile of shoes and bags scattered on the floor. There is a plant on the counter and a towel hanging on the wall. It appears that someone has been in a hurry and left their belongings in disarray.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21063.6, "ram_available_mb": 41777.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21063.8, "ram_available_mb": 41777.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.944}, "power_stats": {"power_gpu_soc_mean_watts": 20.073, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 70.944, "power_watts_avg": 20.073, "energy_joules_est": 207.19, "duration_seconds": 10.322, "sample_count": 89}, "timestamp": "2026-01-26T15:14:29.831391"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8532.356, "latencies_ms": [8532.356], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a bathroom with a white toilet and a green plant on the counter. There are several pairs of shoes lined up on the left side of the image, and a pile of clothes and other items on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21063.8, "ram_available_mb": 41777.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21086.0, "ram_available_mb": 41754.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.466}, "power_stats": {"power_gpu_soc_mean_watts": 20.59, "power_cpu_cv_mean_watts": 1.721, "power_sys_5v0_mean_watts": 8.679, "gpu_utilization_percent_mean": 72.466, "power_watts_avg": 20.59, "energy_joules_est": 175.69, "duration_seconds": 8.533, "sample_count": 73}, "timestamp": "2026-01-26T15:14:40.408539"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11613.576, "latencies_ms": [11613.576], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a polar bear is playing with three colorful balls in a pool of water. The bear is holding a green ball in its mouth, while the other two balls are floating nearby. The bear appears to be enjoying itself, splashing around and having fun with the balls. The scene captures the bear's playful nature and its interaction with the objects in the water", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 21086.0, "ram_available_mb": 41754.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21086.5, "ram_available_mb": 41754.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.58}, "power_stats": {"power_gpu_soc_mean_watts": 19.257, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.648, "gpu_utilization_percent_mean": 69.58, "power_watts_avg": 19.257, "energy_joules_est": 223.65, "duration_seconds": 11.614, "sample_count": 100}, "timestamp": "2026-01-26T15:14:54.051162"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7535.608, "latencies_ms": [7535.608], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "ball: 3\nwater: multiple\npolar bear: 1\nrope: 1\nsand: multiple\nrocks: multiple\nstones: multiple\nleaves: multiple", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21024.3, "ram_available_mb": 41816.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21052.8, "ram_available_mb": 41788.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.312}, "power_stats": {"power_gpu_soc_mean_watts": 21.476, "power_cpu_cv_mean_watts": 1.557, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 74.312, "power_watts_avg": 21.476, "energy_joules_est": 161.85, "duration_seconds": 7.536, "sample_count": 64}, "timestamp": "2026-01-26T15:15:03.613024"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11585.952, "latencies_ms": [11585.952], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, there is a polar bear reaching out towards a green ball that is closer to the viewer than the bear. The bear is surrounded by water, and there are two other balls, one yellow and one green, positioned further away in the background. The bear is in the center of the image, with the balls scattered around it, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21052.8, "ram_available_mb": 41788.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21053.2, "ram_available_mb": 41787.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.091}, "power_stats": {"power_gpu_soc_mean_watts": 19.294, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 68.091, "power_watts_avg": 19.294, "energy_joules_est": 223.55, "duration_seconds": 11.587, "sample_count": 99}, "timestamp": "2026-01-26T15:15:17.216464"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6626.419, "latencies_ms": [6626.419], "images_per_second": 0.151, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "A polar bear is playing with colorful balls in a pool of water. The bear is holding a green ball in its mouth while the other balls are floating nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21053.2, "ram_available_mb": 41787.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 21086.4, "ram_available_mb": 41754.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.125}, "power_stats": {"power_gpu_soc_mean_watts": 22.258, "power_cpu_cv_mean_watts": 1.443, "power_sys_5v0_mean_watts": 8.556, "gpu_utilization_percent_mean": 75.125, "power_watts_avg": 22.258, "energy_joules_est": 147.5, "duration_seconds": 6.627, "sample_count": 56}, "timestamp": "2026-01-26T15:15:25.877174"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8030.722, "latencies_ms": [8030.722], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image features a polar bear in a body of water with three colorful balls: one green, one yellow, and one blue. The lighting is bright and natural, suggesting the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21031.2, "ram_available_mb": 41809.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21050.3, "ram_available_mb": 41790.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.706}, "power_stats": {"power_gpu_soc_mean_watts": 20.625, "power_cpu_cv_mean_watts": 1.648, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 72.706, "power_watts_avg": 20.625, "energy_joules_est": 165.65, "duration_seconds": 8.031, "sample_count": 68}, "timestamp": "2026-01-26T15:15:35.943003"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12405.285, "latencies_ms": [12405.285], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a diptych, split into two parts. On the left, a person is sitting on a wooden chair, their legs crossed, wearing blue jeans and a blue sweater. The chair is positioned in front of a wooden wall, and the person's feet are visible. On the right, a hand is holding a silver flip phone, which is turned on", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 21050.3, "ram_available_mb": 41790.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21048.6, "ram_available_mb": 41792.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.67}, "power_stats": {"power_gpu_soc_mean_watts": 21.585, "power_cpu_cv_mean_watts": 1.797, "power_sys_5v0_mean_watts": 8.937, "gpu_utilization_percent_mean": 72.67, "power_watts_avg": 21.585, "energy_joules_est": 267.78, "duration_seconds": 12.406, "sample_count": 106}, "timestamp": "2026-01-26T15:15:50.400370"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8780.076, "latencies_ms": [8780.076], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "person: 1, leg: 2, chair: 1, cell phone: 1, window: 1, wooden panel: 1, denim: 1, screen: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21048.6, "ram_available_mb": 41792.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21048.8, "ram_available_mb": 41792.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.507}, "power_stats": {"power_gpu_soc_mean_watts": 23.194, "power_cpu_cv_mean_watts": 1.435, "power_sys_5v0_mean_watts": 8.753, "gpu_utilization_percent_mean": 77.507, "power_watts_avg": 23.194, "energy_joules_est": 203.66, "duration_seconds": 8.781, "sample_count": 75}, "timestamp": "2026-01-26T15:16:01.221016"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11038.872, "latencies_ms": [11038.872], "images_per_second": 0.091, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "In the left image, a person's legs are positioned in the foreground, with a wooden chair and a wooden ceiling visible in the background. In the right image, a person's hand is holding a flip phone in the foreground, with a window and a wooden wall in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21048.8, "ram_available_mb": 41792.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21049.5, "ram_available_mb": 41791.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.463}, "power_stats": {"power_gpu_soc_mean_watts": 22.141, "power_cpu_cv_mean_watts": 1.702, "power_sys_5v0_mean_watts": 8.901, "gpu_utilization_percent_mean": 74.463, "power_watts_avg": 22.141, "energy_joules_est": 244.43, "duration_seconds": 11.04, "sample_count": 95}, "timestamp": "2026-01-26T15:16:14.307734"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 12575.193, "latencies_ms": [12575.193], "images_per_second": 0.08, "prompt_tokens": 37, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The image is a split view of a person's lower body and a hand holding a Samsung phone. The left side shows the person sitting on a wooden chair with their legs crossed, wearing blue jeans and a blue sweater. The right side shows the person's hand holding a Samsung phone with the time displayed as 11:26.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21049.5, "ram_available_mb": 41791.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21050.5, "ram_available_mb": 41790.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.435}, "power_stats": {"power_gpu_soc_mean_watts": 21.848, "power_cpu_cv_mean_watts": 1.745, "power_sys_5v0_mean_watts": 8.844, "gpu_utilization_percent_mean": 73.435, "power_watts_avg": 21.848, "energy_joules_est": 274.76, "duration_seconds": 12.576, "sample_count": 108}, "timestamp": "2026-01-26T15:16:28.909338"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8242.331, "latencies_ms": [8242.331], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The image shows a person sitting on a wooden chair with blue jeans, holding a Samsung flip phone. The room has wooden walls and a window with a view of the sky and clouds.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21050.5, "ram_available_mb": 41790.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 21106.2, "ram_available_mb": 41734.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.357}, "power_stats": {"power_gpu_soc_mean_watts": 23.438, "power_cpu_cv_mean_watts": 1.458, "power_sys_5v0_mean_watts": 8.864, "gpu_utilization_percent_mean": 77.357, "power_watts_avg": 23.438, "energy_joules_est": 193.2, "duration_seconds": 8.243, "sample_count": 70}, "timestamp": "2026-01-26T15:16:39.188549"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12405.694, "latencies_ms": [12405.694], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility in a snowy landscape. A yellow train, vibrant against the white backdrop, is making its way down the tracks. The train is moving towards the right side of the image, leaving behind a trail of snow that glistens under the overcast sky. The tracks, also covered in snow, guide the train's journey.", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 21019.0, "ram_available_mb": 41821.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21085.9, "ram_available_mb": 41755.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.714}, "power_stats": {"power_gpu_soc_mean_watts": 21.529, "power_cpu_cv_mean_watts": 1.822, "power_sys_5v0_mean_watts": 8.925, "gpu_utilization_percent_mean": 72.714, "power_watts_avg": 21.529, "energy_joules_est": 267.1, "duration_seconds": 12.406, "sample_count": 105}, "timestamp": "2026-01-26T15:16:53.631876"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11118.453, "latencies_ms": [11118.453], "images_per_second": 0.09, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "- Train: 1\n\n- Train tracks: 2\n\n- Trees: 10\n\n- Snow: 1\n\n- Sky: 1\n\n- Bird: 1\n\n- Train station: 1\n\n- Snow-covered ground: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21022.6, "ram_available_mb": 41818.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21070.4, "ram_available_mb": 41770.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.83}, "power_stats": {"power_gpu_soc_mean_watts": 22.253, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.809, "gpu_utilization_percent_mean": 74.83, "power_watts_avg": 22.253, "energy_joules_est": 247.43, "duration_seconds": 11.119, "sample_count": 94}, "timestamp": "2026-01-26T15:17:06.773441"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11903.069, "latencies_ms": [11903.069], "images_per_second": 0.084, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The train is in the foreground, moving along the tracks which are surrounded by snow-covered trees. The sky is visible in the background, suggesting the train is on a railway line that extends into the distance. The trees on either side of the tracks are closer to the viewer than the train, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21070.4, "ram_available_mb": 41770.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21091.9, "ram_available_mb": 41749.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.782}, "power_stats": {"power_gpu_soc_mean_watts": 21.856, "power_cpu_cv_mean_watts": 1.727, "power_sys_5v0_mean_watts": 8.876, "gpu_utilization_percent_mean": 72.782, "power_watts_avg": 21.856, "energy_joules_est": 260.17, "duration_seconds": 11.904, "sample_count": 101}, "timestamp": "2026-01-26T15:17:20.713752"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8132.897, "latencies_ms": [8132.897], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A train is traveling through a snowy landscape with trees on both sides of the tracks. The sky is overcast and the ground is covered in a thick layer of snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21091.9, "ram_available_mb": 41749.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21065.4, "ram_available_mb": 41775.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_gpu_soc_mean_watts": 23.644, "power_cpu_cv_mean_watts": 1.353, "power_sys_5v0_mean_watts": 8.747, "gpu_utilization_percent_mean": 78.0, "power_watts_avg": 23.644, "energy_joules_est": 192.31, "duration_seconds": 8.134, "sample_count": 68}, "timestamp": "2026-01-26T15:17:30.904462"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9209.806, "latencies_ms": [9209.806], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image depicts a snowy landscape with a train traveling through a snow-covered forest. The sky is overcast, and the ground is blanketed in white snow, indicating a cold and wintry weather condition.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21065.4, "ram_available_mb": 41775.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 21099.7, "ram_available_mb": 41741.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.679}, "power_stats": {"power_gpu_soc_mean_watts": 22.923, "power_cpu_cv_mean_watts": 1.616, "power_sys_5v0_mean_watts": 8.869, "gpu_utilization_percent_mean": 76.679, "power_watts_avg": 22.923, "energy_joules_est": 211.13, "duration_seconds": 9.21, "sample_count": 78}, "timestamp": "2026-01-26T15:17:42.170473"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11619.889, "latencies_ms": [11619.889], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a group of people walking through a snow-covered street. There are five people in total, with some of them carrying handbags. They are navigating the snowy sidewalk, which is covered in a thick layer of snow. \n\nIn the foreground, there is a fire hydrant partially buried under the snow, indicating that the snowfall has", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 21035.4, "ram_available_mb": 41805.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 21057.8, "ram_available_mb": 41783.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.545}, "power_stats": {"power_gpu_soc_mean_watts": 19.343, "power_cpu_cv_mean_watts": 1.92, "power_sys_5v0_mean_watts": 8.659, "gpu_utilization_percent_mean": 70.545, "power_watts_avg": 19.343, "energy_joules_est": 224.78, "duration_seconds": 11.621, "sample_count": 99}, "timestamp": "2026-01-26T15:17:55.846273"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6401.734, "latencies_ms": [6401.734], "images_per_second": 0.156, "prompt_tokens": 39, "response_tokens_est": 32, "n_tiles": 16, "output_text": "- Snow: numerous piles\n- People: 5\n- Fire hydrant: 1\n- Bricks: numerous bricks in the background", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21057.8, "ram_available_mb": 41783.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21057.8, "ram_available_mb": 41783.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.204}, "power_stats": {"power_gpu_soc_mean_watts": 22.474, "power_cpu_cv_mean_watts": 1.393, "power_sys_5v0_mean_watts": 8.577, "gpu_utilization_percent_mean": 76.204, "power_watts_avg": 22.474, "energy_joules_est": 143.89, "duration_seconds": 6.402, "sample_count": 54}, "timestamp": "2026-01-26T15:18:04.277463"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9290.372, "latencies_ms": [9290.372], "images_per_second": 0.108, "prompt_tokens": 44, "response_tokens_est": 59, "n_tiles": 16, "output_text": "In the foreground, there is a large pile of snow covering the ground. To the right of the pile, there is a fire hydrant partially buried in the snow. In the background, there are people walking on the sidewalk, with one person carrying a white bag.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21057.8, "ram_available_mb": 41783.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21102.0, "ram_available_mb": 41738.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.101}, "power_stats": {"power_gpu_soc_mean_watts": 20.13, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.681, "gpu_utilization_percent_mean": 70.101, "power_watts_avg": 20.13, "energy_joules_est": 187.03, "duration_seconds": 9.291, "sample_count": 79}, "timestamp": "2026-01-26T15:18:15.606564"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6841.678, "latencies_ms": [6841.678], "images_per_second": 0.146, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The image depicts a snowy street with a large pile of snow on the side. People are walking around the pile, some of them carrying bags.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21024.3, "ram_available_mb": 41816.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21059.9, "ram_available_mb": 41781.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.362}, "power_stats": {"power_gpu_soc_mean_watts": 22.132, "power_cpu_cv_mean_watts": 1.469, "power_sys_5v0_mean_watts": 8.583, "gpu_utilization_percent_mean": 74.362, "power_watts_avg": 22.132, "energy_joules_est": 151.44, "duration_seconds": 6.842, "sample_count": 58}, "timestamp": "2026-01-26T15:18:24.476637"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7902.5, "latencies_ms": [7902.5], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a large pile of white snow covering the ground and partially covering a fire hydrant. The sky is overcast, and the lighting is dim, indicating that it might be a cold and cloudy day.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21059.9, "ram_available_mb": 41781.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21077.2, "ram_available_mb": 41763.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.761}, "power_stats": {"power_gpu_soc_mean_watts": 20.718, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 8.646, "gpu_utilization_percent_mean": 72.761, "power_watts_avg": 20.718, "energy_joules_est": 163.74, "duration_seconds": 7.903, "sample_count": 67}, "timestamp": "2026-01-26T15:18:34.419717"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11594.334, "latencies_ms": [11594.334], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene on a city street. Dominating the right side of the frame is a red and white sign, standing tall on a metal pole. The sign is a clear warning to drivers: \"No Parking\" and \"Tow Zone\". The pole is positioned on the sidewalk, just next to a tree, adding a touch of nature to the urban setting.\n", "error": null, "sys_before": {"cpu_percent": 3.1, "ram_used_mb": 21077.2, "ram_available_mb": 41763.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21077.4, "ram_available_mb": 41763.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.636}, "power_stats": {"power_gpu_soc_mean_watts": 19.335, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.698, "gpu_utilization_percent_mean": 69.636, "power_watts_avg": 19.335, "energy_joules_est": 224.19, "duration_seconds": 11.595, "sample_count": 99}, "timestamp": "2026-01-26T15:18:48.044911"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7677.609, "latencies_ms": [7677.609], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "tree: 1, sign: 2, pole: 2, building: 1, window: 1, staircase: 1, plant: 1, street light: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21077.4, "ram_available_mb": 41763.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21103.0, "ram_available_mb": 41737.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.227}, "power_stats": {"power_gpu_soc_mean_watts": 21.273, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.59, "gpu_utilization_percent_mean": 74.227, "power_watts_avg": 21.273, "energy_joules_est": 163.34, "duration_seconds": 7.678, "sample_count": 66}, "timestamp": "2026-01-26T15:18:57.754239"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11594.937, "latencies_ms": [11594.937], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a yellow sign with a black symbol of a tooth, positioned to the right of a pole that holds a 'No Parking' sign with a red arrow. The 'No Parking' sign is located further to the right, near the edge of the image, and is positioned above the pole that supports the yellow sign. The background features a tree", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21024.5, "ram_available_mb": 41816.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21065.9, "ram_available_mb": 41775.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.612}, "power_stats": {"power_gpu_soc_mean_watts": 19.351, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 70.612, "power_watts_avg": 19.351, "energy_joules_est": 224.39, "duration_seconds": 11.596, "sample_count": 98}, "timestamp": "2026-01-26T15:19:11.370019"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10807.596, "latencies_ms": [10807.596], "images_per_second": 0.093, "prompt_tokens": 37, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image shows a street scene with a \"No Parking\" sign and a \"Tow Zone\" sign attached to a pole, indicating that parking is not allowed in this area. There is also a yellow sign with a smiley face on it, which seems to be a playful addition to the otherwise strict parking regulations.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21065.9, "ram_available_mb": 41775.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21067.1, "ram_available_mb": 41773.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.72}, "power_stats": {"power_gpu_soc_mean_watts": 19.635, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.601, "gpu_utilization_percent_mean": 70.72, "power_watts_avg": 19.635, "energy_joules_est": 212.22, "duration_seconds": 10.808, "sample_count": 93}, "timestamp": "2026-01-26T15:19:24.190109"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9470.754, "latencies_ms": [9470.754], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The image shows a bright, sunny day with clear skies, as evidenced by the strong shadows cast by the trees and the bright lighting on the signs. The signs are mounted on metal poles and are positioned against a backdrop of lush green foliage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21067.1, "ram_available_mb": 41773.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21068.1, "ram_available_mb": 41772.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.829}, "power_stats": {"power_gpu_soc_mean_watts": 20.087, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 8.702, "gpu_utilization_percent_mean": 71.829, "power_watts_avg": 20.087, "energy_joules_est": 190.25, "duration_seconds": 9.471, "sample_count": 82}, "timestamp": "2026-01-26T15:19:35.682145"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11535.296, "latencies_ms": [11535.296], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a light brown teddy bear is the main subject. The teddy bear is anthropomorphized, wearing round glasses and has a black nose. It's seated on a red desk, which is adorned with a black keyboard and a microphone. The teddy bear is holding a white iPhone in its left hand, suggesting it", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 21068.1, "ram_available_mb": 41772.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21068.3, "ram_available_mb": 41772.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.01}, "power_stats": {"power_gpu_soc_mean_watts": 19.314, "power_cpu_cv_mean_watts": 1.912, "power_sys_5v0_mean_watts": 8.696, "gpu_utilization_percent_mean": 70.01, "power_watts_avg": 19.314, "energy_joules_est": 222.81, "duration_seconds": 11.536, "sample_count": 99}, "timestamp": "2026-01-26T15:19:49.244097"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8033.415, "latencies_ms": [8033.415], "images_per_second": 0.124, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "teddy bear: 1, glasses: 1, phone: 1, microphone: 1, keyboard: 1, mouse: 1, cord: 1, earphones: 1", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 21068.3, "ram_available_mb": 41772.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21068.8, "ram_available_mb": 41772.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.882}, "power_stats": {"power_gpu_soc_mean_watts": 21.219, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.59, "gpu_utilization_percent_mean": 72.882, "power_watts_avg": 21.219, "energy_joules_est": 170.47, "duration_seconds": 8.034, "sample_count": 68}, "timestamp": "2026-01-26T15:19:59.331855"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11355.101, "latencies_ms": [11355.101], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground, there is a black keyboard positioned to the left of a teddy bear that is sitting behind it. The teddy bear is placed on a red surface, and there is a white smartphone and a brown remote control to its right. The background is a dark blue color, providing a contrast to the red surface and the white teddy bear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21068.8, "ram_available_mb": 41772.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21069.8, "ram_available_mb": 41771.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.196}, "power_stats": {"power_gpu_soc_mean_watts": 19.39, "power_cpu_cv_mean_watts": 1.856, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 70.196, "power_watts_avg": 19.39, "energy_joules_est": 220.19, "duration_seconds": 11.356, "sample_count": 97}, "timestamp": "2026-01-26T15:20:12.713994"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9538.528, "latencies_ms": [9538.528], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 59, "n_tiles": 16, "output_text": "In the image, a teddy bear is sitting on a red surface, surrounded by various electronic devices including a keyboard, a mouse, and a cell phone. The setting appears to be a cozy and playful environment, possibly a child's room or a creative workspace.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21069.8, "ram_available_mb": 41771.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21083.3, "ram_available_mb": 41757.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.086}, "power_stats": {"power_gpu_soc_mean_watts": 20.256, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 8.608, "gpu_utilization_percent_mean": 72.086, "power_watts_avg": 20.256, "energy_joules_est": 193.23, "duration_seconds": 9.539, "sample_count": 81}, "timestamp": "2026-01-26T15:20:24.280770"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9161.643, "latencies_ms": [9161.643], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image features a teddy bear with glasses, a white phone, and a microphone, all placed on a red surface. The lighting is dim, creating a moody atmosphere, and the teddy bear appears to be made of a soft, plush material.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21083.3, "ram_available_mb": 41757.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21080.5, "ram_available_mb": 41760.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.156}, "power_stats": {"power_gpu_soc_mean_watts": 20.273, "power_cpu_cv_mean_watts": 1.736, "power_sys_5v0_mean_watts": 8.619, "gpu_utilization_percent_mean": 71.156, "power_watts_avg": 20.273, "energy_joules_est": 185.75, "duration_seconds": 9.162, "sample_count": 77}, "timestamp": "2026-01-26T15:20:35.495378"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12448.426, "latencies_ms": [12448.426], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a skier is captured in mid-air, performing a daring trick on a snowy mountain. The skier, clad in an orange jacket and black pants, is holding two ski poles, adding to the dynamic nature of the scene. The mountain, blanketed in snow, serves as a stunning backdrop to this thrilling moment. The", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 21080.5, "ram_available_mb": 41760.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21081.8, "ram_available_mb": 41759.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.113}, "power_stats": {"power_gpu_soc_mean_watts": 21.479, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.911, "gpu_utilization_percent_mean": 73.113, "power_watts_avg": 21.479, "energy_joules_est": 267.4, "duration_seconds": 12.449, "sample_count": 106}, "timestamp": "2026-01-26T15:20:49.986646"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8945.786, "latencies_ms": [8945.786], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "mountain: 1, ski: 2, snow: 1, tree: 1, snowboard: 1, skier: 1, snowboarder: 1, snow: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21081.8, "ram_available_mb": 41759.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21067.6, "ram_available_mb": 41773.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.373}, "power_stats": {"power_gpu_soc_mean_watts": 22.977, "power_cpu_cv_mean_watts": 1.451, "power_sys_5v0_mean_watts": 8.745, "gpu_utilization_percent_mean": 78.373, "power_watts_avg": 22.977, "energy_joules_est": 205.56, "duration_seconds": 8.946, "sample_count": 75}, "timestamp": "2026-01-26T15:21:00.945075"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11418.967, "latencies_ms": [11418.967], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, a skier is captured in mid-air, performing a jump on the left side of the image. The skier is near the bottom of the frame, with snow and a clear blue sky in the background. The mountain and trees are further in the background, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21067.6, "ram_available_mb": 41773.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21047.5, "ram_available_mb": 41793.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.825}, "power_stats": {"power_gpu_soc_mean_watts": 22.007, "power_cpu_cv_mean_watts": 1.712, "power_sys_5v0_mean_watts": 8.874, "gpu_utilization_percent_mean": 73.825, "power_watts_avg": 22.007, "energy_joules_est": 251.31, "duration_seconds": 11.42, "sample_count": 97}, "timestamp": "2026-01-26T15:21:14.407875"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9822.068, "latencies_ms": [9822.068], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A skier is captured in mid-air, performing a jump on a snowy mountain slope. The date \"27 febbraio 2010\" and the event \"Tour de Sas\" are displayed at the bottom of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21047.5, "ram_available_mb": 41793.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21039.8, "ram_available_mb": 41801.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.554}, "power_stats": {"power_gpu_soc_mean_watts": 22.713, "power_cpu_cv_mean_watts": 1.533, "power_sys_5v0_mean_watts": 8.764, "gpu_utilization_percent_mean": 75.554, "power_watts_avg": 22.713, "energy_joules_est": 223.1, "duration_seconds": 9.823, "sample_count": 83}, "timestamp": "2026-01-26T15:21:26.249027"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10128.425, "latencies_ms": [10128.425], "images_per_second": 0.099, "prompt_tokens": 36, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The image captures a dynamic moment of a skier in mid-air, with the snow splashing around as they perform a jump. The skier is wearing an orange jacket and black pants, contrasting against the white snow and the clear blue sky.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21039.8, "ram_available_mb": 41801.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21025.1, "ram_available_mb": 41815.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.36}, "power_stats": {"power_gpu_soc_mean_watts": 22.461, "power_cpu_cv_mean_watts": 1.619, "power_sys_5v0_mean_watts": 8.888, "gpu_utilization_percent_mean": 75.36, "power_watts_avg": 22.461, "energy_joules_est": 227.51, "duration_seconds": 10.129, "sample_count": 86}, "timestamp": "2026-01-26T15:21:38.393842"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12419.047, "latencies_ms": [12419.047], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a dynamic scene of two surfers riding waves in the ocean. The surfer in the foreground is skillfully maneuvering a wave, while the other surfer is seen further back, also engaged in the sport. The waves are large and powerful, creating a sense of excitement and adventure. The ocean is a deep blue color, and the sky is a", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 21025.1, "ram_available_mb": 41815.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.9, "ram_used_mb": 21090.5, "ram_available_mb": 41750.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.292}, "power_stats": {"power_gpu_soc_mean_watts": 21.62, "power_cpu_cv_mean_watts": 2.024, "power_sys_5v0_mean_watts": 8.938, "gpu_utilization_percent_mean": 73.292, "power_watts_avg": 21.62, "energy_joules_est": 268.51, "duration_seconds": 12.42, "sample_count": 106}, "timestamp": "2026-01-26T15:21:52.888111"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5210.336, "latencies_ms": [5210.336], "images_per_second": 0.192, "prompt_tokens": 39, "response_tokens_est": 12, "n_tiles": 16, "output_text": "wave: 3\nsurfer: 2\n", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21028.6, "ram_available_mb": 41812.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 21063.9, "ram_available_mb": 41777.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 87.512}, "power_stats": {"power_gpu_soc_mean_watts": 26.417, "power_cpu_cv_mean_watts": 0.735, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 87.512, "power_watts_avg": 26.417, "energy_joules_est": 137.66, "duration_seconds": 5.211, "sample_count": 43}, "timestamp": "2026-01-26T15:22:00.114759"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12713.137, "latencies_ms": [12713.137], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a surfer riding a wave on the left side of the image, while another surfer is further back, closer to the horizon, waiting for a wave on the right side. The waves are large and powerful, indicating that the surfers are in the midst of the ocean, which is the main focus of the image. The background shows the sky meeting the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21063.9, "ram_available_mb": 41777.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21085.6, "ram_available_mb": 41755.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.234}, "power_stats": {"power_gpu_soc_mean_watts": 21.641, "power_cpu_cv_mean_watts": 1.791, "power_sys_5v0_mean_watts": 8.902, "gpu_utilization_percent_mean": 72.234, "power_watts_avg": 21.641, "energy_joules_est": 275.14, "duration_seconds": 12.714, "sample_count": 107}, "timestamp": "2026-01-26T15:22:14.844650"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11235.909, "latencies_ms": [11235.909], "images_per_second": 0.089, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image captures a dynamic scene of two surfers riding waves in the ocean. One surfer is skillfully navigating a large wave, while the other is waiting for their turn. The waves are powerful and the water is a deep blue, creating an exhilarating atmosphere for the surfers.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21085.6, "ram_available_mb": 41755.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21085.6, "ram_available_mb": 41755.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.6}, "power_stats": {"power_gpu_soc_mean_watts": 22.235, "power_cpu_cv_mean_watts": 1.639, "power_sys_5v0_mean_watts": 8.801, "gpu_utilization_percent_mean": 74.6, "power_watts_avg": 22.235, "energy_joules_est": 249.84, "duration_seconds": 11.237, "sample_count": 95}, "timestamp": "2026-01-26T15:22:28.095749"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10145.604, "latencies_ms": [10145.604], "images_per_second": 0.099, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image captures a serene beach scene with a surfer riding a wave. The lighting is soft and warm, suggesting it might be either early morning or late afternoon. The waves are large and powerful, indicating strong ocean currents or a recent storm.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21020.4, "ram_available_mb": 41820.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21089.3, "ram_available_mb": 41751.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.756}, "power_stats": {"power_gpu_soc_mean_watts": 22.452, "power_cpu_cv_mean_watts": 1.642, "power_sys_5v0_mean_watts": 8.814, "gpu_utilization_percent_mean": 76.756, "power_watts_avg": 22.452, "energy_joules_est": 227.81, "duration_seconds": 10.147, "sample_count": 86}, "timestamp": "2026-01-26T15:22:40.282890"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11630.365, "latencies_ms": [11630.365], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a delicious pizza sitting on a plate, placed on a dining table. The pizza is topped with various ingredients, including cheese, tomatoes, and possibly onions. The table is covered with a red tablecloth, and there is a fork and a knife placed next to the pizza, ready for someone to enjoy the meal.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 21027.4, "ram_available_mb": 41813.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 21078.4, "ram_available_mb": 41762.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.109}, "power_stats": {"power_gpu_soc_mean_watts": 19.296, "power_cpu_cv_mean_watts": 1.93, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 70.109, "power_watts_avg": 19.296, "energy_joules_est": 224.43, "duration_seconds": 11.631, "sample_count": 101}, "timestamp": "2026-01-26T15:22:53.936591"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9402.816, "latencies_ms": [9402.816], "images_per_second": 0.106, "prompt_tokens": 39, "response_tokens_est": 58, "n_tiles": 16, "output_text": "pizza: 1\nsalt shaker: 1\npepper shaker: 1\nblue napkin: 1\nwhite napkin: 1\nblue plate: 1\nred tablecloth: 1\nred cushion: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21078.4, "ram_available_mb": 41762.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21115.0, "ram_available_mb": 41725.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.763}, "power_stats": {"power_gpu_soc_mean_watts": 20.314, "power_cpu_cv_mean_watts": 1.806, "power_sys_5v0_mean_watts": 8.288, "gpu_utilization_percent_mean": 72.763, "power_watts_avg": 20.314, "energy_joules_est": 191.02, "duration_seconds": 9.403, "sample_count": 80}, "timestamp": "2026-01-26T15:23:05.354440"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11586.568, "latencies_ms": [11586.568], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The pizza is in the foreground, placed on a plate that is on a table covered with a red tablecloth. There is a salt shaker positioned to the left of the plate, and a blue and white striped napkin is folded and placed to the right of the plate. The background is blurred, but it appears to be a dimly lit room with", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21115.0, "ram_available_mb": 41725.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21070.4, "ram_available_mb": 41770.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.96}, "power_stats": {"power_gpu_soc_mean_watts": 19.27, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.32, "gpu_utilization_percent_mean": 69.96, "power_watts_avg": 19.27, "energy_joules_est": 223.29, "duration_seconds": 11.587, "sample_count": 99}, "timestamp": "2026-01-26T15:23:18.974810"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6953.119, "latencies_ms": [6953.119], "images_per_second": 0.144, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A pizza with various toppings is placed on a silver plate on a table with a red and blue tablecloth. A salt shaker is also visible on the table.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 21070.4, "ram_available_mb": 41770.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21075.3, "ram_available_mb": 41765.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.881}, "power_stats": {"power_gpu_soc_mean_watts": 21.926, "power_cpu_cv_mean_watts": 1.478, "power_sys_5v0_mean_watts": 8.218, "gpu_utilization_percent_mean": 74.881, "power_watts_avg": 21.926, "energy_joules_est": 152.47, "duration_seconds": 6.954, "sample_count": 59}, "timestamp": "2026-01-26T15:23:27.951599"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9822.119, "latencies_ms": [9822.119], "images_per_second": 0.102, "prompt_tokens": 36, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image features a pizza with a golden-brown crust, topped with melted cheese, tomato sauce, and various toppings, served on a silver plate. The setting is dimly lit with a red and blue color scheme, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21075.3, "ram_available_mb": 41765.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21100.8, "ram_available_mb": 41740.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.265}, "power_stats": {"power_gpu_soc_mean_watts": 19.898, "power_cpu_cv_mean_watts": 1.808, "power_sys_5v0_mean_watts": 8.303, "gpu_utilization_percent_mean": 71.265, "power_watts_avg": 19.898, "energy_joules_est": 195.45, "duration_seconds": 9.823, "sample_count": 83}, "timestamp": "2026-01-26T15:23:39.790722"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11625.534, "latencies_ms": [11625.534], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene winter scene in a small town. Dominating the foreground is a black clock post, standing tall on a brick sidewalk. The clock face is white, adorned with black numbers and hands, indicating the time. The post is situated on the left side of the image, providing a sense of depth and perspective.\n\nThe sidewalk, made of", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21037.4, "ram_available_mb": 41803.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21104.0, "ram_available_mb": 41736.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.592}, "power_stats": {"power_gpu_soc_mean_watts": 19.062, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.284, "gpu_utilization_percent_mean": 70.592, "power_watts_avg": 19.062, "energy_joules_est": 221.62, "duration_seconds": 11.626, "sample_count": 98}, "timestamp": "2026-01-26T15:23:53.463388"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9852.56, "latencies_ms": [9852.56], "images_per_second": 0.101, "prompt_tokens": 39, "response_tokens_est": 62, "n_tiles": 16, "output_text": "1. Clock: 1\n2. Trees: 2\n3. Cars: 3\n4. Buildings: 3\n5. Snow: 1\n6. Sidewalk: 1\n7. Street: 1\n8. Sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21033.2, "ram_available_mb": 41807.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21077.8, "ram_available_mb": 41763.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.226}, "power_stats": {"power_gpu_soc_mean_watts": 20.228, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 8.272, "gpu_utilization_percent_mean": 71.226, "power_watts_avg": 20.228, "energy_joules_est": 199.31, "duration_seconds": 9.853, "sample_count": 84}, "timestamp": "2026-01-26T15:24:05.362279"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10490.71, "latencies_ms": [10490.71], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The clock is positioned in the foreground on the left side of the image, standing on a brick sidewalk. In the background, there is a street that runs parallel to the sidewalk, with buildings on both sides. The sky is visible in the far background, indicating that the scene is set outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21077.8, "ram_available_mb": 41763.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21029.8, "ram_available_mb": 41811.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.27}, "power_stats": {"power_gpu_soc_mean_watts": 19.77, "power_cpu_cv_mean_watts": 1.821, "power_sys_5v0_mean_watts": 8.308, "gpu_utilization_percent_mean": 71.27, "power_watts_avg": 19.77, "energy_joules_est": 207.41, "duration_seconds": 10.491, "sample_count": 89}, "timestamp": "2026-01-26T15:24:17.887199"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8944.087, "latencies_ms": [8944.087], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image depicts a snowy street scene with a large, ornate clock prominently displayed in the foreground. The clock is located on a pole and is surrounded by a snowy landscape, with buildings and parked cars visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21029.8, "ram_available_mb": 41811.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21070.2, "ram_available_mb": 41770.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.697}, "power_stats": {"power_gpu_soc_mean_watts": 20.496, "power_cpu_cv_mean_watts": 1.69, "power_sys_5v0_mean_watts": 8.25, "gpu_utilization_percent_mean": 72.697, "power_watts_avg": 20.496, "energy_joules_est": 183.33, "duration_seconds": 8.945, "sample_count": 76}, "timestamp": "2026-01-26T15:24:28.852070"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10168.52, "latencies_ms": [10168.52], "images_per_second": 0.098, "prompt_tokens": 36, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The image features a black clock with white numerals and hands, standing on a pole with a snowy ground beneath it. The sky is clear with a gradient of blue to white, indicating either sunrise or sunset, and the buildings have a mix of brick and siding with visible windows and storefronts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21070.2, "ram_available_mb": 41770.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21092.7, "ram_available_mb": 41748.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.736}, "power_stats": {"power_gpu_soc_mean_watts": 19.793, "power_cpu_cv_mean_watts": 1.803, "power_sys_5v0_mean_watts": 8.301, "gpu_utilization_percent_mean": 71.736, "power_watts_avg": 19.793, "energy_joules_est": 201.28, "duration_seconds": 10.169, "sample_count": 87}, "timestamp": "2026-01-26T15:24:41.082665"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11594.683, "latencies_ms": [11594.683], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a baseball game is in progress with a batter and a catcher in the middle of the action. The batter is holding a baseball bat and is in the process of swinging at a pitched ball. The catcher, wearing a baseball glove, is positioned behind the batter, ready to catch the ball if the batter misses.\n\nThere are several other", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 21029.1, "ram_available_mb": 41811.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21103.1, "ram_available_mb": 41737.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.96}, "power_stats": {"power_gpu_soc_mean_watts": 19.367, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.312, "gpu_utilization_percent_mean": 69.96, "power_watts_avg": 19.367, "energy_joules_est": 224.57, "duration_seconds": 11.595, "sample_count": 99}, "timestamp": "2026-01-26T15:24:54.762342"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7665.281, "latencies_ms": [7665.281], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "ball: 1, player: 1, bat: 1, glove: 1, catcher: 1, uniform: 1, helmet: 1, base: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21031.0, "ram_available_mb": 41809.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21073.5, "ram_available_mb": 41767.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.4}, "power_stats": {"power_gpu_soc_mean_watts": 21.256, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.212, "gpu_utilization_percent_mean": 73.4, "power_watts_avg": 21.256, "energy_joules_est": 162.95, "duration_seconds": 7.666, "sample_count": 65}, "timestamp": "2026-01-26T15:25:04.447987"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11602.725, "latencies_ms": [11602.725], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a baseball player in a blue helmet and striped uniform is swinging a bat at an incoming ball. The catcher, wearing a red helmet and uniform, is crouched behind the batter, ready to catch the ball with his glove. The background shows a green fence and a blue banner with the word \"SPORTS\" on it", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21073.5, "ram_available_mb": 41767.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21091.7, "ram_available_mb": 41749.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.778}, "power_stats": {"power_gpu_soc_mean_watts": 19.275, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.297, "gpu_utilization_percent_mean": 69.778, "power_watts_avg": 19.275, "energy_joules_est": 223.65, "duration_seconds": 11.603, "sample_count": 99}, "timestamp": "2026-01-26T15:25:18.064120"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9404.124, "latencies_ms": [9404.124], "images_per_second": 0.106, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image captures a moment from a baseball game where a player in a blue and white uniform is swinging a bat at an incoming ball. The catcher, wearing a red uniform, is crouched behind the batter, ready to catch the ball with his glove.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21091.7, "ram_available_mb": 41749.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21109.9, "ram_available_mb": 41731.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.737}, "power_stats": {"power_gpu_soc_mean_watts": 20.188, "power_cpu_cv_mean_watts": 1.711, "power_sys_5v0_mean_watts": 8.231, "gpu_utilization_percent_mean": 71.737, "power_watts_avg": 20.188, "energy_joules_est": 189.87, "duration_seconds": 9.405, "sample_count": 80}, "timestamp": "2026-01-26T15:25:29.497063"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8149.714, "latencies_ms": [8149.714], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image captures a baseball game in progress with a player in a blue helmet and striped uniform swinging a bat at a baseball. The weather appears to be clear and sunny, casting shadows on the dirt field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21029.7, "ram_available_mb": 41811.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21070.9, "ram_available_mb": 41770.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.232}, "power_stats": {"power_gpu_soc_mean_watts": 20.588, "power_cpu_cv_mean_watts": 1.659, "power_sys_5v0_mean_watts": 8.266, "gpu_utilization_percent_mean": 71.232, "power_watts_avg": 20.588, "energy_joules_est": 167.8, "duration_seconds": 8.15, "sample_count": 69}, "timestamp": "2026-01-26T15:25:39.667815"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11547.656, "latencies_ms": [11547.656], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a brown teddy bear is comfortably seated on a wooden chair. The teddy bear, adorned with a red and white checkered scarf, is facing the camera, giving us a clear view of its features. The chair, with its brown wooden frame and red cushion, provides a warm and inviting seat for the teddy bear", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 21070.9, "ram_available_mb": 41770.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 21074.9, "ram_available_mb": 41766.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.776}, "power_stats": {"power_gpu_soc_mean_watts": 19.298, "power_cpu_cv_mean_watts": 1.903, "power_sys_5v0_mean_watts": 8.315, "gpu_utilization_percent_mean": 69.776, "power_watts_avg": 19.298, "energy_joules_est": 222.86, "duration_seconds": 11.548, "sample_count": 98}, "timestamp": "2026-01-26T15:25:53.244050"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11842.641, "latencies_ms": [11842.641], "images_per_second": 0.084, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "- Chair: 1\n\n- Teddy bear: 1\n\n- Chair cushion: 1\n\n- Curtain: 1\n\n- Teddy bear's bow: 1\n\n- Teddy bear's paws: 2\n\n- Teddy bear's arms: 2\n\n- Teddy bear's legs: ", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21074.9, "ram_available_mb": 41766.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.8, "ram_used_mb": 21080.7, "ram_available_mb": 41760.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.94}, "power_stats": {"power_gpu_soc_mean_watts": 19.289, "power_cpu_cv_mean_watts": 2.089, "power_sys_5v0_mean_watts": 8.276, "gpu_utilization_percent_mean": 70.94, "power_watts_avg": 19.289, "energy_joules_est": 228.44, "duration_seconds": 11.843, "sample_count": 100}, "timestamp": "2026-01-26T15:26:07.102849"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9132.407, "latencies_ms": [9132.407], "images_per_second": 0.11, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The teddy bear is seated on the left side of the chair, positioned in the foreground of the image. It is placed near the front of the chair, which is on the right side, and is relatively close to the viewer compared to the background elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21080.7, "ram_available_mb": 41760.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21110.9, "ram_available_mb": 41730.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.922}, "power_stats": {"power_gpu_soc_mean_watts": 20.311, "power_cpu_cv_mean_watts": 1.731, "power_sys_5v0_mean_watts": 8.276, "gpu_utilization_percent_mean": 71.922, "power_watts_avg": 20.311, "energy_joules_est": 185.5, "duration_seconds": 9.133, "sample_count": 77}, "timestamp": "2026-01-26T15:26:18.257497"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7093.233, "latencies_ms": [7093.233], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A brown teddy bear is sitting on a brown wicker chair with a red cushion. The chair is placed against a wall with a striped curtain to the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21110.9, "ram_available_mb": 41730.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21083.5, "ram_available_mb": 41757.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.85}, "power_stats": {"power_gpu_soc_mean_watts": 21.95, "power_cpu_cv_mean_watts": 1.494, "power_sys_5v0_mean_watts": 8.203, "gpu_utilization_percent_mean": 74.85, "power_watts_avg": 21.95, "energy_joules_est": 155.71, "duration_seconds": 7.094, "sample_count": 60}, "timestamp": "2026-01-26T15:26:27.400526"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6617.625, "latencies_ms": [6617.625], "images_per_second": 0.151, "prompt_tokens": 36, "response_tokens_est": 35, "n_tiles": 16, "output_text": "The chair is made of dark wood with a woven texture and has a brown leather seat. The bear is brown with a red and white checkered scarf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21083.5, "ram_available_mb": 41757.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21102.0, "ram_available_mb": 41738.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.327}, "power_stats": {"power_gpu_soc_mean_watts": 21.949, "power_cpu_cv_mean_watts": 1.462, "power_sys_5v0_mean_watts": 8.261, "gpu_utilization_percent_mean": 74.327, "power_watts_avg": 21.949, "energy_joules_est": 145.26, "duration_seconds": 6.618, "sample_count": 55}, "timestamp": "2026-01-26T15:26:36.041171"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11548.672, "latencies_ms": [11548.672], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two snowboarders are standing on a snowy mountain, preparing to ride down the slope. They are both wearing red jackets and black pants, and one of them is holding a snowboard. The snowboarders are positioned close to each other, with one snowboarder slightly ahead of the other.\n\nThe scene is set against a beautiful back", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 21037.6, "ram_available_mb": 41803.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21080.0, "ram_available_mb": 41760.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.23}, "power_stats": {"power_gpu_soc_mean_watts": 19.242, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.327, "gpu_utilization_percent_mean": 70.23, "power_watts_avg": 19.242, "energy_joules_est": 222.23, "duration_seconds": 11.549, "sample_count": 100}, "timestamp": "2026-01-26T15:26:49.649499"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8264.299, "latencies_ms": [8264.299], "images_per_second": 0.121, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "sun: 1\nmountain: 1\nperson: 2\nsnowboard: 1\nsnow: 1\ngoggles: 1\njacket: 2\nhat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21080.0, "ram_available_mb": 41760.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21081.2, "ram_available_mb": 41759.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.071}, "power_stats": {"power_gpu_soc_mean_watts": 20.859, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.195, "gpu_utilization_percent_mean": 73.071, "power_watts_avg": 20.859, "energy_joules_est": 172.4, "duration_seconds": 8.265, "sample_count": 70}, "timestamp": "2026-01-26T15:26:59.951458"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10645.312, "latencies_ms": [10645.312], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there are two snowboarders standing on a snowy slope. The person on the left is wearing a red jacket and is positioned slightly behind the person on the right, who is wearing a red jacket and holding a snowboard. In the background, there are mountains and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21081.2, "ram_available_mb": 41759.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21107.9, "ram_available_mb": 41732.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.889}, "power_stats": {"power_gpu_soc_mean_watts": 19.678, "power_cpu_cv_mean_watts": 1.824, "power_sys_5v0_mean_watts": 8.3, "gpu_utilization_percent_mean": 70.889, "power_watts_avg": 19.678, "energy_joules_est": 209.49, "duration_seconds": 10.646, "sample_count": 90}, "timestamp": "2026-01-26T15:27:12.611315"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8261.828, "latencies_ms": [8261.828], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "Two snowboarders are standing on a snowy mountain, with the sun shining brightly in the background. They are both wearing red jackets and black pants, and one of them is holding a yellow snowboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21107.9, "ram_available_mb": 41732.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21037.0, "ram_available_mb": 41803.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.171}, "power_stats": {"power_gpu_soc_mean_watts": 20.867, "power_cpu_cv_mean_watts": 1.623, "power_sys_5v0_mean_watts": 8.236, "gpu_utilization_percent_mean": 73.171, "power_watts_avg": 20.867, "energy_joules_est": 172.41, "duration_seconds": 8.262, "sample_count": 70}, "timestamp": "2026-01-26T15:27:22.889899"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7193.471, "latencies_ms": [7193.471], "images_per_second": 0.139, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image features two individuals standing on a snowy surface with a bright sun shining overhead, casting a lens flare effect. The sky is clear and blue, indicating fair weather conditions.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21037.0, "ram_available_mb": 41803.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21113.7, "ram_available_mb": 41727.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.689}, "power_stats": {"power_gpu_soc_mean_watts": 21.372, "power_cpu_cv_mean_watts": 1.535, "power_sys_5v0_mean_watts": 8.244, "gpu_utilization_percent_mean": 73.689, "power_watts_avg": 21.372, "energy_joules_est": 153.75, "duration_seconds": 7.194, "sample_count": 61}, "timestamp": "2026-01-26T15:27:32.112137"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11622.867, "latencies_ms": [11622.867], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a tree with a large number of apples hanging from its branches. The apples are in various stages of ripeness, with some appearing ripe and ready to be picked, while others are still green and unripe. The tree is surrounded by a field, and the scene is set in a natural environment. The apples are scattered across the branches, creating a vis", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 21043.6, "ram_available_mb": 41797.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21108.7, "ram_available_mb": 41732.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.96}, "power_stats": {"power_gpu_soc_mean_watts": 19.247, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.309, "gpu_utilization_percent_mean": 70.96, "power_watts_avg": 19.247, "energy_joules_est": 223.72, "duration_seconds": 11.624, "sample_count": 99}, "timestamp": "2026-01-26T15:27:45.790460"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7655.368, "latencies_ms": [7655.368], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "apple: 5\nleaf: 10\ntree: 1\nbranch: 10\napple tree: 1\napple: 5\napple: 1\napple: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21038.1, "ram_available_mb": 41802.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21077.2, "ram_available_mb": 41763.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.569}, "power_stats": {"power_gpu_soc_mean_watts": 21.34, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.226, "gpu_utilization_percent_mean": 73.569, "power_watts_avg": 21.34, "energy_joules_est": 163.39, "duration_seconds": 7.656, "sample_count": 65}, "timestamp": "2026-01-26T15:27:55.473363"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9826.896, "latencies_ms": [9826.896], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "In the foreground, there are several red apples hanging from a tree branch. The tree trunk is visible on the right side of the image, with a hole near the top. The background is blurred, but it appears to be an orchard with more trees and possibly more apples.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21077.2, "ram_available_mb": 41763.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21078.2, "ram_available_mb": 41762.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.723}, "power_stats": {"power_gpu_soc_mean_watts": 19.952, "power_cpu_cv_mean_watts": 1.798, "power_sys_5v0_mean_watts": 8.314, "gpu_utilization_percent_mean": 70.723, "power_watts_avg": 19.952, "energy_joules_est": 196.08, "duration_seconds": 9.828, "sample_count": 83}, "timestamp": "2026-01-26T15:28:07.316218"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8454.353, "latencies_ms": [8454.353], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image depicts a tree with a hollow trunk, surrounded by branches with red apples hanging from them. The scene appears to be in an orchard or a garden with other trees and foliage in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21078.2, "ram_available_mb": 41762.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21108.6, "ram_available_mb": 41732.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.452}, "power_stats": {"power_gpu_soc_mean_watts": 20.634, "power_cpu_cv_mean_watts": 1.661, "power_sys_5v0_mean_watts": 8.223, "gpu_utilization_percent_mean": 73.452, "power_watts_avg": 20.634, "energy_joules_est": 174.46, "duration_seconds": 8.455, "sample_count": 73}, "timestamp": "2026-01-26T15:28:17.831353"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9396.216, "latencies_ms": [9396.216], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image features a tree with a rough, textured bark and a large, circular hole in the center. The tree is surrounded by several red apples hanging from its branches, and the background is filled with dry, brown leaves and branches, indicating a fall or autumn season.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21108.6, "ram_available_mb": 41732.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21108.6, "ram_available_mb": 41732.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.062}, "power_stats": {"power_gpu_soc_mean_watts": 19.791, "power_cpu_cv_mean_watts": 1.769, "power_sys_5v0_mean_watts": 8.274, "gpu_utilization_percent_mean": 71.062, "power_watts_avg": 19.791, "energy_joules_est": 185.97, "duration_seconds": 9.397, "sample_count": 81}, "timestamp": "2026-01-26T15:28:29.263139"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11547.762, "latencies_ms": [11547.762], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two men are working together in a commercial kitchen, preparing food. One man is standing closer to the left side of the kitchen, while the other man is positioned more towards the center. They are both focused on their tasks, with one man holding a pot and the other man holding a knife.\n\nThe kitchen is well-equipped with various utensils", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 21108.6, "ram_available_mb": 41732.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21108.6, "ram_available_mb": 41732.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.5}, "power_stats": {"power_gpu_soc_mean_watts": 19.342, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.312, "gpu_utilization_percent_mean": 70.5, "power_watts_avg": 19.342, "energy_joules_est": 223.37, "duration_seconds": 11.548, "sample_count": 100}, "timestamp": "2026-01-26T15:28:42.867532"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10751.699, "latencies_ms": [10751.699], "images_per_second": 0.093, "prompt_tokens": 39, "response_tokens_est": 70, "n_tiles": 16, "output_text": "1. Chef: 2\n2. Pots: 5\n3. Pans: 4\n4. Trays: 3\n5. Containers: 4\n6. Utensils: 3\n7. Ingredients: 2\n8. Cooking equipment: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21108.6, "ram_available_mb": 41732.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21037.7, "ram_available_mb": 41803.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.033}, "power_stats": {"power_gpu_soc_mean_watts": 19.719, "power_cpu_cv_mean_watts": 1.81, "power_sys_5v0_mean_watts": 8.26, "gpu_utilization_percent_mean": 71.033, "power_watts_avg": 19.719, "energy_joules_est": 212.02, "duration_seconds": 10.752, "sample_count": 92}, "timestamp": "2026-01-26T15:28:55.650755"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11603.734, "latencies_ms": [11603.734], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, a chef in a white shirt is standing near a stainless steel pot, stirring something in it with a ladle. Behind him, another chef in a white shirt and blue cap is working on a cutting board, preparing food. The background shows a well-equipped kitchen with various cooking utensils and appliances.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21037.7, "ram_available_mb": 41803.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21076.1, "ram_available_mb": 41764.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.01}, "power_stats": {"power_gpu_soc_mean_watts": 19.139, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.293, "gpu_utilization_percent_mean": 70.01, "power_watts_avg": 19.139, "energy_joules_est": 222.1, "duration_seconds": 11.604, "sample_count": 99}, "timestamp": "2026-01-26T15:29:09.289448"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7661.607, "latencies_ms": [7661.607], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "In a professional kitchen, two chefs are working diligently to prepare food. They are surrounded by various cooking utensils and ingredients, indicating a busy and well-equipped environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21076.1, "ram_available_mb": 41764.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21106.9, "ram_available_mb": 41734.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.908}, "power_stats": {"power_gpu_soc_mean_watts": 21.113, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.231, "gpu_utilization_percent_mean": 74.908, "power_watts_avg": 21.113, "energy_joules_est": 161.77, "duration_seconds": 7.662, "sample_count": 65}, "timestamp": "2026-01-26T15:29:18.989153"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10517.604, "latencies_ms": [10517.604], "images_per_second": 0.095, "prompt_tokens": 36, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image depicts a professional kitchen with stainless steel appliances and surfaces, reflecting bright overhead lights. Two chefs, one wearing a white shirt and the other in a white and blue uniform, are working together, one stirring a pot and the other slicing ingredients on a wooden board.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21034.9, "ram_available_mb": 41806.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21094.3, "ram_available_mb": 41746.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.483}, "power_stats": {"power_gpu_soc_mean_watts": 19.472, "power_cpu_cv_mean_watts": 1.839, "power_sys_5v0_mean_watts": 8.298, "gpu_utilization_percent_mean": 70.483, "power_watts_avg": 19.472, "energy_joules_est": 204.81, "duration_seconds": 10.518, "sample_count": 89}, "timestamp": "2026-01-26T15:29:31.541871"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11164.796, "latencies_ms": [11164.796], "images_per_second": 0.09, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of motorcyclists gathered on the side of a road, with several motorcycles parked in a row. There are at least nine motorcycles visible, with some being parked closer to the road and others further back. A total of 11 people can be seen in the scene, with some standing near the motorcycles and others walking around.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 21094.3, "ram_available_mb": 41746.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21039.3, "ram_available_mb": 41801.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 9898.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 66.67}, "power_stats": {"power_gpu_soc_mean_watts": 17.372, "power_cpu_cv_mean_watts": 1.964, "power_sys_5v0_mean_watts": 8.152, "gpu_utilization_percent_mean": 66.67, "power_watts_avg": 17.372, "energy_joules_est": 193.97, "duration_seconds": 11.165, "sample_count": 97}, "timestamp": "2026-01-26T15:29:44.743651"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9488.825, "latencies_ms": [9488.825], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "- Motorcycles: 9\n- People: 10\n- Cars: 1\n- Clothes: 10\n- Helmets: 5\n- Jackets: 10\n- Jeans: 10\n- Boots: 10", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21039.3, "ram_available_mb": 41801.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21044.0, "ram_available_mb": 41796.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 9924.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.488}, "power_stats": {"power_gpu_soc_mean_watts": 18.341, "power_cpu_cv_mean_watts": 1.835, "power_sys_5v0_mean_watts": 8.134, "gpu_utilization_percent_mean": 69.488, "power_watts_avg": 18.341, "energy_joules_est": 174.05, "duration_seconds": 9.489, "sample_count": 82}, "timestamp": "2026-01-26T15:29:56.273521"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11240.415, "latencies_ms": [11240.415], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are several motorcycles parked along the side of the road, with a group of people standing nearby, likely discussing or preparing for a ride. The motorcycles are positioned in a line, with the closest one being the most prominent, and the others gradually receding into the background. The people are standing at varying distances from the motorcy", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21044.0, "ram_available_mb": 41796.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 21046.9, "ram_available_mb": 41794.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 9930.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 67.927}, "power_stats": {"power_gpu_soc_mean_watts": 17.555, "power_cpu_cv_mean_watts": 2.093, "power_sys_5v0_mean_watts": 8.143, "gpu_utilization_percent_mean": 67.927, "power_watts_avg": 17.555, "energy_joules_est": 197.34, "duration_seconds": 11.241, "sample_count": 96}, "timestamp": "2026-01-26T15:30:09.565747"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7582.875, "latencies_ms": [7582.875], "images_per_second": 0.132, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "A group of motorcyclists are gathered on the side of a road, with some sitting on their bikes and others standing. The sky is cloudy, suggesting that it might be a cool or overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21046.9, "ram_available_mb": 41794.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21047.9, "ram_available_mb": 41793.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 9922.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.281}, "power_stats": {"power_gpu_soc_mean_watts": 19.326, "power_cpu_cv_mean_watts": 1.657, "power_sys_5v0_mean_watts": 8.056, "gpu_utilization_percent_mean": 70.281, "power_watts_avg": 19.326, "energy_joules_est": 146.56, "duration_seconds": 7.584, "sample_count": 64}, "timestamp": "2026-01-26T15:30:19.173333"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8024.149, "latencies_ms": [8024.149], "images_per_second": 0.125, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a group of motorcyclists gathered on the side of a road under an overcast sky. The motorcycles are of various colors and models, and the riders are wearing helmets and casual clothing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21047.9, "ram_available_mb": 41793.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21070.8, "ram_available_mb": 41770.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 9921.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.912}, "power_stats": {"power_gpu_soc_mean_watts": 18.931, "power_cpu_cv_mean_watts": 1.754, "power_sys_5v0_mean_watts": 8.115, "gpu_utilization_percent_mean": 68.912, "power_watts_avg": 18.931, "energy_joules_est": 151.92, "duration_seconds": 8.025, "sample_count": 68}, "timestamp": "2026-01-26T15:30:29.233341"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11665.828, "latencies_ms": [11665.828], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of a small, single-engine propeller plane soaring through the sky. The plane, painted in a striking combination of black and white, is flying from left to right, leaving behind a trail of white smoke that contrasts with the grayish-blue backdrop of the sky. The perspective of the image is from below, looking up at the plane, giving", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21070.8, "ram_available_mb": 41770.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21091.1, "ram_available_mb": 41749.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.265}, "power_stats": {"power_gpu_soc_mean_watts": 19.149, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.262, "gpu_utilization_percent_mean": 70.265, "power_watts_avg": 19.149, "energy_joules_est": 223.4, "duration_seconds": 11.666, "sample_count": 98}, "timestamp": "2026-01-26T15:30:42.945240"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7423.908, "latencies_ms": [7423.908], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "airplane: 1, cloud: multiple, smoke: 2, sky: 1, engine: 1, propeller: 1, tail: 1, wing: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21020.9, "ram_available_mb": 41820.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21081.3, "ram_available_mb": 41759.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.161}, "power_stats": {"power_gpu_soc_mean_watts": 21.525, "power_cpu_cv_mean_watts": 1.536, "power_sys_5v0_mean_watts": 8.21, "gpu_utilization_percent_mean": 74.161, "power_watts_avg": 21.525, "energy_joules_est": 159.81, "duration_seconds": 7.424, "sample_count": 62}, "timestamp": "2026-01-26T15:30:52.387059"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9528.499, "latencies_ms": [9528.499], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The airplane is flying in the foreground of the image, appearing closer to the viewer, while the clouds are in the background, creating a sense of depth. The airplane is positioned slightly to the left of the center of the image, leaving ample space on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21027.5, "ram_available_mb": 41813.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21070.0, "ram_available_mb": 41770.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.938}, "power_stats": {"power_gpu_soc_mean_watts": 19.942, "power_cpu_cv_mean_watts": 1.774, "power_sys_5v0_mean_watts": 8.28, "gpu_utilization_percent_mean": 70.938, "power_watts_avg": 19.942, "energy_joules_est": 190.03, "duration_seconds": 9.529, "sample_count": 81}, "timestamp": "2026-01-26T15:31:03.939289"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10828.86, "latencies_ms": [10828.86], "images_per_second": 0.092, "prompt_tokens": 37, "response_tokens_est": 71, "n_tiles": 16, "output_text": "A small propeller-driven aircraft is flying through a cloudy sky, leaving a trail of smoke behind it. The aircraft appears to be a single-engine, propeller-driven plane, and it is flying at a low altitude, with the smoke trailing behind it indicating that it is in the process of taking off or landing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21070.0, "ram_available_mb": 41770.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21025.5, "ram_available_mb": 41815.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.293}, "power_stats": {"power_gpu_soc_mean_watts": 19.784, "power_cpu_cv_mean_watts": 1.827, "power_sys_5v0_mean_watts": 8.294, "gpu_utilization_percent_mean": 71.293, "power_watts_avg": 19.784, "energy_joules_est": 214.25, "duration_seconds": 10.829, "sample_count": 92}, "timestamp": "2026-01-26T15:31:16.780328"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7662.351, "latencies_ms": [7662.351], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image is in black and white, featuring a small propeller-driven aircraft flying through a cloudy sky. The clouds appear to be thick and fluffy, suggesting that the weather may be overcast.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21025.5, "ram_available_mb": 41815.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21061.0, "ram_available_mb": 41779.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.138}, "power_stats": {"power_gpu_soc_mean_watts": 20.79, "power_cpu_cv_mean_watts": 1.625, "power_sys_5v0_mean_watts": 8.272, "gpu_utilization_percent_mean": 72.138, "power_watts_avg": 20.79, "energy_joules_est": 159.32, "duration_seconds": 7.663, "sample_count": 65}, "timestamp": "2026-01-26T15:31:26.503615"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11377.373, "latencies_ms": [11377.373], "images_per_second": 0.088, "prompt_tokens": 24, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the image, there are three sheep standing on a grassy hillside. They are positioned close to each other, with one sheep slightly ahead of the other two. The sheep appear to be looking towards the camera, giving the impression that they are posing for a picture. The background features a beautiful mountainous landscape with a lake, adding to the picturesque setting.", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 21061.0, "ram_available_mb": 41779.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21062.2, "ram_available_mb": 41778.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.856}, "power_stats": {"power_gpu_soc_mean_watts": 19.329, "power_cpu_cv_mean_watts": 1.869, "power_sys_5v0_mean_watts": 8.298, "gpu_utilization_percent_mean": 69.856, "power_watts_avg": 19.329, "energy_joules_est": 219.93, "duration_seconds": 11.378, "sample_count": 97}, "timestamp": "2026-01-26T15:31:39.948565"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7331.567, "latencies_ms": [7331.567], "images_per_second": 0.136, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "mountain: 3, sheep: 4, grass: 1, sky: 1, water: 1, rock: 1, tree: 1, valley: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21062.2, "ram_available_mb": 41778.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21063.2, "ram_available_mb": 41777.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.403}, "power_stats": {"power_gpu_soc_mean_watts": 21.463, "power_cpu_cv_mean_watts": 1.536, "power_sys_5v0_mean_watts": 8.194, "gpu_utilization_percent_mean": 74.403, "power_watts_avg": 21.463, "energy_joules_est": 157.37, "duration_seconds": 7.332, "sample_count": 62}, "timestamp": "2026-01-26T15:31:49.301540"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11598.988, "latencies_ms": [11598.988], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are three sheep standing on a grassy hillside with rocks scattered around. The sheep are positioned near the center of the image, with one slightly to the left, one in the middle, and one to the right. In the background, there are rolling hills that extend into the distance, with a body of water visible at the base of the hills. The sheep", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21063.2, "ram_available_mb": 41777.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21105.9, "ram_available_mb": 41735.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_gpu_soc_mean_watts": 19.142, "power_cpu_cv_mean_watts": 1.879, "power_sys_5v0_mean_watts": 8.308, "gpu_utilization_percent_mean": 70.0, "power_watts_avg": 19.142, "energy_joules_est": 222.04, "duration_seconds": 11.6, "sample_count": 98}, "timestamp": "2026-01-26T15:32:02.929714"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7406.167, "latencies_ms": [7406.167], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "Three sheep are standing on a grassy hillside with a beautiful blue lake and mountains in the background. The sheep appear to be enjoying the scenic view and the peaceful surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21061.0, "ram_available_mb": 41779.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21080.7, "ram_available_mb": 41760.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.254}, "power_stats": {"power_gpu_soc_mean_watts": 21.59, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 8.221, "gpu_utilization_percent_mean": 74.254, "power_watts_avg": 21.59, "energy_joules_est": 159.91, "duration_seconds": 7.407, "sample_count": 63}, "timestamp": "2026-01-26T15:32:12.378553"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8246.954, "latencies_ms": [8246.954], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image features three sheep standing on a grassy hillside with a backdrop of rolling hills and a clear blue sky. The sheep are white and appear to be grazing on the grass, with a body of water visible in the distance.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21025.6, "ram_available_mb": 41815.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21054.6, "ram_available_mb": 41786.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.4}, "power_stats": {"power_gpu_soc_mean_watts": 20.391, "power_cpu_cv_mean_watts": 1.663, "power_sys_5v0_mean_watts": 8.269, "gpu_utilization_percent_mean": 72.4, "power_watts_avg": 20.391, "energy_joules_est": 168.18, "duration_seconds": 8.248, "sample_count": 70}, "timestamp": "2026-01-26T15:32:22.657165"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11581.827, "latencies_ms": [11581.827], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young woman in a wheelchair is holding a tennis racket, ready to play tennis. She is wearing a gray shirt and is positioned in the foreground. Behind her, there is a man in a gray shirt, who is also holding a tennis racket. They are both focused on the game, with the woman in the wheelchair", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 21054.6, "ram_available_mb": 41786.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21055.6, "ram_available_mb": 41785.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.424}, "power_stats": {"power_gpu_soc_mean_watts": 19.287, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.285, "gpu_utilization_percent_mean": 70.424, "power_watts_avg": 19.287, "energy_joules_est": 223.39, "duration_seconds": 11.582, "sample_count": 99}, "timestamp": "2026-01-26T15:32:36.269176"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7791.482, "latencies_ms": [7791.482], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "wheelchair: 1, tennis racket: 1, person: 2, building: 1, wall: 1, window: 1, door: 1, chair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21055.6, "ram_available_mb": 41785.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21055.6, "ram_available_mb": 41785.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.015}, "power_stats": {"power_gpu_soc_mean_watts": 21.149, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.242, "gpu_utilization_percent_mean": 73.015, "power_watts_avg": 21.149, "energy_joules_est": 164.79, "duration_seconds": 7.792, "sample_count": 66}, "timestamp": "2026-01-26T15:32:46.109416"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9988.631, "latencies_ms": [9988.631], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "In the foreground, a person in a wheelchair is holding a tennis racket, positioned near the center of the image. To the right, slightly in the background, another person is standing and also holding a tennis racket. The wheelchair user is closer to the camera than the standing person.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21055.6, "ram_available_mb": 41785.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21056.9, "ram_available_mb": 41784.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.576}, "power_stats": {"power_gpu_soc_mean_watts": 19.77, "power_cpu_cv_mean_watts": 1.794, "power_sys_5v0_mean_watts": 8.256, "gpu_utilization_percent_mean": 70.576, "power_watts_avg": 19.77, "energy_joules_est": 197.49, "duration_seconds": 9.989, "sample_count": 85}, "timestamp": "2026-01-26T15:32:58.129548"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7406.833, "latencies_ms": [7406.833], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A person in a wheelchair is holding a tennis racket, ready to play tennis, with another person standing nearby. The setting appears to be an indoor tennis court or a similar facility.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21056.9, "ram_available_mb": 41784.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21100.9, "ram_available_mb": 41740.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.73}, "power_stats": {"power_gpu_soc_mean_watts": 21.641, "power_cpu_cv_mean_watts": 1.556, "power_sys_5v0_mean_watts": 8.259, "gpu_utilization_percent_mean": 74.73, "power_watts_avg": 21.641, "energy_joules_est": 160.31, "duration_seconds": 7.407, "sample_count": 63}, "timestamp": "2026-01-26T15:33:07.554038"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9850.443, "latencies_ms": [9850.443], "images_per_second": 0.102, "prompt_tokens": 36, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image features a person in a wheelchair holding a tennis racket with a purple and white color scheme. The person is wearing a gray t-shirt with the word \"Empower\" visible on it. The background shows a clear sky and the lighting appears to be natural daylight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21039.0, "ram_available_mb": 41801.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21068.2, "ram_available_mb": 41772.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.405}, "power_stats": {"power_gpu_soc_mean_watts": 19.781, "power_cpu_cv_mean_watts": 1.81, "power_sys_5v0_mean_watts": 8.312, "gpu_utilization_percent_mean": 71.405, "power_watts_avg": 19.781, "energy_joules_est": 194.86, "duration_seconds": 9.851, "sample_count": 84}, "timestamp": "2026-01-26T15:33:19.449178"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12398.671, "latencies_ms": [12398.671], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young child is the main subject, sitting on a brown horse saddle. The child is wearing a black helmet and a pink and white checkered shirt, with blue jeans. The saddle is brown and appears to be made of leather. The child is holding onto the reins with both hands, indicating control over the horse. The background of", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 21068.2, "ram_available_mb": 41772.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21068.5, "ram_available_mb": 41772.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.123}, "power_stats": {"power_gpu_soc_mean_watts": 21.517, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.543, "gpu_utilization_percent_mean": 72.123, "power_watts_avg": 21.517, "energy_joules_est": 266.8, "duration_seconds": 12.399, "sample_count": 106}, "timestamp": "2026-01-26T15:33:33.901489"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 12965.974, "latencies_ms": [12965.974], "images_per_second": 0.077, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "object: horse, count: 1\nobject: saddle, count: 1\nobject: rider, count: 1\nobject: helmet, count: 1\nobject: plaid shirt, count: 1\nobject: jeans, count: 1\nobject: boot, count: 1\nobject: riding boots, count: ", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21068.5, "ram_available_mb": 41772.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21023.7, "ram_available_mb": 41817.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.691}, "power_stats": {"power_gpu_soc_mean_watts": 21.653, "power_cpu_cv_mean_watts": 1.743, "power_sys_5v0_mean_watts": 8.444, "gpu_utilization_percent_mean": 73.691, "power_watts_avg": 21.653, "energy_joules_est": 280.77, "duration_seconds": 12.967, "sample_count": 110}, "timestamp": "2026-01-26T15:33:48.881289"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12667.913, "latencies_ms": [12667.913], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The child is seated on a horse saddle that is positioned in the foreground of the image, indicating they are the main object of focus. The background is filled with greenery, suggesting the setting is outdoors, possibly in a park or a rural area. The saddle and the child are in the near foreground, while the trees and grass fill the background, creating a", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21023.7, "ram_available_mb": 41817.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21087.3, "ram_available_mb": 41753.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.019}, "power_stats": {"power_gpu_soc_mean_watts": 21.61, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 8.537, "gpu_utilization_percent_mean": 74.019, "power_watts_avg": 21.61, "energy_joules_est": 273.77, "duration_seconds": 12.668, "sample_count": 108}, "timestamp": "2026-01-26T15:34:03.610512"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9193.57, "latencies_ms": [9193.57], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A child is riding a brown horse-shaped ride-on toy in a grassy area with trees in the background. The child is wearing a plaid shirt, jeans, and brown boots.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21023.9, "ram_available_mb": 41817.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21014.2, "ram_available_mb": 41826.7, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.936}, "power_stats": {"power_gpu_soc_mean_watts": 23.031, "power_cpu_cv_mean_watts": 1.554, "power_sys_5v0_mean_watts": 8.415, "gpu_utilization_percent_mean": 77.936, "power_watts_avg": 23.031, "energy_joules_est": 211.75, "duration_seconds": 9.194, "sample_count": 78}, "timestamp": "2026-01-26T15:34:14.825657"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10934.993, "latencies_ms": [10934.993], "images_per_second": 0.091, "prompt_tokens": 36, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image shows a child riding a brown horse-shaped ride-on toy with a black helmet, set against a backdrop of lush green trees. The child is wearing a plaid shirt with a mix of pink, white, and red colors, and blue jeans.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21014.2, "ram_available_mb": 41826.7, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21082.6, "ram_available_mb": 41758.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.106}, "power_stats": {"power_gpu_soc_mean_watts": 22.015, "power_cpu_cv_mean_watts": 1.686, "power_sys_5v0_mean_watts": 8.555, "gpu_utilization_percent_mean": 74.106, "power_watts_avg": 22.015, "energy_joules_est": 240.75, "duration_seconds": 10.936, "sample_count": 94}, "timestamp": "2026-01-26T15:34:27.815040"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12319.001, "latencies_ms": [12319.001], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene moment at Raglan, New Zealand, where two surfers are seen riding the waves. The surfer on the left, clad in a black wetsuit, is skillfully maneuvering a white surfboard on a wave that's breaking to the right. A little further away, another surfer in a black wetsuit", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 21020.7, "ram_available_mb": 41820.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21064.8, "ram_available_mb": 41776.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.547}, "power_stats": {"power_gpu_soc_mean_watts": 21.404, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.494, "gpu_utilization_percent_mean": 73.547, "power_watts_avg": 21.404, "energy_joules_est": 263.69, "duration_seconds": 12.32, "sample_count": 106}, "timestamp": "2026-01-26T15:34:42.198963"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6732.397, "latencies_ms": [6732.397], "images_per_second": 0.149, "prompt_tokens": 39, "response_tokens_est": 28, "n_tiles": 16, "output_text": "surfboard: 2\nsurfer: 2\nwave: multiple\nocean: single\ntext: 1\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21064.8, "ram_available_mb": 41776.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 21065.1, "ram_available_mb": 41775.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 80.947}, "power_stats": {"power_gpu_soc_mean_watts": 24.2, "power_cpu_cv_mean_watts": 1.257, "power_sys_5v0_mean_watts": 8.32, "gpu_utilization_percent_mean": 80.947, "power_watts_avg": 24.2, "energy_joules_est": 162.94, "duration_seconds": 6.733, "sample_count": 57}, "timestamp": "2026-01-26T15:34:50.965791"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12150.015, "latencies_ms": [12150.015], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a surfer riding a wave on the left side of the image, while another surfer is further back, closer to the center, also riding a wave. In the background, there is a third surfer who is positioned further away, closer to the right side of the image. The waves create a spatial relationship where the surfers are at different", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21065.1, "ram_available_mb": 41775.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21078.8, "ram_available_mb": 41762.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.398}, "power_stats": {"power_gpu_soc_mean_watts": 21.427, "power_cpu_cv_mean_watts": 1.834, "power_sys_5v0_mean_watts": 8.545, "gpu_utilization_percent_mean": 71.398, "power_watts_avg": 21.427, "energy_joules_est": 260.35, "duration_seconds": 12.151, "sample_count": 103}, "timestamp": "2026-01-26T15:35:05.142673"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9167.428, "latencies_ms": [9167.428], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image captures a serene moment at Raglan, New Zealand, where two individuals are seen surfing on the ocean waves. The sky is clear, and the sea is calm, providing perfect conditions for the surfers.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21017.0, "ram_available_mb": 41823.9, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21084.1, "ram_available_mb": 41756.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.436}, "power_stats": {"power_gpu_soc_mean_watts": 22.607, "power_cpu_cv_mean_watts": 1.524, "power_sys_5v0_mean_watts": 8.376, "gpu_utilization_percent_mean": 76.436, "power_watts_avg": 22.607, "energy_joules_est": 207.26, "duration_seconds": 9.168, "sample_count": 78}, "timestamp": "2026-01-26T15:35:16.327049"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10186.767, "latencies_ms": [10186.767], "images_per_second": 0.098, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image captures a serene ocean scene with a clear blue sky and a calm sea, where two individuals are surfing. The lighting is natural and bright, suggesting it is a sunny day, and the surfers are wearing wetsuits, indicating the water might be cool.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21084.1, "ram_available_mb": 41756.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21085.1, "ram_available_mb": 41755.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.552}, "power_stats": {"power_gpu_soc_mean_watts": 21.884, "power_cpu_cv_mean_watts": 1.725, "power_sys_5v0_mean_watts": 8.519, "gpu_utilization_percent_mean": 73.552, "power_watts_avg": 21.884, "energy_joules_est": 222.94, "duration_seconds": 10.187, "sample_count": 87}, "timestamp": "2026-01-26T15:35:28.566890"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11580.313, "latencies_ms": [11580.313], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a cozy kitchen scene, bathed in soft light that filters through a large window adorned with a diamond-shaped sticker. The window, a silent observer to the culinary adventures, frames a view of a bustling street outside. The kitchen itself is a symphony of colors and textures, with a black stove standing proudly on", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 21085.1, "ram_available_mb": 41755.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21085.8, "ram_available_mb": 41755.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.91}, "power_stats": {"power_gpu_soc_mean_watts": 19.288, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.306, "gpu_utilization_percent_mean": 69.91, "power_watts_avg": 19.288, "energy_joules_est": 223.37, "duration_seconds": 11.581, "sample_count": 100}, "timestamp": "2026-01-26T15:35:42.173809"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7887.859, "latencies_ms": [7887.859], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "stove: 1, pot: 3, pan: 2, window: 1, plant: 1, bookshelf: 1, shelf: 1, plant pot: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21085.8, "ram_available_mb": 41755.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21087.0, "ram_available_mb": 41753.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.239}, "power_stats": {"power_gpu_soc_mean_watts": 21.074, "power_cpu_cv_mean_watts": 1.583, "power_sys_5v0_mean_watts": 8.22, "gpu_utilization_percent_mean": 73.239, "power_watts_avg": 21.074, "energy_joules_est": 166.24, "duration_seconds": 7.889, "sample_count": 67}, "timestamp": "2026-01-26T15:35:52.081058"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11368.52, "latencies_ms": [11368.52], "images_per_second": 0.088, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "In the foreground, there is a small wooden table with a potted plant on it, positioned near the center of the image. The stove is located to the left of the table, and the refrigerator is to the right. In the background, there is a window with a view of the outside, and various kitchen items are scattered around the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21087.0, "ram_available_mb": 41753.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21064.2, "ram_available_mb": 41776.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.208}, "power_stats": {"power_gpu_soc_mean_watts": 19.243, "power_cpu_cv_mean_watts": 1.876, "power_sys_5v0_mean_watts": 8.298, "gpu_utilization_percent_mean": 70.208, "power_watts_avg": 19.243, "energy_joules_est": 218.78, "duration_seconds": 11.369, "sample_count": 96}, "timestamp": "2026-01-26T15:36:05.464060"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8319.631, "latencies_ms": [8319.631], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image depicts a cozy, dimly lit kitchen with a large window that allows natural light to filter in. Various kitchen items and utensils are scattered throughout the space, creating a lived-in and functional atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21064.2, "ram_available_mb": 41776.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21064.9, "ram_available_mb": 41776.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.803}, "power_stats": {"power_gpu_soc_mean_watts": 20.917, "power_cpu_cv_mean_watts": 1.634, "power_sys_5v0_mean_watts": 8.235, "gpu_utilization_percent_mean": 72.803, "power_watts_avg": 20.917, "energy_joules_est": 174.04, "duration_seconds": 8.32, "sample_count": 71}, "timestamp": "2026-01-26T15:36:15.831277"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7445.627, "latencies_ms": [7445.627], "images_per_second": 0.134, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The image shows a kitchen with warm lighting and a combination of wood and metal materials. The walls are painted in a light color, and there is a large window that allows natural light to enter the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21064.9, "ram_available_mb": 41776.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21065.9, "ram_available_mb": 41775.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.944, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.225, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 20.944, "energy_joules_est": 155.96, "duration_seconds": 7.446, "sample_count": 63}, "timestamp": "2026-01-26T15:36:25.335748"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11586.357, "latencies_ms": [11586.357], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a table with a variety of fruits and items arranged on it. There are three oranges placed on a plate, with one orange positioned closer to the left side of the plate and the other two on the right side. A pineapple is also present on the table, occupying a significant portion of the left side. \n\nIn addition to the fruits,", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21065.9, "ram_available_mb": 41775.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21098.9, "ram_available_mb": 41742.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.939}, "power_stats": {"power_gpu_soc_mean_watts": 19.319, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.317, "gpu_utilization_percent_mean": 70.939, "power_watts_avg": 19.319, "energy_joules_est": 223.85, "duration_seconds": 11.587, "sample_count": 98}, "timestamp": "2026-01-26T15:36:38.953671"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9139.715, "latencies_ms": [9139.715], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "Pineapple: 1\nRed incense sticks: 12\nOrange: 3\nRed cups: 6\nRed mat: 1\nWhite plate: 1\nRed candle holder: 1\nRed cloth: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 21028.7, "ram_available_mb": 41812.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21069.3, "ram_available_mb": 41771.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.338}, "power_stats": {"power_gpu_soc_mean_watts": 20.413, "power_cpu_cv_mean_watts": 1.705, "power_sys_5v0_mean_watts": 8.255, "gpu_utilization_percent_mean": 72.338, "power_watts_avg": 20.413, "energy_joules_est": 186.58, "duration_seconds": 9.14, "sample_count": 77}, "timestamp": "2026-01-26T15:36:50.119102"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11617.86, "latencies_ms": [11617.86], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are six red cups arranged in a row, with the cups being the closest objects to the viewer. Behind the cups, there is a pineapple on the left side and a set of oranges on the right side, both of which are further away from the viewer. The pineapple is positioned to the left of the or", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21069.3, "ram_available_mb": 41771.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21096.0, "ram_available_mb": 41744.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.253}, "power_stats": {"power_gpu_soc_mean_watts": 19.113, "power_cpu_cv_mean_watts": 1.871, "power_sys_5v0_mean_watts": 8.288, "gpu_utilization_percent_mean": 70.253, "power_watts_avg": 19.113, "energy_joules_est": 222.06, "duration_seconds": 11.619, "sample_count": 99}, "timestamp": "2026-01-26T15:37:03.798207"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9151.094, "latencies_ms": [9151.094], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image depicts a table with a pineapple, oranges, and red cups, possibly set up for a cultural or religious event. The presence of incense sticks in a container suggests that the scene may be related to a ritual or ceremony.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 21096.0, "ram_available_mb": 41744.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21044.1, "ram_available_mb": 41796.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.234}, "power_stats": {"power_gpu_soc_mean_watts": 20.342, "power_cpu_cv_mean_watts": 1.684, "power_sys_5v0_mean_watts": 8.224, "gpu_utilization_percent_mean": 72.234, "power_watts_avg": 20.342, "energy_joules_est": 186.16, "duration_seconds": 9.152, "sample_count": 77}, "timestamp": "2026-01-26T15:37:14.977900"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10986.881, "latencies_ms": [10986.881], "images_per_second": 0.091, "prompt_tokens": 36, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image features a vibrant red background with a large pineapple on the left side, a small red pot with pink incense sticks in the center, and a white plate with three oranges on the right side. The lighting in the image highlights the textures and colors of the objects, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21044.1, "ram_available_mb": 41796.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21059.3, "ram_available_mb": 41781.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.415}, "power_stats": {"power_gpu_soc_mean_watts": 19.304, "power_cpu_cv_mean_watts": 1.865, "power_sys_5v0_mean_watts": 8.289, "gpu_utilization_percent_mean": 70.415, "power_watts_avg": 19.304, "energy_joules_est": 212.1, "duration_seconds": 10.988, "sample_count": 94}, "timestamp": "2026-01-26T15:37:27.990488"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11596.088, "latencies_ms": [11596.088], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the image, a man is sitting at a table in a restaurant, holding a plate with a sandwich and fries. He is smiling and taking a selfie with his cell phone. The restaurant appears to be busy, with several other people in the background. The man is wearing a black shirt and glasses, and the sandwich on his plate is golden brown.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21059.3, "ram_available_mb": 41781.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21068.5, "ram_available_mb": 41772.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.111}, "power_stats": {"power_gpu_soc_mean_watts": 19.303, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.308, "gpu_utilization_percent_mean": 70.111, "power_watts_avg": 19.303, "energy_joules_est": 223.85, "duration_seconds": 11.597, "sample_count": 99}, "timestamp": "2026-01-26T15:37:41.635464"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10291.157, "latencies_ms": [10291.157], "images_per_second": 0.097, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "- Man: 1\n\n- Fries: 1 serving\n\n- Sandwich: 1\n\n- Ketchup: 1 small container\n\n- Restaurant interior: Multiple objects (at least 5 visible)\n\n- Table: 1\n\n- Chair: 1 visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21068.5, "ram_available_mb": 41772.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21069.5, "ram_available_mb": 41771.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.402}, "power_stats": {"power_gpu_soc_mean_watts": 19.862, "power_cpu_cv_mean_watts": 1.762, "power_sys_5v0_mean_watts": 8.259, "gpu_utilization_percent_mean": 71.402, "power_watts_avg": 19.862, "energy_joules_est": 204.42, "duration_seconds": 10.292, "sample_count": 87}, "timestamp": "2026-01-26T15:37:53.948307"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10446.239, "latencies_ms": [10446.239], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a plate with a sandwich and a side of fries. The sandwich is placed towards the left side of the plate, while the fries are scattered around the bottom left corner. In the background, there are people standing and sitting, with one person standing on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21069.5, "ram_available_mb": 41771.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21069.5, "ram_available_mb": 41771.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.989}, "power_stats": {"power_gpu_soc_mean_watts": 19.442, "power_cpu_cv_mean_watts": 1.839, "power_sys_5v0_mean_watts": 8.277, "gpu_utilization_percent_mean": 70.989, "power_watts_avg": 19.442, "energy_joules_est": 203.11, "duration_seconds": 10.447, "sample_count": 89}, "timestamp": "2026-01-26T15:38:06.434415"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5349.574, "latencies_ms": [5349.574], "images_per_second": 0.187, "prompt_tokens": 37, "response_tokens_est": 23, "n_tiles": 16, "output_text": "A man is taking a selfie in a restaurant, holding a plate with a sandwich and fries.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21069.5, "ram_available_mb": 41771.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 8.4, "ram_used_mb": 21075.5, "ram_available_mb": 41765.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.444}, "power_stats": {"power_gpu_soc_mean_watts": 23.697, "power_cpu_cv_mean_watts": 1.636, "power_sys_5v0_mean_watts": 8.202, "gpu_utilization_percent_mean": 77.444, "power_watts_avg": 23.697, "energy_joules_est": 126.78, "duration_seconds": 5.35, "sample_count": 45}, "timestamp": "2026-01-26T15:38:13.836993"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8574.598, "latencies_ms": [8574.598], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a man indoors with warm lighting, wearing a black t-shirt and glasses. He is holding a plate with a sandwich and a side of fries, and there is a clock on the wall behind him.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 21075.5, "ram_available_mb": 41765.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21075.7, "ram_available_mb": 41765.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.027}, "power_stats": {"power_gpu_soc_mean_watts": 20.278, "power_cpu_cv_mean_watts": 1.738, "power_sys_5v0_mean_watts": 8.284, "gpu_utilization_percent_mean": 72.027, "power_watts_avg": 20.278, "energy_joules_est": 173.89, "duration_seconds": 8.575, "sample_count": 73}, "timestamp": "2026-01-26T15:38:24.466916"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11557.292, "latencies_ms": [11557.292], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene from a high-rise building, viewed from a window. The window, framed in black, offers a clear view of the courtyard below. The courtyard is a mix of gray and green, with a concrete floor that reflects the overcast sky. \n\nTwo bicycles, one blue and one red, are parked on", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21075.7, "ram_available_mb": 41765.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21076.9, "ram_available_mb": 41764.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.09}, "power_stats": {"power_gpu_soc_mean_watts": 19.23, "power_cpu_cv_mean_watts": 1.881, "power_sys_5v0_mean_watts": 8.303, "gpu_utilization_percent_mean": 70.09, "power_watts_avg": 19.23, "energy_joules_est": 222.26, "duration_seconds": 11.558, "sample_count": 100}, "timestamp": "2026-01-26T15:38:38.088179"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7903.588, "latencies_ms": [7903.588], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "building: 1, window: 1, person: 1, umbrella: 1, bicycle: 2, plant: 1, railing: 1, tree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21076.9, "ram_available_mb": 41764.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21077.2, "ram_available_mb": 41763.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.642}, "power_stats": {"power_gpu_soc_mean_watts": 21.053, "power_cpu_cv_mean_watts": 1.577, "power_sys_5v0_mean_watts": 8.231, "gpu_utilization_percent_mean": 72.642, "power_watts_avg": 21.053, "energy_joules_est": 166.41, "duration_seconds": 7.904, "sample_count": 67}, "timestamp": "2026-01-26T15:38:48.015065"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11196.613, "latencies_ms": [11196.613], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there are two bicycles leaning against a metal structure, positioned near the center of the image. In the background, there is a person walking with an umbrella, located on the left side of the image. The metal structure appears to be in the middle ground, extending from the left to the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21077.2, "ram_available_mb": 41763.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21089.9, "ram_available_mb": 41751.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.379}, "power_stats": {"power_gpu_soc_mean_watts": 19.453, "power_cpu_cv_mean_watts": 1.862, "power_sys_5v0_mean_watts": 8.314, "gpu_utilization_percent_mean": 70.379, "power_watts_avg": 19.453, "energy_joules_est": 217.82, "duration_seconds": 11.197, "sample_count": 95}, "timestamp": "2026-01-26T15:39:01.230926"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9869.193, "latencies_ms": [9869.193], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image shows a view from a window looking out onto a courtyard with a person walking with an umbrella, bicycles parked, and a tall metal structure. The scene appears to be in an urban setting, possibly a public space or a courtyard within a building complex.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 21028.0, "ram_available_mb": 41812.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21079.1, "ram_available_mb": 41761.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.554}, "power_stats": {"power_gpu_soc_mean_watts": 20.053, "power_cpu_cv_mean_watts": 1.74, "power_sys_5v0_mean_watts": 8.248, "gpu_utilization_percent_mean": 71.554, "power_watts_avg": 20.053, "energy_joules_est": 197.92, "duration_seconds": 9.87, "sample_count": 83}, "timestamp": "2026-01-26T15:39:13.119743"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6377.609, "latencies_ms": [6377.609], "images_per_second": 0.157, "prompt_tokens": 36, "response_tokens_est": 33, "n_tiles": 16, "output_text": "The image shows a rainy day with a person walking with an umbrella. The buildings in the background are made of concrete and have a modern design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21017.4, "ram_available_mb": 41823.5, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21068.0, "ram_available_mb": 41772.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.778}, "power_stats": {"power_gpu_soc_mean_watts": 22.097, "power_cpu_cv_mean_watts": 1.43, "power_sys_5v0_mean_watts": 8.211, "gpu_utilization_percent_mean": 74.778, "power_watts_avg": 22.097, "energy_joules_est": 140.94, "duration_seconds": 6.378, "sample_count": 54}, "timestamp": "2026-01-26T15:39:21.511665"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11612.396, "latencies_ms": [11612.396], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a close-up view of a plate of spaetzle, a type of soft egg noodle or dumpling of German origin. The spaetzle are coated in a creamy white sauce, giving them a glossy appearance. The plate is white, providing a stark contrast to the creamy color of the spaetzle. In", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21068.0, "ram_available_mb": 41772.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21068.7, "ram_available_mb": 41772.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.434}, "power_stats": {"power_gpu_soc_mean_watts": 19.262, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.297, "gpu_utilization_percent_mean": 70.434, "power_watts_avg": 19.262, "energy_joules_est": 223.69, "duration_seconds": 11.613, "sample_count": 99}, "timestamp": "2026-01-26T15:39:35.150605"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8223.401, "latencies_ms": [8223.401], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "plate: 1, fork: 1, noodles: numerous, sauce: visible, plate edge: visible, noodle texture: visible, noodle color: yellow, noodle shape: spiral", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21068.7, "ram_available_mb": 41772.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21068.9, "ram_available_mb": 41772.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.014}, "power_stats": {"power_gpu_soc_mean_watts": 20.924, "power_cpu_cv_mean_watts": 1.624, "power_sys_5v0_mean_watts": 8.232, "gpu_utilization_percent_mean": 73.014, "power_watts_avg": 20.924, "energy_joules_est": 172.08, "duration_seconds": 8.224, "sample_count": 70}, "timestamp": "2026-01-26T15:39:45.406782"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10101.368, "latencies_ms": [10101.368], "images_per_second": 0.099, "prompt_tokens": 44, "response_tokens_est": 66, "n_tiles": 16, "output_text": "In the foreground, there is a pile of cooked noodles with a creamy sauce, occupying the majority of the frame. In the background, slightly out of focus, there is a fork and what appears to be a plate, suggesting that the noodles are ready to be eaten.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21068.9, "ram_available_mb": 41772.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21068.9, "ram_available_mb": 41772.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.046}, "power_stats": {"power_gpu_soc_mean_watts": 19.78, "power_cpu_cv_mean_watts": 1.808, "power_sys_5v0_mean_watts": 8.286, "gpu_utilization_percent_mean": 71.046, "power_watts_avg": 19.78, "energy_joules_est": 199.82, "duration_seconds": 10.102, "sample_count": 87}, "timestamp": "2026-01-26T15:39:57.570146"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9607.039, "latencies_ms": [9607.039], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image shows a close-up of a plate of cooked noodles, possibly a type of pasta, with a fork resting on the edge of the plate. The setting appears to be a dining table with a blurred background, suggesting a mealtime scenario.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21068.9, "ram_available_mb": 41772.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21069.4, "ram_available_mb": 41771.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.988}, "power_stats": {"power_gpu_soc_mean_watts": 20.148, "power_cpu_cv_mean_watts": 1.729, "power_sys_5v0_mean_watts": 8.259, "gpu_utilization_percent_mean": 71.988, "power_watts_avg": 20.148, "energy_joules_est": 193.58, "duration_seconds": 9.608, "sample_count": 81}, "timestamp": "2026-01-26T15:40:09.214056"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8155.073, "latencies_ms": [8155.073], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a close-up of a plate of spaghetti with a fork resting on the edge of the plate. The lighting is dim, casting a warm glow on the pasta and creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 21069.4, "ram_available_mb": 41771.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21080.6, "ram_available_mb": 41760.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.609}, "power_stats": {"power_gpu_soc_mean_watts": 20.484, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.246, "gpu_utilization_percent_mean": 72.609, "power_watts_avg": 20.484, "energy_joules_est": 167.06, "duration_seconds": 8.156, "sample_count": 69}, "timestamp": "2026-01-26T15:40:19.414172"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11604.458, "latencies_ms": [11604.458], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a scene of a workspace, bathed in soft light filtering through a white curtain. Dominating the scene is a silver laptop, its screen alive with a vibrant green cactus wallpaper. The laptop is positioned on a white desk, which also hosts a black mouse and a black speaker, both of which are turned off. \n\nTo", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 21080.6, "ram_available_mb": 41760.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21075.9, "ram_available_mb": 41765.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.596}, "power_stats": {"power_gpu_soc_mean_watts": 19.226, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.304, "gpu_utilization_percent_mean": 69.596, "power_watts_avg": 19.226, "energy_joules_est": 223.12, "duration_seconds": 11.605, "sample_count": 99}, "timestamp": "2026-01-26T15:40:33.069099"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7551.645, "latencies_ms": [7551.645], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "computer: 2, mouse: 2, keyboard: 1, monitor: 1, speaker: 1, mouse pad: 1, wall: 1, curtain: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21075.9, "ram_available_mb": 41765.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21076.4, "ram_available_mb": 41764.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.062}, "power_stats": {"power_gpu_soc_mean_watts": 21.329, "power_cpu_cv_mean_watts": 1.557, "power_sys_5v0_mean_watts": 8.185, "gpu_utilization_percent_mean": 74.062, "power_watts_avg": 21.329, "energy_joules_est": 161.08, "duration_seconds": 7.552, "sample_count": 64}, "timestamp": "2026-01-26T15:40:42.637752"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11579.903, "latencies_ms": [11579.903], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "On the left side of the image, there is a desktop computer with a monitor displaying a green leafy wallpaper, positioned near the foreground. In the background, slightly to the right, there is a laptop with the same leafy wallpaper on its screen, placed on a desk. In the foreground, there is a black mouse and a speaker, both of which are position", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21076.4, "ram_available_mb": 41764.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21077.4, "ram_available_mb": 41763.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.101}, "power_stats": {"power_gpu_soc_mean_watts": 19.283, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.297, "gpu_utilization_percent_mean": 70.101, "power_watts_avg": 19.283, "energy_joules_est": 223.31, "duration_seconds": 11.581, "sample_count": 99}, "timestamp": "2026-01-26T15:40:56.249887"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8568.977, "latencies_ms": [8568.977], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a workspace with a laptop open on a white desk, displaying a green leafy wallpaper. There is a computer mouse and a speaker placed on the desk, and a keyboard is visible to the left of the laptop.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21077.4, "ram_available_mb": 41763.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21078.1, "ram_available_mb": 41762.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.274}, "power_stats": {"power_gpu_soc_mean_watts": 20.72, "power_cpu_cv_mean_watts": 1.667, "power_sys_5v0_mean_watts": 8.225, "gpu_utilization_percent_mean": 72.274, "power_watts_avg": 20.72, "energy_joules_est": 177.56, "duration_seconds": 8.57, "sample_count": 73}, "timestamp": "2026-01-26T15:41:06.871282"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8907.832, "latencies_ms": [8907.832], "images_per_second": 0.112, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a well-lit room with a laptop displaying a vibrant green leaf pattern on its screen. The laptop is placed on a white desk, accompanied by a black mouse and a speaker, with a window covered by white curtains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21078.1, "ram_available_mb": 41762.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21083.3, "ram_available_mb": 41757.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.171}, "power_stats": {"power_gpu_soc_mean_watts": 20.215, "power_cpu_cv_mean_watts": 1.753, "power_sys_5v0_mean_watts": 8.273, "gpu_utilization_percent_mean": 71.171, "power_watts_avg": 20.215, "energy_joules_est": 180.08, "duration_seconds": 8.908, "sample_count": 76}, "timestamp": "2026-01-26T15:41:17.814348"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11626.03, "latencies_ms": [11626.03], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young girl is sitting on a bed in a bedroom. She is wearing a green shirt and is surrounded by a variety of items. There are several books scattered around the room, with some on the bed and others on the floor. A toy truck can be seen on the bed, and a toy car is placed on the floor. \n\n", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 21020.9, "ram_available_mb": 41820.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21061.4, "ram_available_mb": 41779.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.424}, "power_stats": {"power_gpu_soc_mean_watts": 19.226, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.281, "gpu_utilization_percent_mean": 70.424, "power_watts_avg": 19.226, "energy_joules_est": 223.53, "duration_seconds": 11.627, "sample_count": 99}, "timestamp": "2026-01-26T15:41:31.492334"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9501.342, "latencies_ms": [9501.342], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Bed: 1\n- Pillow: 2\n- Nightstand: 1\n- Lamp: 1\n- Magazines: 1\n- Toy trucks: 1\n- Toy cars: 1\n- Toy blocks: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21061.7, "ram_available_mb": 41779.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21061.7, "ram_available_mb": 41779.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.074}, "power_stats": {"power_gpu_soc_mean_watts": 20.198, "power_cpu_cv_mean_watts": 1.714, "power_sys_5v0_mean_watts": 8.231, "gpu_utilization_percent_mean": 71.074, "power_watts_avg": 20.198, "energy_joules_est": 191.92, "duration_seconds": 9.502, "sample_count": 81}, "timestamp": "2026-01-26T15:41:43.030520"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11132.827, "latencies_ms": [11132.827], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 75, "n_tiles": 16, "output_text": "In the foreground, there is a bed with a brown blanket and a girl sitting on it. Behind the bed, there is a nightstand with a lamp and some books. To the left of the bed, there is a clothes rack with a white shirt hanging on it. The room has orange walls and the floor is made of stone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21061.7, "ram_available_mb": 41779.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21098.9, "ram_available_mb": 41742.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.421}, "power_stats": {"power_gpu_soc_mean_watts": 19.252, "power_cpu_cv_mean_watts": 1.866, "power_sys_5v0_mean_watts": 8.292, "gpu_utilization_percent_mean": 70.421, "power_watts_avg": 19.252, "energy_joules_est": 214.35, "duration_seconds": 11.134, "sample_count": 95}, "timestamp": "2026-01-26T15:41:56.188291"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7424.599, "latencies_ms": [7424.599], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A young girl is sitting on a bed in a bedroom with orange walls. She is surrounded by toys and books, and there is a wooden nightstand with a lamp next to the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21020.6, "ram_available_mb": 41820.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21075.9, "ram_available_mb": 41765.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.222}, "power_stats": {"power_gpu_soc_mean_watts": 21.476, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 8.217, "gpu_utilization_percent_mean": 74.222, "power_watts_avg": 21.476, "energy_joules_est": 159.47, "duration_seconds": 7.425, "sample_count": 63}, "timestamp": "2026-01-26T15:42:05.646147"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5628.086, "latencies_ms": [5628.086], "images_per_second": 0.178, "prompt_tokens": 36, "response_tokens_est": 27, "n_tiles": 16, "output_text": "The room has orange walls and a wooden bed frame. There is a lamp on a small wooden nightstand next to the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21075.9, "ram_available_mb": 41765.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21098.9, "ram_available_mb": 41742.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.708}, "power_stats": {"power_gpu_soc_mean_watts": 22.416, "power_cpu_cv_mean_watts": 1.359, "power_sys_5v0_mean_watts": 8.226, "gpu_utilization_percent_mean": 76.708, "power_watts_avg": 22.416, "energy_joules_est": 126.17, "duration_seconds": 5.629, "sample_count": 48}, "timestamp": "2026-01-26T15:42:13.316027"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11586.104, "latencies_ms": [11586.104], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a baseball game in progress, with a batter standing at home plate, holding a baseball bat and preparing to swing. The catcher is crouched behind the batter, wearing a baseball glove, and the umpire is standing nearby, observing the game. \n\nThere are several other people in the scene, including a player standing close to the batter", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 21098.9, "ram_available_mb": 41742.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 21101.6, "ram_available_mb": 41739.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.303}, "power_stats": {"power_gpu_soc_mean_watts": 19.311, "power_cpu_cv_mean_watts": 2.001, "power_sys_5v0_mean_watts": 8.309, "gpu_utilization_percent_mean": 71.303, "power_watts_avg": 19.311, "energy_joules_est": 223.75, "duration_seconds": 11.587, "sample_count": 99}, "timestamp": "2026-01-26T15:42:26.945367"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10390.664, "latencies_ms": [10390.664], "images_per_second": 0.096, "prompt_tokens": 39, "response_tokens_est": 67, "n_tiles": 16, "output_text": "1. Player: 1\n2. Baseball bat: 1\n3. Pitcher: 1\n4. Catcher: 1\n5. Umpire: 1\n6. Home plate: 1\n7. Mound: 1\n8. Grass: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21101.6, "ram_available_mb": 41739.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21067.9, "ram_available_mb": 41773.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.614}, "power_stats": {"power_gpu_soc_mean_watts": 19.92, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.257, "gpu_utilization_percent_mean": 71.614, "power_watts_avg": 19.92, "energy_joules_est": 206.99, "duration_seconds": 10.391, "sample_count": 88}, "timestamp": "2026-01-26T15:42:39.379879"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10239.731, "latencies_ms": [10239.731], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "In the foreground, a baseball player is standing with a bat, ready to swing. Behind the player, there is a catcher crouched down with a baseball glove, and further back, an umpire is standing with a mask on. The background shows a well-maintained grassy field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21067.9, "ram_available_mb": 41773.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21068.8, "ram_available_mb": 41772.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.241}, "power_stats": {"power_gpu_soc_mean_watts": 19.68, "power_cpu_cv_mean_watts": 1.812, "power_sys_5v0_mean_watts": 8.283, "gpu_utilization_percent_mean": 70.241, "power_watts_avg": 19.68, "energy_joules_est": 201.53, "duration_seconds": 10.24, "sample_count": 87}, "timestamp": "2026-01-26T15:42:51.643233"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9625.2, "latencies_ms": [9625.2], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image captures a moment from a baseball game, with a batter in mid-swing, a catcher crouched behind him, and an umpire observing the play. The scene is set on a well-maintained baseball field with a lush green outfield.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21068.8, "ram_available_mb": 41772.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21094.6, "ram_available_mb": 41746.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.78}, "power_stats": {"power_gpu_soc_mean_watts": 20.191, "power_cpu_cv_mean_watts": 1.747, "power_sys_5v0_mean_watts": 8.231, "gpu_utilization_percent_mean": 71.78, "power_watts_avg": 20.191, "energy_joules_est": 194.35, "duration_seconds": 9.626, "sample_count": 82}, "timestamp": "2026-01-26T15:43:03.315225"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9378.248, "latencies_ms": [9378.248], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image shows a baseball game in progress with a player in a red jersey swinging a bat on a field with visible dirt patches and well-maintained grass. The lighting suggests it is a sunny day, casting shadows of the players on the ground.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21094.6, "ram_available_mb": 41746.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21094.8, "ram_available_mb": 41746.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.075}, "power_stats": {"power_gpu_soc_mean_watts": 19.994, "power_cpu_cv_mean_watts": 1.776, "power_sys_5v0_mean_watts": 8.28, "gpu_utilization_percent_mean": 71.075, "power_watts_avg": 19.994, "energy_joules_est": 187.52, "duration_seconds": 9.379, "sample_count": 80}, "timestamp": "2026-01-26T15:43:14.734595"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12352.08, "latencies_ms": [12352.08], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a white cat is standing on a concrete surface, holding a small bird in its mouth. The bird appears to be a pigeon, and the cat seems to be in the process of eating it. The scene is set outdoors, with the cat and the bird being the main focus of the image.\n\nThere are several leaves scattered around the area, adding", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 21031.1, "ram_available_mb": 41809.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21091.7, "ram_available_mb": 41749.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.324}, "power_stats": {"power_gpu_soc_mean_watts": 21.334, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.488, "gpu_utilization_percent_mean": 71.324, "power_watts_avg": 21.334, "energy_joules_est": 263.53, "duration_seconds": 12.353, "sample_count": 105}, "timestamp": "2026-01-26T15:43:29.141283"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8612.198, "latencies_ms": [8612.198], "images_per_second": 0.116, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "cat: 1, bird: 1, leaves: numerous, ground: 1, sunlight: 1, shadows: 1, feathers: numerous, bird's head: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21028.4, "ram_available_mb": 41812.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21028.1, "ram_available_mb": 41812.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.904}, "power_stats": {"power_gpu_soc_mean_watts": 22.888, "power_cpu_cv_mean_watts": 1.458, "power_sys_5v0_mean_watts": 8.376, "gpu_utilization_percent_mean": 75.904, "power_watts_avg": 22.888, "energy_joules_est": 197.13, "duration_seconds": 8.613, "sample_count": 73}, "timestamp": "2026-01-26T15:43:39.797584"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11794.405, "latencies_ms": [11794.405], "images_per_second": 0.085, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "In the foreground, there is a bird with its head near the ground, close to the camera's viewpoint. Behind the bird, a cat's head is visible, positioned higher up and further away from the camera. The bird is in the lower part of the image, while the cat is in the upper part, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21028.1, "ram_available_mb": 41812.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21037.4, "ram_available_mb": 41803.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.51}, "power_stats": {"power_gpu_soc_mean_watts": 21.47, "power_cpu_cv_mean_watts": 1.809, "power_sys_5v0_mean_watts": 8.51, "gpu_utilization_percent_mean": 71.51, "power_watts_avg": 21.47, "energy_joules_est": 253.24, "duration_seconds": 11.795, "sample_count": 102}, "timestamp": "2026-01-26T15:43:53.620747"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7665.72, "latencies_ms": [7665.72], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 36, "n_tiles": 16, "output_text": "A cat is seen eating a bird on the ground, surrounded by leaves and twigs. The scene takes place outdoors, possibly in a garden or a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21037.4, "ram_available_mb": 41803.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21111.5, "ram_available_mb": 41729.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.646}, "power_stats": {"power_gpu_soc_mean_watts": 23.474, "power_cpu_cv_mean_watts": 1.354, "power_sys_5v0_mean_watts": 8.354, "gpu_utilization_percent_mean": 78.646, "power_watts_avg": 23.474, "energy_joules_est": 179.96, "duration_seconds": 7.666, "sample_count": 65}, "timestamp": "2026-01-26T15:44:03.330436"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9035.033, "latencies_ms": [9035.033], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image features a cat with a white and brown coat, standing on a concrete surface with a shadow cast to the right side of the frame. The lighting is bright and appears to be natural sunlight, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21024.3, "ram_available_mb": 41816.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21088.2, "ram_available_mb": 41752.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.382}, "power_stats": {"power_gpu_soc_mean_watts": 22.395, "power_cpu_cv_mean_watts": 1.627, "power_sys_5v0_mean_watts": 8.509, "gpu_utilization_percent_mean": 75.382, "power_watts_avg": 22.395, "energy_joules_est": 202.35, "duration_seconds": 9.036, "sample_count": 76}, "timestamp": "2026-01-26T15:44:14.382614"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12387.273, "latencies_ms": [12387.273], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a hand is holding a sandwich with a bite taken out of it. The sandwich is made with a white bread roll, filled with a slice of tomato, a slice of cucumber, and a slice of spinach. The hand is holding the sandwich from the top, and the bite taken out of it is visible. The background is blur", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21088.2, "ram_available_mb": 41752.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21026.7, "ram_available_mb": 41814.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.143}, "power_stats": {"power_gpu_soc_mean_watts": 21.52, "power_cpu_cv_mean_watts": 1.81, "power_sys_5v0_mean_watts": 8.577, "gpu_utilization_percent_mean": 73.143, "power_watts_avg": 21.52, "energy_joules_est": 266.59, "duration_seconds": 12.388, "sample_count": 105}, "timestamp": "2026-01-26T15:44:28.799124"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9003.303, "latencies_ms": [9003.303], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "hand: 1, sandwich: 1, tomato: 1, lettuce: 1, bread: 1, knife: 1, cutting board: 1, kitchen: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21026.7, "ram_available_mb": 41814.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21061.6, "ram_available_mb": 41779.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.816}, "power_stats": {"power_gpu_soc_mean_watts": 23.079, "power_cpu_cv_mean_watts": 1.443, "power_sys_5v0_mean_watts": 8.375, "gpu_utilization_percent_mean": 75.816, "power_watts_avg": 23.079, "energy_joules_est": 207.8, "duration_seconds": 9.004, "sample_count": 76}, "timestamp": "2026-01-26T15:44:39.825153"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12702.984, "latencies_ms": [12702.984], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a hand holding a sandwich with a bite taken out of it, indicating it is being eaten. The sandwich is composed of a white bread roll, a slice of red tomato, and a green leafy vegetable, possibly spinach. In the background, there is a blurred view of what appears to be a kitchen counter with a", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21061.6, "ram_available_mb": 41779.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21091.8, "ram_available_mb": 41749.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.78}, "power_stats": {"power_gpu_soc_mean_watts": 21.567, "power_cpu_cv_mean_watts": 1.769, "power_sys_5v0_mean_watts": 8.512, "gpu_utilization_percent_mean": 71.78, "power_watts_avg": 21.567, "energy_joules_est": 273.98, "duration_seconds": 12.704, "sample_count": 109}, "timestamp": "2026-01-26T15:44:54.543784"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8370.822, "latencies_ms": [8370.822], "images_per_second": 0.119, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "A person is holding a sandwich with a bite taken out of it, and the sandwich is on a white plate. The background appears to be a kitchen counter with a stove.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21091.8, "ram_available_mb": 41749.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 21031.0, "ram_available_mb": 41809.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.236}, "power_stats": {"power_gpu_soc_mean_watts": 23.415, "power_cpu_cv_mean_watts": 1.395, "power_sys_5v0_mean_watts": 8.386, "gpu_utilization_percent_mean": 78.236, "power_watts_avg": 23.415, "energy_joules_est": 196.02, "duration_seconds": 8.371, "sample_count": 72}, "timestamp": "2026-01-26T15:45:04.953237"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9968.915, "latencies_ms": [9968.915], "images_per_second": 0.1, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a hand holding a sandwich with a slice of tomato and a leafy green vegetable, possibly spinach, on a white plate. The lighting in the image is dim, and the focus is shallow, with the background out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21031.0, "ram_available_mb": 41809.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21071.0, "ram_available_mb": 41769.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.655}, "power_stats": {"power_gpu_soc_mean_watts": 22.496, "power_cpu_cv_mean_watts": 1.615, "power_sys_5v0_mean_watts": 8.51, "gpu_utilization_percent_mean": 75.655, "power_watts_avg": 22.496, "energy_joules_est": 224.28, "duration_seconds": 9.97, "sample_count": 84}, "timestamp": "2026-01-26T15:45:16.936651"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11647.921, "latencies_ms": [11647.921], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, two young girls are sitting on the back of a boat, enjoying their time on the water. They are both wearing hats, with one girl wearing a white hat and the other wearing a gray hat. The girls are sitting close to each other, with one girl on the left side and the other on the right side of the boat.\n\nThe boat", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 21071.0, "ram_available_mb": 41769.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21071.0, "ram_available_mb": 41769.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.778}, "power_stats": {"power_gpu_soc_mean_watts": 19.185, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.273, "gpu_utilization_percent_mean": 69.778, "power_watts_avg": 19.185, "energy_joules_est": 223.48, "duration_seconds": 11.649, "sample_count": 99}, "timestamp": "2026-01-26T15:45:30.631016"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8569.29, "latencies_ms": [8569.29], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "boat: 1\nrope: 10\nlife preserver: 1\nperson: 2\nhat: 1\npants: 2\ntie-dye: 1\nseat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21071.0, "ram_available_mb": 41769.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21071.7, "ram_available_mb": 41769.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.808}, "power_stats": {"power_gpu_soc_mean_watts": 20.688, "power_cpu_cv_mean_watts": 1.655, "power_sys_5v0_mean_watts": 8.225, "gpu_utilization_percent_mean": 72.808, "power_watts_avg": 20.688, "energy_joules_est": 177.29, "duration_seconds": 8.57, "sample_count": 73}, "timestamp": "2026-01-26T15:45:41.225493"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11570.451, "latencies_ms": [11570.451], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "Two individuals are seated on the bow of a boat, with one person in the foreground wearing a pink top and orange pants, and the other in the background wearing a white top and blue jeans. They are positioned near the edge of the boat, which is in the foreground, while the calm blue water and the boat's rigging form the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21071.7, "ram_available_mb": 41769.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21072.7, "ram_available_mb": 41768.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.263}, "power_stats": {"power_gpu_soc_mean_watts": 19.287, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.327, "gpu_utilization_percent_mean": 70.263, "power_watts_avg": 19.287, "energy_joules_est": 223.18, "duration_seconds": 11.571, "sample_count": 99}, "timestamp": "2026-01-26T15:45:54.819472"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7545.433, "latencies_ms": [7545.433], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "Two young girls are sitting on the bow of a sailboat, enjoying the view of the ocean. They are both wearing casual summer clothes and one of them is wearing a hat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21072.7, "ram_available_mb": 41768.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21090.4, "ram_available_mb": 41750.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.556}, "power_stats": {"power_gpu_soc_mean_watts": 21.449, "power_cpu_cv_mean_watts": 1.543, "power_sys_5v0_mean_watts": 8.228, "gpu_utilization_percent_mean": 74.556, "power_watts_avg": 21.449, "energy_joules_est": 161.85, "duration_seconds": 7.546, "sample_count": 63}, "timestamp": "2026-01-26T15:46:04.404389"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9152.4, "latencies_ms": [9152.4], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "Two individuals are seated on a boat, with one wearing a light-colored hat and the other in a pink top and orange pants. The boat is equipped with a white railing and an orange lifebuoy, indicating safety measures for water activities.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21090.4, "ram_available_mb": 41750.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21082.5, "ram_available_mb": 41758.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.385}, "power_stats": {"power_gpu_soc_mean_watts": 20.079, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 8.281, "gpu_utilization_percent_mean": 71.385, "power_watts_avg": 20.079, "energy_joules_est": 183.78, "duration_seconds": 9.153, "sample_count": 78}, "timestamp": "2026-01-26T15:46:15.580343"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11568.627, "latencies_ms": [11568.627], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a **white sheep** standing in a field. The sheep is facing the camera and appears to be looking directly at it. The field is covered in **green grass** and there are **yellow flowers** scattered throughout. In the background, there is a **stone wall**. The sheep is positioned in the center of the image, with the stone wall behind it", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 21082.5, "ram_available_mb": 41758.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.0, "ram_used_mb": 21084.8, "ram_available_mb": 41756.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.776}, "power_stats": {"power_gpu_soc_mean_watts": 19.32, "power_cpu_cv_mean_watts": 1.968, "power_sys_5v0_mean_watts": 8.321, "gpu_utilization_percent_mean": 69.776, "power_watts_avg": 19.32, "energy_joules_est": 223.52, "duration_seconds": 11.569, "sample_count": 98}, "timestamp": "2026-01-26T15:46:29.200124"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7092.212, "latencies_ms": [7092.212], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 38, "n_tiles": 16, "output_text": "sheep: 1, grass: many, stones: many, flowers: yellow, green plants: many, sunlight: bright, shadows: 1, sky: not visible", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21084.8, "ram_available_mb": 41756.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21032.2, "ram_available_mb": 41808.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.05}, "power_stats": {"power_gpu_soc_mean_watts": 21.542, "power_cpu_cv_mean_watts": 1.501, "power_sys_5v0_mean_watts": 8.192, "gpu_utilization_percent_mean": 75.05, "power_watts_avg": 21.542, "energy_joules_est": 152.79, "duration_seconds": 7.093, "sample_count": 60}, "timestamp": "2026-01-26T15:46:38.332995"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11590.284, "latencies_ms": [11590.284], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The sheep is standing in the foreground of the image, facing the camera with its body positioned slightly to the left of the frame. In the background, there is a stone wall with patches of yellow lichen growing on it, indicating that the sheep is in a grassy area near a man-made structure. The sheep appears to be at a moderate distance from the wall, as", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21032.2, "ram_available_mb": 41808.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21085.8, "ram_available_mb": 41755.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.222}, "power_stats": {"power_gpu_soc_mean_watts": 19.245, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.281, "gpu_utilization_percent_mean": 70.222, "power_watts_avg": 19.245, "energy_joules_est": 223.07, "duration_seconds": 11.591, "sample_count": 99}, "timestamp": "2026-01-26T15:46:51.989309"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6064.758, "latencies_ms": [6064.758], "images_per_second": 0.165, "prompt_tokens": 37, "response_tokens_est": 29, "n_tiles": 16, "output_text": "A sheep is standing in a grassy field with a stone wall in the background. The sheep appears to be looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21022.6, "ram_available_mb": 41818.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 21086.1, "ram_available_mb": 41754.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.471}, "power_stats": {"power_gpu_soc_mean_watts": 22.816, "power_cpu_cv_mean_watts": 1.342, "power_sys_5v0_mean_watts": 8.137, "gpu_utilization_percent_mean": 76.471, "power_watts_avg": 22.816, "energy_joules_est": 138.39, "duration_seconds": 6.065, "sample_count": 51}, "timestamp": "2026-01-26T15:47:00.102425"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7131.927, "latencies_ms": [7131.927], "images_per_second": 0.14, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The sheep is predominantly white with some black markings on its face and legs. It is standing on a grassy field with a stone wall covered in yellow lichen in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21032.8, "ram_available_mb": 41808.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21075.2, "ram_available_mb": 41765.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.574}, "power_stats": {"power_gpu_soc_mean_watts": 20.95, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 8.234, "gpu_utilization_percent_mean": 73.574, "power_watts_avg": 20.95, "energy_joules_est": 149.43, "duration_seconds": 7.133, "sample_count": 61}, "timestamp": "2026-01-26T15:47:09.271927"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11581.676, "latencies_ms": [11581.676], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is standing next to a large truck, which is carrying a large black pipe. The man appears to be inspecting the pipe, possibly checking its condition or preparing it for transport. The truck is parked in a parking lot, and there are other vehicles in the vicinity, including a car and a truck in the background.\n\nThere", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 21075.2, "ram_available_mb": 41765.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21076.1, "ram_available_mb": 41764.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.97}, "power_stats": {"power_gpu_soc_mean_watts": 19.314, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.281, "gpu_utilization_percent_mean": 69.97, "power_watts_avg": 19.314, "energy_joules_est": 223.7, "duration_seconds": 11.582, "sample_count": 99}, "timestamp": "2026-01-26T15:47:22.891872"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8576.622, "latencies_ms": [8576.622], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "- Truck: 1\n- Trailer: 1\n- Man: 2\n- Hose: 1\n- Container: 2\n- Tree: 1\n- Car: 1\n- Watch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21076.1, "ram_available_mb": 41764.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21077.1, "ram_available_mb": 41763.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.726}, "power_stats": {"power_gpu_soc_mean_watts": 20.607, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.245, "gpu_utilization_percent_mean": 72.726, "power_watts_avg": 20.607, "energy_joules_est": 176.75, "duration_seconds": 8.577, "sample_count": 73}, "timestamp": "2026-01-26T15:47:33.489729"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10186.91, "latencies_ms": [10186.91], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "In the foreground, there is a large truck with a flatbed trailer loaded with multiple black and yellow pipes. A man is standing on the trailer, pointing towards the pipes. In the background, there are trees and a building, indicating that the truck is parked in an outdoor area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21077.1, "ram_available_mb": 41763.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21048.4, "ram_available_mb": 41792.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.698}, "power_stats": {"power_gpu_soc_mean_watts": 19.693, "power_cpu_cv_mean_watts": 1.829, "power_sys_5v0_mean_watts": 8.297, "gpu_utilization_percent_mean": 70.698, "power_watts_avg": 19.693, "energy_joules_est": 200.62, "duration_seconds": 10.187, "sample_count": 86}, "timestamp": "2026-01-26T15:47:45.701092"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8666.279, "latencies_ms": [8666.279], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "In the image, a man is standing next to a large truck loaded with multiple black pipes. The truck is parked in a lot with trees in the background, and the man appears to be inspecting or discussing the pipes.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21048.4, "ram_available_mb": 41792.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21065.6, "ram_available_mb": 41775.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.12}, "power_stats": {"power_gpu_soc_mean_watts": 20.682, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.25, "gpu_utilization_percent_mean": 71.12, "power_watts_avg": 20.682, "energy_joules_est": 179.25, "duration_seconds": 8.667, "sample_count": 75}, "timestamp": "2026-01-26T15:47:56.412596"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6504.429, "latencies_ms": [6504.429], "images_per_second": 0.154, "prompt_tokens": 36, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A large black and blue pipe is being transported on a yellow trailer. The sky is overcast, and the ground appears to be wet, suggesting recent rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21065.6, "ram_available_mb": 41775.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21066.9, "ram_available_mb": 41774.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.375}, "power_stats": {"power_gpu_soc_mean_watts": 21.822, "power_cpu_cv_mean_watts": 1.508, "power_sys_5v0_mean_watts": 8.21, "gpu_utilization_percent_mean": 74.375, "power_watts_avg": 21.822, "energy_joules_est": 141.95, "duration_seconds": 6.505, "sample_count": 56}, "timestamp": "2026-01-26T15:48:04.930153"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11592.163, "latencies_ms": [11592.163], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a group of three giraffes walking together in a dirt field near a pond. They are all walking in the same direction, with one giraffe slightly ahead of the other two. The giraffes are positioned close to each other, creating a sense of unity and togetherness.\n\nIn addition to the giraff", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 21066.9, "ram_available_mb": 41774.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21067.1, "ram_available_mb": 41773.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.15}, "power_stats": {"power_gpu_soc_mean_watts": 19.162, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.315, "gpu_utilization_percent_mean": 70.15, "power_watts_avg": 19.162, "energy_joules_est": 222.14, "duration_seconds": 11.593, "sample_count": 100}, "timestamp": "2026-01-26T15:48:18.588040"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10267.343, "latencies_ms": [10267.343], "images_per_second": 0.097, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "1. Giraffe: 3\n2. Deer: 1\n3. Watering hole: 1\n4. Fence: 1\n5. Bushes: 1\n6. Trees: 1\n7. Rock: 1\n8. Ground: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21067.1, "ram_available_mb": 41773.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21067.8, "ram_available_mb": 41773.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.011}, "power_stats": {"power_gpu_soc_mean_watts": 19.902, "power_cpu_cv_mean_watts": 1.789, "power_sys_5v0_mean_watts": 8.272, "gpu_utilization_percent_mean": 72.011, "power_watts_avg": 19.902, "energy_joules_est": 204.35, "duration_seconds": 10.268, "sample_count": 89}, "timestamp": "2026-01-26T15:48:30.876011"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11572.126, "latencies_ms": [11572.126], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a small deer sitting near the edge of a pond, while three giraffes are walking in the background, with one closer to the left side, one in the middle, and one on the right side of the image. The giraffes are positioned further away from the viewer compared to the deer, creating a sense of depth", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21067.8, "ram_available_mb": 41773.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21068.3, "ram_available_mb": 41772.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.424}, "power_stats": {"power_gpu_soc_mean_watts": 19.239, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.303, "gpu_utilization_percent_mean": 69.424, "power_watts_avg": 19.239, "energy_joules_est": 222.65, "duration_seconds": 11.573, "sample_count": 99}, "timestamp": "2026-01-26T15:48:44.485092"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8217.333, "latencies_ms": [8217.333], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "Three giraffes are walking in a line across a dirt path, with one giraffe walking towards a pond with water and lily pads. A deer is sitting on the ground near the pond.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21068.3, "ram_available_mb": 41772.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21087.3, "ram_available_mb": 41753.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.057}, "power_stats": {"power_gpu_soc_mean_watts": 20.924, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.236, "gpu_utilization_percent_mean": 73.057, "power_watts_avg": 20.924, "energy_joules_est": 171.95, "duration_seconds": 8.218, "sample_count": 70}, "timestamp": "2026-01-26T15:48:54.765028"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9708.291, "latencies_ms": [9708.291], "images_per_second": 0.103, "prompt_tokens": 36, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image shows three giraffes with distinct brown and white spotted patterns walking in a line across a dirt path. The weather appears to be overcast, with no direct sunlight visible, and the environment is a mix of greenery and trees, suggesting a natural, possibly wild, habitat.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21025.4, "ram_available_mb": 41815.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21077.2, "ram_available_mb": 41763.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.354}, "power_stats": {"power_gpu_soc_mean_watts": 19.765, "power_cpu_cv_mean_watts": 1.796, "power_sys_5v0_mean_watts": 8.294, "gpu_utilization_percent_mean": 71.354, "power_watts_avg": 19.765, "energy_joules_est": 191.9, "duration_seconds": 9.709, "sample_count": 82}, "timestamp": "2026-01-26T15:49:06.529619"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11583.812, "latencies_ms": [11583.812], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large pizza sitting on a white plate, placed on a dining table. The pizza is topped with various ingredients, including mushrooms and ham. There are two wine glasses on the table, one closer to the left side and the other near the right side. A bowl can also be seen on the table, possibly containing a side dish", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21077.0, "ram_available_mb": 41763.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21077.5, "ram_available_mb": 41763.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.96}, "power_stats": {"power_gpu_soc_mean_watts": 19.323, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.28, "gpu_utilization_percent_mean": 69.96, "power_watts_avg": 19.323, "energy_joules_est": 223.85, "duration_seconds": 11.584, "sample_count": 99}, "timestamp": "2026-01-26T15:49:20.166537"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8766.984, "latencies_ms": [8766.984], "images_per_second": 0.114, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "table: 1\nchair: 1\nglass: 2\npizza: 1\nartichoke: 1\nmushrooms: 1\nham: 1\nbottle: 1\nperson: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21077.5, "ram_available_mb": 41763.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21078.0, "ram_available_mb": 41762.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.041}, "power_stats": {"power_gpu_soc_mean_watts": 20.626, "power_cpu_cv_mean_watts": 1.682, "power_sys_5v0_mean_watts": 8.257, "gpu_utilization_percent_mean": 73.041, "power_watts_avg": 20.626, "energy_joules_est": 180.84, "duration_seconds": 8.768, "sample_count": 74}, "timestamp": "2026-01-26T15:49:30.958426"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11569.567, "latencies_ms": [11569.567], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large pizza on a white plate, which is positioned centrally in the image. To the right of the pizza, there are two glasses of beer, one closer to the viewer and the other slightly further away. In the background, there is a dining area with tables and chairs, and a person is visible on the", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21078.0, "ram_available_mb": 41762.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21078.7, "ram_available_mb": 41762.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.378}, "power_stats": {"power_gpu_soc_mean_watts": 19.218, "power_cpu_cv_mean_watts": 1.903, "power_sys_5v0_mean_watts": 8.317, "gpu_utilization_percent_mean": 70.378, "power_watts_avg": 19.218, "energy_joules_est": 222.36, "duration_seconds": 11.57, "sample_count": 98}, "timestamp": "2026-01-26T15:49:44.577566"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9946.017, "latencies_ms": [9946.017], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image depicts a cozy restaurant setting with a large pizza placed on a white plate, accompanied by two glasses of beer. The table is set with a white tablecloth, and there are other tables and chairs visible in the background, suggesting a casual dining atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21078.7, "ram_available_mb": 41762.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21078.9, "ram_available_mb": 41762.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.435}, "power_stats": {"power_gpu_soc_mean_watts": 20.001, "power_cpu_cv_mean_watts": 1.761, "power_sys_5v0_mean_watts": 8.244, "gpu_utilization_percent_mean": 71.435, "power_watts_avg": 20.001, "energy_joules_est": 198.94, "duration_seconds": 9.947, "sample_count": 85}, "timestamp": "2026-01-26T15:49:56.570464"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9364.656, "latencies_ms": [9364.656], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The pizza is placed on a white plate, which contrasts with the golden-brown crust and the colorful toppings. The lighting in the restaurant is warm and ambient, highlighting the textures of the pizza and the glasses of beer on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21078.9, "ram_available_mb": 41762.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21078.9, "ram_available_mb": 41762.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.2}, "power_stats": {"power_gpu_soc_mean_watts": 20.009, "power_cpu_cv_mean_watts": 1.786, "power_sys_5v0_mean_watts": 8.296, "gpu_utilization_percent_mean": 71.2, "power_watts_avg": 20.009, "energy_joules_est": 187.39, "duration_seconds": 9.365, "sample_count": 80}, "timestamp": "2026-01-26T15:50:07.987078"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11570.584, "latencies_ms": [11570.584], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a black cat is standing on a white bathroom sink, drinking water from a silver faucet. The cat is positioned on the right side of the sink, with its head lowered to the water. The sink is located in a bathroom, and there is a bottle of hand soap placed on the left side of the sink. The scene captures a", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 21078.9, "ram_available_mb": 41762.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 21080.0, "ram_available_mb": 41760.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.677}, "power_stats": {"power_gpu_soc_mean_watts": 19.319, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.284, "gpu_utilization_percent_mean": 70.677, "power_watts_avg": 19.319, "energy_joules_est": 223.54, "duration_seconds": 11.571, "sample_count": 99}, "timestamp": "2026-01-26T15:50:21.585297"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7978.598, "latencies_ms": [7978.598], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "faucet: 1, cat: 1, sink: 1, water: 1, bottle: 1, dish: 1, toothpaste: 1, label: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21080.0, "ram_available_mb": 41760.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.0, "ram_used_mb": 21083.1, "ram_available_mb": 41757.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.353}, "power_stats": {"power_gpu_soc_mean_watts": 21.197, "power_cpu_cv_mean_watts": 1.848, "power_sys_5v0_mean_watts": 8.28, "gpu_utilization_percent_mean": 73.353, "power_watts_avg": 21.197, "energy_joules_est": 169.14, "duration_seconds": 7.979, "sample_count": 68}, "timestamp": "2026-01-26T15:50:31.606396"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11589.006, "latencies_ms": [11589.006], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a black cat is positioned near the edge of a white sink, with its head close to the faucet, suggesting it is drinking water. The faucet is located on the right side of the sink, and the water is flowing from it towards the cat. In the background, there is a bottle of liquid soap on the left side of the", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21083.1, "ram_available_mb": 41757.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21082.9, "ram_available_mb": 41758.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.48}, "power_stats": {"power_gpu_soc_mean_watts": 19.279, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.304, "gpu_utilization_percent_mean": 69.48, "power_watts_avg": 19.279, "energy_joules_est": 223.44, "duration_seconds": 11.59, "sample_count": 98}, "timestamp": "2026-01-26T15:50:45.212176"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7768.158, "latencies_ms": [7768.158], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A black cat is standing on a white bathroom sink, with its head dipped into the running water from a silver faucet. There is a bottle of liquid soap on the edge of the sink.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21082.9, "ram_available_mb": 41758.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21029.9, "ram_available_mb": 41811.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.848}, "power_stats": {"power_gpu_soc_mean_watts": 21.24, "power_cpu_cv_mean_watts": 1.577, "power_sys_5v0_mean_watts": 8.203, "gpu_utilization_percent_mean": 73.848, "power_watts_avg": 21.24, "energy_joules_est": 165.01, "duration_seconds": 7.769, "sample_count": 66}, "timestamp": "2026-01-26T15:50:55.007203"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7082.081, "latencies_ms": [7082.081], "images_per_second": 0.141, "prompt_tokens": 36, "response_tokens_est": 40, "n_tiles": 16, "output_text": "The image shows a black cat drinking water from a silver faucet in a white sink. The lighting in the room is bright, and the walls are painted a light blue color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21029.9, "ram_available_mb": 41811.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21026.4, "ram_available_mb": 41814.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.254}, "power_stats": {"power_gpu_soc_mean_watts": 21.44, "power_cpu_cv_mean_watts": 1.574, "power_sys_5v0_mean_watts": 8.274, "gpu_utilization_percent_mean": 73.254, "power_watts_avg": 21.44, "energy_joules_est": 151.86, "duration_seconds": 7.083, "sample_count": 59}, "timestamp": "2026-01-26T15:51:04.115653"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12420.849, "latencies_ms": [12420.849], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man and a woman are riding a horse-drawn carriage through a puddle of water. The horse is pulling the carriage, and the two people are sitting in it, enjoying their ride. The puddle of water is large enough to partially submerge the carriage, creating a reflection of the horse and the people on the water's surface", "error": null, "sys_before": {"cpu_percent": 9.7, "ram_used_mb": 21026.4, "ram_available_mb": 41814.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21070.3, "ram_available_mb": 41770.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.736}, "power_stats": {"power_gpu_soc_mean_watts": 21.48, "power_cpu_cv_mean_watts": 1.805, "power_sys_5v0_mean_watts": 8.547, "gpu_utilization_percent_mean": 72.736, "power_watts_avg": 21.48, "energy_joules_est": 266.81, "duration_seconds": 12.422, "sample_count": 106}, "timestamp": "2026-01-26T15:51:18.583122"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11434.859, "latencies_ms": [11434.859], "images_per_second": 0.087, "prompt_tokens": 39, "response_tokens_est": 66, "n_tiles": 16, "output_text": "1. Carriage: 1\n2. Horse: 1\n3. Puddle: 1\n4. Reflection: 1\n5. Man: 2\n6. Hat: 1\n7. Wagon wheel: 2\n8. Trees: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21070.3, "ram_available_mb": 41770.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21071.3, "ram_available_mb": 41769.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.309}, "power_stats": {"power_gpu_soc_mean_watts": 22.107, "power_cpu_cv_mean_watts": 1.659, "power_sys_5v0_mean_watts": 8.423, "gpu_utilization_percent_mean": 75.309, "power_watts_avg": 22.107, "energy_joules_est": 252.8, "duration_seconds": 11.435, "sample_count": 97}, "timestamp": "2026-01-26T15:51:32.073493"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10841.495, "latencies_ms": [10841.495], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 63, "n_tiles": 16, "output_text": "In the foreground, there is a horse pulling a wooden cart with two people seated inside. The horse is positioned on the right side of the image, while the cart is located in the center. The background features a plowed field and a barn, with a clear blue sky above.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21071.3, "ram_available_mb": 41769.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21096.8, "ram_available_mb": 41744.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.848}, "power_stats": {"power_gpu_soc_mean_watts": 22.177, "power_cpu_cv_mean_watts": 1.67, "power_sys_5v0_mean_watts": 8.494, "gpu_utilization_percent_mean": 74.848, "power_watts_avg": 22.177, "energy_joules_est": 240.45, "duration_seconds": 10.842, "sample_count": 92}, "timestamp": "2026-01-26T15:51:44.931403"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9569.156, "latencies_ms": [9569.156], "images_per_second": 0.105, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "Two people are riding in a horse-drawn carriage through a puddle on a sunny day. The horse is brown and is pulling the carriage, while the riders are wearing hats and casual clothing.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21096.8, "ram_available_mb": 41744.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21114.8, "ram_available_mb": 41726.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.333}, "power_stats": {"power_gpu_soc_mean_watts": 22.796, "power_cpu_cv_mean_watts": 1.512, "power_sys_5v0_mean_watts": 8.402, "gpu_utilization_percent_mean": 77.333, "power_watts_avg": 22.796, "energy_joules_est": 218.15, "duration_seconds": 9.57, "sample_count": 81}, "timestamp": "2026-01-26T15:51:56.531613"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9538.187, "latencies_ms": [9538.187], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image features a clear blue sky and a bright sun casting shadows on the ground, creating a reflection of the horse-drawn carriage on the wet surface. The carriage is made of wood and metal, with the horse being a rich brown color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21027.9, "ram_available_mb": 41813.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21092.9, "ram_available_mb": 41748.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.889}, "power_stats": {"power_gpu_soc_mean_watts": 22.619, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 8.48, "gpu_utilization_percent_mean": 75.889, "power_watts_avg": 22.619, "energy_joules_est": 215.76, "duration_seconds": 9.539, "sample_count": 81}, "timestamp": "2026-01-26T15:52:08.088683"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11586.539, "latencies_ms": [11586.539], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a bride and groom are standing together on a grassy lawn, holding an umbrella to protect themselves from the sun. The bride is wearing a white wedding dress and holding a bouquet of flowers, while the groom is dressed in a suit. They are both smiling and appear to be enjoying their special day.\n\nThere are", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 21029.5, "ram_available_mb": 41811.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21063.4, "ram_available_mb": 41777.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.031}, "power_stats": {"power_gpu_soc_mean_watts": 19.116, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.308, "gpu_utilization_percent_mean": 70.031, "power_watts_avg": 19.116, "energy_joules_est": 221.5, "duration_seconds": 11.587, "sample_count": 98}, "timestamp": "2026-01-26T15:52:21.742290"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8120.847, "latencies_ms": [8120.847], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "umbrella: 1, bride: 1, groom: 1, flower bouquet: 1, grass: 1, building: 1, stone wall: 1, people: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21063.4, "ram_available_mb": 41777.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21065.8, "ram_available_mb": 41775.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.826}, "power_stats": {"power_gpu_soc_mean_watts": 21.128, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.24, "gpu_utilization_percent_mean": 73.826, "power_watts_avg": 21.128, "energy_joules_est": 171.59, "duration_seconds": 8.122, "sample_count": 69}, "timestamp": "2026-01-26T15:52:31.891138"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9039.059, "latencies_ms": [9039.059], "images_per_second": 0.111, "prompt_tokens": 44, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The bride and groom are standing close together in the foreground, with the bride holding a bouquet of flowers to her left and the groom holding a sword to his right. In the background, there are other people and a building with a red roof.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21065.8, "ram_available_mb": 41775.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21066.1, "ram_available_mb": 41774.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.468}, "power_stats": {"power_gpu_soc_mean_watts": 20.254, "power_cpu_cv_mean_watts": 1.71, "power_sys_5v0_mean_watts": 8.272, "gpu_utilization_percent_mean": 72.468, "power_watts_avg": 20.254, "energy_joules_est": 183.09, "duration_seconds": 9.04, "sample_count": 77}, "timestamp": "2026-01-26T15:52:42.960106"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9160.399, "latencies_ms": [9160.399], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "A bride and groom are standing under a black and white umbrella in a garden, with a stone building in the background. They appear to be at their wedding, as the bride is wearing a white dress and holding a bouquet of flowers.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21066.1, "ram_available_mb": 41774.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21067.5, "ram_available_mb": 41773.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.346}, "power_stats": {"power_gpu_soc_mean_watts": 20.389, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 8.238, "gpu_utilization_percent_mean": 72.346, "power_watts_avg": 20.389, "energy_joules_est": 186.79, "duration_seconds": 9.161, "sample_count": 78}, "timestamp": "2026-01-26T15:52:54.163441"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7400.443, "latencies_ms": [7400.443], "images_per_second": 0.135, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image features a bride and groom standing under a black and white umbrella. The bride is wearing a white wedding dress and holding a bouquet of orange and yellow flowers.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21067.5, "ram_available_mb": 41773.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21067.5, "ram_available_mb": 41773.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.619}, "power_stats": {"power_gpu_soc_mean_watts": 21.222, "power_cpu_cv_mean_watts": 1.575, "power_sys_5v0_mean_watts": 8.246, "gpu_utilization_percent_mean": 73.619, "power_watts_avg": 21.222, "energy_joules_est": 157.07, "duration_seconds": 7.401, "sample_count": 63}, "timestamp": "2026-01-26T15:53:03.594137"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12355.6, "latencies_ms": [12355.6], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is sitting on a sandy beach, enjoying a relaxing day. He is wearing a white shirt and brown shorts, and his feet are propped up on the sand. The man is also wearing a watch on his left wrist. \n\nA colorful kite is flying in the background, adding a vibrant touch to", "error": null, "sys_before": {"cpu_percent": 13.8, "ram_used_mb": 21067.5, "ram_available_mb": 41773.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21026.2, "ram_available_mb": 41814.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.905}, "power_stats": {"power_gpu_soc_mean_watts": 21.342, "power_cpu_cv_mean_watts": 1.799, "power_sys_5v0_mean_watts": 8.468, "gpu_utilization_percent_mean": 72.905, "power_watts_avg": 21.342, "energy_joules_est": 263.71, "duration_seconds": 12.356, "sample_count": 105}, "timestamp": "2026-01-26T15:53:17.997089"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9662.144, "latencies_ms": [9662.144], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "sand: numerous\nkite: 1\nperson: 2\nwatch: 1\nkite string: 4\nkite: 1\nkite string: 4\nkite: 1\nkite string: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21026.2, "ram_available_mb": 41814.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21096.2, "ram_available_mb": 41744.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.366}, "power_stats": {"power_gpu_soc_mean_watts": 22.348, "power_cpu_cv_mean_watts": 1.566, "power_sys_5v0_mean_watts": 8.369, "gpu_utilization_percent_mean": 75.366, "power_watts_avg": 22.348, "energy_joules_est": 215.94, "duration_seconds": 9.663, "sample_count": 82}, "timestamp": "2026-01-26T15:53:29.674241"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11094.426, "latencies_ms": [11094.426], "images_per_second": 0.09, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there is a person lying on the sand with their legs up, near the center of the image. To the right, there is another person sitting on the sand, holding onto a kite string. The kite is in the background, flying high in the sky, and the ocean is visible in the far background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21033.4, "ram_available_mb": 41807.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21067.8, "ram_available_mb": 41773.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.926}, "power_stats": {"power_gpu_soc_mean_watts": 21.684, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 8.528, "gpu_utilization_percent_mean": 72.926, "power_watts_avg": 21.684, "energy_joules_est": 240.59, "duration_seconds": 11.095, "sample_count": 94}, "timestamp": "2026-01-26T15:53:42.816687"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8257.111, "latencies_ms": [8257.111], "images_per_second": 0.121, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A man is sitting on the beach, enjoying the sun and the waves. He is holding a kite that is flying in the air, with its string stretched out in front of him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21067.8, "ram_available_mb": 41773.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 21069.0, "ram_available_mb": 41771.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.1}, "power_stats": {"power_gpu_soc_mean_watts": 23.079, "power_cpu_cv_mean_watts": 1.429, "power_sys_5v0_mean_watts": 8.337, "gpu_utilization_percent_mean": 76.1, "power_watts_avg": 23.079, "energy_joules_est": 190.58, "duration_seconds": 8.258, "sample_count": 70}, "timestamp": "2026-01-26T15:53:53.102758"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10874.093, "latencies_ms": [10874.093], "images_per_second": 0.092, "prompt_tokens": 36, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The image shows a sunny day at the beach with clear skies and bright sunlight casting shadows on the sand. A person is lying on the beach, wearing a white shirt and brown shorts, with a colorful kite in the background that has red, blue, green, and yellow ribbons.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21069.0, "ram_available_mb": 41771.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21019.5, "ram_available_mb": 41821.4, "ram_percent": 33.4}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.946}, "power_stats": {"power_gpu_soc_mean_watts": 21.733, "power_cpu_cv_mean_watts": 1.748, "power_sys_5v0_mean_watts": 8.526, "gpu_utilization_percent_mean": 72.946, "power_watts_avg": 21.733, "energy_joules_est": 236.34, "duration_seconds": 10.875, "sample_count": 92}, "timestamp": "2026-01-26T15:54:05.997118"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11595.512, "latencies_ms": [11595.512], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a brown couch positioned on the left side, a red chair on the right side, and a small black table in the middle. On the table, there is a lamp providing light to the room. The room also features a television placed on the right side, and a dining table with chairs nearby. \n\nThere are", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 21019.5, "ram_available_mb": 41821.4, "ram_percent": 33.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21067.2, "ram_available_mb": 41773.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.364}, "power_stats": {"power_gpu_soc_mean_watts": 19.295, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.303, "gpu_utilization_percent_mean": 70.364, "power_watts_avg": 19.295, "energy_joules_est": 223.75, "duration_seconds": 11.596, "sample_count": 99}, "timestamp": "2026-01-26T15:54:19.658084"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7540.61, "latencies_ms": [7540.61], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "couch: 1, lamp: 2, table: 1, chair: 1, window: 2, blinds: 2, wall: 1, floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21067.2, "ram_available_mb": 41773.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 21070.1, "ram_available_mb": 41770.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.413}, "power_stats": {"power_gpu_soc_mean_watts": 21.429, "power_cpu_cv_mean_watts": 1.531, "power_sys_5v0_mean_watts": 8.21, "gpu_utilization_percent_mean": 74.413, "power_watts_avg": 21.429, "energy_joules_est": 161.6, "duration_seconds": 7.541, "sample_count": 63}, "timestamp": "2026-01-26T15:54:29.218074"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11556.636, "latencies_ms": [11556.636], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a brown leather couch positioned to the left, with a wooden side table and a red armchair to its right. The main objects are arranged in a semi-circle, creating a cozy seating area. In the background, there is a black television placed on a stand to the right, and a window with wooden shutters is visible through", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 21070.1, "ram_available_mb": 41770.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 21076.8, "ram_available_mb": 41764.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.586}, "power_stats": {"power_gpu_soc_mean_watts": 19.27, "power_cpu_cv_mean_watts": 2.244, "power_sys_5v0_mean_watts": 8.322, "gpu_utilization_percent_mean": 68.586, "power_watts_avg": 19.27, "energy_joules_est": 222.71, "duration_seconds": 11.557, "sample_count": 99}, "timestamp": "2026-01-26T15:54:42.832788"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9157.555, "latencies_ms": [9157.555], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image depicts a cozy living room with a brown leather couch, a black table with a lamp, and a red chair. There is a television on the right side of the room, and a window with wooden shutters is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21076.8, "ram_available_mb": 41764.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21074.8, "ram_available_mb": 41766.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.823}, "power_stats": {"power_gpu_soc_mean_watts": 20.207, "power_cpu_cv_mean_watts": 1.707, "power_sys_5v0_mean_watts": 8.213, "gpu_utilization_percent_mean": 71.823, "power_watts_avg": 20.207, "energy_joules_est": 185.06, "duration_seconds": 9.158, "sample_count": 79}, "timestamp": "2026-01-26T15:54:54.026617"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9485.293, "latencies_ms": [9485.293], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The room is lit by a warm, yellow-orange light from a floor lamp and a table lamp, creating a cozy atmosphere. The walls are painted in a light beige color, and the windows are covered with wooden shutters that are partially open, allowing natural light to filter in.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21074.8, "ram_available_mb": 41766.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21094.2, "ram_available_mb": 41746.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.817}, "power_stats": {"power_gpu_soc_mean_watts": 19.94, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 8.291, "gpu_utilization_percent_mean": 71.817, "power_watts_avg": 19.94, "energy_joules_est": 189.15, "duration_seconds": 9.486, "sample_count": 82}, "timestamp": "2026-01-26T15:55:05.540989"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12376.03, "latencies_ms": [12376.03], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is captured in the midst of a casual outdoor meal. He's dressed in a blue t-shirt, which contrasts with the lush greenery of the park around him. The man is holding a white plate in his right hand, which is filled with a slice of cake. His left hand is holding a fork, poised to", "error": null, "sys_before": {"cpu_percent": 11.5, "ram_used_mb": 21032.3, "ram_available_mb": 41808.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21072.0, "ram_available_mb": 41768.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.571}, "power_stats": {"power_gpu_soc_mean_watts": 21.538, "power_cpu_cv_mean_watts": 1.806, "power_sys_5v0_mean_watts": 8.538, "gpu_utilization_percent_mean": 72.571, "power_watts_avg": 21.538, "energy_joules_est": 266.57, "duration_seconds": 12.377, "sample_count": 105}, "timestamp": "2026-01-26T15:55:19.967080"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8880.753, "latencies_ms": [8880.753], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "person: 1, plate: 1, piece of cake: 1, fork: 1, bite: 1, tree: 1, leaf: 1, grass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21072.0, "ram_available_mb": 41768.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21073.2, "ram_available_mb": 41767.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.027}, "power_stats": {"power_gpu_soc_mean_watts": 23.184, "power_cpu_cv_mean_watts": 1.435, "power_sys_5v0_mean_watts": 8.387, "gpu_utilization_percent_mean": 78.027, "power_watts_avg": 23.184, "energy_joules_est": 205.91, "duration_seconds": 8.881, "sample_count": 75}, "timestamp": "2026-01-26T15:55:30.877976"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10482.548, "latencies_ms": [10482.548], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The person is standing in the foreground on the left side of the image, holding a plate with food on it. The background consists of trees and grass, indicating an outdoor setting. The person is positioned closer to the camera than the trees, which are further away in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21073.2, "ram_available_mb": 41767.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21073.0, "ram_available_mb": 41767.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.27}, "power_stats": {"power_gpu_soc_mean_watts": 22.259, "power_cpu_cv_mean_watts": 1.637, "power_sys_5v0_mean_watts": 8.499, "gpu_utilization_percent_mean": 75.27, "power_watts_avg": 22.259, "energy_joules_est": 233.35, "duration_seconds": 10.483, "sample_count": 89}, "timestamp": "2026-01-26T15:55:43.384559"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7113.453, "latencies_ms": [7113.453], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 29, "n_tiles": 16, "output_text": "A person is eating a piece of cake outdoors in a park-like setting with trees and grass visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21073.0, "ram_available_mb": 41767.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 21073.0, "ram_available_mb": 41767.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 79.933}, "power_stats": {"power_gpu_soc_mean_watts": 24.429, "power_cpu_cv_mean_watts": 1.194, "power_sys_5v0_mean_watts": 8.331, "gpu_utilization_percent_mean": 79.933, "power_watts_avg": 24.429, "energy_joules_est": 173.79, "duration_seconds": 7.114, "sample_count": 60}, "timestamp": "2026-01-26T15:55:52.539090"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9053.551, "latencies_ms": [9053.551], "images_per_second": 0.11, "prompt_tokens": 36, "response_tokens_est": 48, "n_tiles": 16, "output_text": "The image shows a person holding a plate with a piece of cake, set against a backdrop of lush green trees under a clear blue sky. The lighting is natural and bright, suggesting it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21073.0, "ram_available_mb": 41767.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21099.2, "ram_available_mb": 41741.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.221}, "power_stats": {"power_gpu_soc_mean_watts": 22.884, "power_cpu_cv_mean_watts": 1.538, "power_sys_5v0_mean_watts": 8.469, "gpu_utilization_percent_mean": 76.221, "power_watts_avg": 22.884, "energy_joules_est": 207.19, "duration_seconds": 9.054, "sample_count": 77}, "timestamp": "2026-01-26T15:56:03.621896"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11572.456, "latencies_ms": [11572.456], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is walking alongside a horse, which is carrying a large load of luggage. The horse is pulling the luggage, which includes a suitcase and a backpack, on its back. The man appears to be guiding the horse and ensuring it moves in the right direction.\n\nThe scene takes place in a wooded area, with trees surrounding", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 21037.4, "ram_available_mb": 41803.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21085.5, "ram_available_mb": 41755.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.131}, "power_stats": {"power_gpu_soc_mean_watts": 19.335, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.3, "gpu_utilization_percent_mean": 71.131, "power_watts_avg": 19.335, "energy_joules_est": 223.77, "duration_seconds": 11.573, "sample_count": 99}, "timestamp": "2026-01-26T15:56:17.222843"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10155.8, "latencies_ms": [10155.8], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. Man: 1\n2. Horse: 1\n3. Basket: 1\n4. Luggage: 2\n5. Rope: 1\n6. Trees: 4\n7. Ground: 1\n8. Stones: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21085.5, "ram_available_mb": 41755.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21086.5, "ram_available_mb": 41754.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.773}, "power_stats": {"power_gpu_soc_mean_watts": 19.896, "power_cpu_cv_mean_watts": 1.787, "power_sys_5v0_mean_watts": 8.259, "gpu_utilization_percent_mean": 71.773, "power_watts_avg": 19.896, "energy_joules_est": 202.07, "duration_seconds": 10.156, "sample_count": 88}, "timestamp": "2026-01-26T15:56:29.412249"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10679.125, "latencies_ms": [10679.125], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "A man is walking in the foreground, leading a horse that is carrying luggage in the background. The horse is positioned near the right side of the image, while the man is on the left side. The luggage is placed on top of the horse's back, indicating that the horse is being used for transportation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21086.5, "ram_available_mb": 41754.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21086.8, "ram_available_mb": 41754.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.154}, "power_stats": {"power_gpu_soc_mean_watts": 19.402, "power_cpu_cv_mean_watts": 1.842, "power_sys_5v0_mean_watts": 8.284, "gpu_utilization_percent_mean": 71.154, "power_watts_avg": 19.402, "energy_joules_est": 207.21, "duration_seconds": 10.68, "sample_count": 91}, "timestamp": "2026-01-26T15:56:42.119709"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7066.782, "latencies_ms": [7066.782], "images_per_second": 0.142, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A man is walking with a horse carrying luggage on its back, likely on a journey or trip. The horse is walking on a dirt path with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21086.8, "ram_available_mb": 41754.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21087.0, "ram_available_mb": 41753.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.483}, "power_stats": {"power_gpu_soc_mean_watts": 21.828, "power_cpu_cv_mean_watts": 1.501, "power_sys_5v0_mean_watts": 8.225, "gpu_utilization_percent_mean": 74.483, "power_watts_avg": 21.828, "energy_joules_est": 154.27, "duration_seconds": 7.067, "sample_count": 60}, "timestamp": "2026-01-26T15:56:51.225414"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7534.346, "latencies_ms": [7534.346], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image shows a donkey carrying a large red and grey suitcase on its back. The donkey is wearing a colorful blanket and is walking on a dirt path with trees in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21087.0, "ram_available_mb": 41753.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21087.7, "ram_available_mb": 41753.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.891}, "power_stats": {"power_gpu_soc_mean_watts": 21.027, "power_cpu_cv_mean_watts": 1.613, "power_sys_5v0_mean_watts": 8.256, "gpu_utilization_percent_mean": 72.891, "power_watts_avg": 21.027, "energy_joules_est": 158.44, "duration_seconds": 7.535, "sample_count": 64}, "timestamp": "2026-01-26T15:57:00.779210"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12077.685, "latencies_ms": [12077.685], "images_per_second": 0.083, "prompt_tokens": 24, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The image captures a serene night scene at a river, where a bridge is illuminated with blue lights. A boat is floating on the water, and several people are gathered on the riverbank, enjoying the view. The bridge and the boat are the main focal points of the image, with the blue lights creating a beautiful contrast against the dark night sky.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 21087.7, "ram_available_mb": 41753.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21088.0, "ram_available_mb": 41752.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.333}, "power_stats": {"power_gpu_soc_mean_watts": 21.378, "power_cpu_cv_mean_watts": 1.795, "power_sys_5v0_mean_watts": 8.479, "gpu_utilization_percent_mean": 72.333, "power_watts_avg": 21.378, "energy_joules_est": 258.21, "duration_seconds": 12.078, "sample_count": 105}, "timestamp": "2026-01-26T15:57:14.880803"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8109.149, "latencies_ms": [8109.149], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "people: 5, bridge: 1, boat: 1, water: 1, lights: 1, trees: 1, sky: 1, road: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21088.0, "ram_available_mb": 41752.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 21088.7, "ram_available_mb": 41752.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.043}, "power_stats": {"power_gpu_soc_mean_watts": 23.206, "power_cpu_cv_mean_watts": 1.415, "power_sys_5v0_mean_watts": 8.366, "gpu_utilization_percent_mean": 77.043, "power_watts_avg": 23.206, "energy_joules_est": 188.19, "duration_seconds": 8.11, "sample_count": 69}, "timestamp": "2026-01-26T15:57:25.026968"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12160.853, "latencies_ms": [12160.853], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a group of people standing on the left side of the image, near the water's edge. The bridge, which is the main object in the background, spans across the body of water and is lit up with blue lights. The boat is situated on the water, closer to the bridge than the people, and is also illuminated by the blue lights", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21088.7, "ram_available_mb": 41752.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21094.9, "ram_available_mb": 41746.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.124}, "power_stats": {"power_gpu_soc_mean_watts": 21.287, "power_cpu_cv_mean_watts": 1.833, "power_sys_5v0_mean_watts": 8.521, "gpu_utilization_percent_mean": 72.124, "power_watts_avg": 21.287, "energy_joules_est": 258.88, "duration_seconds": 12.161, "sample_count": 105}, "timestamp": "2026-01-26T15:57:39.250227"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9957.071, "latencies_ms": [9957.071], "images_per_second": 0.1, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image captures a serene night scene at a river, where a bridge is illuminated with blue lights, creating a beautiful reflection on the water. A group of people are gathered on the riverbank, admiring the view and the boat passing under the bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21031.3, "ram_available_mb": 41809.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21080.9, "ram_available_mb": 41760.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.365}, "power_stats": {"power_gpu_soc_mean_watts": 22.317, "power_cpu_cv_mean_watts": 1.596, "power_sys_5v0_mean_watts": 8.404, "gpu_utilization_percent_mean": 74.365, "power_watts_avg": 22.317, "energy_joules_est": 222.22, "duration_seconds": 9.958, "sample_count": 85}, "timestamp": "2026-01-26T15:57:51.227011"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8096.499, "latencies_ms": [8096.499], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The bridge is illuminated with blue lights, creating a striking contrast against the night sky. The reflection of the lights can be seen on the water's surface, adding to the visual appeal of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21080.9, "ram_available_mb": 41760.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21093.4, "ram_available_mb": 41747.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.493}, "power_stats": {"power_gpu_soc_mean_watts": 22.878, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 8.491, "gpu_utilization_percent_mean": 75.493, "power_watts_avg": 22.878, "energy_joules_est": 185.25, "duration_seconds": 8.097, "sample_count": 69}, "timestamp": "2026-01-26T15:58:01.358010"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11576.991, "latencies_ms": [11576.991], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person's foot is adorned with a vibrant pink shoe, which stands out against the backdrop of a blue bench. The shoe, with its pointed toe and a bow on the top, is a striking contrast to the blue denim jeans worn by the person. The bench, showing signs of age and weathering, is", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 21093.4, "ram_available_mb": 41747.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21069.3, "ram_available_mb": 41771.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.663}, "power_stats": {"power_gpu_soc_mean_watts": 19.259, "power_cpu_cv_mean_watts": 1.894, "power_sys_5v0_mean_watts": 8.288, "gpu_utilization_percent_mean": 70.663, "power_watts_avg": 19.259, "energy_joules_est": 222.97, "duration_seconds": 11.578, "sample_count": 101}, "timestamp": "2026-01-26T15:58:14.963105"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8149.523, "latencies_ms": [8149.523], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "shoe: 1, heel: 1, toe: 1, bow: 1, blue jeans: 1, bench: 1, paint: 1, peeling paint: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21069.3, "ram_available_mb": 41771.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21070.0, "ram_available_mb": 41770.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.871}, "power_stats": {"power_gpu_soc_mean_watts": 20.746, "power_cpu_cv_mean_watts": 1.624, "power_sys_5v0_mean_watts": 8.202, "gpu_utilization_percent_mean": 73.871, "power_watts_avg": 20.746, "energy_joules_est": 169.08, "duration_seconds": 8.15, "sample_count": 70}, "timestamp": "2026-01-26T15:58:25.124408"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8594.741, "latencies_ms": [8594.741], "images_per_second": 0.116, "prompt_tokens": 44, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The pink shoe is positioned in the foreground, resting on a blue bench that occupies the middle ground of the image. The bench has a worn appearance with peeling paint, indicating it is the main object in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21070.0, "ram_available_mb": 41770.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21071.0, "ram_available_mb": 41769.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.507}, "power_stats": {"power_gpu_soc_mean_watts": 20.393, "power_cpu_cv_mean_watts": 1.721, "power_sys_5v0_mean_watts": 8.284, "gpu_utilization_percent_mean": 71.507, "power_watts_avg": 20.393, "energy_joules_est": 175.28, "duration_seconds": 8.595, "sample_count": 73}, "timestamp": "2026-01-26T15:58:35.776747"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7406.93, "latencies_ms": [7406.93], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A person is sitting on a blue bench with green and blue paint splattered on it. The person is wearing blue jeans and pink shoes with a bow on the front.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21071.0, "ram_available_mb": 41769.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.5, "ram_used_mb": 21098.8, "ram_available_mb": 41742.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.73}, "power_stats": {"power_gpu_soc_mean_watts": 21.526, "power_cpu_cv_mean_watts": 1.785, "power_sys_5v0_mean_watts": 8.243, "gpu_utilization_percent_mean": 74.73, "power_watts_avg": 21.526, "energy_joules_est": 159.45, "duration_seconds": 7.408, "sample_count": 63}, "timestamp": "2026-01-26T15:58:45.203032"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8811.617, "latencies_ms": [8811.617], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image shows a person's foot wearing a bright pink shoe with a bow on the side, placed on a blue bench with peeling paint. The lighting appears to be natural, suggesting the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 21037.1, "ram_available_mb": 41803.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21076.5, "ram_available_mb": 41764.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.541}, "power_stats": {"power_gpu_soc_mean_watts": 20.296, "power_cpu_cv_mean_watts": 1.725, "power_sys_5v0_mean_watts": 8.294, "gpu_utilization_percent_mean": 71.541, "power_watts_avg": 20.296, "energy_joules_est": 178.85, "duration_seconds": 8.812, "sample_count": 74}, "timestamp": "2026-01-26T15:58:56.050954"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11568.81, "latencies_ms": [11568.81], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman and a young boy are standing in a room with red walls. The woman is holding a knife in her hand, and the boy is standing next to her. They are both smiling, possibly enjoying a playful moment together. \n\nThe room features a dining table with a cake on it, suggesting that they might be celebrating a special", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 21076.5, "ram_available_mb": 41764.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21032.6, "ram_available_mb": 41808.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.27}, "power_stats": {"power_gpu_soc_mean_watts": 19.302, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.312, "gpu_utilization_percent_mean": 70.27, "power_watts_avg": 19.302, "energy_joules_est": 223.31, "duration_seconds": 11.569, "sample_count": 100}, "timestamp": "2026-01-26T15:59:09.652276"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7531.742, "latencies_ms": [7531.742], "images_per_second": 0.133, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "woman: 1, knife: 1, boy: 1, table: 1, plate: 1, cup: 1, cake: 1, wall: 1", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 21032.6, "ram_available_mb": 41808.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21072.6, "ram_available_mb": 41768.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.969}, "power_stats": {"power_gpu_soc_mean_watts": 21.422, "power_cpu_cv_mean_watts": 1.557, "power_sys_5v0_mean_watts": 8.222, "gpu_utilization_percent_mean": 73.969, "power_watts_avg": 21.422, "energy_joules_est": 161.36, "duration_seconds": 7.532, "sample_count": 64}, "timestamp": "2026-01-26T15:59:19.200497"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10751.818, "latencies_ms": [10751.818], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "In the foreground, a woman is standing to the left of a table, holding a knife in her right hand. On the table, there is a red cake with a yellow design on it, a white mug, and a plate. The woman is standing near the table, and the cake is in the center of the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21072.6, "ram_available_mb": 41768.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21074.0, "ram_available_mb": 41766.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.75}, "power_stats": {"power_gpu_soc_mean_watts": 19.558, "power_cpu_cv_mean_watts": 1.857, "power_sys_5v0_mean_watts": 8.307, "gpu_utilization_percent_mean": 69.75, "power_watts_avg": 19.558, "energy_joules_est": 210.3, "duration_seconds": 10.752, "sample_count": 92}, "timestamp": "2026-01-26T15:59:32.005261"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8228.48, "latencies_ms": [8228.48], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A woman and a boy are standing in a room with red walls, and the woman is holding a knife. There is a cake on the table in front of them, suggesting that they may be celebrating a special occasion.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21074.0, "ram_available_mb": 41766.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21075.0, "ram_available_mb": 41765.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.271}, "power_stats": {"power_gpu_soc_mean_watts": 21.027, "power_cpu_cv_mean_watts": 1.63, "power_sys_5v0_mean_watts": 8.226, "gpu_utilization_percent_mean": 73.271, "power_watts_avg": 21.027, "energy_joules_est": 173.03, "duration_seconds": 8.229, "sample_count": 70}, "timestamp": "2026-01-26T15:59:42.262040"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7685.408, "latencies_ms": [7685.408], "images_per_second": 0.13, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image features a woman and a boy standing in a room with red walls. The woman is wearing a green cardigan and holding a knife, while the boy is wearing a blue plaid shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21075.0, "ram_available_mb": 41765.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21075.3, "ram_available_mb": 41765.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.938}, "power_stats": {"power_gpu_soc_mean_watts": 20.651, "power_cpu_cv_mean_watts": 1.626, "power_sys_5v0_mean_watts": 8.254, "gpu_utilization_percent_mean": 72.938, "power_watts_avg": 20.651, "energy_joules_est": 158.73, "duration_seconds": 7.686, "sample_count": 65}, "timestamp": "2026-01-26T15:59:51.981733"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11597.562, "latencies_ms": [11597.562], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a large indoor arena, where two elephants are the main subjects. The elephant on the left, with its light brown skin, stands calmly, its trunk extended towards the ground. Its companion, a gray elephant, is also on the ground, its trunk reaching out towards the elephant on the left. The", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21075.3, "ram_available_mb": 41765.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21075.5, "ram_available_mb": 41765.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.848}, "power_stats": {"power_gpu_soc_mean_watts": 19.271, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.304, "gpu_utilization_percent_mean": 69.848, "power_watts_avg": 19.271, "energy_joules_est": 223.51, "duration_seconds": 11.598, "sample_count": 99}, "timestamp": "2026-01-26T16:00:05.607279"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10480.432, "latencies_ms": [10480.432], "images_per_second": 0.095, "prompt_tokens": 39, "response_tokens_est": 68, "n_tiles": 16, "output_text": "1. Elephant: 2\n2. Barrier: 1\n3. Camera: 1\n4. Lighting equipment: 1\n5. Folding chair: 1\n6. Trunk: 2\n7. Stroller: 1\n8. Cone: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21075.5, "ram_available_mb": 41765.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21076.2, "ram_available_mb": 41764.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.289}, "power_stats": {"power_gpu_soc_mean_watts": 19.919, "power_cpu_cv_mean_watts": 1.806, "power_sys_5v0_mean_watts": 8.295, "gpu_utilization_percent_mean": 70.289, "power_watts_avg": 19.919, "energy_joules_est": 208.77, "duration_seconds": 10.481, "sample_count": 90}, "timestamp": "2026-01-26T16:00:18.110931"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11598.908, "latencies_ms": [11598.908], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two elephants, one with a brown skin tone and the other with a grey skin tone, standing close to each other. In the background, there are bleachers and a person walking towards the left side of the image. The elephants are positioned in the center of the image, with the bleachers behind them and the person in the", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21076.2, "ram_available_mb": 41764.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21055.1, "ram_available_mb": 41785.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.449}, "power_stats": {"power_gpu_soc_mean_watts": 19.234, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.283, "gpu_utilization_percent_mean": 69.449, "power_watts_avg": 19.234, "energy_joules_est": 223.11, "duration_seconds": 11.6, "sample_count": 98}, "timestamp": "2026-01-26T16:00:31.743250"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9357.689, "latencies_ms": [9357.689], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "In an indoor arena with a high ceiling and metal beams, two elephants are standing side by side, with a person standing to the left of the elephants. The arena has a patterned floor and there are bleachers in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21055.1, "ram_available_mb": 41785.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21080.1, "ram_available_mb": 41760.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.05}, "power_stats": {"power_gpu_soc_mean_watts": 20.17, "power_cpu_cv_mean_watts": 1.736, "power_sys_5v0_mean_watts": 8.262, "gpu_utilization_percent_mean": 71.05, "power_watts_avg": 20.17, "energy_joules_est": 188.76, "duration_seconds": 9.358, "sample_count": 80}, "timestamp": "2026-01-26T16:00:43.127334"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11583.292, "latencies_ms": [11583.292], "images_per_second": 0.086, "prompt_tokens": 36, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The image features two elephants, one with a light brown skin and the other with a darker grey skin, standing in an indoor arena with a patterned floor. The lighting is artificial, with spotlights directed towards the elephants, and the arena has a concrete ceiling with metal beams and a set of bleachers in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21080.1, "ram_available_mb": 41760.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21042.1, "ram_available_mb": 41798.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.38}, "power_stats": {"power_gpu_soc_mean_watts": 19.189, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.297, "gpu_utilization_percent_mean": 69.38, "power_watts_avg": 19.189, "energy_joules_est": 222.28, "duration_seconds": 11.584, "sample_count": 100}, "timestamp": "2026-01-26T16:00:56.770398"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11601.594, "latencies_ms": [11601.594], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a thrilling moment of a horse race on a beach. Two jockeys, clad in white helmets and uniforms, are seen riding their horses in a race. The horses are galloping on the sandy beach, with the ocean in the background. The scene is set in black and white, adding a timeless and classic feel to the", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 21042.1, "ram_available_mb": 41798.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21084.9, "ram_available_mb": 41756.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.041}, "power_stats": {"power_gpu_soc_mean_watts": 19.304, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.289, "gpu_utilization_percent_mean": 70.041, "power_watts_avg": 19.304, "energy_joules_est": 223.97, "duration_seconds": 11.602, "sample_count": 98}, "timestamp": "2026-01-26T16:01:10.416693"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7658.175, "latencies_ms": [7658.175], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "horse: 2, jockey: 2, beach: 1, water: 1, sky: 1, clouds: 1, sand: 1, ripples: 1", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21084.9, "ram_available_mb": 41756.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21085.1, "ram_available_mb": 41755.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.985}, "power_stats": {"power_gpu_soc_mean_watts": 21.216, "power_cpu_cv_mean_watts": 1.57, "power_sys_5v0_mean_watts": 8.217, "gpu_utilization_percent_mean": 73.985, "power_watts_avg": 21.216, "energy_joules_est": 162.49, "duration_seconds": 7.659, "sample_count": 65}, "timestamp": "2026-01-26T16:01:20.094922"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11564.759, "latencies_ms": [11564.759], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two jockeys riding horses, positioned close to each other and moving towards the right side of the image. The background features a vast, open landscape that appears to be a beach, with the ocean extending to the horizon. The horses and jockeys are in the near foreground, while the beach and ocean are in the far background,", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21085.1, "ram_available_mb": 41755.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21085.1, "ram_available_mb": 41755.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.031}, "power_stats": {"power_gpu_soc_mean_watts": 19.31, "power_cpu_cv_mean_watts": 1.883, "power_sys_5v0_mean_watts": 8.305, "gpu_utilization_percent_mean": 70.031, "power_watts_avg": 19.31, "energy_joules_est": 223.33, "duration_seconds": 11.565, "sample_count": 98}, "timestamp": "2026-01-26T16:01:33.703709"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9823.134, "latencies_ms": [9823.134], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image captures a dynamic scene of three jockeys riding horses on a beach. The horses are galloping along the wet sand, leaving a trail of splashes behind them, suggesting a recent rain or a high tide that has left the beach temporarily submerged.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21085.1, "ram_available_mb": 41755.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21089.6, "ram_available_mb": 41751.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.094}, "power_stats": {"power_gpu_soc_mean_watts": 20.113, "power_cpu_cv_mean_watts": 1.751, "power_sys_5v0_mean_watts": 8.245, "gpu_utilization_percent_mean": 72.094, "power_watts_avg": 20.113, "energy_joules_est": 197.58, "duration_seconds": 9.824, "sample_count": 85}, "timestamp": "2026-01-26T16:01:45.547515"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7660.908, "latencies_ms": [7660.908], "images_per_second": 0.131, "prompt_tokens": 36, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image is a sepia-toned photograph, giving it a vintage or timeless feel. The lighting is soft and diffused, with no harsh shadows, suggesting an overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21035.8, "ram_available_mb": 41805.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21084.2, "ram_available_mb": 41756.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.308}, "power_stats": {"power_gpu_soc_mean_watts": 20.927, "power_cpu_cv_mean_watts": 1.613, "power_sys_5v0_mean_watts": 8.285, "gpu_utilization_percent_mean": 72.308, "power_watts_avg": 20.927, "energy_joules_est": 160.33, "duration_seconds": 7.662, "sample_count": 65}, "timestamp": "2026-01-26T16:01:55.226466"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11559.951, "latencies_ms": [11559.951], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is standing in a snowy landscape, talking on his cell phone. He is wearing a black jacket and a black hat, which contrasts with the white snow around him. The man appears to be engaged in a conversation, as he holds the phone to his ear with his left hand. The background features a snowy forest, adding to the wintry", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 21084.2, "ram_available_mb": 41756.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21096.7, "ram_available_mb": 41744.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.52}, "power_stats": {"power_gpu_soc_mean_watts": 19.23, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.3, "gpu_utilization_percent_mean": 70.52, "power_watts_avg": 19.23, "energy_joules_est": 222.32, "duration_seconds": 11.561, "sample_count": 98}, "timestamp": "2026-01-26T16:02:08.824172"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10125.733, "latencies_ms": [10125.733], "images_per_second": 0.099, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "1. Boy: 1\n2. Jacket: 1\n3. Glove: 1\n4. Hat: 1\n5. Goggles: 1\n6. Cell phone: 1\n7. Trees: 1\n8. Snow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21034.9, "ram_available_mb": 41806.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21054.2, "ram_available_mb": 41786.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.047}, "power_stats": {"power_gpu_soc_mean_watts": 19.958, "power_cpu_cv_mean_watts": 1.756, "power_sys_5v0_mean_watts": 8.229, "gpu_utilization_percent_mean": 72.047, "power_watts_avg": 19.958, "energy_joules_est": 202.1, "duration_seconds": 10.126, "sample_count": 85}, "timestamp": "2026-01-26T16:02:20.982794"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8718.652, "latencies_ms": [8718.652], "images_per_second": 0.115, "prompt_tokens": 44, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The person is standing in the foreground with a snowy landscape in the background. The trees are behind the person, and the ground is covered in snow. The person is wearing a black jacket and a blue and black goggle on their head.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21054.2, "ram_available_mb": 41786.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21076.2, "ram_available_mb": 41764.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.784}, "power_stats": {"power_gpu_soc_mean_watts": 20.405, "power_cpu_cv_mean_watts": 1.693, "power_sys_5v0_mean_watts": 8.272, "gpu_utilization_percent_mean": 71.784, "power_watts_avg": 20.405, "energy_joules_est": 177.92, "duration_seconds": 8.719, "sample_count": 74}, "timestamp": "2026-01-26T16:02:31.717853"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7907.866, "latencies_ms": [7907.866], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A person is standing in a snowy area, wearing a black jacket and a goggles on their head. They are holding a cell phone to their ear and appear to be talking on the phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21076.2, "ram_available_mb": 41764.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21092.9, "ram_available_mb": 41748.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.552}, "power_stats": {"power_gpu_soc_mean_watts": 21.097, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.225, "gpu_utilization_percent_mean": 73.552, "power_watts_avg": 21.097, "energy_joules_est": 166.85, "duration_seconds": 7.908, "sample_count": 67}, "timestamp": "2026-01-26T16:02:41.650736"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6941.861, "latencies_ms": [6941.861], "images_per_second": 0.144, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The person in the image is wearing a black jacket and a blue and black goggle on their head. The background shows a snowy landscape with trees and a blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21031.1, "ram_available_mb": 41809.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 21082.4, "ram_available_mb": 41758.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.695}, "power_stats": {"power_gpu_soc_mean_watts": 21.663, "power_cpu_cv_mean_watts": 1.709, "power_sys_5v0_mean_watts": 8.27, "gpu_utilization_percent_mean": 74.695, "power_watts_avg": 21.663, "energy_joules_est": 150.39, "duration_seconds": 6.942, "sample_count": 59}, "timestamp": "2026-01-26T16:02:50.636330"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11582.281, "latencies_ms": [11582.281], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene camping scene at sunset. A green tent is pitched on a grassy field, with a motorcycle parked in front of it. The motorcycle, with its black seat and handlebars, is equipped with a large black bag and a smaller black bag attached to the back. The motorcycle is facing towards the right side of the image,", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 21082.4, "ram_available_mb": 41758.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21082.3, "ram_available_mb": 41758.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.235}, "power_stats": {"power_gpu_soc_mean_watts": 19.361, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.297, "gpu_utilization_percent_mean": 70.235, "power_watts_avg": 19.361, "energy_joules_est": 224.26, "duration_seconds": 11.583, "sample_count": 98}, "timestamp": "2026-01-26T16:03:04.246022"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7646.582, "latencies_ms": [7646.582], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "tent: 1, motorcycle: 1, trees: numerous, sun: 1, sky: 1, grass: 1, forest: 1, camping gear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.3, "ram_available_mb": 41758.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21082.8, "ram_available_mb": 41758.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.323}, "power_stats": {"power_gpu_soc_mean_watts": 21.295, "power_cpu_cv_mean_watts": 1.57, "power_sys_5v0_mean_watts": 8.206, "gpu_utilization_percent_mean": 73.323, "power_watts_avg": 21.295, "energy_joules_est": 162.85, "duration_seconds": 7.647, "sample_count": 65}, "timestamp": "2026-01-26T16:03:13.920471"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11244.724, "latencies_ms": [11244.724], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "A motorcycle is parked in the foreground on the left side of the image, while a green tent is situated in the background on the right side. The motorcycle is positioned closer to the camera, making it appear larger, and the tent is further away, appearing smaller. The sun is setting in the background, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.8, "ram_available_mb": 41758.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21114.2, "ram_available_mb": 41726.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.695}, "power_stats": {"power_gpu_soc_mean_watts": 19.38, "power_cpu_cv_mean_watts": 1.866, "power_sys_5v0_mean_watts": 8.295, "gpu_utilization_percent_mean": 70.695, "power_watts_avg": 19.38, "energy_joules_est": 217.93, "duration_seconds": 11.245, "sample_count": 95}, "timestamp": "2026-01-26T16:03:27.187317"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7217.654, "latencies_ms": [7217.654], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A motorcycle is parked in a field with a green tent nearby, suggesting a camping trip. The sun is setting in the background, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21044.0, "ram_available_mb": 41796.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21085.5, "ram_available_mb": 41755.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.639}, "power_stats": {"power_gpu_soc_mean_watts": 21.617, "power_cpu_cv_mean_watts": 1.509, "power_sys_5v0_mean_watts": 8.193, "gpu_utilization_percent_mean": 73.639, "power_watts_avg": 21.617, "energy_joules_est": 156.04, "duration_seconds": 7.218, "sample_count": 61}, "timestamp": "2026-01-26T16:03:36.461020"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8697.162, "latencies_ms": [8697.162], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image features a serene outdoor setting with a green tent and a motorcycle parked on dry grass. The lighting suggests it's either dawn or dusk, with the sun low in the sky casting a warm glow and long shadows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21085.5, "ram_available_mb": 41755.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21086.2, "ram_available_mb": 41754.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.054}, "power_stats": {"power_gpu_soc_mean_watts": 20.243, "power_cpu_cv_mean_watts": 1.72, "power_sys_5v0_mean_watts": 8.243, "gpu_utilization_percent_mean": 72.054, "power_watts_avg": 20.243, "energy_joules_est": 176.07, "duration_seconds": 8.698, "sample_count": 74}, "timestamp": "2026-01-26T16:03:47.185952"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11585.036, "latencies_ms": [11585.036], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment frozen in time, featuring a vintage steam locomotive, numbered 67371, pulling a passenger train along a railway track. The locomotive, painted in a striking black, is adorned with a white number and a red emblem on its side, adding a touch of color to its otherwise monochrome appearance. The train", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21086.2, "ram_available_mb": 41754.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21086.9, "ram_available_mb": 41754.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.05}, "power_stats": {"power_gpu_soc_mean_watts": 19.29, "power_cpu_cv_mean_watts": 1.893, "power_sys_5v0_mean_watts": 8.298, "gpu_utilization_percent_mean": 70.05, "power_watts_avg": 19.29, "energy_joules_est": 223.49, "duration_seconds": 11.586, "sample_count": 100}, "timestamp": "2026-01-26T16:04:00.817355"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9725.323, "latencies_ms": [9725.323], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Train: 1\n\n- Train car: 2\n\n- Train engine: 1\n\n- Train tracks: 1\n\n- Station platform: 1\n\n- Station sign: 1\n\n- People: 4\n\n- Buildings: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21086.9, "ram_available_mb": 41754.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21088.1, "ram_available_mb": 41752.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.595}, "power_stats": {"power_gpu_soc_mean_watts": 19.962, "power_cpu_cv_mean_watts": 1.753, "power_sys_5v0_mean_watts": 8.241, "gpu_utilization_percent_mean": 71.595, "power_watts_avg": 19.962, "energy_joules_est": 194.15, "duration_seconds": 9.726, "sample_count": 84}, "timestamp": "2026-01-26T16:04:12.586330"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11544.834, "latencies_ms": [11544.834], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a vintage steam locomotive with the number 67371 prominently displayed on its front. It is situated on the railway tracks, positioned towards the left side of the image. In the background, there is a train station platform where a group of people is standing near the edge, facing the locomotive. The platform extends from the", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21088.1, "ram_available_mb": 41752.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21106.1, "ram_available_mb": 41734.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.545}, "power_stats": {"power_gpu_soc_mean_watts": 19.331, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.308, "gpu_utilization_percent_mean": 70.545, "power_watts_avg": 19.331, "energy_joules_est": 223.19, "duration_seconds": 11.545, "sample_count": 99}, "timestamp": "2026-01-26T16:04:26.151762"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7190.552, "latencies_ms": [7190.552], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image depicts a vintage black and white scene of a steam locomotive at a train station. There are several people standing on the platform, waiting to board the train.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21042.0, "ram_available_mb": 41798.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21081.4, "ram_available_mb": 41759.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.672}, "power_stats": {"power_gpu_soc_mean_watts": 21.635, "power_cpu_cv_mean_watts": 1.522, "power_sys_5v0_mean_watts": 8.207, "gpu_utilization_percent_mean": 74.672, "power_watts_avg": 21.635, "energy_joules_est": 155.58, "duration_seconds": 7.191, "sample_count": 61}, "timestamp": "2026-01-26T16:04:35.374030"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9133.963, "latencies_ms": [9133.963], "images_per_second": 0.109, "prompt_tokens": 36, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image is a black and white photograph, indicating it was taken during a time when color photography was not available or commonly used. The lighting appears to be natural daylight, and the weather seems to be clear as there are no signs of rain or snow in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21081.4, "ram_available_mb": 41759.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21103.1, "ram_available_mb": 41737.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.705}, "power_stats": {"power_gpu_soc_mean_watts": 20.015, "power_cpu_cv_mean_watts": 1.765, "power_sys_5v0_mean_watts": 8.295, "gpu_utilization_percent_mean": 71.705, "power_watts_avg": 20.015, "energy_joules_est": 182.83, "duration_seconds": 9.135, "sample_count": 78}, "timestamp": "2026-01-26T16:04:46.558418"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11617.12, "latencies_ms": [11617.12], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a bustling street scene in Japan, bathed in the stark contrast of black and white. The perspective is from a low angle, looking upwards towards the sky, giving a sense of the towering buildings that line the street. These buildings, constructed from concrete, are adorned with numerous signs and banners, their black text standing out against the white background", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21041.3, "ram_available_mb": 41799.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21080.7, "ram_available_mb": 41760.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.092}, "power_stats": {"power_gpu_soc_mean_watts": 19.275, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.301, "gpu_utilization_percent_mean": 70.092, "power_watts_avg": 19.275, "energy_joules_est": 223.93, "duration_seconds": 11.618, "sample_count": 98}, "timestamp": "2026-01-26T16:05:00.213041"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8016.05, "latencies_ms": [8016.05], "images_per_second": 0.125, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "sign: 20, building: 2, window: 15, sky: 1, wire: 5, banner: 10, text: 30, character: 25", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21080.7, "ram_available_mb": 41760.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21081.2, "ram_available_mb": 41759.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.088}, "power_stats": {"power_gpu_soc_mean_watts": 21.032, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.205, "gpu_utilization_percent_mean": 73.088, "power_watts_avg": 21.032, "energy_joules_est": 168.61, "duration_seconds": 8.017, "sample_count": 68}, "timestamp": "2026-01-26T16:05:10.289000"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11591.045, "latencies_ms": [11591.045], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a dense cluster of signs hanging from above, with the signs appearing to be in the foreground, creating a sense of depth as they lead the eye towards the buildings in the background. The signs are arranged in a somewhat chaotic manner, with some signs overlapping others, and they vary in size and orientation, adding to the complexity of the scene. The buildings in the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21081.2, "ram_available_mb": 41759.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21081.2, "ram_available_mb": 41759.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.26}, "power_stats": {"power_gpu_soc_mean_watts": 19.125, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.296, "gpu_utilization_percent_mean": 70.26, "power_watts_avg": 19.125, "energy_joules_est": 221.69, "duration_seconds": 11.592, "sample_count": 100}, "timestamp": "2026-01-26T16:05:23.921101"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8125.661, "latencies_ms": [8125.661], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image depicts a bustling urban scene with numerous signs hanging from above, likely indicating shops or businesses. The signs are densely packed, creating a sense of a busy and crowded environment.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21081.2, "ram_available_mb": 41759.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21082.2, "ram_available_mb": 41758.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.671}, "power_stats": {"power_gpu_soc_mean_watts": 20.775, "power_cpu_cv_mean_watts": 1.612, "power_sys_5v0_mean_watts": 8.208, "gpu_utilization_percent_mean": 73.671, "power_watts_avg": 20.775, "energy_joules_est": 168.82, "duration_seconds": 8.126, "sample_count": 70}, "timestamp": "2026-01-26T16:05:34.097265"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8830.513, "latencies_ms": [8830.513], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image is a black and white photograph, showcasing a dense array of Chinese characters on various signs and banners. The signs are made of a reflective material, possibly metal, and are suspended from above, creating a canopy-like effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.2, "ram_available_mb": 41758.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21082.4, "ram_available_mb": 41758.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.333}, "power_stats": {"power_gpu_soc_mean_watts": 20.073, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.283, "gpu_utilization_percent_mean": 71.333, "power_watts_avg": 20.073, "energy_joules_est": 177.27, "duration_seconds": 8.831, "sample_count": 75}, "timestamp": "2026-01-26T16:05:44.948505"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11556.018, "latencies_ms": [11556.018], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is sitting on a concrete ledge by the water, enjoying the view. He is wearing blue shorts and appears to be relaxed. The ledge is located near a grassy area, and there is a bench nearby. The bench has a sign on it that reads \"Closed\" and \"Lakefront Trail.\" The man seems", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 21082.4, "ram_available_mb": 41758.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21083.4, "ram_available_mb": 41757.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.96}, "power_stats": {"power_gpu_soc_mean_watts": 19.233, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.304, "gpu_utilization_percent_mean": 69.96, "power_watts_avg": 19.233, "energy_joules_est": 222.27, "duration_seconds": 11.557, "sample_count": 100}, "timestamp": "2026-01-26T16:05:58.536972"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8938.527, "latencies_ms": [8938.527], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "path: 2\n\nlakefront trail: 1\n\nclosed: 1\n\nman: 1\n\nshirt: 1\n\nshorts: 1\n\nsocks: 1\n\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21083.4, "ram_available_mb": 41757.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21083.9, "ram_available_mb": 41757.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.312}, "power_stats": {"power_gpu_soc_mean_watts": 20.378, "power_cpu_cv_mean_watts": 1.689, "power_sys_5v0_mean_watts": 8.255, "gpu_utilization_percent_mean": 73.312, "power_watts_avg": 20.378, "energy_joules_est": 182.16, "duration_seconds": 8.939, "sample_count": 77}, "timestamp": "2026-01-26T16:06:09.529883"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11645.717, "latencies_ms": [11645.717], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "A man is lying on a concrete barrier near the water's edge, which is in the background. The barrier is positioned in the foreground, and there is a 'CLOSED' sign in front of it, indicating that the area is off-limits. The grassy area where the man is lying appears to be near the trail, as suggested by the sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21083.9, "ram_available_mb": 41757.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21100.6, "ram_available_mb": 41740.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.73}, "power_stats": {"power_gpu_soc_mean_watts": 19.282, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.319, "gpu_utilization_percent_mean": 70.73, "power_watts_avg": 19.282, "energy_joules_est": 224.56, "duration_seconds": 11.646, "sample_count": 100}, "timestamp": "2026-01-26T16:06:23.215806"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7909.919, "latencies_ms": [7909.919], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A person is lying on a concrete barrier near a body of water, with a closed sign for the Lakefront Trail in the foreground. The grassy area appears to be a park or recreational space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21100.6, "ram_available_mb": 41740.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21080.5, "ram_available_mb": 41760.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.701}, "power_stats": {"power_gpu_soc_mean_watts": 21.095, "power_cpu_cv_mean_watts": 1.583, "power_sys_5v0_mean_watts": 8.207, "gpu_utilization_percent_mean": 73.701, "power_watts_avg": 21.095, "energy_joules_est": 166.87, "duration_seconds": 7.911, "sample_count": 67}, "timestamp": "2026-01-26T16:06:33.155261"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7512.901, "latencies_ms": [7512.901], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A man is lying on a concrete barrier by the water, wearing blue shorts and black shoes. The barrier is blue with white text that reads \"Lakefront Trail Closed\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21080.5, "ram_available_mb": 41760.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21081.9, "ram_available_mb": 41759.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.111}, "power_stats": {"power_gpu_soc_mean_watts": 21.418, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.277, "gpu_utilization_percent_mean": 74.111, "power_watts_avg": 21.418, "energy_joules_est": 160.93, "duration_seconds": 7.514, "sample_count": 63}, "timestamp": "2026-01-26T16:06:42.690018"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11570.478, "latencies_ms": [11570.478], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the serene expanse of a lush green field, a brown horse and a white dog share a moment of tranquility. The horse, adorned with a black halter, stands majestically on the right side of the image. Its coat, a rich shade of brown, contrasts beautifully with the verdant surroundings. \n\nOn the left", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 21081.9, "ram_available_mb": 41759.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.8, "ram_used_mb": 21112.5, "ram_available_mb": 41728.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.091}, "power_stats": {"power_gpu_soc_mean_watts": 19.157, "power_cpu_cv_mean_watts": 2.017, "power_sys_5v0_mean_watts": 8.294, "gpu_utilization_percent_mean": 70.091, "power_watts_avg": 19.157, "energy_joules_est": 221.67, "duration_seconds": 11.571, "sample_count": 99}, "timestamp": "2026-01-26T16:06:56.313427"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7565.079, "latencies_ms": [7565.079], "images_per_second": 0.132, "prompt_tokens": 39, "response_tokens_est": 42, "n_tiles": 16, "output_text": "horse: 1, dog: 1, grass: many, sky: 1, trees: many, field: 1, sun: 1, horse's mane: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21112.5, "ram_available_mb": 41728.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21112.8, "ram_available_mb": 41728.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.016}, "power_stats": {"power_gpu_soc_mean_watts": 21.291, "power_cpu_cv_mean_watts": 1.551, "power_sys_5v0_mean_watts": 8.225, "gpu_utilization_percent_mean": 74.016, "power_watts_avg": 21.291, "energy_joules_est": 161.08, "duration_seconds": 7.566, "sample_count": 64}, "timestamp": "2026-01-26T16:07:05.903726"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10389.319, "latencies_ms": [10389.319], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, there is a golden retriever dog sitting in the grass, positioned near the center of the image. Behind the dog, a brown horse with a black halter is standing further back in the grassy field. The background features a clear blue sky and a distant landscape with trees and shrubs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21112.8, "ram_available_mb": 41728.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21113.2, "ram_available_mb": 41727.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.284}, "power_stats": {"power_gpu_soc_mean_watts": 19.775, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 8.284, "gpu_utilization_percent_mean": 71.284, "power_watts_avg": 19.775, "energy_joules_est": 205.46, "duration_seconds": 10.39, "sample_count": 88}, "timestamp": "2026-01-26T16:07:18.313478"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8966.302, "latencies_ms": [8966.302], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "In a serene field, a brown horse and a golden retriever are standing close to each other, both looking towards the camera. The horse is wearing a bridle, and the dog appears to be enjoying the sunny day outdoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21113.2, "ram_available_mb": 41727.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21113.5, "ram_available_mb": 41727.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.447}, "power_stats": {"power_gpu_soc_mean_watts": 20.421, "power_cpu_cv_mean_watts": 1.685, "power_sys_5v0_mean_watts": 8.225, "gpu_utilization_percent_mean": 71.447, "power_watts_avg": 20.421, "energy_joules_est": 183.11, "duration_seconds": 8.967, "sample_count": 76}, "timestamp": "2026-01-26T16:07:29.307085"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7881.554, "latencies_ms": [7881.554], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image features a golden retriever and a chestnut-colored horse standing in a field of tall green grass. The lighting is bright and natural, suggesting the photo was taken on a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21043.9, "ram_available_mb": 41797.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21082.7, "ram_available_mb": 41758.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.948, "power_cpu_cv_mean_watts": 1.625, "power_sys_5v0_mean_watts": 8.271, "gpu_utilization_percent_mean": 73.0, "power_watts_avg": 20.948, "energy_joules_est": 165.12, "duration_seconds": 7.882, "sample_count": 66}, "timestamp": "2026-01-26T16:07:39.243313"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11543.102, "latencies_ms": [11543.102], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of people playing volleyball in a gymnasium. There are at least 13 people visible in the scene, with some players actively participating in the game and others standing or waiting for the ball. The volleyball is in the air, and the players are focused on the game.\n\nThe gymnasium has a blue floor, and there are two", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21082.7, "ram_available_mb": 41758.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21082.7, "ram_available_mb": 41758.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.69}, "power_stats": {"power_gpu_soc_mean_watts": 19.326, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.322, "gpu_utilization_percent_mean": 70.69, "power_watts_avg": 19.326, "energy_joules_est": 223.09, "duration_seconds": 11.544, "sample_count": 100}, "timestamp": "2026-01-26T16:07:52.837385"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10058.246, "latencies_ms": [10058.246], "images_per_second": 0.099, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "- Volleyball: 1\n\n- Players: 15\n\n- Net: 1\n\n- Gym floor: 1\n\n- Wall: 1\n\n- Ceiling: 1\n\n- Basketball hoop: 1\n\n- Basketball: 1", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 21082.7, "ram_available_mb": 41758.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21110.3, "ram_available_mb": 41730.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.802}, "power_stats": {"power_gpu_soc_mean_watts": 20.01, "power_cpu_cv_mean_watts": 1.759, "power_sys_5v0_mean_watts": 8.253, "gpu_utilization_percent_mean": 71.802, "power_watts_avg": 20.01, "energy_joules_est": 201.28, "duration_seconds": 10.059, "sample_count": 86}, "timestamp": "2026-01-26T16:08:04.916358"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11442.777, "latencies_ms": [11442.777], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "In the foreground, a person in a blue outfit is holding a volleyball, preparing to serve. In the background, there are multiple players in green and blue uniforms, positioned near the net, likely waiting for the serve. The players are spread out across the court, with some closer to the net and others further away, indicating a game in progress.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21110.3, "ram_available_mb": 41730.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21137.0, "ram_available_mb": 41703.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.062}, "power_stats": {"power_gpu_soc_mean_watts": 19.325, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.304, "gpu_utilization_percent_mean": 70.062, "power_watts_avg": 19.325, "energy_joules_est": 221.15, "duration_seconds": 11.444, "sample_count": 97}, "timestamp": "2026-01-26T16:08:18.379971"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7229.165, "latencies_ms": [7229.165], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image depicts a group of people in a gymnasium playing volleyball. The players are wearing blue and green uniforms, and the court is marked with white lines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21137.0, "ram_available_mb": 41703.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21035.2, "ram_available_mb": 41805.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.295}, "power_stats": {"power_gpu_soc_mean_watts": 21.616, "power_cpu_cv_mean_watts": 1.509, "power_sys_5v0_mean_watts": 8.177, "gpu_utilization_percent_mean": 74.295, "power_watts_avg": 21.616, "energy_joules_est": 156.28, "duration_seconds": 7.23, "sample_count": 61}, "timestamp": "2026-01-26T16:08:27.621748"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7821.076, "latencies_ms": [7821.076], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows an indoor volleyball court with a bright and well-lit environment, featuring players in blue and green uniforms. The court is marked with white lines and has a blue floor with orange boundary lines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21035.2, "ram_available_mb": 41805.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21087.9, "ram_available_mb": 41753.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.075}, "power_stats": {"power_gpu_soc_mean_watts": 20.529, "power_cpu_cv_mean_watts": 1.643, "power_sys_5v0_mean_watts": 8.262, "gpu_utilization_percent_mean": 71.075, "power_watts_avg": 20.529, "energy_joules_est": 160.57, "duration_seconds": 7.822, "sample_count": 67}, "timestamp": "2026-01-26T16:08:37.469849"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11568.805, "latencies_ms": [11568.805], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large herd of zebras and wildebeests grazing in a grassy field. There are at least nine zebras and four wildebeests visible in the scene. The zebras are spread out across the field, with some standing closer to the wildebeests. The wildebeests are grazing on the grass, while the z", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 21087.9, "ram_available_mb": 41753.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21089.4, "ram_available_mb": 41751.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.01}, "power_stats": {"power_gpu_soc_mean_watts": 19.319, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.326, "gpu_utilization_percent_mean": 71.01, "power_watts_avg": 19.319, "energy_joules_est": 223.51, "duration_seconds": 11.569, "sample_count": 99}, "timestamp": "2026-01-26T16:08:51.071625"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9239.946, "latencies_ms": [9239.946], "images_per_second": 0.108, "prompt_tokens": 39, "response_tokens_est": 57, "n_tiles": 16, "output_text": "zebras: 5, wildebeests: 3, flamingos: 100, birds: 0, elephants: 0, giraffes: 0, lions: 0, crocodiles: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21089.4, "ram_available_mb": 41751.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21089.9, "ram_available_mb": 41751.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.385}, "power_stats": {"power_gpu_soc_mean_watts": 20.47, "power_cpu_cv_mean_watts": 1.719, "power_sys_5v0_mean_watts": 8.267, "gpu_utilization_percent_mean": 72.385, "power_watts_avg": 20.47, "energy_joules_est": 189.15, "duration_seconds": 9.241, "sample_count": 78}, "timestamp": "2026-01-26T16:09:02.336487"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11569.341, "latencies_ms": [11569.341], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are several zebras and wildebeests grazing on the grass, with one zebra standing slightly apart from the rest. In the background, there is a large herd of flamingos gathered in the water, creating a striking contrast between the two groups. The zebras and wildebeests are closer to the viewer, while the", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21089.9, "ram_available_mb": 41751.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21090.4, "ram_available_mb": 41750.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.102}, "power_stats": {"power_gpu_soc_mean_watts": 19.318, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.273, "gpu_utilization_percent_mean": 70.102, "power_watts_avg": 19.318, "energy_joules_est": 223.51, "duration_seconds": 11.57, "sample_count": 98}, "timestamp": "2026-01-26T16:09:15.917687"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9594.803, "latencies_ms": [9594.803], "images_per_second": 0.104, "prompt_tokens": 37, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image depicts a serene scene of a grassy field where a group of zebras and wildebeests are grazing. In the background, there is a large herd of flamingos gathered in the water, creating a picturesque and diverse wildlife view.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21090.4, "ram_available_mb": 41750.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21090.4, "ram_available_mb": 41750.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.297, "power_cpu_cv_mean_watts": 1.719, "power_sys_5v0_mean_watts": 8.234, "gpu_utilization_percent_mean": 72.0, "power_watts_avg": 20.297, "energy_joules_est": 194.76, "duration_seconds": 9.595, "sample_count": 81}, "timestamp": "2026-01-26T16:09:27.539082"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8367.751, "latencies_ms": [8367.751], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image features a serene landscape with a group of zebras and wildebeests grazing in a grassy field. The sky is overcast, and the overall lighting is soft, suggesting it might be a cloudy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21090.4, "ram_available_mb": 41750.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21104.1, "ram_available_mb": 41736.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.111}, "power_stats": {"power_gpu_soc_mean_watts": 20.393, "power_cpu_cv_mean_watts": 1.712, "power_sys_5v0_mean_watts": 8.264, "gpu_utilization_percent_mean": 72.111, "power_watts_avg": 20.393, "energy_joules_est": 170.66, "duration_seconds": 8.368, "sample_count": 72}, "timestamp": "2026-01-26T16:09:37.967966"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11577.676, "latencies_ms": [11577.676], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there are two cats with orange and white fur sitting on a wooden deck. They are facing each other, and one of them is licking the other's face. The deck has a greenish-yellow color, and the cats are positioned in such a way that they are looking at each other. The image captures a moment of interaction between the two", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21104.1, "ram_available_mb": 41736.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21104.3, "ram_available_mb": 41736.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.737}, "power_stats": {"power_gpu_soc_mean_watts": 19.279, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.305, "gpu_utilization_percent_mean": 69.737, "power_watts_avg": 19.279, "energy_joules_est": 223.22, "duration_seconds": 11.578, "sample_count": 99}, "timestamp": "2026-01-26T16:09:51.592087"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7754.552, "latencies_ms": [7754.552], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "cat: 2\nreflection: 1\nfence: 1\nboard: 1\nnail: 1\nmirror: 1\nwindow: 1\nrain: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21042.6, "ram_available_mb": 41798.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21078.3, "ram_available_mb": 41762.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.896}, "power_stats": {"power_gpu_soc_mean_watts": 21.173, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.221, "gpu_utilization_percent_mean": 73.896, "power_watts_avg": 21.173, "energy_joules_est": 164.2, "duration_seconds": 7.755, "sample_count": 67}, "timestamp": "2026-01-26T16:10:01.374270"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11578.345, "latencies_ms": [11578.345], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The two cats are positioned in the foreground, with one cat appearing larger and more dominant, while the other is smaller and seems to be looking up at the larger cat. They are standing on a wooden surface that extends into the background, which is blurred and indistinct. The larger cat is closer to the camera, making it appear more prominent in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21078.3, "ram_available_mb": 41762.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21112.0, "ram_available_mb": 41728.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.99}, "power_stats": {"power_gpu_soc_mean_watts": 19.215, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.28, "gpu_utilization_percent_mean": 69.99, "power_watts_avg": 19.215, "energy_joules_est": 222.49, "duration_seconds": 11.579, "sample_count": 99}, "timestamp": "2026-01-26T16:10:14.997816"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7543.49, "latencies_ms": [7543.49], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "Two orange and white cats are sitting on a wooden deck, looking at their reflections in a mirror. The deck appears to be outdoors and the cats seem curious about their own image.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 21112.0, "ram_available_mb": 41728.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21070.1, "ram_available_mb": 41770.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.046}, "power_stats": {"power_gpu_soc_mean_watts": 21.369, "power_cpu_cv_mean_watts": 1.551, "power_sys_5v0_mean_watts": 8.218, "gpu_utilization_percent_mean": 74.046, "power_watts_avg": 21.369, "energy_joules_est": 161.21, "duration_seconds": 7.544, "sample_count": 65}, "timestamp": "2026-01-26T16:10:24.602524"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11234.889, "latencies_ms": [11234.889], "images_per_second": 0.089, "prompt_tokens": 36, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The image features two cats with a vibrant orange and white coat, sitting on a wooden surface that appears to be a deck or a similar structure. The lighting is soft and diffused, suggesting an overcast day or a shaded area, and the wooden planks have a weathered look with some discoloration, indicating exposure to the elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21070.1, "ram_available_mb": 41770.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21071.6, "ram_available_mb": 41769.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.649}, "power_stats": {"power_gpu_soc_mean_watts": 19.168, "power_cpu_cv_mean_watts": 1.881, "power_sys_5v0_mean_watts": 8.333, "gpu_utilization_percent_mean": 70.649, "power_watts_avg": 19.168, "energy_joules_est": 215.36, "duration_seconds": 11.236, "sample_count": 97}, "timestamp": "2026-01-26T16:10:37.863967"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11571.209, "latencies_ms": [11571.209], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a serene lake with several boats docked along the shore. There are at least five boats visible, with some closer to the foreground and others further away. The boats vary in size and design, adding to the picturesque scene.\n\nIn addition to the boats, there are a few people scattered around the area, possibly enjoying the view or attending to their boats", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21071.6, "ram_available_mb": 41769.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21106.0, "ram_available_mb": 41734.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.327}, "power_stats": {"power_gpu_soc_mean_watts": 19.281, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.299, "gpu_utilization_percent_mean": 70.327, "power_watts_avg": 19.281, "energy_joules_est": 223.12, "duration_seconds": 11.572, "sample_count": 101}, "timestamp": "2026-01-26T16:10:51.505480"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7148.974, "latencies_ms": [7148.974], "images_per_second": 0.14, "prompt_tokens": 39, "response_tokens_est": 39, "n_tiles": 16, "output_text": "boat: 5, building: 1, tree: many, mountain: 1, water: 1, lamp post: 1, grass: many, road: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21043.9, "ram_available_mb": 41797.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 8.6, "ram_used_mb": 21086.9, "ram_available_mb": 41754.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.617}, "power_stats": {"power_gpu_soc_mean_watts": 21.857, "power_cpu_cv_mean_watts": 1.915, "power_sys_5v0_mean_watts": 8.279, "gpu_utilization_percent_mean": 75.617, "power_watts_avg": 21.857, "energy_joules_est": 156.27, "duration_seconds": 7.15, "sample_count": 60}, "timestamp": "2026-01-26T16:11:00.675795"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9969.078, "latencies_ms": [9969.078], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "In the foreground, there is a street lamp on the left side of the image, with a tree in front of it. The boats are docked in the water, with some closer to the shore and others further away. The buildings and trees are in the background, with the mountains visible in the far distance.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21086.9, "ram_available_mb": 41754.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21087.1, "ram_available_mb": 41753.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.326}, "power_stats": {"power_gpu_soc_mean_watts": 19.751, "power_cpu_cv_mean_watts": 1.82, "power_sys_5v0_mean_watts": 8.289, "gpu_utilization_percent_mean": 71.326, "power_watts_avg": 19.751, "energy_joules_est": 196.91, "duration_seconds": 9.97, "sample_count": 86}, "timestamp": "2026-01-26T16:11:12.670757"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7758.584, "latencies_ms": [7758.584], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image depicts a serene lakeside scene with several boats docked at a pier. In the background, there are buildings and lush greenery, creating a picturesque and peaceful atmosphere.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21087.1, "ram_available_mb": 41753.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21087.1, "ram_available_mb": 41753.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.5}, "power_stats": {"power_gpu_soc_mean_watts": 21.199, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.227, "gpu_utilization_percent_mean": 74.5, "power_watts_avg": 21.199, "energy_joules_est": 164.49, "duration_seconds": 7.759, "sample_count": 66}, "timestamp": "2026-01-26T16:11:22.461091"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7550.815, "latencies_ms": [7550.815], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image shows a body of water with several boats docked along the shore. The sky is overcast, and the overall lighting is dim, suggesting it might be an early morning or a cloudy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21087.1, "ram_available_mb": 41753.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21087.1, "ram_available_mb": 41753.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.125}, "power_stats": {"power_gpu_soc_mean_watts": 20.921, "power_cpu_cv_mean_watts": 1.62, "power_sys_5v0_mean_watts": 8.256, "gpu_utilization_percent_mean": 73.125, "power_watts_avg": 20.921, "energy_joules_est": 157.98, "duration_seconds": 7.551, "sample_count": 64}, "timestamp": "2026-01-26T16:11:32.056033"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11549.244, "latencies_ms": [11549.244], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photo, a man is standing on a street, holding a bicycle. He is wearing a towel around his neck, suggesting he might have been cycling in the heat. The street is lined with various shops and restaurants, each with their own unique signage. The man is positioned in front of a shop with a sign that reads \"", "error": null, "sys_before": {"cpu_percent": 11.8, "ram_used_mb": 21087.1, "ram_available_mb": 41753.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21038.4, "ram_available_mb": 41802.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.28}, "power_stats": {"power_gpu_soc_mean_watts": 19.328, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.302, "gpu_utilization_percent_mean": 69.28, "power_watts_avg": 19.328, "energy_joules_est": 223.24, "duration_seconds": 11.55, "sample_count": 100}, "timestamp": "2026-01-26T16:11:45.632199"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7887.037, "latencies_ms": [7887.037], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "bicycle: 1, building: 2, umbrella: 1, person: 2, sign: 3, window: 1, door: 1, street light: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21038.4, "ram_available_mb": 41802.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21082.9, "ram_available_mb": 41758.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.716}, "power_stats": {"power_gpu_soc_mean_watts": 21.013, "power_cpu_cv_mean_watts": 1.613, "power_sys_5v0_mean_watts": 8.196, "gpu_utilization_percent_mean": 71.716, "power_watts_avg": 21.013, "energy_joules_est": 165.74, "duration_seconds": 7.888, "sample_count": 67}, "timestamp": "2026-01-26T16:11:55.533109"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11550.518, "latencies_ms": [11550.518], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 80, "n_tiles": 16, "output_text": "In the foreground, a man is standing with a bicycle, positioned on the right side of the image. The bicycle is near the center of the image, with the man standing to its right. In the background, there is a building with Chinese characters on it, and a person holding an umbrella is partially visible behind the man with the bicycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.9, "ram_available_mb": 41758.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21091.9, "ram_available_mb": 41749.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.071}, "power_stats": {"power_gpu_soc_mean_watts": 19.318, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.322, "gpu_utilization_percent_mean": 70.071, "power_watts_avg": 19.318, "energy_joules_est": 223.14, "duration_seconds": 11.551, "sample_count": 99}, "timestamp": "2026-01-26T16:12:09.125975"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7773.358, "latencies_ms": [7773.358], "images_per_second": 0.129, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A man is standing on a street holding a bicycle, with a woman holding an umbrella in the background. The setting appears to be a commercial area with shops and signage in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21091.9, "ram_available_mb": 41749.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21093.1, "ram_available_mb": 41747.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.53}, "power_stats": {"power_gpu_soc_mean_watts": 21.219, "power_cpu_cv_mean_watts": 1.565, "power_sys_5v0_mean_watts": 8.22, "gpu_utilization_percent_mean": 73.53, "power_watts_avg": 21.219, "energy_joules_est": 164.96, "duration_seconds": 7.774, "sample_count": 66}, "timestamp": "2026-01-26T16:12:18.960659"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7540.407, "latencies_ms": [7540.407], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image is in black and white, featuring a man with an umbrella and a bicycle. The weather appears to be overcast, as the sky is grey and the overall lighting is dim.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21030.5, "ram_available_mb": 41810.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21082.6, "ram_available_mb": 41758.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.857}, "power_stats": {"power_gpu_soc_mean_watts": 20.931, "power_cpu_cv_mean_watts": 1.633, "power_sys_5v0_mean_watts": 8.274, "gpu_utilization_percent_mean": 71.857, "power_watts_avg": 20.931, "energy_joules_est": 157.84, "duration_seconds": 7.541, "sample_count": 63}, "timestamp": "2026-01-26T16:12:28.513893"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12367.667, "latencies_ms": [12367.667], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a market stall with a large number of bananas hanging from the ceiling. There are several bunches of bananas suspended in various positions, creating an abundant display. The bananas are hanging in different sizes and are spread across the entire ceiling, giving the impression of a bountiful harvest. The stall appears to be a fruit stand,", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 21082.6, "ram_available_mb": 41758.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21083.0, "ram_available_mb": 41757.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.198}, "power_stats": {"power_gpu_soc_mean_watts": 21.515, "power_cpu_cv_mean_watts": 1.809, "power_sys_5v0_mean_watts": 8.549, "gpu_utilization_percent_mean": 73.198, "power_watts_avg": 21.515, "energy_joules_est": 266.1, "duration_seconds": 12.368, "sample_count": 106}, "timestamp": "2026-01-26T16:12:42.937924"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11869.226, "latencies_ms": [11869.226], "images_per_second": 0.084, "prompt_tokens": 39, "response_tokens_est": 70, "n_tiles": 16, "output_text": "Banana: 100\nBanana bunch: 10\nBanana tree: 1\nBanana curtain: 1\nBanana bowl: 1\nBanana curtain: 1\nBanana curtain: 1\nBanana curtain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21083.0, "ram_available_mb": 41757.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21099.3, "ram_available_mb": 41741.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.188}, "power_stats": {"power_gpu_soc_mean_watts": 21.947, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.44, "gpu_utilization_percent_mean": 74.188, "power_watts_avg": 21.947, "energy_joules_est": 260.51, "duration_seconds": 11.87, "sample_count": 101}, "timestamp": "2026-01-26T16:12:56.868270"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9757.1, "latencies_ms": [9757.1], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The bananas are hanging in bunches from the ceiling, with some bunches closer to the foreground and others further away, creating a sense of depth. The background features a wall and a door, providing context for the location of the bananas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21037.9, "ram_available_mb": 41802.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21074.8, "ram_available_mb": 41766.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.542}, "power_stats": {"power_gpu_soc_mean_watts": 22.597, "power_cpu_cv_mean_watts": 1.582, "power_sys_5v0_mean_watts": 8.47, "gpu_utilization_percent_mean": 74.542, "power_watts_avg": 22.597, "energy_joules_est": 220.5, "duration_seconds": 9.758, "sample_count": 83}, "timestamp": "2026-01-26T16:13:08.651262"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8145.533, "latencies_ms": [8145.533], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image depicts a market scene with a bunch of bananas hanging from the ceiling. The bananas are arranged in a way that they are easily accessible for customers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21074.8, "ram_available_mb": 41766.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 21076.3, "ram_available_mb": 41764.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.246}, "power_stats": {"power_gpu_soc_mean_watts": 23.666, "power_cpu_cv_mean_watts": 1.369, "power_sys_5v0_mean_watts": 8.373, "gpu_utilization_percent_mean": 78.246, "power_watts_avg": 23.666, "energy_joules_est": 192.79, "duration_seconds": 8.146, "sample_count": 69}, "timestamp": "2026-01-26T16:13:18.817353"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8267.977, "latencies_ms": [8267.977], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The bananas are hanging in bunches and appear to be ripe and ready to eat. The lighting in the image is bright and natural, suggesting that it was taken during the day.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21076.3, "ram_available_mb": 41764.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 21077.3, "ram_available_mb": 41763.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.2}, "power_stats": {"power_gpu_soc_mean_watts": 23.151, "power_cpu_cv_mean_watts": 1.407, "power_sys_5v0_mean_watts": 8.466, "gpu_utilization_percent_mean": 78.2, "power_watts_avg": 23.151, "energy_joules_est": 191.43, "duration_seconds": 8.269, "sample_count": 70}, "timestamp": "2026-01-26T16:13:29.150528"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11575.289, "latencies_ms": [11575.289], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene rural landscape, dominated by a green field that stretches out to meet a backdrop of majestic mountains. A train, painted in vibrant shades of red and green, is the focal point of the scene. It's moving from the left to the right of the frame, adding a dynamic element to the otherwise tranquil setting", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 21077.3, "ram_available_mb": 41763.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21077.6, "ram_available_mb": 41763.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.49}, "power_stats": {"power_gpu_soc_mean_watts": 19.283, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.317, "gpu_utilization_percent_mean": 70.49, "power_watts_avg": 19.283, "energy_joules_est": 223.22, "duration_seconds": 11.576, "sample_count": 100}, "timestamp": "2026-01-26T16:13:42.764774"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7944.063, "latencies_ms": [7944.063], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "Train: 1\nCar: 2\nHouse: 1\nTrees: 1\nPower line: 1\nMountain: 1\nHouse: 1\nHouse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21077.6, "ram_available_mb": 41763.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21077.8, "ram_available_mb": 41763.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.426}, "power_stats": {"power_gpu_soc_mean_watts": 21.222, "power_cpu_cv_mean_watts": 1.619, "power_sys_5v0_mean_watts": 8.251, "gpu_utilization_percent_mean": 74.426, "power_watts_avg": 21.222, "energy_joules_est": 168.6, "duration_seconds": 7.945, "sample_count": 68}, "timestamp": "2026-01-26T16:13:52.747605"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9834.638, "latencies_ms": [9834.638], "images_per_second": 0.102, "prompt_tokens": 44, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The train is positioned in the foreground on the left side of the image, moving from left to right. The telephone pole is situated in the foreground on the right side, standing vertically. The hills are in the background, creating a sense of depth behind the train and the telephone pole.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21077.8, "ram_available_mb": 41763.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21077.5, "ram_available_mb": 41763.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.369}, "power_stats": {"power_gpu_soc_mean_watts": 19.835, "power_cpu_cv_mean_watts": 1.801, "power_sys_5v0_mean_watts": 8.281, "gpu_utilization_percent_mean": 70.369, "power_watts_avg": 19.835, "energy_joules_est": 195.08, "duration_seconds": 9.835, "sample_count": 84}, "timestamp": "2026-01-26T16:14:04.598356"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7971.29, "latencies_ms": [7971.29], "images_per_second": 0.125, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "A green and white train with red carriages is traveling through a rural countryside with a mountainous backdrop. The train is passing by a grassy field with a wooden utility pole in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21077.5, "ram_available_mb": 41763.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21091.2, "ram_available_mb": 41749.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.739}, "power_stats": {"power_gpu_soc_mean_watts": 21.082, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.229, "gpu_utilization_percent_mean": 73.739, "power_watts_avg": 21.082, "energy_joules_est": 168.06, "duration_seconds": 7.972, "sample_count": 69}, "timestamp": "2026-01-26T16:14:14.596357"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6520.109, "latencies_ms": [6520.109], "images_per_second": 0.153, "prompt_tokens": 36, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A green and white train with red carriages is on tracks through a grassy field. The sky is blue with some clouds, and there are mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21029.5, "ram_available_mb": 41811.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21075.8, "ram_available_mb": 41765.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.589}, "power_stats": {"power_gpu_soc_mean_watts": 21.725, "power_cpu_cv_mean_watts": 1.522, "power_sys_5v0_mean_watts": 8.289, "gpu_utilization_percent_mean": 74.589, "power_watts_avg": 21.725, "energy_joules_est": 141.66, "duration_seconds": 6.521, "sample_count": 56}, "timestamp": "2026-01-26T16:14:23.153509"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11588.474, "latencies_ms": [11588.474], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment on a beach where a man is standing with his left arm raised, possibly in a gesture of excitement or celebration. He is dressed in a dark shirt, khaki shorts, and a hat, suggesting a casual, sunny day at the beach. The ocean is visible in the background with waves crashing onto the shore, indicating a coastal", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 21075.8, "ram_available_mb": 41765.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21075.8, "ram_available_mb": 41765.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.616}, "power_stats": {"power_gpu_soc_mean_watts": 19.288, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.294, "gpu_utilization_percent_mean": 70.616, "power_watts_avg": 19.288, "energy_joules_est": 223.53, "duration_seconds": 11.589, "sample_count": 99}, "timestamp": "2026-01-26T16:14:36.781727"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8357.179, "latencies_ms": [8357.179], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "person: 1, arm: 1, kite: 1, kite tail: 1, kite string: 1, kite: 1, kite tail: 1, kite: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21075.8, "ram_available_mb": 41765.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21077.0, "ram_available_mb": 41763.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.366}, "power_stats": {"power_gpu_soc_mean_watts": 20.813, "power_cpu_cv_mean_watts": 1.646, "power_sys_5v0_mean_watts": 8.237, "gpu_utilization_percent_mean": 73.366, "power_watts_avg": 20.813, "energy_joules_est": 173.95, "duration_seconds": 8.358, "sample_count": 71}, "timestamp": "2026-01-26T16:14:47.173167"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7662.892, "latencies_ms": [7662.892], "images_per_second": 0.13, "prompt_tokens": 44, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The person is standing in the foreground on the left side of the image, near the edge of the water. The kite is in the background, flying high in the sky on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21077.0, "ram_available_mb": 41763.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21077.0, "ram_available_mb": 41763.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.846}, "power_stats": {"power_gpu_soc_mean_watts": 21.027, "power_cpu_cv_mean_watts": 1.626, "power_sys_5v0_mean_watts": 8.28, "gpu_utilization_percent_mean": 71.846, "power_watts_avg": 21.027, "energy_joules_est": 161.14, "duration_seconds": 7.663, "sample_count": 65}, "timestamp": "2026-01-26T16:14:56.895836"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8638.917, "latencies_ms": [8638.917], "images_per_second": 0.116, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A person is standing on a sandy beach with their arms raised, possibly celebrating or enjoying the moment. The ocean is visible in the background with waves coming in, and there is a colorful kite flying in the sky to the right.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21077.0, "ram_available_mb": 41763.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.6, "ram_used_mb": 21082.1, "ram_available_mb": 41758.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.459}, "power_stats": {"power_gpu_soc_mean_watts": 20.74, "power_cpu_cv_mean_watts": 1.942, "power_sys_5v0_mean_watts": 8.267, "gpu_utilization_percent_mean": 71.459, "power_watts_avg": 20.74, "energy_joules_est": 179.18, "duration_seconds": 8.64, "sample_count": 74}, "timestamp": "2026-01-26T16:15:07.574779"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7539.323, "latencies_ms": [7539.323], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The image shows a person standing on a sandy beach with the ocean waves in the background. The sky is partly cloudy, and the person is wearing a dark shirt and khaki shorts.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 21082.1, "ram_available_mb": 41758.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21082.3, "ram_available_mb": 41758.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.359}, "power_stats": {"power_gpu_soc_mean_watts": 20.96, "power_cpu_cv_mean_watts": 1.632, "power_sys_5v0_mean_watts": 8.273, "gpu_utilization_percent_mean": 72.359, "power_watts_avg": 20.96, "energy_joules_est": 158.04, "duration_seconds": 7.54, "sample_count": 64}, "timestamp": "2026-01-26T16:15:17.176061"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11584.945, "latencies_ms": [11584.945], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene garden scene, where two terracotta pots cradle a vibrant green plant. The pots, with their orange hue, stand out against the backdrop of the earthy brown soil. The plant, a lush green, is in full bloom with large leaves that reach out, almost as if in a welcoming gesture. The", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 21082.3, "ram_available_mb": 41758.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21082.3, "ram_available_mb": 41758.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.717}, "power_stats": {"power_gpu_soc_mean_watts": 19.278, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.307, "gpu_utilization_percent_mean": 69.717, "power_watts_avg": 19.278, "energy_joules_est": 223.35, "duration_seconds": 11.586, "sample_count": 99}, "timestamp": "2026-01-26T16:15:30.794918"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9029.578, "latencies_ms": [9029.578], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Broccoli plant: 2\n- Leafy greens: 2\n- Plants: 2\n- Pot: 2\n- Soil: 2\n- Flower: 1\n- Potting tray: 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21082.3, "ram_available_mb": 41758.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21083.0, "ram_available_mb": 41757.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.273}, "power_stats": {"power_gpu_soc_mean_watts": 20.436, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 8.21, "gpu_utilization_percent_mean": 72.273, "power_watts_avg": 20.436, "energy_joules_est": 184.54, "duration_seconds": 9.03, "sample_count": 77}, "timestamp": "2026-01-26T16:15:41.865495"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11679.06, "latencies_ms": [11679.06], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there are two terracotta pots placed close to each other, each containing a plant with large, green leaves. The plants appear to be in the early stages of growth, with small clusters of broccoli forming at the top of the leaves. In the background, there is a sandy ground surface, and a small pink flower can be seen at the top", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 21083.0, "ram_available_mb": 41757.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21083.0, "ram_available_mb": 41757.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.255}, "power_stats": {"power_gpu_soc_mean_watts": 19.061, "power_cpu_cv_mean_watts": 1.871, "power_sys_5v0_mean_watts": 8.258, "gpu_utilization_percent_mean": 70.255, "power_watts_avg": 19.061, "energy_joules_est": 222.63, "duration_seconds": 11.68, "sample_count": 98}, "timestamp": "2026-01-26T16:15:55.559014"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10777.147, "latencies_ms": [10777.147], "images_per_second": 0.093, "prompt_tokens": 37, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The image shows a gardening scene with two terracotta pots containing leafy green plants, possibly kale or a similar variety, growing in a sandy or gravelly soil. The plants are healthy and appear to be well-maintained, suggesting they are being cared for in a garden or a controlled environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21083.0, "ram_available_mb": 41757.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21083.5, "ram_available_mb": 41757.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.882}, "power_stats": {"power_gpu_soc_mean_watts": 19.525, "power_cpu_cv_mean_watts": 1.829, "power_sys_5v0_mean_watts": 8.242, "gpu_utilization_percent_mean": 70.882, "power_watts_avg": 19.525, "energy_joules_est": 210.44, "duration_seconds": 10.778, "sample_count": 93}, "timestamp": "2026-01-26T16:16:08.393097"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9367.538, "latencies_ms": [9367.538], "images_per_second": 0.107, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image shows a garden with two large, healthy green plants with large leaves, possibly kale or a similar leafy green, growing in terracotta pots. The lighting is natural and appears to be daylight, suggesting the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21083.5, "ram_available_mb": 41757.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21083.5, "ram_available_mb": 41757.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.325}, "power_stats": {"power_gpu_soc_mean_watts": 20.04, "power_cpu_cv_mean_watts": 1.776, "power_sys_5v0_mean_watts": 8.278, "gpu_utilization_percent_mean": 71.325, "power_watts_avg": 20.04, "energy_joules_est": 187.74, "duration_seconds": 9.368, "sample_count": 80}, "timestamp": "2026-01-26T16:16:19.773728"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11554.58, "latencies_ms": [11554.58], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young boy is sitting on a brown and white pony, while an older man stands nearby, holding the pony's reins. The boy appears to be enjoying the experience of riding the pony, and the older man is likely the pony's owner or trainer. \n\nThe scene takes place in front of a house, with a", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21083.5, "ram_available_mb": 41757.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21103.0, "ram_available_mb": 41737.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.05}, "power_stats": {"power_gpu_soc_mean_watts": 19.315, "power_cpu_cv_mean_watts": 1.901, "power_sys_5v0_mean_watts": 8.326, "gpu_utilization_percent_mean": 71.05, "power_watts_avg": 19.315, "energy_joules_est": 223.19, "duration_seconds": 11.555, "sample_count": 100}, "timestamp": "2026-01-26T16:16:33.362792"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7436.898, "latencies_ms": [7436.898], "images_per_second": 0.134, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "boy: 1, horse: 1, man: 1, rope: 1, chair: 1, building: 1, window: 2, door: 1", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21103.0, "ram_available_mb": 41737.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21104.5, "ram_available_mb": 41736.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.27}, "power_stats": {"power_gpu_soc_mean_watts": 21.402, "power_cpu_cv_mean_watts": 1.531, "power_sys_5v0_mean_watts": 8.177, "gpu_utilization_percent_mean": 74.27, "power_watts_avg": 21.402, "energy_joules_est": 159.18, "duration_seconds": 7.438, "sample_count": 63}, "timestamp": "2026-01-26T16:16:42.818055"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11562.706, "latencies_ms": [11562.706], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "In the foreground, a young boy is seated on a small brown and white pony, positioned near the center of the image. To the right, an older man stands holding the pony's lead rope, appearing to guide or assist the boy. The background features a red building with white trim and windows, providing a vibrant backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21104.5, "ram_available_mb": 41736.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21052.3, "ram_available_mb": 41788.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.081}, "power_stats": {"power_gpu_soc_mean_watts": 19.312, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.315, "gpu_utilization_percent_mean": 70.081, "power_watts_avg": 19.312, "energy_joules_est": 223.31, "duration_seconds": 11.563, "sample_count": 99}, "timestamp": "2026-01-26T16:16:56.423833"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7225.938, "latencies_ms": [7225.938], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A young boy is sitting on a small pony while an older man holds the reins and guides the pony. They are standing in front of a red building with white windows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21052.3, "ram_available_mb": 41788.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21094.6, "ram_available_mb": 41746.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.508}, "power_stats": {"power_gpu_soc_mean_watts": 21.618, "power_cpu_cv_mean_watts": 1.509, "power_sys_5v0_mean_watts": 8.191, "gpu_utilization_percent_mean": 74.508, "power_watts_avg": 21.618, "energy_joules_est": 156.22, "duration_seconds": 7.227, "sample_count": 61}, "timestamp": "2026-01-26T16:17:05.676318"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8096.053, "latencies_ms": [8096.053], "images_per_second": 0.124, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a bright and sunny day with clear skies, casting shadows on the ground. The main subjects, an older man and a young boy, are standing on a paved area with a red building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21031.1, "ram_available_mb": 41809.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21071.0, "ram_available_mb": 41769.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.214}, "power_stats": {"power_gpu_soc_mean_watts": 20.663, "power_cpu_cv_mean_watts": 1.681, "power_sys_5v0_mean_watts": 8.292, "gpu_utilization_percent_mean": 73.214, "power_watts_avg": 20.663, "energy_joules_est": 167.3, "duration_seconds": 8.097, "sample_count": 70}, "timestamp": "2026-01-26T16:17:15.793707"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11585.493, "latencies_ms": [11585.493], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young boy is walking through a field of blue flowers. He is wearing a striped shirt and blue jeans, and he is holding a teddy bear in his hand. The boy is walking on a dirt path, and the field of blue flowers is on either side of him. The flowers are in full bloom, creating a beautiful and serene atmosphere", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 21071.0, "ram_available_mb": 41769.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21071.2, "ram_available_mb": 41769.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.765}, "power_stats": {"power_gpu_soc_mean_watts": 19.301, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.288, "gpu_utilization_percent_mean": 69.765, "power_watts_avg": 19.301, "energy_joules_est": 223.62, "duration_seconds": 11.586, "sample_count": 98}, "timestamp": "2026-01-26T16:17:29.426919"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8908.918, "latencies_ms": [8908.918], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "child: 1, striped shirt: 1, blue jeans: 1, brown teddy bear: 1, blue flowers: numerous, green plants: numerous, dirt path: 1, sunlight: visible, trees: visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21071.2, "ram_available_mb": 41769.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21036.8, "ram_available_mb": 41804.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.867}, "power_stats": {"power_gpu_soc_mean_watts": 20.54, "power_cpu_cv_mean_watts": 1.681, "power_sys_5v0_mean_watts": 8.234, "gpu_utilization_percent_mean": 72.867, "power_watts_avg": 20.54, "energy_joules_est": 183.0, "duration_seconds": 8.91, "sample_count": 75}, "timestamp": "2026-01-26T16:17:40.363815"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11234.002, "latencies_ms": [11234.002], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The boy is standing in the foreground on a dirt path, facing away from the camera, with a field of blue flowers to his left and a blurred background that suggests a wooded area. The boy is positioned to the right of the frame, and the flowers are densely packed, creating a vibrant contrast with the dirt path.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21036.8, "ram_available_mb": 41804.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21049.8, "ram_available_mb": 41791.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.337}, "power_stats": {"power_gpu_soc_mean_watts": 19.298, "power_cpu_cv_mean_watts": 1.867, "power_sys_5v0_mean_watts": 8.283, "gpu_utilization_percent_mean": 70.337, "power_watts_avg": 19.298, "energy_joules_est": 216.81, "duration_seconds": 11.235, "sample_count": 95}, "timestamp": "2026-01-26T16:17:53.616351"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6734.594, "latencies_ms": [6734.594], "images_per_second": 0.148, "prompt_tokens": 37, "response_tokens_est": 35, "n_tiles": 16, "output_text": "A young child is walking through a field of bluebells, holding a teddy bear. The child is wearing a striped shirt and blue jeans.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21049.8, "ram_available_mb": 41791.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21073.5, "ram_available_mb": 41767.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.754}, "power_stats": {"power_gpu_soc_mean_watts": 22.064, "power_cpu_cv_mean_watts": 1.446, "power_sys_5v0_mean_watts": 8.189, "gpu_utilization_percent_mean": 74.754, "power_watts_avg": 22.064, "energy_joules_est": 148.61, "duration_seconds": 6.735, "sample_count": 57}, "timestamp": "2026-01-26T16:18:02.403750"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10637.269, "latencies_ms": [10637.269], "images_per_second": 0.094, "prompt_tokens": 36, "response_tokens_est": 71, "n_tiles": 16, "output_text": "The image features a child in a field of vibrant blue flowers under bright sunlight, creating a serene and colorful outdoor scene. The child is wearing a striped blue and white shirt and jeans, and is holding a brown teddy bear, adding a sense of innocence and playfulness to the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21073.5, "ram_available_mb": 41767.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21074.5, "ram_available_mb": 41766.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.511}, "power_stats": {"power_gpu_soc_mean_watts": 19.606, "power_cpu_cv_mean_watts": 1.845, "power_sys_5v0_mean_watts": 8.314, "gpu_utilization_percent_mean": 70.511, "power_watts_avg": 19.606, "energy_joules_est": 208.57, "duration_seconds": 10.638, "sample_count": 90}, "timestamp": "2026-01-26T16:18:15.103789"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11633.096, "latencies_ms": [11633.096], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a single orange placed on the side of a road, with a parking lot in the background. The orange is positioned in the center of the scene, drawing attention to itself. The parking lot is filled with various cars, including a white car and a black car, parked in different spots. The cars are of different sizes and colors, creating a diverse and busy", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 21074.5, "ram_available_mb": 41766.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21082.7, "ram_available_mb": 41758.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.677}, "power_stats": {"power_gpu_soc_mean_watts": 19.206, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.285, "gpu_utilization_percent_mean": 69.677, "power_watts_avg": 19.206, "energy_joules_est": 223.44, "duration_seconds": 11.634, "sample_count": 99}, "timestamp": "2026-01-26T16:18:28.796623"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10048.24, "latencies_ms": [10048.24], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 64, "n_tiles": 16, "output_text": "1. Orange: 1\n2. Cars: 10\n3. Trees: 4\n4. Parking lot: 1\n5. White line: 1\n6. Road: 1\n7. Clouds: 1\n8. Sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.7, "ram_available_mb": 41758.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21083.7, "ram_available_mb": 41757.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.765}, "power_stats": {"power_gpu_soc_mean_watts": 20.01, "power_cpu_cv_mean_watts": 1.77, "power_sys_5v0_mean_watts": 8.246, "gpu_utilization_percent_mean": 71.765, "power_watts_avg": 20.01, "energy_joules_est": 201.08, "duration_seconds": 10.049, "sample_count": 85}, "timestamp": "2026-01-26T16:18:40.877574"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11562.064, "latencies_ms": [11562.064], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The main object, an orange, is positioned in the foreground on the left side of the image, standing out against the asphalt. In the background, there is a row of cars parked parallel to the road, which extends from the middle to the right side of the image. The sky is visible in the upper part of the image, indicating that the scene is set outdo", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21083.7, "ram_available_mb": 41757.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21084.2, "ram_available_mb": 41756.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.636}, "power_stats": {"power_gpu_soc_mean_watts": 19.331, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.306, "gpu_utilization_percent_mean": 70.636, "power_watts_avg": 19.331, "energy_joules_est": 223.52, "duration_seconds": 11.563, "sample_count": 99}, "timestamp": "2026-01-26T16:18:54.460824"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7294.813, "latencies_ms": [7294.813], "images_per_second": 0.137, "prompt_tokens": 37, "response_tokens_est": 40, "n_tiles": 16, "output_text": "An orange is placed on the side of a road, with cars parked in the background. The sky is overcast, and the overall scene appears to be a quiet street with minimal traffic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21084.2, "ram_available_mb": 41756.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21084.4, "ram_available_mb": 41756.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.016}, "power_stats": {"power_gpu_soc_mean_watts": 21.59, "power_cpu_cv_mean_watts": 1.497, "power_sys_5v0_mean_watts": 8.216, "gpu_utilization_percent_mean": 74.016, "power_watts_avg": 21.59, "energy_joules_est": 157.51, "duration_seconds": 7.295, "sample_count": 62}, "timestamp": "2026-01-26T16:19:03.777815"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9263.447, "latencies_ms": [9263.447], "images_per_second": 0.108, "prompt_tokens": 36, "response_tokens_est": 59, "n_tiles": 16, "output_text": "The image features a single, large, and round orange with a rough texture, placed on a gray asphalt surface. The lighting is soft and diffused, suggesting an overcast day, and the background shows a parking lot with cars and trees, indicating an urban setting.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21084.4, "ram_available_mb": 41756.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 9.6, "ram_used_mb": 21093.6, "ram_available_mb": 41747.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.494}, "power_stats": {"power_gpu_soc_mean_watts": 19.91, "power_cpu_cv_mean_watts": 2.133, "power_sys_5v0_mean_watts": 8.301, "gpu_utilization_percent_mean": 69.494, "power_watts_avg": 19.91, "energy_joules_est": 184.45, "duration_seconds": 9.264, "sample_count": 79}, "timestamp": "2026-01-26T16:19:15.080899"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12370.387, "latencies_ms": [12370.387], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seated at a wooden table, wearing a gray suit and a white shirt. He is smiling and has a bowl of food in front of him. On the table, there are two bottles of beer, a glass of beer, and a set of keys. The background is a plain white wall. The man's face is", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 21093.6, "ram_available_mb": 41747.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 21096.9, "ram_available_mb": 41744.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.009}, "power_stats": {"power_gpu_soc_mean_watts": 21.497, "power_cpu_cv_mean_watts": 1.997, "power_sys_5v0_mean_watts": 8.576, "gpu_utilization_percent_mean": 73.009, "power_watts_avg": 21.497, "energy_joules_est": 265.94, "duration_seconds": 12.371, "sample_count": 106}, "timestamp": "2026-01-26T16:19:29.484000"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8996.018, "latencies_ms": [8996.018], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "bowl: 1, bottle: 2, keys: 5, remote control: 1, wallet: 1, glass: 1, spoon: 1, tie: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21096.9, "ram_available_mb": 41744.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21097.4, "ram_available_mb": 41743.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.539}, "power_stats": {"power_gpu_soc_mean_watts": 23.081, "power_cpu_cv_mean_watts": 1.448, "power_sys_5v0_mean_watts": 8.385, "gpu_utilization_percent_mean": 77.539, "power_watts_avg": 23.081, "energy_joules_est": 207.65, "duration_seconds": 8.997, "sample_count": 76}, "timestamp": "2026-01-26T16:19:40.494556"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9910.445, "latencies_ms": [9910.445], "images_per_second": 0.101, "prompt_tokens": 44, "response_tokens_est": 55, "n_tiles": 16, "output_text": "In the foreground, there is a wooden table with a bowl in the center, a bottle of beer to the right, and a set of keys to the left. The background features a plain wall and a partial view of another person's arm.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21097.4, "ram_available_mb": 41743.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21098.9, "ram_available_mb": 41742.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.952}, "power_stats": {"power_gpu_soc_mean_watts": 22.399, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.511, "gpu_utilization_percent_mean": 75.952, "power_watts_avg": 22.399, "energy_joules_est": 222.0, "duration_seconds": 9.911, "sample_count": 84}, "timestamp": "2026-01-26T16:19:52.452706"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9339.783, "latencies_ms": [9339.783], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 48, "n_tiles": 16, "output_text": "A man is seated at a table with a bowl of food in front of him, accompanied by two bottles of beer. The setting appears to be a casual dining environment, possibly a restaurant or a home.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21098.9, "ram_available_mb": 41742.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21043.7, "ram_available_mb": 41797.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.519}, "power_stats": {"power_gpu_soc_mean_watts": 22.95, "power_cpu_cv_mean_watts": 1.495, "power_sys_5v0_mean_watts": 8.387, "gpu_utilization_percent_mean": 76.519, "power_watts_avg": 22.95, "energy_joules_est": 214.36, "duration_seconds": 9.34, "sample_count": 79}, "timestamp": "2026-01-26T16:20:03.835030"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10665.008, "latencies_ms": [10665.008], "images_per_second": 0.094, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image shows a man sitting at a table with a bowl of food in front of him. The table is made of wood and there are two bottles of beer on the table. The man is wearing a gray suit and a white shirt. The background is a plain white wall.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21043.7, "ram_available_mb": 41797.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21089.2, "ram_available_mb": 41751.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.978}, "power_stats": {"power_gpu_soc_mean_watts": 22.184, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.512, "gpu_utilization_percent_mean": 74.978, "power_watts_avg": 22.184, "energy_joules_est": 236.61, "duration_seconds": 10.666, "sample_count": 91}, "timestamp": "2026-01-26T16:20:16.550478"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11576.459, "latencies_ms": [11576.459], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a neatly made bed in a bedroom. The bed is covered with white sheets and pillows, creating a clean and inviting atmosphere. A chair is placed next to the bed, providing a comfortable spot for relaxation or reading. A window can be seen in the background, allowing natural light to enter the room.\n\nThere is a suitcase placed on the floor near", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 21089.1, "ram_available_mb": 41751.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21089.9, "ram_available_mb": 41751.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.192}, "power_stats": {"power_gpu_soc_mean_watts": 19.275, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.315, "gpu_utilization_percent_mean": 70.192, "power_watts_avg": 19.275, "energy_joules_est": 223.15, "duration_seconds": 11.577, "sample_count": 99}, "timestamp": "2026-01-26T16:20:30.160207"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8580.147, "latencies_ms": [8580.147], "images_per_second": 0.117, "prompt_tokens": 39, "response_tokens_est": 51, "n_tiles": 16, "output_text": "bed: 1\npillow: 4\nchair: 1\ndesk: 1\nphone: 1\nlamp: 1\nwindow: 1\nframe: 1\ncarpet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21089.9, "ram_available_mb": 41751.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21089.6, "ram_available_mb": 41751.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.425}, "power_stats": {"power_gpu_soc_mean_watts": 20.567, "power_cpu_cv_mean_watts": 1.655, "power_sys_5v0_mean_watts": 8.236, "gpu_utilization_percent_mean": 73.425, "power_watts_avg": 20.567, "energy_joules_est": 176.48, "duration_seconds": 8.581, "sample_count": 73}, "timestamp": "2026-01-26T16:20:40.753070"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10337.121, "latencies_ms": [10337.121], "images_per_second": 0.097, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "The bed is positioned in the foreground of the image, taking up a significant portion of the space. It is placed against a wall with a window above it, allowing natural light to enter the room. The desk and chair are located to the left of the bed, creating a workspace adjacent to the sleeping area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21089.6, "ram_available_mb": 41751.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21090.1, "ram_available_mb": 41750.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.101}, "power_stats": {"power_gpu_soc_mean_watts": 19.484, "power_cpu_cv_mean_watts": 1.844, "power_sys_5v0_mean_watts": 8.285, "gpu_utilization_percent_mean": 71.101, "power_watts_avg": 19.484, "energy_joules_est": 201.42, "duration_seconds": 10.338, "sample_count": 89}, "timestamp": "2026-01-26T16:20:53.126602"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8665.001, "latencies_ms": [8665.001], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image shows a neatly made bed in a small, well-lit room with a window on the wall. There is a desk with a chair and a phone on it, and a suitcase is placed on the floor next to the bed.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21090.1, "ram_available_mb": 41750.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21090.1, "ram_available_mb": 41750.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.658}, "power_stats": {"power_gpu_soc_mean_watts": 20.735, "power_cpu_cv_mean_watts": 1.672, "power_sys_5v0_mean_watts": 8.24, "gpu_utilization_percent_mean": 72.658, "power_watts_avg": 20.735, "energy_joules_est": 179.68, "duration_seconds": 8.666, "sample_count": 73}, "timestamp": "2026-01-26T16:21:03.827583"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8113.631, "latencies_ms": [8113.631], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The room is well-lit with natural light coming from the window, which shows a clear sky outside. The bed is neatly made with white linens, and there is a white desk with a phone and some items on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21090.1, "ram_available_mb": 41750.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21090.6, "ram_available_mb": 41750.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.42}, "power_stats": {"power_gpu_soc_mean_watts": 20.625, "power_cpu_cv_mean_watts": 1.67, "power_sys_5v0_mean_watts": 8.269, "gpu_utilization_percent_mean": 72.42, "power_watts_avg": 20.625, "energy_joules_est": 167.36, "duration_seconds": 8.114, "sample_count": 69}, "timestamp": "2026-01-26T16:21:13.966597"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11546.382, "latencies_ms": [11546.382], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of the image, a trio of plush toys are gathered, each with its own unique charm. The bear, a symbol of warmth and comfort, is adorned with a vibrant green hat and a red scarf, adding a pop of color to its soft brown fur. It sits next to a snowman, who is dressed in a classic white", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21090.6, "ram_available_mb": 41750.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21090.6, "ram_available_mb": 41750.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.061}, "power_stats": {"power_gpu_soc_mean_watts": 19.24, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.289, "gpu_utilization_percent_mean": 69.061, "power_watts_avg": 19.24, "energy_joules_est": 222.17, "duration_seconds": 11.547, "sample_count": 99}, "timestamp": "2026-01-26T16:21:27.566860"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8914.909, "latencies_ms": [8914.909], "images_per_second": 0.112, "prompt_tokens": 39, "response_tokens_est": 54, "n_tiles": 16, "output_text": "bear: 2, coca cola hat: 1, snowman: 1, berlin tag: 1, red scarf: 1, green hat: 1, orange background: 1, blue floral pattern: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21090.6, "ram_available_mb": 41750.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21091.1, "ram_available_mb": 41749.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.896}, "power_stats": {"power_gpu_soc_mean_watts": 20.67, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 8.251, "gpu_utilization_percent_mean": 72.896, "power_watts_avg": 20.67, "energy_joules_est": 184.29, "duration_seconds": 8.916, "sample_count": 77}, "timestamp": "2026-01-26T16:21:38.510657"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11655.912, "latencies_ms": [11655.912], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a teddy bear wearing a green hat and a red scarf, sitting next to a stuffed bear wearing a red hat and a green scarf. In the background, there is a snowman wearing a black hat and a red scarf, sitting next to a teddy bear wearing a green hat and a red scarf. The snow", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21091.1, "ram_available_mb": 41749.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21092.1, "ram_available_mb": 41748.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.24}, "power_stats": {"power_gpu_soc_mean_watts": 19.44, "power_cpu_cv_mean_watts": 1.889, "power_sys_5v0_mean_watts": 8.293, "gpu_utilization_percent_mean": 69.24, "power_watts_avg": 19.44, "energy_joules_est": 226.6, "duration_seconds": 11.656, "sample_count": 100}, "timestamp": "2026-01-26T16:21:52.195212"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8107.09, "latencies_ms": [8107.09], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "In the image, a teddy bear is sitting on a bed with a red and green floral background. The teddy bear is wearing a green hat and a red scarf, and is holding a black bottle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21092.1, "ram_available_mb": 41748.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21092.6, "ram_available_mb": 41748.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.265}, "power_stats": {"power_gpu_soc_mean_watts": 21.237, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.231, "gpu_utilization_percent_mean": 73.265, "power_watts_avg": 21.237, "energy_joules_est": 172.18, "duration_seconds": 8.108, "sample_count": 68}, "timestamp": "2026-01-26T16:22:02.323720"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7843.243, "latencies_ms": [7843.243], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image features a plush teddy bear wearing a green hat and a red scarf, sitting next to a snowman plush toy. The background has a floral pattern with orange and pink colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21092.6, "ram_available_mb": 41748.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21092.8, "ram_available_mb": 41748.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.136}, "power_stats": {"power_gpu_soc_mean_watts": 21.204, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.255, "gpu_utilization_percent_mean": 73.136, "power_watts_avg": 21.204, "energy_joules_est": 166.32, "duration_seconds": 7.844, "sample_count": 66}, "timestamp": "2026-01-26T16:22:12.212781"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12379.237, "latencies_ms": [12379.237], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image presents a clear glass bowl filled with a group of oranges. The bowl is placed on a table, and the oranges are arranged in a circular pattern, with some overlapping each other. The oranges are bright orange in color, and the bowl is transparent, allowing the oranges to be seen clearly. The table has a textured surface, possibly a table", "error": null, "sys_before": {"cpu_percent": 6.1, "ram_used_mb": 21092.8, "ram_available_mb": 41748.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21092.6, "ram_available_mb": 41748.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.231}, "power_stats": {"power_gpu_soc_mean_watts": 21.553, "power_cpu_cv_mean_watts": 1.805, "power_sys_5v0_mean_watts": 8.539, "gpu_utilization_percent_mean": 73.231, "power_watts_avg": 21.553, "energy_joules_est": 266.82, "duration_seconds": 12.38, "sample_count": 104}, "timestamp": "2026-01-26T16:22:26.620779"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4336.73, "latencies_ms": [4336.73], "images_per_second": 0.231, "prompt_tokens": 39, "response_tokens_est": 5, "n_tiles": 16, "output_text": "orange: 8", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21092.6, "ram_available_mb": 41748.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 21110.5, "ram_available_mb": 41730.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 90.861}, "power_stats": {"power_gpu_soc_mean_watts": 27.657, "power_cpu_cv_mean_watts": 0.511, "power_sys_5v0_mean_watts": 8.19, "gpu_utilization_percent_mean": 90.861, "power_watts_avg": 27.657, "energy_joules_est": 119.96, "duration_seconds": 4.337, "sample_count": 36}, "timestamp": "2026-01-26T16:22:32.978530"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12632.001, "latencies_ms": [12632.001], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The oranges are clustered together in the foreground of the image, with some appearing closer to the viewer and others further away, creating a sense of depth. The bowl is placed centrally in the image, with the oranges filling most of the space within it. The background is a textured surface that extends beyond the bowl, providing a contrast that highlights the or", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21110.5, "ram_available_mb": 41730.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21046.8, "ram_available_mb": 41794.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.296}, "power_stats": {"power_gpu_soc_mean_watts": 21.632, "power_cpu_cv_mean_watts": 1.79, "power_sys_5v0_mean_watts": 8.508, "gpu_utilization_percent_mean": 73.296, "power_watts_avg": 21.632, "energy_joules_est": 273.27, "duration_seconds": 12.633, "sample_count": 108}, "timestamp": "2026-01-26T16:22:47.630956"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8706.468, "latencies_ms": [8706.468], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A bowl filled with oranges is placed on a textured surface, possibly a table or countertop. The oranges are arranged neatly in the bowl, creating a visually appealing display.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21046.8, "ram_available_mb": 41794.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21106.3, "ram_available_mb": 41734.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.068}, "power_stats": {"power_gpu_soc_mean_watts": 23.424, "power_cpu_cv_mean_watts": 1.433, "power_sys_5v0_mean_watts": 8.421, "gpu_utilization_percent_mean": 77.068, "power_watts_avg": 23.424, "energy_joules_est": 203.96, "duration_seconds": 8.707, "sample_count": 74}, "timestamp": "2026-01-26T16:22:58.385054"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8570.541, "latencies_ms": [8570.541], "images_per_second": 0.117, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The oranges are a vibrant orange color, indicating they are ripe and likely sweet. They are placed in a clear glass bowl, which reflects light and highlights their glossy texture.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21043.6, "ram_available_mb": 41797.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 21081.5, "ram_available_mb": 41759.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.135}, "power_stats": {"power_gpu_soc_mean_watts": 23.12, "power_cpu_cv_mean_watts": 1.487, "power_sys_5v0_mean_watts": 8.497, "gpu_utilization_percent_mean": 76.135, "power_watts_avg": 23.12, "energy_joules_est": 198.17, "duration_seconds": 8.571, "sample_count": 74}, "timestamp": "2026-01-26T16:23:08.993168"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12396.919, "latencies_ms": [12396.919], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a surfer is skillfully riding a large wave in the ocean. The surfer, clad in a black wetsuit, is positioned on a white surfboard. The wave, a magnificent shade of blue, is breaking to the right, creating a dramatic scene. The sky above is a uniform gray, suggesting an overcast day. The", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21081.5, "ram_available_mb": 41759.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.6, "ram_used_mb": 21085.2, "ram_available_mb": 41755.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.657}, "power_stats": {"power_gpu_soc_mean_watts": 21.246, "power_cpu_cv_mean_watts": 1.933, "power_sys_5v0_mean_watts": 8.447, "gpu_utilization_percent_mean": 71.657, "power_watts_avg": 21.246, "energy_joules_est": 263.4, "duration_seconds": 12.398, "sample_count": 105}, "timestamp": "2026-01-26T16:23:23.449865"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9029.793, "latencies_ms": [9029.793], "images_per_second": 0.111, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "wave: 1\nsurfer: 1\nocean: 1\nspray: 1\nclouds: 1\nsky: 1\nsand: 1\nsea foam: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21085.2, "ram_available_mb": 41755.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 8.8, "ram_used_mb": 21091.6, "ram_available_mb": 41749.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.286}, "power_stats": {"power_gpu_soc_mean_watts": 22.714, "power_cpu_cv_mean_watts": 1.913, "power_sys_5v0_mean_watts": 8.394, "gpu_utilization_percent_mean": 76.286, "power_watts_avg": 22.714, "energy_joules_est": 205.12, "duration_seconds": 9.03, "sample_count": 77}, "timestamp": "2026-01-26T16:23:34.492655"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11888.219, "latencies_ms": [11888.219], "images_per_second": 0.084, "prompt_tokens": 44, "response_tokens_est": 77, "n_tiles": 16, "output_text": "The surfer is positioned near the foreground, riding a wave that is closer to the viewer than the horizon. The wave is large and forms the central focus of the image, with the surfer appearing small in comparison to the wave's size. The background is the ocean extending to the horizon, which appears far away and is less detailed than the wave.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21091.6, "ram_available_mb": 41749.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 9.6, "ram_used_mb": 21103.9, "ram_available_mb": 41737.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.5}, "power_stats": {"power_gpu_soc_mean_watts": 21.451, "power_cpu_cv_mean_watts": 2.163, "power_sys_5v0_mean_watts": 8.547, "gpu_utilization_percent_mean": 72.5, "power_watts_avg": 21.451, "energy_joules_est": 255.03, "duration_seconds": 11.889, "sample_count": 102}, "timestamp": "2026-01-26T16:23:48.422155"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6539.821, "latencies_ms": [6539.821], "images_per_second": 0.153, "prompt_tokens": 37, "response_tokens_est": 26, "n_tiles": 16, "output_text": "A surfer is riding a large wave in the ocean. The sky is overcast and the water is choppy.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21103.9, "ram_available_mb": 41737.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 8.6, "ram_used_mb": 21135.5, "ram_available_mb": 41705.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 80.618}, "power_stats": {"power_gpu_soc_mean_watts": 24.289, "power_cpu_cv_mean_watts": 1.521, "power_sys_5v0_mean_watts": 8.314, "gpu_utilization_percent_mean": 80.618, "power_watts_avg": 24.289, "energy_joules_est": 158.86, "duration_seconds": 6.54, "sample_count": 55}, "timestamp": "2026-01-26T16:23:57.026324"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7775.827, "latencies_ms": [7775.827], "images_per_second": 0.129, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The surfer is riding a large wave with a deep blue color and white foam at the crest. The sky is overcast, suggesting a cloudy day with no direct sunlight.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21072.0, "ram_available_mb": 41768.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 8.4, "ram_used_mb": 21115.7, "ram_available_mb": 41725.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.723}, "power_stats": {"power_gpu_soc_mean_watts": 23.12, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.54, "gpu_utilization_percent_mean": 76.723, "power_watts_avg": 23.12, "energy_joules_est": 179.79, "duration_seconds": 7.776, "sample_count": 65}, "timestamp": "2026-01-26T16:24:06.815317"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11579.604, "latencies_ms": [11579.604], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a cat is sitting on a couch, looking at a laptop screen. The cat appears to be focused on the screen, possibly watching something interesting or playing with the laptop. The laptop is placed on the couch, and the cat is positioned in front of it, occupying a significant portion of the scene. The cat's presence adds a playful and cozy", "error": null, "sys_before": {"cpu_percent": 13.3, "ram_used_mb": 21115.7, "ram_available_mb": 41725.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 9.7, "ram_used_mb": 21117.8, "ram_available_mb": 41723.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.908}, "power_stats": {"power_gpu_soc_mean_watts": 19.3, "power_cpu_cv_mean_watts": 2.075, "power_sys_5v0_mean_watts": 8.33, "gpu_utilization_percent_mean": 69.908, "power_watts_avg": 19.3, "energy_joules_est": 223.5, "duration_seconds": 11.58, "sample_count": 98}, "timestamp": "2026-01-26T16:24:20.428156"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7764.073, "latencies_ms": [7764.073], "images_per_second": 0.129, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "couch: 1\nlaptop: 1\nscreen: 1\ntwitter: 1\nweb page: 1\nsearch bar: 1\nkeyword: 1\ncat: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21117.8, "ram_available_mb": 41723.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 8.2, "ram_used_mb": 21141.0, "ram_available_mb": 41699.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.909}, "power_stats": {"power_gpu_soc_mean_watts": 21.164, "power_cpu_cv_mean_watts": 2.475, "power_sys_5v0_mean_watts": 8.336, "gpu_utilization_percent_mean": 73.909, "power_watts_avg": 21.164, "energy_joules_est": 164.33, "duration_seconds": 7.765, "sample_count": 66}, "timestamp": "2026-01-26T16:24:30.209319"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11593.953, "latencies_ms": [11593.953], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a cat sitting on the left side of a laptop, with its body facing the screen. The laptop is placed on a surface, likely a table or a couch, and the cat is positioned in such a way that it appears to be looking at the screen. The background is less distinct but seems to be a part of a room with a couch and", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21068.3, "ram_available_mb": 41772.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21105.8, "ram_available_mb": 41735.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.263}, "power_stats": {"power_gpu_soc_mean_watts": 19.276, "power_cpu_cv_mean_watts": 1.876, "power_sys_5v0_mean_watts": 8.288, "gpu_utilization_percent_mean": 70.263, "power_watts_avg": 19.276, "energy_joules_est": 223.5, "duration_seconds": 11.595, "sample_count": 99}, "timestamp": "2026-01-26T16:24:43.837221"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8451.369, "latencies_ms": [8451.369], "images_per_second": 0.118, "prompt_tokens": 37, "response_tokens_est": 50, "n_tiles": 16, "output_text": "A cat is sitting on a couch with its head resting on a laptop, appearing to be using the Twitter website. The laptop is open and the screen is visible, showing the Twitter homepage with the search bar and the Twitter logo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21105.8, "ram_available_mb": 41735.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21117.5, "ram_available_mb": 41723.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.375}, "power_stats": {"power_gpu_soc_mean_watts": 20.766, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 8.233, "gpu_utilization_percent_mean": 73.375, "power_watts_avg": 20.766, "energy_joules_est": 175.52, "duration_seconds": 8.452, "sample_count": 72}, "timestamp": "2026-01-26T16:24:54.347406"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5587.6, "latencies_ms": [5587.6], "images_per_second": 0.179, "prompt_tokens": 36, "response_tokens_est": 27, "n_tiles": 16, "output_text": "The image shows a cat with a white and brown coat sitting in front of a laptop. The laptop screen displays the Twitter website.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21056.6, "ram_available_mb": 41784.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 21092.1, "ram_available_mb": 41748.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.146}, "power_stats": {"power_gpu_soc_mean_watts": 22.769, "power_cpu_cv_mean_watts": 1.343, "power_sys_5v0_mean_watts": 8.244, "gpu_utilization_percent_mean": 76.146, "power_watts_avg": 22.769, "energy_joules_est": 127.24, "duration_seconds": 5.588, "sample_count": 48}, "timestamp": "2026-01-26T16:25:01.965369"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12361.493, "latencies_ms": [12361.493], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of horses in a field, with some of them standing close together and others scattered around. There are at least five horses visible in the scene, with one horse standing alone in the background. The horses are of various sizes, and they appear to be enjoying their time in the field.\n\nIn the background, there is a house, which suggests that the horses are", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 21092.1, "ram_available_mb": 41748.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21127.5, "ram_available_mb": 41713.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.327}, "power_stats": {"power_gpu_soc_mean_watts": 21.266, "power_cpu_cv_mean_watts": 1.811, "power_sys_5v0_mean_watts": 8.461, "gpu_utilization_percent_mean": 72.327, "power_watts_avg": 21.266, "energy_joules_est": 262.9, "duration_seconds": 12.362, "sample_count": 107}, "timestamp": "2026-01-26T16:25:16.397085"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10724.873, "latencies_ms": [10724.873], "images_per_second": 0.093, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Horses: 5\n2. Hay: 1\n3. Grass: 1\n4. Fence: 1\n5. House: 1\n6. Trees: 1\n7. Power lines: 1\n8. Sky: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21127.5, "ram_available_mb": 41713.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21128.5, "ram_available_mb": 41712.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.967}, "power_stats": {"power_gpu_soc_mean_watts": 22.071, "power_cpu_cv_mean_watts": 1.667, "power_sys_5v0_mean_watts": 8.429, "gpu_utilization_percent_mean": 74.967, "power_watts_avg": 22.071, "energy_joules_est": 236.73, "duration_seconds": 10.726, "sample_count": 91}, "timestamp": "2026-01-26T16:25:29.133781"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9481.082, "latencies_ms": [9481.082], "images_per_second": 0.105, "prompt_tokens": 44, "response_tokens_est": 56, "n_tiles": 16, "output_text": "In the foreground, there is a group of horses standing close together, with one horse slightly ahead of the others. In the background, there is a house and trees. The horses are in the middle ground of the image, with the house and trees in the background.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 21128.5, "ram_available_mb": 41712.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21129.7, "ram_available_mb": 41711.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.469}, "power_stats": {"power_gpu_soc_mean_watts": 22.178, "power_cpu_cv_mean_watts": 1.665, "power_sys_5v0_mean_watts": 8.506, "gpu_utilization_percent_mean": 74.469, "power_watts_avg": 22.178, "energy_joules_est": 210.29, "duration_seconds": 9.482, "sample_count": 81}, "timestamp": "2026-01-26T16:25:40.646626"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9135.699, "latencies_ms": [9135.699], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A group of horses, including a foal, are standing in a grassy field with hay scattered around. The horses are of different colors, including brown and black, and they appear to be grazing or resting in the field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21129.7, "ram_available_mb": 41711.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21129.9, "ram_available_mb": 41711.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.974}, "power_stats": {"power_gpu_soc_mean_watts": 22.638, "power_cpu_cv_mean_watts": 1.55, "power_sys_5v0_mean_watts": 8.398, "gpu_utilization_percent_mean": 74.974, "power_watts_avg": 22.638, "energy_joules_est": 206.83, "duration_seconds": 9.136, "sample_count": 78}, "timestamp": "2026-01-26T16:25:51.836673"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9126.579, "latencies_ms": [9126.579], "images_per_second": 0.11, "prompt_tokens": 36, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image features a group of horses with varying shades of brown and black, standing in a field with dry, yellowish grass. The lighting is natural and appears to be from an overcast sky, giving the scene a soft, diffused light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21129.9, "ram_available_mb": 41711.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21143.1, "ram_available_mb": 41697.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.779}, "power_stats": {"power_gpu_soc_mean_watts": 22.376, "power_cpu_cv_mean_watts": 1.637, "power_sys_5v0_mean_watts": 8.512, "gpu_utilization_percent_mean": 74.779, "power_watts_avg": 22.376, "energy_joules_est": 204.23, "duration_seconds": 9.127, "sample_count": 77}, "timestamp": "2026-01-26T16:26:02.975681"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11594.043, "latencies_ms": [11594.043], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seen surfing on a wave in the ocean. He is wearing a black wetsuit and is skillfully riding the wave on a white surfboard. The surfer is positioned in the center of the image, with the wave beneath him. The ocean extends to the right side of the image, while the left side is occupied by a", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 21143.1, "ram_available_mb": 41697.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21143.1, "ram_available_mb": 41697.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.051}, "power_stats": {"power_gpu_soc_mean_watts": 19.305, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.303, "gpu_utilization_percent_mean": 70.051, "power_watts_avg": 19.305, "energy_joules_est": 223.84, "duration_seconds": 11.595, "sample_count": 99}, "timestamp": "2026-01-26T16:26:16.628404"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7666.266, "latencies_ms": [7666.266], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "surfboard: 1, water: multiple, surfer: 1, wave: 1, cliff: 1, sky: 1, sand: 1, wind: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21143.1, "ram_available_mb": 41697.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21143.4, "ram_available_mb": 41697.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.8}, "power_stats": {"power_gpu_soc_mean_watts": 21.28, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.217, "gpu_utilization_percent_mean": 73.8, "power_watts_avg": 21.28, "energy_joules_est": 163.15, "duration_seconds": 7.667, "sample_count": 65}, "timestamp": "2026-01-26T16:26:26.325993"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9353.453, "latencies_ms": [9353.453], "images_per_second": 0.107, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The surfer is in the foreground, riding a wave that is breaking near the shore. The coastline is visible in the background, with cliffs rising steeply from the water's edge. The sky is above the scene, suggesting the surfer is at sea level.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21061.5, "ram_available_mb": 41779.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21096.7, "ram_available_mb": 41744.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_gpu_soc_mean_watts": 20.142, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.298, "gpu_utilization_percent_mean": 71.0, "power_watts_avg": 20.142, "energy_joules_est": 188.41, "duration_seconds": 9.354, "sample_count": 79}, "timestamp": "2026-01-26T16:26:37.736644"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6612.083, "latencies_ms": [6612.083], "images_per_second": 0.151, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "A man in a black wetsuit is surfing on a wave in the ocean. The background shows a rocky cliff and a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 21096.7, "ram_available_mb": 41744.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21114.2, "ram_available_mb": 41726.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.411}, "power_stats": {"power_gpu_soc_mean_watts": 22.211, "power_cpu_cv_mean_watts": 1.43, "power_sys_5v0_mean_watts": 8.196, "gpu_utilization_percent_mean": 75.411, "power_watts_avg": 22.211, "energy_joules_est": 146.87, "duration_seconds": 6.613, "sample_count": 56}, "timestamp": "2026-01-26T16:26:46.404616"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7018.582, "latencies_ms": [7018.582], "images_per_second": 0.142, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The surfer is wearing a black wetsuit and is riding a wave on a yellow surfboard. The water is a deep blue color and the sky is cloudy.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21114.2, "ram_available_mb": 41726.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21096.5, "ram_available_mb": 41744.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.017}, "power_stats": {"power_gpu_soc_mean_watts": 21.077, "power_cpu_cv_mean_watts": 1.554, "power_sys_5v0_mean_watts": 8.245, "gpu_utilization_percent_mean": 74.017, "power_watts_avg": 21.077, "energy_joules_est": 147.94, "duration_seconds": 7.019, "sample_count": 60}, "timestamp": "2026-01-26T16:26:55.466321"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11567.806, "latencies_ms": [11567.806], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a collection of carved pumpkins, each with a unique design, displayed on a table. There are three pumpkins in total, with one large pumpkin taking up most of the space on the table. The large pumpkin has a face carved into it, while the other two pumpkins have different designs.\n\nIn addition to the pump", "error": null, "sys_before": {"cpu_percent": 10.3, "ram_used_mb": 21096.3, "ram_available_mb": 41744.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21096.8, "ram_available_mb": 41744.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.576}, "power_stats": {"power_gpu_soc_mean_watts": 19.323, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.307, "gpu_utilization_percent_mean": 69.576, "power_watts_avg": 19.323, "energy_joules_est": 223.54, "duration_seconds": 11.568, "sample_count": 99}, "timestamp": "2026-01-26T16:27:09.067676"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5244.597, "latencies_ms": [5244.597], "images_per_second": 0.191, "prompt_tokens": 39, "response_tokens_est": 22, "n_tiles": 16, "output_text": "pumpkin: 3, flower: 2, drawing: 8, figure: 2", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21096.8, "ram_available_mb": 41744.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 21097.0, "ram_available_mb": 41743.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.932}, "power_stats": {"power_gpu_soc_mean_watts": 23.818, "power_cpu_cv_mean_watts": 1.164, "power_sys_5v0_mean_watts": 8.131, "gpu_utilization_percent_mean": 77.932, "power_watts_avg": 23.818, "energy_joules_est": 124.93, "duration_seconds": 5.245, "sample_count": 44}, "timestamp": "2026-01-26T16:27:16.345525"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11564.723, "latencies_ms": [11564.723], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large pumpkin with a carved face and a smaller pumpkin with a carved face nearby. To the right, there is a vase filled with pink and white flowers. The vase is placed on top of the smaller pumpkin, creating a layered effect. In the background, there are various drawings and posters on", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21097.0, "ram_available_mb": 41743.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21097.2, "ram_available_mb": 41743.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.163}, "power_stats": {"power_gpu_soc_mean_watts": 19.349, "power_cpu_cv_mean_watts": 1.887, "power_sys_5v0_mean_watts": 8.318, "gpu_utilization_percent_mean": 70.163, "power_watts_avg": 19.349, "energy_joules_est": 223.78, "duration_seconds": 11.565, "sample_count": 98}, "timestamp": "2026-01-26T16:27:29.951126"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8817.564, "latencies_ms": [8817.564], "images_per_second": 0.113, "prompt_tokens": 37, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The image shows a collection of carved pumpkins on display, likely for a Halloween event. There are three pumpkins in the foreground, each with a unique carving, and a vase with pink flowers in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21097.2, "ram_available_mb": 41743.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21051.1, "ram_available_mb": 41789.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.4}, "power_stats": {"power_gpu_soc_mean_watts": 20.395, "power_cpu_cv_mean_watts": 1.681, "power_sys_5v0_mean_watts": 8.234, "gpu_utilization_percent_mean": 72.4, "power_watts_avg": 20.395, "energy_joules_est": 179.85, "duration_seconds": 8.818, "sample_count": 75}, "timestamp": "2026-01-26T16:27:40.812819"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9937.051, "latencies_ms": [9937.051], "images_per_second": 0.101, "prompt_tokens": 36, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image features a collection of carved pumpkins with various designs, including a smiling face and a bat, set against a backdrop of a bookshelf filled with books. The pumpkins are illuminated from within, casting a warm glow and highlighting the intricate carvings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21051.1, "ram_available_mb": 41789.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21089.6, "ram_available_mb": 41751.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.2}, "power_stats": {"power_gpu_soc_mean_watts": 19.705, "power_cpu_cv_mean_watts": 1.827, "power_sys_5v0_mean_watts": 8.306, "gpu_utilization_percent_mean": 70.2, "power_watts_avg": 19.705, "energy_joules_est": 195.82, "duration_seconds": 9.938, "sample_count": 85}, "timestamp": "2026-01-26T16:27:52.791314"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12367.078, "latencies_ms": [12367.078], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a small, somewhat cluttered bathroom. Dominating the scene is a white sink, its surface gleaming under the light. Above it, a mirror reflects the room's interior, revealing a white door on the right and a white wall on the left. The door, slightly ajar, invites curiosity about what lies beyond.\n\nOn the", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 21089.6, "ram_available_mb": 41751.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21104.0, "ram_available_mb": 41736.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11229.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.019}, "power_stats": {"power_gpu_soc_mean_watts": 21.377, "power_cpu_cv_mean_watts": 1.813, "power_sys_5v0_mean_watts": 8.49, "gpu_utilization_percent_mean": 73.019, "power_watts_avg": 21.377, "energy_joules_est": 264.38, "duration_seconds": 12.368, "sample_count": 106}, "timestamp": "2026-01-26T16:28:07.215950"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8856.153, "latencies_ms": [8856.153], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 46, "n_tiles": 16, "output_text": "wall: 1\nmirror: 1\nshelf: 1\nfaucet: 1\nsink: 1\npipe: 1\ndoor: 1\nbags: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21042.2, "ram_available_mb": 41798.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21081.9, "ram_available_mb": 41759.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11258.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.92}, "power_stats": {"power_gpu_soc_mean_watts": 22.803, "power_cpu_cv_mean_watts": 1.5, "power_sys_5v0_mean_watts": 8.372, "gpu_utilization_percent_mean": 75.92, "power_watts_avg": 22.803, "energy_joules_est": 201.96, "duration_seconds": 8.857, "sample_count": 75}, "timestamp": "2026-01-26T16:28:18.083558"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10012.903, "latencies_ms": [10012.903], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 60, "n_tiles": 16, "output_text": "In the foreground, there is a white sink with a silver faucet on the left side, and a white door on the right side. In the background, there is a small black shelf with a few items on it, and a white wall with some stickers on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21081.9, "ram_available_mb": 41759.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 9.7, "ram_used_mb": 21118.1, "ram_available_mb": 41722.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11268.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.259}, "power_stats": {"power_gpu_soc_mean_watts": 22.041, "power_cpu_cv_mean_watts": 2.115, "power_sys_5v0_mean_watts": 8.518, "gpu_utilization_percent_mean": 74.259, "power_watts_avg": 22.041, "energy_joules_est": 220.71, "duration_seconds": 10.014, "sample_count": 85}, "timestamp": "2026-01-26T16:28:30.157310"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7928.904, "latencies_ms": [7928.904], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image shows a small, cluttered bathroom with a white sink, a mirror, and a white door. There is a black bag on the floor next to the sink.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21118.1, "ram_available_mb": 41722.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 8.5, "ram_used_mb": 21137.7, "ram_available_mb": 41703.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11254.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.403}, "power_stats": {"power_gpu_soc_mean_watts": 23.357, "power_cpu_cv_mean_watts": 1.756, "power_sys_5v0_mean_watts": 8.379, "gpu_utilization_percent_mean": 78.403, "power_watts_avg": 23.357, "energy_joules_est": 185.21, "duration_seconds": 7.93, "sample_count": 67}, "timestamp": "2026-01-26T16:28:40.143465"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11733.516, "latencies_ms": [11733.516], "images_per_second": 0.085, "prompt_tokens": 36, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The image shows an interior space with a white sink and a white door. The lighting appears to be artificial, and the walls are a light color, possibly white or a light shade of beige. There is a small shelf with a few items on it, including a blue container and a white container. A black bag is also visible on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21137.7, "ram_available_mb": 41703.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 10.2, "ram_used_mb": 21065.4, "ram_available_mb": 41775.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11252.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.46}, "power_stats": {"power_gpu_soc_mean_watts": 21.507, "power_cpu_cv_mean_watts": 2.19, "power_sys_5v0_mean_watts": 8.545, "gpu_utilization_percent_mean": 71.46, "power_watts_avg": 21.507, "energy_joules_est": 252.37, "duration_seconds": 11.734, "sample_count": 100}, "timestamp": "2026-01-26T16:28:53.893091"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11569.58, "latencies_ms": [11569.58], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photo, a young child is the main subject, sitting on a bed with a laptop in front of them. The child is wearing a white shirt and has their feet up on the bed, giving the impression of a relaxed and comfortable environment. The laptop screen is turned on, displaying an image of a person. The child appears to be focused on the laptop screen", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 21065.4, "ram_available_mb": 41775.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 9.6, "ram_used_mb": 21099.7, "ram_available_mb": 41741.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_gpu_soc_mean_watts": 19.362, "power_cpu_cv_mean_watts": 2.087, "power_sys_5v0_mean_watts": 8.33, "gpu_utilization_percent_mean": 70.0, "power_watts_avg": 19.362, "energy_joules_est": 224.02, "duration_seconds": 11.57, "sample_count": 98}, "timestamp": "2026-01-26T16:29:07.500282"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7665.193, "latencies_ms": [7665.193], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "laptop: 1, child: 1, bed: 1, pillow: 1, blanket: 1, wall: 1, window: 1, curtain: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21099.7, "ram_available_mb": 41741.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 8.2, "ram_used_mb": 21108.4, "ram_available_mb": 41732.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.523}, "power_stats": {"power_gpu_soc_mean_watts": 21.203, "power_cpu_cv_mean_watts": 1.749, "power_sys_5v0_mean_watts": 8.239, "gpu_utilization_percent_mean": 73.523, "power_watts_avg": 21.203, "energy_joules_est": 162.54, "duration_seconds": 7.666, "sample_count": 65}, "timestamp": "2026-01-26T16:29:17.207202"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9412.658, "latencies_ms": [9412.658], "images_per_second": 0.106, "prompt_tokens": 44, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The laptop is positioned on the left side of the image, slightly in the foreground, while the child is seated on the right side of the image, closer to the viewer. The background is a plain, light-colored wall that provides a neutral backdrop for the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21108.4, "ram_available_mb": 41732.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 8.4, "ram_used_mb": 21107.7, "ram_available_mb": 41733.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.888}, "power_stats": {"power_gpu_soc_mean_watts": 20.236, "power_cpu_cv_mean_watts": 2.337, "power_sys_5v0_mean_watts": 8.379, "gpu_utilization_percent_mean": 71.888, "power_watts_avg": 20.236, "energy_joules_est": 190.49, "duration_seconds": 9.413, "sample_count": 80}, "timestamp": "2026-01-26T16:29:28.637077"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5941.719, "latencies_ms": [5941.719], "images_per_second": 0.168, "prompt_tokens": 37, "response_tokens_est": 28, "n_tiles": 16, "output_text": "A young child is sitting on a bed with a laptop in front of them. The child appears to be focused on the laptop screen.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21107.7, "ram_available_mb": 41733.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 21106.6, "ram_available_mb": 41734.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.84}, "power_stats": {"power_gpu_soc_mean_watts": 22.665, "power_cpu_cv_mean_watts": 1.305, "power_sys_5v0_mean_watts": 8.164, "gpu_utilization_percent_mean": 76.84, "power_watts_avg": 22.665, "energy_joules_est": 134.69, "duration_seconds": 5.943, "sample_count": 50}, "timestamp": "2026-01-26T16:29:36.591860"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8679.449, "latencies_ms": [8679.449], "images_per_second": 0.115, "prompt_tokens": 36, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image is in black and white, with the laptop being the only object in color, creating a stark contrast against the white background. The lighting is soft and diffused, coming from the left side, casting gentle shadows to the right of the objects.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 21106.6, "ram_available_mb": 41734.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21104.8, "ram_available_mb": 41736.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.053}, "power_stats": {"power_gpu_soc_mean_watts": 20.348, "power_cpu_cv_mean_watts": 1.729, "power_sys_5v0_mean_watts": 8.269, "gpu_utilization_percent_mean": 72.053, "power_watts_avg": 20.348, "energy_joules_est": 176.62, "duration_seconds": 8.68, "sample_count": 75}, "timestamp": "2026-01-26T16:29:47.289104"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12349.81, "latencies_ms": [12349.81], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is skiing down a snowy mountain slope. He is wearing a brown jacket and is in the process of making a turn on his skis. The skier is leaning to the right, indicating that he is turning right. The snowy mountain slope is surrounded by trees, creating a picturesque winter landscape. The skier appears to be enjoying", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 21104.8, "ram_available_mb": 41736.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21053.5, "ram_available_mb": 41787.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.594}, "power_stats": {"power_gpu_soc_mean_watts": 21.254, "power_cpu_cv_mean_watts": 1.809, "power_sys_5v0_mean_watts": 8.479, "gpu_utilization_percent_mean": 72.594, "power_watts_avg": 21.254, "energy_joules_est": 262.5, "duration_seconds": 12.351, "sample_count": 106}, "timestamp": "2026-01-26T16:30:01.685592"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11010.413, "latencies_ms": [11010.413], "images_per_second": 0.091, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. Skier: 1\n2. Ski: 2\n3. Snow: 1\n4. Trees: 1\n5. Pine trees: 1\n6. Snowflakes: 1\n7. Mountain: 1\n8. Snowboard: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21053.5, "ram_available_mb": 41787.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21110.9, "ram_available_mb": 41730.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.065}, "power_stats": {"power_gpu_soc_mean_watts": 21.855, "power_cpu_cv_mean_watts": 1.691, "power_sys_5v0_mean_watts": 8.425, "gpu_utilization_percent_mean": 72.065, "power_watts_avg": 21.855, "energy_joules_est": 240.65, "duration_seconds": 11.011, "sample_count": 93}, "timestamp": "2026-01-26T16:30:14.733562"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10815.366, "latencies_ms": [10815.366], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The skier is in the foreground, moving downhill on the snow-covered slope. The trees in the background are also covered in snow, indicating that the skier is at a higher elevation than the trees. The skier is closer to the camera than the trees, making them appear larger in the frame.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21110.9, "ram_available_mb": 41730.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21135.8, "ram_available_mb": 41705.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.602}, "power_stats": {"power_gpu_soc_mean_watts": 21.575, "power_cpu_cv_mean_watts": 1.76, "power_sys_5v0_mean_watts": 8.495, "gpu_utilization_percent_mean": 72.602, "power_watts_avg": 21.575, "energy_joules_est": 233.35, "duration_seconds": 10.816, "sample_count": 93}, "timestamp": "2026-01-26T16:30:27.595352"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8690.516, "latencies_ms": [8690.516], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A skier is seen in action, carving through the snow on a mountain slope. The skier is wearing a brown jacket, blue pants, and a blue helmet with goggles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21046.5, "ram_available_mb": 41794.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 21051.7, "ram_available_mb": 41789.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.973}, "power_stats": {"power_gpu_soc_mean_watts": 22.895, "power_cpu_cv_mean_watts": 1.498, "power_sys_5v0_mean_watts": 8.383, "gpu_utilization_percent_mean": 76.973, "power_watts_avg": 22.895, "energy_joules_est": 198.98, "duration_seconds": 8.691, "sample_count": 74}, "timestamp": "2026-01-26T16:30:38.318255"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7804.945, "latencies_ms": [7804.945], "images_per_second": 0.128, "prompt_tokens": 36, "response_tokens_est": 41, "n_tiles": 16, "output_text": "The skier is wearing a brown jacket and blue helmet, and is surrounded by snow-covered trees. The snow is falling heavily, creating a white and fluffy atmosphere.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21051.7, "ram_available_mb": 41789.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21111.6, "ram_available_mb": 41729.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.545}, "power_stats": {"power_gpu_soc_mean_watts": 22.902, "power_cpu_cv_mean_watts": 1.492, "power_sys_5v0_mean_watts": 8.45, "gpu_utilization_percent_mean": 76.545, "power_watts_avg": 22.902, "energy_joules_est": 178.77, "duration_seconds": 7.806, "sample_count": 66}, "timestamp": "2026-01-26T16:30:48.139779"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11605.916, "latencies_ms": [11605.916], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene hotel room bathed in soft light. Dominating the scene is a large bed, dressed in a pristine white comforter and adorned with two pillows, one a vibrant shade of green and the other a soothing blue. The bed is positioned against a wall, which is punctuated by a window draped", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 21046.9, "ram_available_mb": 41794.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21086.4, "ram_available_mb": 41754.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.697}, "power_stats": {"power_gpu_soc_mean_watts": 19.276, "power_cpu_cv_mean_watts": 1.88, "power_sys_5v0_mean_watts": 8.304, "gpu_utilization_percent_mean": 69.697, "power_watts_avg": 19.276, "energy_joules_est": 223.73, "duration_seconds": 11.607, "sample_count": 99}, "timestamp": "2026-01-26T16:31:01.809937"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8221.143, "latencies_ms": [8221.143], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 48, "n_tiles": 16, "output_text": "bed: 1\nlamp: 2\nchair: 1\nsofa: 1\nwindow: 1\ncurtain: 1\npillow: 3\nbedspread: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 21086.4, "ram_available_mb": 41754.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21085.1, "ram_available_mb": 41755.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.914}, "power_stats": {"power_gpu_soc_mean_watts": 20.888, "power_cpu_cv_mean_watts": 1.641, "power_sys_5v0_mean_watts": 8.247, "gpu_utilization_percent_mean": 72.914, "power_watts_avg": 20.888, "energy_joules_est": 171.74, "duration_seconds": 8.222, "sample_count": 70}, "timestamp": "2026-01-26T16:31:12.049653"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11230.056, "latencies_ms": [11230.056], "images_per_second": 0.089, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The bed is in the foreground of the image, occupying the right side and extending from the bottom to the top of the frame. The chair is positioned in the background, to the left of the bed, and is smaller in size compared to the bed. The window is in the far background, behind the chair, and is the largest object in the room.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21085.1, "ram_available_mb": 41755.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21086.1, "ram_available_mb": 41754.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.165}, "power_stats": {"power_gpu_soc_mean_watts": 19.36, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.299, "gpu_utilization_percent_mean": 70.165, "power_watts_avg": 19.36, "energy_joules_est": 217.43, "duration_seconds": 11.231, "sample_count": 97}, "timestamp": "2026-01-26T16:31:25.295335"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9922.102, "latencies_ms": [9922.102], "images_per_second": 0.101, "prompt_tokens": 37, "response_tokens_est": 63, "n_tiles": 16, "output_text": "The image depicts a well-lit hotel room with a large bed in the center, flanked by two nightstands with lamps on them. There is a blue armchair and a suitcase on the left side of the room, suggesting that someone is staying in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21086.1, "ram_available_mb": 41754.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21100.3, "ram_available_mb": 41740.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.357}, "power_stats": {"power_gpu_soc_mean_watts": 20.135, "power_cpu_cv_mean_watts": 1.753, "power_sys_5v0_mean_watts": 8.264, "gpu_utilization_percent_mean": 71.357, "power_watts_avg": 20.135, "energy_joules_est": 199.79, "duration_seconds": 9.923, "sample_count": 84}, "timestamp": "2026-01-26T16:31:37.240526"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7578.121, "latencies_ms": [7578.121], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The room is well-lit with natural light coming from the window, which has white curtains. The bed has a white comforter and is adorned with two blue and two green pillows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21037.8, "ram_available_mb": 41803.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21074.1, "ram_available_mb": 41766.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.859}, "power_stats": {"power_gpu_soc_mean_watts": 20.744, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.271, "gpu_utilization_percent_mean": 72.859, "power_watts_avg": 20.744, "energy_joules_est": 157.21, "duration_seconds": 7.579, "sample_count": 64}, "timestamp": "2026-01-26T16:31:46.830516"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10813.456, "latencies_ms": [10813.456], "images_per_second": 0.092, "prompt_tokens": 24, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The image captures a snowy mountain slope with a group of people skiing and snowboarding. A skier is in the process of jumping over a red pipe, while other skiers and snowboarders are scattered across the slope. The scene is lively and full of action, with people enjoying the winter sports on the mountain.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21074.1, "ram_available_mb": 41766.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21044.8, "ram_available_mb": 41796.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.143}, "power_stats": {"power_gpu_soc_mean_watts": 19.519, "power_cpu_cv_mean_watts": 1.838, "power_sys_5v0_mean_watts": 8.295, "gpu_utilization_percent_mean": 70.143, "power_watts_avg": 19.519, "energy_joules_est": 211.08, "duration_seconds": 10.814, "sample_count": 91}, "timestamp": "2026-01-26T16:31:59.709132"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11808.168, "latencies_ms": [11808.168], "images_per_second": 0.085, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "object: skiers, count: 10\nobject: ski lifts, count: 2\nobject: snowboarders, count: 1\nobject: snow, count: uncountable\nobject: ski equipment, count: 1\nobject: ski tracks, count: uncountable\nobject: ski poles, count: 1\nobject: snow-cover", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21044.8, "ram_available_mb": 41796.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21081.3, "ram_available_mb": 41759.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_gpu_soc_mean_watts": 19.343, "power_cpu_cv_mean_watts": 1.861, "power_sys_5v0_mean_watts": 8.273, "gpu_utilization_percent_mean": 71.0, "power_watts_avg": 19.343, "energy_joules_est": 228.42, "duration_seconds": 11.809, "sample_count": 100}, "timestamp": "2026-01-26T16:32:13.542895"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11569.127, "latencies_ms": [11569.127], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large red pipe that extends from the left side of the image towards the center, where a skier is performing a trick. The skier is positioned near the top of the pipe, slightly to the right of the center. In the background, there are multiple skiers on the slopes, with some closer to the viewer and others further away", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21081.3, "ram_available_mb": 41759.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21040.2, "ram_available_mb": 41800.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.596}, "power_stats": {"power_gpu_soc_mean_watts": 19.206, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.308, "gpu_utilization_percent_mean": 70.596, "power_watts_avg": 19.206, "energy_joules_est": 222.21, "duration_seconds": 11.57, "sample_count": 99}, "timestamp": "2026-01-26T16:32:27.131371"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10101.261, "latencies_ms": [10101.261], "images_per_second": 0.099, "prompt_tokens": 37, "response_tokens_est": 64, "n_tiles": 16, "output_text": "The image captures a dynamic scene at a ski resort where a skier is in mid-air, performing a jump over a red metal ramp. Other skiers can be seen on the slopes, some in the background and others closer to the foreground, all enjoying the snowy terrain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21040.2, "ram_available_mb": 41800.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21115.4, "ram_available_mb": 41725.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.529}, "power_stats": {"power_gpu_soc_mean_watts": 19.965, "power_cpu_cv_mean_watts": 1.757, "power_sys_5v0_mean_watts": 8.218, "gpu_utilization_percent_mean": 71.529, "power_watts_avg": 19.965, "energy_joules_est": 201.68, "duration_seconds": 10.102, "sample_count": 87}, "timestamp": "2026-01-26T16:32:39.288954"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9604.49, "latencies_ms": [9604.49], "images_per_second": 0.104, "prompt_tokens": 36, "response_tokens_est": 62, "n_tiles": 16, "output_text": "The image captures a bright and sunny day at a ski resort with clear blue skies. The snow-covered slopes are bustling with skiers and snowboarders, and the main feature is a large red metal rail that they are using for jumps and tricks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21043.5, "ram_available_mb": 41797.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21074.1, "ram_available_mb": 41766.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.889}, "power_stats": {"power_gpu_soc_mean_watts": 19.907, "power_cpu_cv_mean_watts": 1.784, "power_sys_5v0_mean_watts": 8.287, "gpu_utilization_percent_mean": 70.889, "power_watts_avg": 19.907, "energy_joules_est": 191.21, "duration_seconds": 9.605, "sample_count": 81}, "timestamp": "2026-01-26T16:32:50.922686"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11606.168, "latencies_ms": [11606.168], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a vibrant scene of urban street art. Dominating the left side of the frame is a wall, its surface a canvas for a graffiti mural. The mural is a riot of colors, with black and white text and shapes forming a complex pattern. The text \"THE ONE N' ONLY\" is prominently displayed, suggesting a theme", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21074.1, "ram_available_mb": 41766.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21101.1, "ram_available_mb": 41739.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.827}, "power_stats": {"power_gpu_soc_mean_watts": 19.12, "power_cpu_cv_mean_watts": 1.874, "power_sys_5v0_mean_watts": 8.273, "gpu_utilization_percent_mean": 69.827, "power_watts_avg": 19.12, "energy_joules_est": 221.93, "duration_seconds": 11.607, "sample_count": 98}, "timestamp": "2026-01-26T16:33:04.571772"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6429.461, "latencies_ms": [6429.461], "images_per_second": 0.156, "prompt_tokens": 39, "response_tokens_est": 32, "n_tiles": 16, "output_text": "1. Graffiti: multiple instances\n2. Parking meter: 1\n3. Wall: large surface area covered with graffiti", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21047.7, "ram_available_mb": 41793.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 21073.6, "ram_available_mb": 41767.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.704}, "power_stats": {"power_gpu_soc_mean_watts": 22.298, "power_cpu_cv_mean_watts": 1.378, "power_sys_5v0_mean_watts": 8.165, "gpu_utilization_percent_mean": 75.704, "power_watts_avg": 22.298, "energy_joules_est": 143.38, "duration_seconds": 6.43, "sample_count": 54}, "timestamp": "2026-01-26T16:33:13.038732"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10498.583, "latencies_ms": [10498.583], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The graffiti-covered wall serves as the background, with various tags and designs in the foreground. A metal pole with a parking meter is positioned in the foreground, slightly to the right of the center of the image. The pole is closer to the viewer than the wall, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21073.6, "ram_available_mb": 41767.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 8.0, "ram_used_mb": 21082.0, "ram_available_mb": 41758.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.978}, "power_stats": {"power_gpu_soc_mean_watts": 19.772, "power_cpu_cv_mean_watts": 2.051, "power_sys_5v0_mean_watts": 8.307, "gpu_utilization_percent_mean": 69.978, "power_watts_avg": 19.772, "energy_joules_est": 207.59, "duration_seconds": 10.499, "sample_count": 89}, "timestamp": "2026-01-26T16:33:25.577313"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8922.965, "latencies_ms": [8922.965], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "The image shows a graffiti-covered wall with a metal gate in the background. A metal pole with a parking meter is in the foreground, and the graffiti includes the phrase \"THE ONE N' ONLY CHAKA.\"", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21082.0, "ram_available_mb": 41758.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21082.7, "ram_available_mb": 41758.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.267}, "power_stats": {"power_gpu_soc_mean_watts": 20.565, "power_cpu_cv_mean_watts": 1.686, "power_sys_5v0_mean_watts": 8.245, "gpu_utilization_percent_mean": 72.267, "power_watts_avg": 20.565, "energy_joules_est": 183.51, "duration_seconds": 8.924, "sample_count": 75}, "timestamp": "2026-01-26T16:33:36.520792"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9016.6, "latencies_ms": [9016.6], "images_per_second": 0.111, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image features a wall covered in vibrant graffiti art with a mix of colors including black, white, red, and blue. The wall is adjacent to a metal fence, and there is a street lamp with a reflective surface in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.7, "ram_available_mb": 41758.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21043.1, "ram_available_mb": 41797.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.605}, "power_stats": {"power_gpu_soc_mean_watts": 20.299, "power_cpu_cv_mean_watts": 1.706, "power_sys_5v0_mean_watts": 8.238, "gpu_utilization_percent_mean": 72.605, "power_watts_avg": 20.299, "energy_joules_est": 183.04, "duration_seconds": 9.017, "sample_count": 76}, "timestamp": "2026-01-26T16:33:47.565919"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11575.597, "latencies_ms": [11575.597], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of tranquility and adventure in the vast expanse of the ocean. A surfer, clad in a black wetsuit, is skillfully riding a wave. The surfer is crouched on a white surfboard, leaning into the wave with a sense of balance and control. The wave, a beautiful shade of blue,", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21043.1, "ram_available_mb": 41797.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21079.1, "ram_available_mb": 41761.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.273}, "power_stats": {"power_gpu_soc_mean_watts": 19.347, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.306, "gpu_utilization_percent_mean": 70.273, "power_watts_avg": 19.347, "energy_joules_est": 223.97, "duration_seconds": 11.576, "sample_count": 99}, "timestamp": "2026-01-26T16:34:01.195772"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7863.669, "latencies_ms": [7863.669], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "water: numerous\nsurfboard: 1\nwave: 1\nsurfer: 1\ndark area: 1\nlight area: 1\nshadow: 1\ncolor contrast: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21079.1, "ram_available_mb": 41761.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21079.6, "ram_available_mb": 41761.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.015}, "power_stats": {"power_gpu_soc_mean_watts": 21.18, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.236, "gpu_utilization_percent_mean": 73.015, "power_watts_avg": 21.18, "energy_joules_est": 166.57, "duration_seconds": 7.864, "sample_count": 67}, "timestamp": "2026-01-26T16:34:11.105567"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11570.457, "latencies_ms": [11570.457], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The surfer is positioned on the right side of the image, riding a wave that is breaking towards the left. The wave is in the foreground, while the vast expanse of the ocean extends into the background, creating a sense of depth. The surfer appears small in comparison to the wave, emphasizing the wave's size and the distance between the surfer and the horizon", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21079.6, "ram_available_mb": 41761.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21107.5, "ram_available_mb": 41733.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.02}, "power_stats": {"power_gpu_soc_mean_watts": 19.182, "power_cpu_cv_mean_watts": 1.885, "power_sys_5v0_mean_watts": 8.299, "gpu_utilization_percent_mean": 70.02, "power_watts_avg": 19.182, "energy_joules_est": 221.96, "duration_seconds": 11.571, "sample_count": 100}, "timestamp": "2026-01-26T16:34:24.707683"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7247.921, "latencies_ms": [7247.921], "images_per_second": 0.138, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A surfer is riding a wave in the deep blue ocean. The wave is breaking to the right, and the surfer is skillfully maneuvering the surfboard.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21107.5, "ram_available_mb": 41733.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21113.2, "ram_available_mb": 41727.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.935}, "power_stats": {"power_gpu_soc_mean_watts": 21.352, "power_cpu_cv_mean_watts": 1.504, "power_sys_5v0_mean_watts": 8.168, "gpu_utilization_percent_mean": 74.935, "power_watts_avg": 21.352, "energy_joules_est": 154.77, "duration_seconds": 7.248, "sample_count": 62}, "timestamp": "2026-01-26T16:34:34.013526"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6269.94, "latencies_ms": [6269.94], "images_per_second": 0.159, "prompt_tokens": 36, "response_tokens_est": 33, "n_tiles": 16, "output_text": "The surfer is riding a wave in the deep blue ocean under a clear sky. The wave is a bright white as it breaks around the surfer.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21041.1, "ram_available_mb": 41799.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21076.5, "ram_available_mb": 41764.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.396}, "power_stats": {"power_gpu_soc_mean_watts": 22.135, "power_cpu_cv_mean_watts": 1.457, "power_sys_5v0_mean_watts": 8.252, "gpu_utilization_percent_mean": 74.396, "power_watts_avg": 22.135, "energy_joules_est": 138.8, "duration_seconds": 6.271, "sample_count": 53}, "timestamp": "2026-01-26T16:34:42.338635"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11588.73, "latencies_ms": [11588.73], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a yellow double-decker bus parked at a bus stop on a city street. The bus is stopped next to a bus stop shelter, and there are several people waiting to board the bus. One man is standing near the bus, while another man is standing further away, closer to the bus stop shelter. \n\nThere are also two other people in the scene, one", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21076.5, "ram_available_mb": 41764.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21081.0, "ram_available_mb": 41759.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.444}, "power_stats": {"power_gpu_soc_mean_watts": 19.069, "power_cpu_cv_mean_watts": 1.888, "power_sys_5v0_mean_watts": 8.283, "gpu_utilization_percent_mean": 70.444, "power_watts_avg": 19.069, "energy_joules_est": 221.0, "duration_seconds": 11.589, "sample_count": 99}, "timestamp": "2026-01-26T16:34:55.962526"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7410.616, "latencies_ms": [7410.616], "images_per_second": 0.135, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "bus: 1, bus stop: 1, flower: multiple, building: multiple, people: 2, street: multiple, pavement: multiple, bus route: 11", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21081.0, "ram_available_mb": 41759.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21081.0, "ram_available_mb": 41759.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.032}, "power_stats": {"power_gpu_soc_mean_watts": 21.206, "power_cpu_cv_mean_watts": 1.53, "power_sys_5v0_mean_watts": 8.158, "gpu_utilization_percent_mean": 74.032, "power_watts_avg": 21.206, "energy_joules_est": 157.16, "duration_seconds": 7.411, "sample_count": 62}, "timestamp": "2026-01-26T16:35:05.415305"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9179.311, "latencies_ms": [9179.311], "images_per_second": 0.109, "prompt_tokens": 44, "response_tokens_est": 57, "n_tiles": 16, "output_text": "The bus is parked on the left side of the image, near the curb, while a person is standing on the right side, close to the bus, at a bus stop. The bus is in the foreground, and the bus stop shelter is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21081.0, "ram_available_mb": 41759.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21078.7, "ram_available_mb": 41762.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.513}, "power_stats": {"power_gpu_soc_mean_watts": 20.051, "power_cpu_cv_mean_watts": 1.724, "power_sys_5v0_mean_watts": 8.256, "gpu_utilization_percent_mean": 71.513, "power_watts_avg": 20.051, "energy_joules_est": 184.07, "duration_seconds": 9.18, "sample_count": 78}, "timestamp": "2026-01-26T16:35:16.609803"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7693.186, "latencies_ms": [7693.186], "images_per_second": 0.13, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A double-decker bus is parked at a bus stop, with a man waiting to board. The bus displays the route number 11 and the destination \"Lyham & St Rines\".", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 21078.7, "ram_available_mb": 41762.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21120.7, "ram_available_mb": 41720.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.215}, "power_stats": {"power_gpu_soc_mean_watts": 21.226, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.203, "gpu_utilization_percent_mean": 73.215, "power_watts_avg": 21.226, "energy_joules_est": 163.31, "duration_seconds": 7.694, "sample_count": 65}, "timestamp": "2026-01-26T16:35:26.366819"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11213.093, "latencies_ms": [11213.093], "images_per_second": 0.089, "prompt_tokens": 36, "response_tokens_est": 75, "n_tiles": 16, "output_text": "The image features a vibrant yellow double-decker bus with the destination \"Lyham & St Rnes\" displayed on its front. The bus is parked at a bus stop with a blue shelter, and there are people waiting to board. The sky is overcast, suggesting a cloudy day, and the ground is wet, indicating recent rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21033.1, "ram_available_mb": 41807.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21076.6, "ram_available_mb": 41764.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.635}, "power_stats": {"power_gpu_soc_mean_watts": 19.427, "power_cpu_cv_mean_watts": 1.859, "power_sys_5v0_mean_watts": 8.279, "gpu_utilization_percent_mean": 70.635, "power_watts_avg": 19.427, "energy_joules_est": 217.85, "duration_seconds": 11.214, "sample_count": 96}, "timestamp": "2026-01-26T16:35:39.611452"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11567.84, "latencies_ms": [11567.84], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the vast expanse of the gray sky, a small yellow airplane with a blue propeller is captured in mid-flight. The airplane, with its red wings and tail, is the main subject of this image. It's soaring from the left to the right, leaving behind a trail of red smoke that adds a dynamic element to the scene. The airplane's", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 21076.6, "ram_available_mb": 41764.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21094.6, "ram_available_mb": 41746.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.646}, "power_stats": {"power_gpu_soc_mean_watts": 19.291, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.314, "gpu_utilization_percent_mean": 69.646, "power_watts_avg": 19.291, "energy_joules_est": 223.17, "duration_seconds": 11.568, "sample_count": 99}, "timestamp": "2026-01-26T16:35:53.210083"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8139.488, "latencies_ms": [8139.488], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "airplane: 1, wing: 2, propeller: 1, tail: 1, engine: 1, landing gear: 2, fuselage: 1, tail fin: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21039.4, "ram_available_mb": 41801.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21067.4, "ram_available_mb": 41773.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.101}, "power_stats": {"power_gpu_soc_mean_watts": 20.863, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.222, "gpu_utilization_percent_mean": 73.101, "power_watts_avg": 20.863, "energy_joules_est": 169.83, "duration_seconds": 8.14, "sample_count": 69}, "timestamp": "2026-01-26T16:36:03.395353"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10547.709, "latencies_ms": [10547.709], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The airplane is in the foreground, flying against a clear sky in the background. It is positioned slightly to the left of the center of the image, with its wings spread wide and its nose pointed upwards. The airplane appears to be in motion, as it is captured mid-flight with its landing gear extended.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21067.4, "ram_available_mb": 41773.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 21073.1, "ram_available_mb": 41767.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_gpu_soc_mean_watts": 19.553, "power_cpu_cv_mean_watts": 1.931, "power_sys_5v0_mean_watts": 8.338, "gpu_utilization_percent_mean": 71.0, "power_watts_avg": 19.553, "energy_joules_est": 206.25, "duration_seconds": 10.548, "sample_count": 91}, "timestamp": "2026-01-26T16:36:15.961593"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9820.538, "latencies_ms": [9820.538], "images_per_second": 0.102, "prompt_tokens": 37, "response_tokens_est": 62, "n_tiles": 16, "output_text": "A yellow and blue biplane with the registration SP-AWF is captured in mid-flight against a backdrop of a cloudy sky. The aircraft is equipped with a single propeller and appears to be in good condition, suggesting it may be used for recreational flying or training purposes.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21073.1, "ram_available_mb": 41767.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 21082.5, "ram_available_mb": 41758.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.548}, "power_stats": {"power_gpu_soc_mean_watts": 20.31, "power_cpu_cv_mean_watts": 1.839, "power_sys_5v0_mean_watts": 8.487, "gpu_utilization_percent_mean": 70.548, "power_watts_avg": 20.31, "energy_joules_est": 199.47, "duration_seconds": 9.821, "sample_count": 84}, "timestamp": "2026-01-26T16:36:27.837681"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8949.223, "latencies_ms": [8949.223], "images_per_second": 0.112, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The airplane in the image is a small, single-engine, propeller-driven aircraft with a yellow and red color scheme. It is flying in the sky with its landing gear extended, indicating that it may be preparing for landing or taking off.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.5, "ram_available_mb": 41758.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21083.5, "ram_available_mb": 41757.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.224}, "power_stats": {"power_gpu_soc_mean_watts": 19.99, "power_cpu_cv_mean_watts": 1.759, "power_sys_5v0_mean_watts": 8.598, "gpu_utilization_percent_mean": 72.224, "power_watts_avg": 19.99, "energy_joules_est": 178.91, "duration_seconds": 8.95, "sample_count": 76}, "timestamp": "2026-01-26T16:36:38.813684"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11601.487, "latencies_ms": [11601.487], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment from an airplane's perspective, just before it begins its descent. The airplane's wing is prominently visible in the foreground, with its blue and white colors standing out against the clear blue sky. The wing's design and structure are clearly visible, indicating the plane's readiness for landing.\n\nBelow the wing, a", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21083.5, "ram_available_mb": 41757.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21104.0, "ram_available_mb": 41736.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.724}, "power_stats": {"power_gpu_soc_mean_watts": 19.324, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 69.724, "power_watts_avg": 19.324, "energy_joules_est": 224.2, "duration_seconds": 11.602, "sample_count": 98}, "timestamp": "2026-01-26T16:36:52.480221"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11778.683, "latencies_ms": [11778.683], "images_per_second": 0.085, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "- Cars: numerous, exact count not visible\n- Parking lot: large, exact area not visible\n- Trees: scattered throughout, exact count not visible\n- Buildings: several, exact number not visible\n- Airport: visible in the background, exact size not visible\n- Clouds: scattered across the sky, exact count not visible\n- Runway: visible in the", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21042.2, "ram_available_mb": 41798.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21088.9, "ram_available_mb": 41752.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.04}, "power_stats": {"power_gpu_soc_mean_watts": 19.414, "power_cpu_cv_mean_watts": 1.861, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 70.04, "power_watts_avg": 19.414, "energy_joules_est": 228.68, "duration_seconds": 11.779, "sample_count": 100}, "timestamp": "2026-01-26T16:37:06.310219"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11588.311, "latencies_ms": [11588.311], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The airplane wing is in the foreground on the left side of the image, indicating it is closer to the viewer than the parking lot and buildings in the background. The parking lot is situated in the middle ground, occupying a large portion of the image, while the buildings are in the background, further away from the viewpoint. The landscape in the far background appears to be", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21088.9, "ram_available_mb": 41752.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21110.4, "ram_available_mb": 41730.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.485}, "power_stats": {"power_gpu_soc_mean_watts": 19.29, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 69.485, "power_watts_avg": 19.29, "energy_joules_est": 223.55, "duration_seconds": 11.589, "sample_count": 99}, "timestamp": "2026-01-26T16:37:19.913180"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10093.355, "latencies_ms": [10093.355], "images_per_second": 0.099, "prompt_tokens": 37, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The image captures an aerial view of a bustling airport parking lot filled with numerous cars, with a large airplane wing visible in the top left corner. The scene conveys a sense of travel and transportation, with the airplane and cars indicating the presence of an active airport.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21047.0, "ram_available_mb": 41793.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.4, "ram_used_mb": 21087.9, "ram_available_mb": 41753.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.942}, "power_stats": {"power_gpu_soc_mean_watts": 20.225, "power_cpu_cv_mean_watts": 1.996, "power_sys_5v0_mean_watts": 8.683, "gpu_utilization_percent_mean": 71.942, "power_watts_avg": 20.225, "energy_joules_est": 204.15, "duration_seconds": 10.094, "sample_count": 86}, "timestamp": "2026-01-26T16:37:32.044211"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6852.104, "latencies_ms": [6852.104], "images_per_second": 0.146, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The image shows a clear day with a bright blue sky and a few scattered clouds. The sunlight is casting shadows on the ground, indicating it is either morning or late afternoon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21087.9, "ram_available_mb": 41753.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21088.6, "ram_available_mb": 41752.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.5}, "power_stats": {"power_gpu_soc_mean_watts": 21.623, "power_cpu_cv_mean_watts": 1.539, "power_sys_5v0_mean_watts": 8.619, "gpu_utilization_percent_mean": 73.5, "power_watts_avg": 21.623, "energy_joules_est": 148.18, "duration_seconds": 6.853, "sample_count": 58}, "timestamp": "2026-01-26T16:37:40.911961"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11563.424, "latencies_ms": [11563.424], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is holding a pink flip phone with a screen displaying a picture of a woman. The person is sitting on a couch, and there are other people in the room, with one person sitting on the left side and another person on the right side. \n\nThere are two cups in the scene, one placed on the left side of the room and", "error": null, "sys_before": {"cpu_percent": 15.8, "ram_used_mb": 21088.6, "ram_available_mb": 41752.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21056.4, "ram_available_mb": 41784.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.424}, "power_stats": {"power_gpu_soc_mean_watts": 19.376, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.665, "gpu_utilization_percent_mean": 70.424, "power_watts_avg": 19.376, "energy_joules_est": 224.07, "duration_seconds": 11.564, "sample_count": 99}, "timestamp": "2026-01-26T16:37:54.522632"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7645.321, "latencies_ms": [7645.321], "images_per_second": 0.131, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "person: 2, hand: 2, remote control: 1, cup: 1, couch: 1, blanket: 1, dog: 1, gift: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21056.4, "ram_available_mb": 41784.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21095.1, "ram_available_mb": 41745.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.364}, "power_stats": {"power_gpu_soc_mean_watts": 21.302, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.596, "gpu_utilization_percent_mean": 74.364, "power_watts_avg": 21.302, "energy_joules_est": 162.87, "duration_seconds": 7.646, "sample_count": 66}, "timestamp": "2026-01-26T16:38:04.205071"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10422.529, "latencies_ms": [10422.529], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, a person is holding a pink flip phone with a picture of a girl on the screen. Behind them, another person is seated on a couch, partially visible, with a white cup on a table nearby. The setting appears to be a living room with a casual, relaxed atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21095.1, "ram_available_mb": 41745.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21050.2, "ram_available_mb": 41790.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.1}, "power_stats": {"power_gpu_soc_mean_watts": 19.713, "power_cpu_cv_mean_watts": 1.841, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 70.1, "power_watts_avg": 19.713, "energy_joules_est": 205.47, "duration_seconds": 10.423, "sample_count": 90}, "timestamp": "2026-01-26T16:38:16.677807"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7806.629, "latencies_ms": [7806.629], "images_per_second": 0.128, "prompt_tokens": 37, "response_tokens_est": 44, "n_tiles": 16, "output_text": "In the image, a person is holding a pink flip phone with a picture of a girl on the screen. The person is sitting on a couch with a cup and a can on the table nearby.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21050.2, "ram_available_mb": 41790.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21083.7, "ram_available_mb": 41757.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.2}, "power_stats": {"power_gpu_soc_mean_watts": 21.279, "power_cpu_cv_mean_watts": 1.57, "power_sys_5v0_mean_watts": 8.59, "gpu_utilization_percent_mean": 73.2, "power_watts_avg": 21.279, "energy_joules_est": 166.13, "duration_seconds": 7.807, "sample_count": 65}, "timestamp": "2026-01-26T16:38:26.515608"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8223.735, "latencies_ms": [8223.735], "images_per_second": 0.122, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image shows a person holding a pink flip phone with a screen displaying an image of a cartoon character. The phone is held in a dimly lit room with a red cup and a can of soda visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21083.7, "ram_available_mb": 41757.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21084.4, "ram_available_mb": 41756.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.814}, "power_stats": {"power_gpu_soc_mean_watts": 20.546, "power_cpu_cv_mean_watts": 1.704, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 71.814, "power_watts_avg": 20.546, "energy_joules_est": 168.98, "duration_seconds": 8.224, "sample_count": 70}, "timestamp": "2026-01-26T16:38:36.776070"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11593.361, "latencies_ms": [11593.361], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the vast expanse of a dry grassland, two zebras stand side by side, their black and white stripes contrasting sharply with the golden hue of the grass. The zebra on the left, slightly ahead of its companion, gazes directly into the camera, its eyes reflecting a sense of curiosity and alertness. Its companion, on the other hand,", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 21084.4, "ram_available_mb": 41756.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21085.4, "ram_available_mb": 41755.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.677}, "power_stats": {"power_gpu_soc_mean_watts": 19.319, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.675, "gpu_utilization_percent_mean": 68.677, "power_watts_avg": 19.319, "energy_joules_est": 223.98, "duration_seconds": 11.594, "sample_count": 99}, "timestamp": "2026-01-26T16:38:50.407506"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10161.135, "latencies_ms": [10161.135], "images_per_second": 0.098, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "1. Zebra: 2\n2. Grass: numerous\n3. Trees: scattered background\n4. Bushes: scattered background\n5. Sky: visible in distance\n6. Mountains: visible in distance\n7. Sun: shining on scene\n8. Water: not visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21085.4, "ram_available_mb": 41755.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21086.4, "ram_available_mb": 41754.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.828}, "power_stats": {"power_gpu_soc_mean_watts": 20.015, "power_cpu_cv_mean_watts": 1.785, "power_sys_5v0_mean_watts": 8.609, "gpu_utilization_percent_mean": 71.828, "power_watts_avg": 20.015, "energy_joules_est": 203.39, "duration_seconds": 10.162, "sample_count": 87}, "timestamp": "2026-01-26T16:39:02.622987"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10795.772, "latencies_ms": [10795.772], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 72, "n_tiles": 16, "output_text": "The two zebras are positioned in the foreground of the image, standing in a field of tall, dry grass. They are relatively close to each other, with one zebra slightly ahead of the other. In the background, there are trees and shrubs, indicating that the zebras are in a natural habitat with vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21086.4, "ram_available_mb": 41754.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21116.0, "ram_available_mb": 41724.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.967}, "power_stats": {"power_gpu_soc_mean_watts": 19.416, "power_cpu_cv_mean_watts": 1.847, "power_sys_5v0_mean_watts": 8.683, "gpu_utilization_percent_mean": 69.967, "power_watts_avg": 19.416, "energy_joules_est": 209.62, "duration_seconds": 10.796, "sample_count": 91}, "timestamp": "2026-01-26T16:39:15.428588"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7890.059, "latencies_ms": [7890.059], "images_per_second": 0.127, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "Two zebras are standing in a field of tall, dry grass with trees and bushes in the background. The zebras appear to be in a natural habitat, possibly a savannah or grassland.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 21116.0, "ram_available_mb": 41724.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21078.5, "ram_available_mb": 41762.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.433}, "power_stats": {"power_gpu_soc_mean_watts": 21.204, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.578, "gpu_utilization_percent_mean": 73.433, "power_watts_avg": 21.204, "energy_joules_est": 167.32, "duration_seconds": 7.891, "sample_count": 67}, "timestamp": "2026-01-26T16:39:25.335789"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8236.362, "latencies_ms": [8236.362], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image features two zebras standing in a field of tall, dry grass with a backdrop of trees and shrubs. The lighting is bright and natural, suggesting that the photo was taken during the day in a sunny environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21078.5, "ram_available_mb": 41762.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21116.2, "ram_available_mb": 41724.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.789}, "power_stats": {"power_gpu_soc_mean_watts": 20.579, "power_cpu_cv_mean_watts": 1.697, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 72.789, "power_watts_avg": 20.579, "energy_joules_est": 169.51, "duration_seconds": 8.237, "sample_count": 71}, "timestamp": "2026-01-26T16:39:35.633700"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11616.837, "latencies_ms": [11616.837], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is seen walking into the ocean with a yellow surfboard under his arm. He is wearing black shorts and appears to be heading towards the waves. The ocean is a deep blue color, and the waves are white and frothy. The sky is clear and blue, indicating a sunny day. The man seems to be preparing to surf, as", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 21036.9, "ram_available_mb": 41804.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21115.3, "ram_available_mb": 41725.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.465}, "power_stats": {"power_gpu_soc_mean_watts": 19.347, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.661, "gpu_utilization_percent_mean": 70.465, "power_watts_avg": 19.347, "energy_joules_est": 224.76, "duration_seconds": 11.617, "sample_count": 99}, "timestamp": "2026-01-26T16:39:49.311071"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7333.837, "latencies_ms": [7333.837], "images_per_second": 0.136, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "person: 1, surfboard: 1, wave: multiple, ocean: 1, sky: 1, sun: 1, sand: 1, water: multiple", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21115.3, "ram_available_mb": 41725.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21043.7, "ram_available_mb": 41797.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.919}, "power_stats": {"power_gpu_soc_mean_watts": 21.57, "power_cpu_cv_mean_watts": 1.53, "power_sys_5v0_mean_watts": 8.561, "gpu_utilization_percent_mean": 73.919, "power_watts_avg": 21.57, "energy_joules_est": 158.2, "duration_seconds": 7.334, "sample_count": 62}, "timestamp": "2026-01-26T16:39:58.657384"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 9977.062, "latencies_ms": [9977.062], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground on the left side of the image, walking towards the right side where the waves are breaking in the background. The waves appear closer to the viewer and are more detailed, while the surfer and the sandy beach are in the midground and appear less detailed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21043.7, "ram_available_mb": 41797.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21090.5, "ram_available_mb": 41750.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.106}, "power_stats": {"power_gpu_soc_mean_watts": 19.853, "power_cpu_cv_mean_watts": 1.803, "power_sys_5v0_mean_watts": 8.623, "gpu_utilization_percent_mean": 72.106, "power_watts_avg": 19.853, "energy_joules_est": 198.09, "duration_seconds": 9.978, "sample_count": 85}, "timestamp": "2026-01-26T16:40:10.650374"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7440.248, "latencies_ms": [7440.248], "images_per_second": 0.134, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A man is seen walking into the ocean with a yellow surfboard, likely preparing to surf. The waves are crashing onto the shore, creating a beautiful scene for the surfer.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21090.5, "ram_available_mb": 41750.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21104.0, "ram_available_mb": 41736.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.762}, "power_stats": {"power_gpu_soc_mean_watts": 21.457, "power_cpu_cv_mean_watts": 1.531, "power_sys_5v0_mean_watts": 8.568, "gpu_utilization_percent_mean": 73.762, "power_watts_avg": 21.457, "energy_joules_est": 159.66, "duration_seconds": 7.441, "sample_count": 63}, "timestamp": "2026-01-26T16:40:20.110972"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8806.89, "latencies_ms": [8806.89], "images_per_second": 0.114, "prompt_tokens": 36, "response_tokens_est": 55, "n_tiles": 16, "output_text": "The image features a person holding a bright yellow surfboard, standing in the shallow water of a beach. The sky is clear and the sunlight is reflecting off the water's surface, creating a serene and inviting atmosphere for surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21042.3, "ram_available_mb": 41798.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21084.3, "ram_available_mb": 41756.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.987}, "power_stats": {"power_gpu_soc_mean_watts": 20.308, "power_cpu_cv_mean_watts": 1.734, "power_sys_5v0_mean_watts": 8.664, "gpu_utilization_percent_mean": 70.987, "power_watts_avg": 20.308, "energy_joules_est": 178.86, "duration_seconds": 8.808, "sample_count": 75}, "timestamp": "2026-01-26T16:40:30.942786"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 10298.329, "latencies_ms": [10298.329], "images_per_second": 0.097, "prompt_tokens": 24, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the image, a black and white cow stands on a sandy beach. The cow is positioned in the center of the frame, facing the camera. The beach is sandy, and the ocean can be seen in the background. The cow appears to be calmly standing on the beach, enjoying the serene environment.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 21084.3, "ram_available_mb": 41756.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21089.3, "ram_available_mb": 41751.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.607}, "power_stats": {"power_gpu_soc_mean_watts": 19.762, "power_cpu_cv_mean_watts": 1.834, "power_sys_5v0_mean_watts": 8.647, "gpu_utilization_percent_mean": 71.607, "power_watts_avg": 19.762, "energy_joules_est": 203.53, "duration_seconds": 10.299, "sample_count": 89}, "timestamp": "2026-01-26T16:40:43.289878"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7090.844, "latencies_ms": [7090.844], "images_per_second": 0.141, "prompt_tokens": 39, "response_tokens_est": 38, "n_tiles": 16, "output_text": "cow: 1, rock: 1, sand: many, water: many, sky: not visible, grass: not visible, tree: not visible, cloud: not visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21089.3, "ram_available_mb": 41751.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21115.9, "ram_available_mb": 41725.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.6}, "power_stats": {"power_gpu_soc_mean_watts": 21.721, "power_cpu_cv_mean_watts": 1.521, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 74.6, "power_watts_avg": 21.721, "energy_joules_est": 154.03, "duration_seconds": 7.091, "sample_count": 60}, "timestamp": "2026-01-26T16:40:52.434678"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10909.886, "latencies_ms": [10909.886], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The cow is positioned in the foreground of the image, standing on the sandy ground. The background consists of a vast expanse of water, which appears to be a beach or a shoreline. The cow is near the water's edge, with a small rock or piece of debris visible in the foreground to its left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21115.9, "ram_available_mb": 41725.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21115.9, "ram_available_mb": 41725.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.14}, "power_stats": {"power_gpu_soc_mean_watts": 19.386, "power_cpu_cv_mean_watts": 1.85, "power_sys_5v0_mean_watts": 8.637, "gpu_utilization_percent_mean": 70.14, "power_watts_avg": 19.386, "energy_joules_est": 211.51, "duration_seconds": 10.911, "sample_count": 93}, "timestamp": "2026-01-26T16:41:05.359032"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7000.525, "latencies_ms": [7000.525], "images_per_second": 0.143, "prompt_tokens": 37, "response_tokens_est": 37, "n_tiles": 16, "output_text": "A black and white image features a cow standing on a sandy beach with the ocean in the background. The cow appears to be calmly enjoying its time near the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21115.9, "ram_available_mb": 41725.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21122.4, "ram_available_mb": 41718.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.39}, "power_stats": {"power_gpu_soc_mean_watts": 21.697, "power_cpu_cv_mean_watts": 1.485, "power_sys_5v0_mean_watts": 8.553, "gpu_utilization_percent_mean": 74.39, "power_watts_avg": 21.697, "energy_joules_est": 151.9, "duration_seconds": 7.001, "sample_count": 59}, "timestamp": "2026-01-26T16:41:14.372258"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11538.276, "latencies_ms": [11538.276], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image is a black and white photograph featuring a cow standing on a beach. The cow is predominantly black with white patches on its face and legs. The lighting is natural and appears to be coming from the side, casting shadows on the cow and the sand. The beach appears to be wet, possibly due to the tide, and there is a small rock or piece of", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21122.4, "ram_available_mb": 41718.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 21046.2, "ram_available_mb": 41794.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.16}, "power_stats": {"power_gpu_soc_mean_watts": 19.29, "power_cpu_cv_mean_watts": 1.909, "power_sys_5v0_mean_watts": 8.664, "gpu_utilization_percent_mean": 70.16, "power_watts_avg": 19.29, "energy_joules_est": 222.59, "duration_seconds": 11.539, "sample_count": 100}, "timestamp": "2026-01-26T16:41:27.971150"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12353.551, "latencies_ms": [12353.551], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In this black and white photograph, a woman is the central figure, standing on a snowy mountain. She is dressed in a white blouse adorned with black floral embroidery and black pants. Her feet are clad in black boots, and she is holding a pair of skis in her left hand. In her right hand, she holds a ski pole. The", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 21046.2, "ram_available_mb": 41794.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 21087.8, "ram_available_mb": 41753.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11229.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.123}, "power_stats": {"power_gpu_soc_mean_watts": 21.511, "power_cpu_cv_mean_watts": 2.039, "power_sys_5v0_mean_watts": 8.874, "gpu_utilization_percent_mean": 73.123, "power_watts_avg": 21.511, "energy_joules_est": 265.75, "duration_seconds": 12.354, "sample_count": 106}, "timestamp": "2026-01-26T16:41:42.381700"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9203.595, "latencies_ms": [9203.595], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "woman: 1, ski poles: 2, skis: 2, snow: multiple patches, trees: multiple, snow pants: 1, white blouse: 1, black gloves: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21087.8, "ram_available_mb": 41753.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21088.8, "ram_available_mb": 41752.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11258.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.654}, "power_stats": {"power_gpu_soc_mean_watts": 22.661, "power_cpu_cv_mean_watts": 1.534, "power_sys_5v0_mean_watts": 8.721, "gpu_utilization_percent_mean": 75.654, "power_watts_avg": 22.661, "energy_joules_est": 208.58, "duration_seconds": 9.204, "sample_count": 78}, "timestamp": "2026-01-26T16:41:53.638394"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12218.033, "latencies_ms": [12218.033], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a person stands prominently, positioned centrally in the frame, with skis and ski poles to their left, suggesting they are ready to ski. The background features a snowy landscape with trees, indicating the setting is a snowy outdoor area. The person appears to be at a distance from the viewer, as they are smaller in size compared", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21088.8, "ram_available_mb": 41752.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21088.5, "ram_available_mb": 41752.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11268.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.114}, "power_stats": {"power_gpu_soc_mean_watts": 21.451, "power_cpu_cv_mean_watts": 1.829, "power_sys_5v0_mean_watts": 8.866, "gpu_utilization_percent_mean": 72.114, "power_watts_avg": 21.451, "energy_joules_est": 262.1, "duration_seconds": 12.219, "sample_count": 105}, "timestamp": "2026-01-26T16:42:07.911132"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9191.651, "latencies_ms": [9191.651], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 49, "n_tiles": 16, "output_text": "A woman is standing on a snowy landscape, holding ski poles and dressed in winter attire, suggesting a skiing scene. The background features a cloudy sky and pine trees, indicating a mountainous or forested area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21088.5, "ram_available_mb": 41752.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21089.5, "ram_available_mb": 41751.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11254.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.821}, "power_stats": {"power_gpu_soc_mean_watts": 22.713, "power_cpu_cv_mean_watts": 1.539, "power_sys_5v0_mean_watts": 8.721, "gpu_utilization_percent_mean": 75.821, "power_watts_avg": 22.713, "energy_joules_est": 208.78, "duration_seconds": 9.192, "sample_count": 78}, "timestamp": "2026-01-26T16:42:19.165679"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8867.642, "latencies_ms": [8867.642], "images_per_second": 0.113, "prompt_tokens": 36, "response_tokens_est": 50, "n_tiles": 16, "output_text": "The image is in black and white, featuring a person dressed in winter attire, standing in a snowy landscape. The lighting appears to be natural, coming from the side, casting shadows to the right of the person and objects.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21089.5, "ram_available_mb": 41751.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21089.7, "ram_available_mb": 41751.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11252.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.747}, "power_stats": {"power_gpu_soc_mean_watts": 22.635, "power_cpu_cv_mean_watts": 1.595, "power_sys_5v0_mean_watts": 8.896, "gpu_utilization_percent_mean": 73.747, "power_watts_avg": 22.635, "energy_joules_est": 200.73, "duration_seconds": 8.868, "sample_count": 75}, "timestamp": "2026-01-26T16:42:30.075574"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11575.372, "latencies_ms": [11575.372], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a dog is standing on a sandy beach, holding a yellow frisbee in its mouth. The dog appears to be enjoying its time at the beach, possibly playing fetch with the frisbee. The beach is sandy, and the dog is positioned in the center of the scene.\n\nThere are several people visible in the background, scattered across", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 21089.7, "ram_available_mb": 41751.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21089.5, "ram_available_mb": 41751.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.051}, "power_stats": {"power_gpu_soc_mean_watts": 19.342, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.688, "gpu_utilization_percent_mean": 71.051, "power_watts_avg": 19.342, "energy_joules_est": 223.9, "duration_seconds": 11.576, "sample_count": 99}, "timestamp": "2026-01-26T16:42:43.693957"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7484.199, "latencies_ms": [7484.199], "images_per_second": 0.134, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "dog: 1, frisbee: 1, sand: numerous, water: multiple, waves: multiple, beach: 1, people: 1, rock formation: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21089.5, "ram_available_mb": 41751.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21129.5, "ram_available_mb": 41711.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.825}, "power_stats": {"power_gpu_soc_mean_watts": 21.145, "power_cpu_cv_mean_watts": 1.531, "power_sys_5v0_mean_watts": 8.526, "gpu_utilization_percent_mean": 73.825, "power_watts_avg": 21.145, "energy_joules_est": 158.27, "duration_seconds": 7.485, "sample_count": 63}, "timestamp": "2026-01-26T16:42:53.189395"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8262.859, "latencies_ms": [8262.859], "images_per_second": 0.121, "prompt_tokens": 44, "response_tokens_est": 50, "n_tiles": 16, "output_text": "In the foreground, there is a dog holding a yellow frisbee in its mouth. The dog is standing on a sandy beach with the ocean in the background. There are a few people visible in the distance near the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21129.5, "ram_available_mb": 41711.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21077.4, "ram_available_mb": 41763.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.771}, "power_stats": {"power_gpu_soc_mean_watts": 20.809, "power_cpu_cv_mean_watts": 1.675, "power_sys_5v0_mean_watts": 8.617, "gpu_utilization_percent_mean": 72.771, "power_watts_avg": 20.809, "energy_joules_est": 171.96, "duration_seconds": 8.263, "sample_count": 70}, "timestamp": "2026-01-26T16:43:03.480773"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7086.274, "latencies_ms": [7086.274], "images_per_second": 0.141, "prompt_tokens": 37, "response_tokens_est": 38, "n_tiles": 16, "output_text": "A dog is standing on a sandy beach holding a yellow frisbee in its mouth. The ocean can be seen in the background with a small island visible on the horizon.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 21077.4, "ram_available_mb": 41763.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21109.2, "ram_available_mb": 41731.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.867}, "power_stats": {"power_gpu_soc_mean_watts": 21.769, "power_cpu_cv_mean_watts": 1.501, "power_sys_5v0_mean_watts": 8.547, "gpu_utilization_percent_mean": 73.867, "power_watts_avg": 21.769, "energy_joules_est": 154.27, "duration_seconds": 7.087, "sample_count": 60}, "timestamp": "2026-01-26T16:43:12.584209"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7580.449, "latencies_ms": [7580.449], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The dog is holding a yellow frisbee on a sandy beach with turquoise water and a cloudy sky in the background. The beach appears to be deserted with no people in sight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21039.0, "ram_available_mb": 41801.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21118.9, "ram_available_mb": 41722.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.219}, "power_stats": {"power_gpu_soc_mean_watts": 20.796, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 72.219, "power_watts_avg": 20.796, "energy_joules_est": 157.66, "duration_seconds": 7.581, "sample_count": 64}, "timestamp": "2026-01-26T16:43:22.176414"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11630.979, "latencies_ms": [11630.979], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment in a kitchen where a group of people, including at least one in military attire, are gathered around a large metal pot on a stove. The kitchen is equipped with a refrigerator, a sink, and various kitchen items like bowls and a cutting board. The room has a concrete wall and a window, suggesting a simple, utilitarian", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 21118.9, "ram_available_mb": 41722.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21047.6, "ram_available_mb": 41793.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.04}, "power_stats": {"power_gpu_soc_mean_watts": 19.306, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.63, "gpu_utilization_percent_mean": 71.04, "power_watts_avg": 19.306, "energy_joules_est": 224.56, "duration_seconds": 11.632, "sample_count": 99}, "timestamp": "2026-01-26T16:43:35.852153"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8118.604, "latencies_ms": [8118.604], "images_per_second": 0.123, "prompt_tokens": 39, "response_tokens_est": 47, "n_tiles": 16, "output_text": "pot: 1, bowl: 1, refrigerator: 1, box: 1, container: 1, cutting board: 1, utensil: 1, container: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21047.6, "ram_available_mb": 41793.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21099.2, "ram_available_mb": 41741.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.261}, "power_stats": {"power_gpu_soc_mean_watts": 21.01, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 73.261, "power_watts_avg": 21.01, "energy_joules_est": 170.59, "duration_seconds": 8.119, "sample_count": 69}, "timestamp": "2026-01-26T16:43:46.003860"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11554.804, "latencies_ms": [11554.804], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a large pot on a stove with a lid, and to the left, there is a cutting board with a knife and some food items. In the background, there is a refrigerator and a group of people standing around the kitchen area. The people are positioned near the refrigerator, suggesting they might be preparing a meal", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 21037.4, "ram_available_mb": 41803.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21082.3, "ram_available_mb": 41758.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.65}, "power_stats": {"power_gpu_soc_mean_watts": 19.302, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.642, "gpu_utilization_percent_mean": 70.65, "power_watts_avg": 19.302, "energy_joules_est": 223.04, "duration_seconds": 11.555, "sample_count": 100}, "timestamp": "2026-01-26T16:43:59.575342"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9260.957, "latencies_ms": [9260.957], "images_per_second": 0.108, "prompt_tokens": 37, "response_tokens_est": 57, "n_tiles": 16, "output_text": "A group of people, including at least one in military uniform, are gathered in a kitchen with a large metal refrigerator, a stove, and various kitchen items on the counter. It appears to be a casual gathering or a meal preparation event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.3, "ram_available_mb": 41758.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21083.1, "ram_available_mb": 41757.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.405}, "power_stats": {"power_gpu_soc_mean_watts": 20.242, "power_cpu_cv_mean_watts": 1.722, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 72.405, "power_watts_avg": 20.242, "energy_joules_est": 187.47, "duration_seconds": 9.262, "sample_count": 79}, "timestamp": "2026-01-26T16:44:10.871661"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7319.179, "latencies_ms": [7319.179], "images_per_second": 0.137, "prompt_tokens": 36, "response_tokens_est": 42, "n_tiles": 16, "output_text": "The image shows a group of people in a kitchen with concrete walls and a metal refrigerator. The lighting is artificial, and the materials visible include metal, wood, and various kitchen items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21083.1, "ram_available_mb": 41757.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21084.3, "ram_available_mb": 41756.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.613}, "power_stats": {"power_gpu_soc_mean_watts": 21.169, "power_cpu_cv_mean_watts": 1.601, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 72.613, "power_watts_avg": 21.169, "energy_joules_est": 154.95, "duration_seconds": 7.32, "sample_count": 62}, "timestamp": "2026-01-26T16:44:20.222992"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11590.979, "latencies_ms": [11590.979], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene and well-organized bathroom. Dominating the scene is a pristine white toilet, its lid closed, standing against a wall adorned with beige tiles. Above the toilet, a silver towel rack is mounted, holding two neatly folded white towels. The rack is positioned above", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21084.3, "ram_available_mb": 41756.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21083.8, "ram_available_mb": 41757.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.306}, "power_stats": {"power_gpu_soc_mean_watts": 19.155, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 70.306, "power_watts_avg": 19.155, "energy_joules_est": 222.04, "duration_seconds": 11.592, "sample_count": 98}, "timestamp": "2026-01-26T16:44:33.862955"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11796.644, "latencies_ms": [11796.644], "images_per_second": 0.085, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "toilet: 1, toilet paper: 1, toilet brush: 1, toilet paper holder: 1, toilet paper roll: 1, toilet paper: 1, toilet paper: 1, toilet paper: 1, toilet paper: 1, toilet paper: ", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21083.8, "ram_available_mb": 41757.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21084.1, "ram_available_mb": 41756.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.653}, "power_stats": {"power_gpu_soc_mean_watts": 19.526, "power_cpu_cv_mean_watts": 1.866, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 70.653, "power_watts_avg": 19.526, "energy_joules_est": 230.35, "duration_seconds": 11.797, "sample_count": 101}, "timestamp": "2026-01-26T16:44:47.701346"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11660.708, "latencies_ms": [11660.708], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground of the image, there is a toilet positioned near the bottom right corner, with a toilet paper roll and a toilet brush placed to its left. Behind the toilet, there is a wall-mounted shelf with two white towels on top and several bottles of toiletries arranged below. The shelf", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 21084.1, "ram_available_mb": 41756.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21084.1, "ram_available_mb": 41756.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.808}, "power_stats": {"power_gpu_soc_mean_watts": 19.379, "power_cpu_cv_mean_watts": 1.871, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 70.808, "power_watts_avg": 19.379, "energy_joules_est": 225.99, "duration_seconds": 11.661, "sample_count": 99}, "timestamp": "2026-01-26T16:45:01.387221"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9187.292, "latencies_ms": [9187.292], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image shows a small, well-lit bathroom with a toilet, a shelf with toiletries, and a towel rack. The toilet is white and there is a toilet paper roll on the wall next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21084.1, "ram_available_mb": 41756.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21084.0, "ram_available_mb": 41756.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.859}, "power_stats": {"power_gpu_soc_mean_watts": 20.356, "power_cpu_cv_mean_watts": 1.713, "power_sys_5v0_mean_watts": 8.585, "gpu_utilization_percent_mean": 72.859, "power_watts_avg": 20.356, "energy_joules_est": 187.03, "duration_seconds": 9.188, "sample_count": 78}, "timestamp": "2026-01-26T16:45:12.591246"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11045.827, "latencies_ms": [11045.827], "images_per_second": 0.091, "prompt_tokens": 36, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image shows a bathroom with a beige color scheme, featuring a toilet, a towel rack with folded white towels, and a shelf with various bottles. The lighting appears to be artificial, coming from a ceiling light, and the materials used in the bathroom are primarily tiles and ceramics.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21084.0, "ram_available_mb": 41756.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21084.0, "ram_available_mb": 41756.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.202}, "power_stats": {"power_gpu_soc_mean_watts": 19.609, "power_cpu_cv_mean_watts": 1.852, "power_sys_5v0_mean_watts": 8.673, "gpu_utilization_percent_mean": 71.202, "power_watts_avg": 19.609, "energy_joules_est": 216.61, "duration_seconds": 11.046, "sample_count": 94}, "timestamp": "2026-01-26T16:45:25.653481"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11620.723, "latencies_ms": [11620.723], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman and a dog are sitting together in the passenger seat of a car. The dog is wearing a green hat, which is a common accessory for St. Patrick's Day celebrations. The woman is looking out the window, possibly observing the surroundings or waiting for something. The car's side mirror is visible, and it has a green sh", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 21084.0, "ram_available_mb": 41756.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 21086.7, "ram_available_mb": 41754.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.81}, "power_stats": {"power_gpu_soc_mean_watts": 19.282, "power_cpu_cv_mean_watts": 1.961, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 69.81, "power_watts_avg": 19.282, "energy_joules_est": 224.08, "duration_seconds": 11.621, "sample_count": 100}, "timestamp": "2026-01-26T16:45:39.303358"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9643.135, "latencies_ms": [9643.135], "images_per_second": 0.104, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "1. Car: 1\n2. Mirror: 1\n3. Window: 1\n4. Shamrock: 1\n5. Dog: 1\n6. Hat: 1\n7. Tree: 1\n8. Person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21086.7, "ram_available_mb": 41754.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21086.7, "ram_available_mb": 41754.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.915}, "power_stats": {"power_gpu_soc_mean_watts": 20.167, "power_cpu_cv_mean_watts": 1.747, "power_sys_5v0_mean_watts": 8.596, "gpu_utilization_percent_mean": 71.915, "power_watts_avg": 20.167, "energy_joules_est": 194.49, "duration_seconds": 9.644, "sample_count": 82}, "timestamp": "2026-01-26T16:45:50.964383"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11589.051, "latencies_ms": [11589.051], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a reflection of a person and a dog in the side mirror of a vehicle. The person is seated in the driver's seat, wearing a green hat, and the dog is sitting in the passenger seat. The background shows the interior of the vehicle, with a green shamrock decoration on the side mirror, indicating a festive or celebratory", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21086.7, "ram_available_mb": 41754.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21114.2, "ram_available_mb": 41726.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.253}, "power_stats": {"power_gpu_soc_mean_watts": 19.198, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.648, "gpu_utilization_percent_mean": 70.253, "power_watts_avg": 19.198, "energy_joules_est": 222.5, "duration_seconds": 11.59, "sample_count": 99}, "timestamp": "2026-01-26T16:46:04.596028"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8181.007, "latencies_ms": [8181.007], "images_per_second": 0.122, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A person is sitting in a vehicle with a dog, both wearing green hats, likely celebrating St. Patrick's Day. The side mirror of the vehicle has a green shamrock decoration attached to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21033.4, "ram_available_mb": 41807.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21080.0, "ram_available_mb": 41760.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.441}, "power_stats": {"power_gpu_soc_mean_watts": 20.738, "power_cpu_cv_mean_watts": 1.618, "power_sys_5v0_mean_watts": 8.551, "gpu_utilization_percent_mean": 73.441, "power_watts_avg": 20.738, "energy_joules_est": 169.67, "duration_seconds": 8.182, "sample_count": 68}, "timestamp": "2026-01-26T16:46:14.816816"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7009.761, "latencies_ms": [7009.761], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image shows a reflection of a person and a dog wearing a green hat in a vehicle's side mirror. The mirror has a green shamrock sticker attached to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21080.0, "ram_available_mb": 41760.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21084.7, "ram_available_mb": 41756.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.085}, "power_stats": {"power_gpu_soc_mean_watts": 21.29, "power_cpu_cv_mean_watts": 1.54, "power_sys_5v0_mean_watts": 8.601, "gpu_utilization_percent_mean": 72.085, "power_watts_avg": 21.29, "energy_joules_est": 149.25, "duration_seconds": 7.01, "sample_count": 59}, "timestamp": "2026-01-26T16:46:23.857508"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11613.973, "latencies_ms": [11613.973], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, there is a large elephant standing in a pool of water, surrounded by rocks. The elephant appears to be enjoying its time in the water, possibly cooling off or playing. There are several people in the background, watching the elephant and enjoying the scene. Some of the people are standing closer to the elephant, while others are further", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 21084.7, "ram_available_mb": 41756.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21085.9, "ram_available_mb": 41755.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.707}, "power_stats": {"power_gpu_soc_mean_watts": 19.292, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.644, "gpu_utilization_percent_mean": 69.707, "power_watts_avg": 19.292, "energy_joules_est": 224.07, "duration_seconds": 11.615, "sample_count": 99}, "timestamp": "2026-01-26T16:46:37.522135"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9662.576, "latencies_ms": [9662.576], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 60, "n_tiles": 16, "output_text": "1. Elephant: 1\n2. Rock: 3\n3. Water: 2\n4. Fence: 1\n5. Log: 1\n6. Bush: 1\n7. Tree: 1\n8. People: 3", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21085.9, "ram_available_mb": 41755.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21085.9, "ram_available_mb": 41755.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.556}, "power_stats": {"power_gpu_soc_mean_watts": 20.213, "power_cpu_cv_mean_watts": 1.739, "power_sys_5v0_mean_watts": 8.603, "gpu_utilization_percent_mean": 72.556, "power_watts_avg": 20.213, "energy_joules_est": 195.32, "duration_seconds": 9.663, "sample_count": 81}, "timestamp": "2026-01-26T16:46:49.223451"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8154.416, "latencies_ms": [8154.416], "images_per_second": 0.123, "prompt_tokens": 44, "response_tokens_est": 49, "n_tiles": 16, "output_text": "In the foreground, there is a large elephant standing in a pool of water. To the left of the elephant, there is a large rock. In the background, there are several people walking around the enclosure.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21085.9, "ram_available_mb": 41755.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21086.2, "ram_available_mb": 41754.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.058}, "power_stats": {"power_gpu_soc_mean_watts": 20.578, "power_cpu_cv_mean_watts": 1.676, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 72.058, "power_watts_avg": 20.578, "energy_joules_est": 167.81, "duration_seconds": 8.155, "sample_count": 69}, "timestamp": "2026-01-26T16:46:59.437407"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 6652.825, "latencies_ms": [6652.825], "images_per_second": 0.15, "prompt_tokens": 37, "response_tokens_est": 34, "n_tiles": 16, "output_text": "In the image, there is an elephant standing in a pool of water at a zoo. People are watching the elephant from behind a fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21086.2, "ram_available_mb": 41754.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21086.4, "ram_available_mb": 41754.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.482}, "power_stats": {"power_gpu_soc_mean_watts": 21.929, "power_cpu_cv_mean_watts": 1.444, "power_sys_5v0_mean_watts": 8.535, "gpu_utilization_percent_mean": 75.482, "power_watts_avg": 21.929, "energy_joules_est": 145.9, "duration_seconds": 6.653, "sample_count": 56}, "timestamp": "2026-01-26T16:47:08.149404"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7002.279, "latencies_ms": [7002.279], "images_per_second": 0.143, "prompt_tokens": 36, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The elephant is standing in a shallow pool of water, with its trunk extended towards the water. The pool is surrounded by a sandy area with some rocks scattered around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21086.4, "ram_available_mb": 41754.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21101.1, "ram_available_mb": 41739.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.153}, "power_stats": {"power_gpu_soc_mean_watts": 21.474, "power_cpu_cv_mean_watts": 1.546, "power_sys_5v0_mean_watts": 8.613, "gpu_utilization_percent_mean": 73.153, "power_watts_avg": 21.474, "energy_joules_est": 150.38, "duration_seconds": 7.003, "sample_count": 59}, "timestamp": "2026-01-26T16:47:17.205496"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11627.838, "latencies_ms": [11627.838], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, four people are standing on a snow-covered slope, posing for a picture. They are all wearing skis and holding ski poles, indicating that they are skiing enthusiasts. The group consists of two men and two women, all dressed in winter clothing suitable for skiing. The snowy mountain in the background adds to the overall winter sports atmosphere", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21038.0, "ram_available_mb": 41802.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21045.5, "ram_available_mb": 41795.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.959}, "power_stats": {"power_gpu_soc_mean_watts": 19.292, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 69.959, "power_watts_avg": 19.292, "energy_joules_est": 224.34, "duration_seconds": 11.628, "sample_count": 98}, "timestamp": "2026-01-26T16:47:30.890754"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9721.193, "latencies_ms": [9721.193], "images_per_second": 0.103, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Skis: 4\n\n- Skis: 4\n\n- People: 4\n\n- Mountains: 1\n\n- Trees: 1\n\n- Jackets: 4\n\n- Gloves: 4\n\n- Boots: 4", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21045.5, "ram_available_mb": 41795.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21089.9, "ram_available_mb": 41751.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.122}, "power_stats": {"power_gpu_soc_mean_watts": 20.195, "power_cpu_cv_mean_watts": 1.742, "power_sys_5v0_mean_watts": 8.613, "gpu_utilization_percent_mean": 72.122, "power_watts_avg": 20.195, "energy_joules_est": 196.33, "duration_seconds": 9.722, "sample_count": 82}, "timestamp": "2026-01-26T16:47:42.634950"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10566.191, "latencies_ms": [10566.191], "images_per_second": 0.095, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "In the foreground, there are four skiers standing close together, with one skier slightly ahead of the others, creating a sense of depth. They are positioned on a snowy slope with a mountain range in the background, which appears to be at a considerable distance from the skiers, emphasizing the vastness of the landscape.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21089.9, "ram_available_mb": 41751.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21090.9, "ram_available_mb": 41750.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.341}, "power_stats": {"power_gpu_soc_mean_watts": 19.455, "power_cpu_cv_mean_watts": 1.86, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 70.341, "power_watts_avg": 19.455, "energy_joules_est": 205.58, "duration_seconds": 10.567, "sample_count": 91}, "timestamp": "2026-01-26T16:47:55.225476"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7204.682, "latencies_ms": [7204.682], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "Four skiers are standing on a snowy slope with a mountain in the background. They are all wearing ski gear and holding ski poles, ready to ski down the slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21090.9, "ram_available_mb": 41750.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21109.1, "ram_available_mb": 41731.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.738}, "power_stats": {"power_gpu_soc_mean_watts": 21.721, "power_cpu_cv_mean_watts": 1.516, "power_sys_5v0_mean_watts": 8.539, "gpu_utilization_percent_mean": 74.738, "power_watts_avg": 21.721, "energy_joules_est": 156.51, "duration_seconds": 7.205, "sample_count": 61}, "timestamp": "2026-01-26T16:48:04.452627"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8355.002, "latencies_ms": [8355.002], "images_per_second": 0.12, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image shows a group of four skiers on a snowy mountain under clear blue skies. They are wearing winter clothing in bright colors like blue, black, and white, and are equipped with ski poles and skis.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21038.0, "ram_available_mb": 41802.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21063.4, "ram_available_mb": 41777.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.437}, "power_stats": {"power_gpu_soc_mean_watts": 20.54, "power_cpu_cv_mean_watts": 1.691, "power_sys_5v0_mean_watts": 8.615, "gpu_utilization_percent_mean": 71.437, "power_watts_avg": 20.54, "energy_joules_est": 171.62, "duration_seconds": 8.356, "sample_count": 71}, "timestamp": "2026-01-26T16:48:14.844785"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11629.395, "latencies_ms": [11629.395], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person's hand is holding a black iPhone with a white keyboard. The phone is displaying a photo of a snowy landscape. The time displayed on the phone is 9:45. The person's fingers are visible, and they are holding the phone in a way that suggests they are either taking a photo or viewing one. The background is blurred", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 21063.4, "ram_available_mb": 41777.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21063.4, "ram_available_mb": 41777.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.68}, "power_stats": {"power_gpu_soc_mean_watts": 19.219, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.648, "gpu_utilization_percent_mean": 69.68, "power_watts_avg": 19.219, "energy_joules_est": 223.52, "duration_seconds": 11.63, "sample_count": 100}, "timestamp": "2026-01-26T16:48:28.521261"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7329.258, "latencies_ms": [7329.258], "images_per_second": 0.136, "prompt_tokens": 39, "response_tokens_est": 40, "n_tiles": 16, "output_text": "hand: 1, thumb: 2, fingers: 5, phone: 1, screen: 1, keyboard: 1, tree: 1, snow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21063.4, "ram_available_mb": 41777.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21084.6, "ram_available_mb": 41756.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.952}, "power_stats": {"power_gpu_soc_mean_watts": 21.596, "power_cpu_cv_mean_watts": 1.53, "power_sys_5v0_mean_watts": 8.574, "gpu_utilization_percent_mean": 73.952, "power_watts_avg": 21.596, "energy_joules_est": 158.3, "duration_seconds": 7.33, "sample_count": 62}, "timestamp": "2026-01-26T16:48:37.877578"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10642.428, "latencies_ms": [10642.428], "images_per_second": 0.094, "prompt_tokens": 44, "response_tokens_est": 70, "n_tiles": 16, "output_text": "The phone is held in the foreground, with the screen displaying an image of a snowy scene. The background is less distinct but appears to be a wooden surface, possibly a table or desk. The person's fingers are positioned near the bottom of the phone, suggesting they are either holding it or about to interact with it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21084.6, "ram_available_mb": 41756.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21084.9, "ram_available_mb": 41756.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.756}, "power_stats": {"power_gpu_soc_mean_watts": 19.825, "power_cpu_cv_mean_watts": 1.828, "power_sys_5v0_mean_watts": 8.671, "gpu_utilization_percent_mean": 71.756, "power_watts_avg": 19.825, "energy_joules_est": 211.0, "duration_seconds": 10.643, "sample_count": 90}, "timestamp": "2026-01-26T16:48:50.547373"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8160.034, "latencies_ms": [8160.034], "images_per_second": 0.123, "prompt_tokens": 37, "response_tokens_est": 47, "n_tiles": 16, "output_text": "A person is holding a smartphone with their left hand, displaying a photo of a snowy landscape on the screen. The phone's screen is illuminated, and the time is displayed as 9:45.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21084.9, "ram_available_mb": 41756.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21101.4, "ram_available_mb": 41739.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.536}, "power_stats": {"power_gpu_soc_mean_watts": 20.926, "power_cpu_cv_mean_watts": 1.607, "power_sys_5v0_mean_watts": 8.576, "gpu_utilization_percent_mean": 73.536, "power_watts_avg": 20.926, "energy_joules_est": 170.77, "duration_seconds": 8.161, "sample_count": 69}, "timestamp": "2026-01-26T16:49:00.735352"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7876.734, "latencies_ms": [7876.734], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 46, "n_tiles": 16, "output_text": "The image shows a person holding a smartphone with a black case, displaying a photo with a snowy landscape and a tree. The phone's screen is off, and the time displayed is 9:45.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21101.4, "ram_available_mb": 41739.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21101.3, "ram_available_mb": 41739.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.701}, "power_stats": {"power_gpu_soc_mean_watts": 20.959, "power_cpu_cv_mean_watts": 1.625, "power_sys_5v0_mean_watts": 8.603, "gpu_utilization_percent_mean": 72.701, "power_watts_avg": 20.959, "energy_joules_est": 165.1, "duration_seconds": 7.877, "sample_count": 67}, "timestamp": "2026-01-26T16:49:10.656638"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12413.543, "latencies_ms": [12413.543], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a red and blue donation box situated on a sidewalk near a street. The box is designed to collect money for a campaign to end homelessness. It is positioned next to a sign that reads \"Campaign to End Homelessness.\" \n\nIn the background, there are several cars parked along the street, indicating that the location is likely a busy", "error": null, "sys_before": {"cpu_percent": 7.4, "ram_used_mb": 21101.3, "ram_available_mb": 41739.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21039.7, "ram_available_mb": 41801.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.143}, "power_stats": {"power_gpu_soc_mean_watts": 21.582, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.924, "gpu_utilization_percent_mean": 73.143, "power_watts_avg": 21.582, "energy_joules_est": 267.92, "duration_seconds": 12.414, "sample_count": 105}, "timestamp": "2026-01-26T16:49:25.098370"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 12848.402, "latencies_ms": [12848.402], "images_per_second": 0.078, "prompt_tokens": 39, "response_tokens_est": 78, "n_tiles": 16, "output_text": "1. Denver's Road Home donation box: 1\n2. Donate Home sign: 1\n3. Campaign to End Homelessness sign: 1\n4. Parking sign: 1\n5. Trees: multiple\n6. Bushes: multiple\n7. Sidewalk: 1\n8. Street: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21039.7, "ram_available_mb": 41801.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 21062.2, "ram_available_mb": 41778.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.309}, "power_stats": {"power_gpu_soc_mean_watts": 21.709, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 8.82, "gpu_utilization_percent_mean": 73.309, "power_watts_avg": 21.709, "energy_joules_est": 278.94, "duration_seconds": 12.849, "sample_count": 110}, "timestamp": "2026-01-26T16:49:39.999054"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8044.308, "latencies_ms": [8044.308], "images_per_second": 0.124, "prompt_tokens": 44, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The parking meter is located on the right side of the image, near the foreground. The sign below it is in the background, slightly to the left of the parking meter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21062.2, "ram_available_mb": 41778.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.9, "ram_used_mb": 21066.3, "ram_available_mb": 41774.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_gpu_soc_mean_watts": 23.508, "power_cpu_cv_mean_watts": 1.878, "power_sys_5v0_mean_watts": 8.869, "gpu_utilization_percent_mean": 78.0, "power_watts_avg": 23.508, "energy_joules_est": 189.12, "duration_seconds": 8.045, "sample_count": 68}, "timestamp": "2026-01-26T16:49:50.073667"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10860.192, "latencies_ms": [10860.192], "images_per_second": 0.092, "prompt_tokens": 37, "response_tokens_est": 61, "n_tiles": 16, "output_text": "A red and black donation box is placed on a sidewalk next to a sign that reads \"Campaign to End Homelessness.\" The box is located on Denver's Road Home, and it is likely that it is used to collect donations for the homelessness campaign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21066.3, "ram_available_mb": 41774.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21066.3, "ram_available_mb": 41774.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.935}, "power_stats": {"power_gpu_soc_mean_watts": 22.365, "power_cpu_cv_mean_watts": 1.623, "power_sys_5v0_mean_watts": 8.798, "gpu_utilization_percent_mean": 74.935, "power_watts_avg": 22.365, "energy_joules_est": 242.9, "duration_seconds": 10.861, "sample_count": 92}, "timestamp": "2026-01-26T16:50:02.969524"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11503.737, "latencies_ms": [11503.737], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 69, "n_tiles": 16, "output_text": "The image features a red and black parking meter with a white label that reads \"DENVER'S ROAD HOME\" and a sign below it that says \"CAMPAIGN TO END HOMELESSNESS.\" The parking meter is located on a sidewalk next to a concrete wall and some greenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21066.3, "ram_available_mb": 41774.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21066.6, "ram_available_mb": 41774.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.378}, "power_stats": {"power_gpu_soc_mean_watts": 22.006, "power_cpu_cv_mean_watts": 1.716, "power_sys_5v0_mean_watts": 8.892, "gpu_utilization_percent_mean": 74.378, "power_watts_avg": 22.006, "energy_joules_est": 253.16, "duration_seconds": 11.504, "sample_count": 98}, "timestamp": "2026-01-26T16:50:16.496790"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11586.206, "latencies_ms": [11586.206], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a group of zebras grazing in a grassy field. There are four zebras in total, with one zebra prominently in the foreground, and the other three zebras positioned further back in the scene. The zebras are standing on a lush green field, which provides them with ample space to graze and move around", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 21066.6, "ram_available_mb": 41774.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21092.3, "ram_available_mb": 41748.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.959}, "power_stats": {"power_gpu_soc_mean_watts": 19.394, "power_cpu_cv_mean_watts": 1.882, "power_sys_5v0_mean_watts": 8.684, "gpu_utilization_percent_mean": 69.959, "power_watts_avg": 19.394, "energy_joules_est": 224.72, "duration_seconds": 11.587, "sample_count": 98}, "timestamp": "2026-01-26T16:50:30.126839"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4249.11, "latencies_ms": [4249.11], "images_per_second": 0.235, "prompt_tokens": 39, "response_tokens_est": 13, "n_tiles": 16, "output_text": "zebra: 4\ngrass: many\n", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21092.3, "ram_available_mb": 41748.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 21093.3, "ram_available_mb": 41747.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_gpu_soc_mean_watts": 25.106, "power_cpu_cv_mean_watts": 0.878, "power_sys_5v0_mean_watts": 8.482, "gpu_utilization_percent_mean": 82.0, "power_watts_avg": 25.106, "energy_joules_est": 106.7, "duration_seconds": 4.25, "sample_count": 36}, "timestamp": "2026-01-26T16:50:36.412654"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11630.833, "latencies_ms": [11630.833], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a zebra grazing on the left side of the image, while the other two zebras are positioned further back, with one on the right side and the other partially visible on the far left. The zebra in the foreground is closer to the camera, making it appear larger, while the other two are farther away, appearing smaller.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21093.3, "ram_available_mb": 41747.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21093.0, "ram_available_mb": 41747.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.343}, "power_stats": {"power_gpu_soc_mean_watts": 19.081, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.653, "gpu_utilization_percent_mean": 70.343, "power_watts_avg": 19.081, "energy_joules_est": 221.94, "duration_seconds": 11.631, "sample_count": 99}, "timestamp": "2026-01-26T16:50:50.075070"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7545.426, "latencies_ms": [7545.426], "images_per_second": 0.133, "prompt_tokens": 37, "response_tokens_est": 42, "n_tiles": 16, "output_text": "A group of zebras are grazing in a grassy field with trees and bushes in the background. The zebras are standing close together and appear to be enjoying their meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21093.0, "ram_available_mb": 41747.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21118.2, "ram_available_mb": 41722.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.906}, "power_stats": {"power_gpu_soc_mean_watts": 21.422, "power_cpu_cv_mean_watts": 1.557, "power_sys_5v0_mean_watts": 8.563, "gpu_utilization_percent_mean": 73.906, "power_watts_avg": 21.422, "energy_joules_est": 161.65, "duration_seconds": 7.546, "sample_count": 64}, "timestamp": "2026-01-26T16:50:59.679432"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6639.258, "latencies_ms": [6639.258], "images_per_second": 0.151, "prompt_tokens": 36, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The zebras are grazing in a field with dry grass and green bushes in the background. The lighting is natural and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21118.2, "ram_available_mb": 41722.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21042.0, "ram_available_mb": 41798.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.946}, "power_stats": {"power_gpu_soc_mean_watts": 21.836, "power_cpu_cv_mean_watts": 1.486, "power_sys_5v0_mean_watts": 8.629, "gpu_utilization_percent_mean": 73.946, "power_watts_avg": 21.836, "energy_joules_est": 144.99, "duration_seconds": 6.64, "sample_count": 56}, "timestamp": "2026-01-26T16:51:08.347499"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11596.397, "latencies_ms": [11596.397], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is skillfully riding a wave on a surfboard. He is wearing a black wetsuit, which contrasts with the blue-green color of the ocean. The wave, a beautiful shade of green, is breaking to the right, creating a dynamic scene. The man is crouched on the surfboard, his arms outstretch", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21042.0, "ram_available_mb": 41798.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21082.4, "ram_available_mb": 41758.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.697}, "power_stats": {"power_gpu_soc_mean_watts": 19.291, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 70.697, "power_watts_avg": 19.291, "energy_joules_est": 223.72, "duration_seconds": 11.597, "sample_count": 99}, "timestamp": "2026-01-26T16:51:21.978641"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7877.267, "latencies_ms": [7877.267], "images_per_second": 0.127, "prompt_tokens": 39, "response_tokens_est": 45, "n_tiles": 16, "output_text": "water: numerous\nsurfboard: 1\nwetsuit: 1\nman: 1\nwave: 1\nsplash: multiple\nsun: visible in reflection\nsand: not visible", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.4, "ram_available_mb": 41758.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21123.1, "ram_available_mb": 41717.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.576}, "power_stats": {"power_gpu_soc_mean_watts": 21.288, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 73.576, "power_watts_avg": 21.288, "energy_joules_est": 167.71, "duration_seconds": 7.878, "sample_count": 66}, "timestamp": "2026-01-26T16:51:31.865582"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11461.465, "latencies_ms": [11461.465], "images_per_second": 0.087, "prompt_tokens": 44, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The surfer is positioned in the foreground, riding a wave that is breaking to the right. The wave originates in the background and extends towards the left side of the image, creating a dynamic spatial relationship between the surfer and the wave. The surfer is near the wave's crest, indicating they are actively engaged in surfing the wave.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21123.1, "ram_available_mb": 41717.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21123.1, "ram_available_mb": 41717.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.173}, "power_stats": {"power_gpu_soc_mean_watts": 19.312, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 70.173, "power_watts_avg": 19.312, "energy_joules_est": 221.36, "duration_seconds": 11.462, "sample_count": 98}, "timestamp": "2026-01-26T16:51:45.383096"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7648.509, "latencies_ms": [7648.509], "images_per_second": 0.131, "prompt_tokens": 37, "response_tokens_est": 43, "n_tiles": 16, "output_text": "A surfer in a black wetsuit is skillfully riding a wave in the ocean. The surfer is crouched on a surfboard, navigating the powerful force of the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21044.8, "ram_available_mb": 41796.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21120.2, "ram_available_mb": 41720.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.547}, "power_stats": {"power_gpu_soc_mean_watts": 21.341, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.603, "gpu_utilization_percent_mean": 74.547, "power_watts_avg": 21.341, "energy_joules_est": 163.24, "duration_seconds": 7.649, "sample_count": 64}, "timestamp": "2026-01-26T16:51:55.058014"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7568.208, "latencies_ms": [7568.208], "images_per_second": 0.132, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "The surfer is wearing a black wetsuit that contrasts with the bright blue of the ocean water. The wave is white and frothy, indicating it's a good wave for surfing.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21120.2, "ram_available_mb": 41720.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21120.2, "ram_available_mb": 41720.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.25}, "power_stats": {"power_gpu_soc_mean_watts": 21.046, "power_cpu_cv_mean_watts": 1.613, "power_sys_5v0_mean_watts": 8.604, "gpu_utilization_percent_mean": 72.25, "power_watts_avg": 21.046, "energy_joules_est": 159.29, "duration_seconds": 7.569, "sample_count": 64}, "timestamp": "2026-01-26T16:52:04.662600"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11607.125, "latencies_ms": [11607.125], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a serene moment on a snowy mountain. Two skiers, clad in white jackets and helmets, are seen in the foreground. The skier on the left is holding a pair of skis, ready to descend the slope, while the one on the right is adjusting their backpack. The sun is setting, casting a warm glow on", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 21120.2, "ram_available_mb": 41720.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21071.0, "ram_available_mb": 41769.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.374}, "power_stats": {"power_gpu_soc_mean_watts": 19.331, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.669, "gpu_utilization_percent_mean": 70.374, "power_watts_avg": 19.331, "energy_joules_est": 224.39, "duration_seconds": 11.608, "sample_count": 99}, "timestamp": "2026-01-26T16:52:18.339464"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8346.152, "latencies_ms": [8346.152], "images_per_second": 0.12, "prompt_tokens": 39, "response_tokens_est": 49, "n_tiles": 16, "output_text": "person: 2, ski poles: 2, backpacks: 2, ski boots: 2, ski gear: 2, ski tracks: 2, snow: 2, sun: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21071.0, "ram_available_mb": 41769.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21109.2, "ram_available_mb": 41731.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.306}, "power_stats": {"power_gpu_soc_mean_watts": 20.826, "power_cpu_cv_mean_watts": 1.645, "power_sys_5v0_mean_watts": 8.613, "gpu_utilization_percent_mean": 73.306, "power_watts_avg": 20.826, "energy_joules_est": 173.83, "duration_seconds": 8.347, "sample_count": 72}, "timestamp": "2026-01-26T16:52:28.744894"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11575.456, "latencies_ms": [11575.456], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a person standing on skis, holding ski poles, and wearing a backpack, positioned near the bottom right of the image. Another person is seen in the background, also on skis, standing further up the slope. The sun is positioned in the background, slightly to the left, casting light on the snow-covered mountain and the", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 21037.5, "ram_available_mb": 41803.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21077.4, "ram_available_mb": 41763.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.133}, "power_stats": {"power_gpu_soc_mean_watts": 19.332, "power_cpu_cv_mean_watts": 1.899, "power_sys_5v0_mean_watts": 8.689, "gpu_utilization_percent_mean": 70.133, "power_watts_avg": 19.332, "energy_joules_est": 223.79, "duration_seconds": 11.576, "sample_count": 98}, "timestamp": "2026-01-26T16:52:42.349715"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9374.539, "latencies_ms": [9374.539], "images_per_second": 0.107, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image captures a serene moment on a snowy mountain peak during sunrise or sunset, with two skiers preparing to descend. The warm glow of the sun illuminates the scene, casting long shadows and highlighting the crisp white snow.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21077.4, "ram_available_mb": 41763.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21108.8, "ram_available_mb": 41732.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.987}, "power_stats": {"power_gpu_soc_mean_watts": 20.39, "power_cpu_cv_mean_watts": 1.721, "power_sys_5v0_mean_watts": 8.588, "gpu_utilization_percent_mean": 71.987, "power_watts_avg": 20.39, "energy_joules_est": 191.16, "duration_seconds": 9.375, "sample_count": 80}, "timestamp": "2026-01-26T16:52:53.761260"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11003.177, "latencies_ms": [11003.177], "images_per_second": 0.091, "prompt_tokens": 36, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image captures a serene winter scene with the sun low on the horizon casting a warm glow on the snow-covered mountains. Two skiers, clad in white gear, are silhouetted against the bright light, one holding a ski pole and the other with a backpack, suggesting they are on a skiing adventure.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21036.9, "ram_available_mb": 41804.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21084.5, "ram_available_mb": 41756.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.505}, "power_stats": {"power_gpu_soc_mean_watts": 19.332, "power_cpu_cv_mean_watts": 1.866, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 70.505, "power_watts_avg": 19.332, "energy_joules_est": 212.73, "duration_seconds": 11.004, "sample_count": 95}, "timestamp": "2026-01-26T16:53:06.816941"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11602.428, "latencies_ms": [11602.428], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a baseball game in progress, with a batter standing at home plate, holding a baseball bat and preparing to swing. The catcher is crouched behind him, wearing a baseball glove and ready to catch the ball. The umpire is also present, closely observing the game.\n\nIn the background, there are several people watching the game, including", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 21084.5, "ram_available_mb": 41756.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21084.8, "ram_available_mb": 41756.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_gpu_soc_mean_watts": 19.336, "power_cpu_cv_mean_watts": 1.876, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 70.0, "power_watts_avg": 19.336, "energy_joules_est": 224.36, "duration_seconds": 11.603, "sample_count": 99}, "timestamp": "2026-01-26T16:53:20.450920"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10483.872, "latencies_ms": [10483.872], "images_per_second": 0.095, "prompt_tokens": 39, "response_tokens_est": 68, "n_tiles": 16, "output_text": "1. Baseball players: 3\n2. Baseball bat: 1\n3. Baseball glove: 1\n4. Catcher: 1\n5. Umpire: 1\n6. Spectators: 3\n7. Cars: 3\n8. Fence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21084.8, "ram_available_mb": 41756.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21085.0, "ram_available_mb": 41755.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.056}, "power_stats": {"power_gpu_soc_mean_watts": 19.857, "power_cpu_cv_mean_watts": 1.806, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 72.056, "power_watts_avg": 19.857, "energy_joules_est": 208.19, "duration_seconds": 10.484, "sample_count": 90}, "timestamp": "2026-01-26T16:53:32.963031"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10026.728, "latencies_ms": [10026.728], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "In the foreground, a baseball player is standing near the home plate, holding a bat and ready to swing. Behind the batter, the catcher is crouched behind home plate, wearing a baseball glove. In the background, there are spectators seated on chairs, watching the game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21085.0, "ram_available_mb": 41755.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21086.0, "ram_available_mb": 41754.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.365}, "power_stats": {"power_gpu_soc_mean_watts": 19.661, "power_cpu_cv_mean_watts": 1.813, "power_sys_5v0_mean_watts": 8.645, "gpu_utilization_percent_mean": 71.365, "power_watts_avg": 19.661, "energy_joules_est": 197.15, "duration_seconds": 10.027, "sample_count": 85}, "timestamp": "2026-01-26T16:53:45.013851"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8576.851, "latencies_ms": [8576.851], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image captures a moment from a baseball game, with a batter in a red shirt and white pants at the plate, ready to swing. In the background, there are spectators seated on chairs, watching the game unfold.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21086.0, "ram_available_mb": 41754.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 8.9, "ram_used_mb": 21090.6, "ram_available_mb": 41750.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.548}, "power_stats": {"power_gpu_soc_mean_watts": 20.612, "power_cpu_cv_mean_watts": 1.941, "power_sys_5v0_mean_watts": 8.622, "gpu_utilization_percent_mean": 73.548, "power_watts_avg": 20.612, "energy_joules_est": 176.8, "duration_seconds": 8.577, "sample_count": 73}, "timestamp": "2026-01-26T16:53:55.609277"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7862.016, "latencies_ms": [7862.016], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a baseball game in progress with players wearing helmets and holding baseball gloves. The weather appears to be sunny and clear, as indicated by the bright lighting and shadows cast on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21090.6, "ram_available_mb": 41750.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 9.7, "ram_used_mb": 21098.9, "ram_available_mb": 41742.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.682}, "power_stats": {"power_gpu_soc_mean_watts": 21.076, "power_cpu_cv_mean_watts": 2.183, "power_sys_5v0_mean_watts": 8.732, "gpu_utilization_percent_mean": 73.682, "power_watts_avg": 21.076, "energy_joules_est": 165.71, "duration_seconds": 7.863, "sample_count": 66}, "timestamp": "2026-01-26T16:54:05.494507"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12405.557, "latencies_ms": [12405.557], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a moment of indulgence, featuring a slice of cake and a milkshake on a table. The cake, with its light yellow hue, is placed on a white plate, accompanied by a fork and knife. The milkshake, served in a tall glass, is topped with whipped cream, adding a touch of elegance to the scene", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 21098.6, "ram_available_mb": 41742.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21098.7, "ram_available_mb": 41742.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.439}, "power_stats": {"power_gpu_soc_mean_watts": 21.5, "power_cpu_cv_mean_watts": 1.847, "power_sys_5v0_mean_watts": 8.927, "gpu_utilization_percent_mean": 72.439, "power_watts_avg": 21.5, "energy_joules_est": 266.73, "duration_seconds": 12.406, "sample_count": 107}, "timestamp": "2026-01-26T16:54:19.922983"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11335.791, "latencies_ms": [11335.791], "images_per_second": 0.088, "prompt_tokens": 39, "response_tokens_est": 65, "n_tiles": 16, "output_text": "- Glass: 1\n\n- Whipped cream: 1\n\n- Ice cream: 1\n\n- Fork: 2\n\n- Knife: 2\n\n- Napkin: 1\n\n- Cake: 1\n\n- Table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21098.7, "ram_available_mb": 41742.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21098.7, "ram_available_mb": 41742.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.729}, "power_stats": {"power_gpu_soc_mean_watts": 22.145, "power_cpu_cv_mean_watts": 1.642, "power_sys_5v0_mean_watts": 8.784, "gpu_utilization_percent_mean": 74.729, "power_watts_avg": 22.145, "energy_joules_est": 251.05, "duration_seconds": 11.336, "sample_count": 96}, "timestamp": "2026-01-26T16:54:33.296956"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12715.795, "latencies_ms": [12715.795], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a slice of cake on a white plate, positioned near the center of the image. Behind the cake, slightly to the left, is a tall glass filled with a creamy beverage, which appears to be the main focus of the image. The background is less distinct but shows the interior of a cafe with other patrons and tables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21098.7, "ram_available_mb": 41742.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21116.9, "ram_available_mb": 41724.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.822}, "power_stats": {"power_gpu_soc_mean_watts": 21.638, "power_cpu_cv_mean_watts": 1.784, "power_sys_5v0_mean_watts": 8.887, "gpu_utilization_percent_mean": 73.822, "power_watts_avg": 21.638, "energy_joules_est": 275.16, "duration_seconds": 12.716, "sample_count": 107}, "timestamp": "2026-01-26T16:54:48.037681"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8547.571, "latencies_ms": [8547.571], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "In the image, there is a dessert table with a slice of cake and a milkshake on it. The table is in a restaurant with people sitting at the tables in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21055.1, "ram_available_mb": 41785.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21086.3, "ram_available_mb": 41754.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.708}, "power_stats": {"power_gpu_soc_mean_watts": 23.411, "power_cpu_cv_mean_watts": 1.4, "power_sys_5v0_mean_watts": 8.748, "gpu_utilization_percent_mean": 76.708, "power_watts_avg": 23.411, "energy_joules_est": 200.12, "duration_seconds": 8.548, "sample_count": 72}, "timestamp": "2026-01-26T16:54:58.606833"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10482.681, "latencies_ms": [10482.681], "images_per_second": 0.095, "prompt_tokens": 36, "response_tokens_est": 60, "n_tiles": 16, "output_text": "The image features a dessert setting with a slice of cake on a white plate and a milkshake in a tall glass with a straw, placed on a dark wooden table. The lighting in the room is warm and ambient, suggesting an indoor setting with soft lighting.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 21086.3, "ram_available_mb": 41754.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21086.0, "ram_available_mb": 41754.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.856}, "power_stats": {"power_gpu_soc_mean_watts": 22.175, "power_cpu_cv_mean_watts": 1.667, "power_sys_5v0_mean_watts": 8.882, "gpu_utilization_percent_mean": 74.856, "power_watts_avg": 22.175, "energy_joules_est": 232.47, "duration_seconds": 10.483, "sample_count": 90}, "timestamp": "2026-01-26T16:55:11.109952"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11553.906, "latencies_ms": [11553.906], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a large, multi-tiered wedding cake placed on a dining table. The cake is decorated with white and blue frosting, and it has a white and blue theme. The table is surrounded by numerous chairs, indicating that it is set up for a wedding reception. \n\nIn addition to the cake, there are several wine", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 21086.0, "ram_available_mb": 41754.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21085.2, "ram_available_mb": 41755.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.485}, "power_stats": {"power_gpu_soc_mean_watts": 19.14, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.658, "gpu_utilization_percent_mean": 69.485, "power_watts_avg": 19.14, "energy_joules_est": 221.15, "duration_seconds": 11.555, "sample_count": 99}, "timestamp": "2026-01-26T16:55:24.696329"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7673.175, "latencies_ms": [7673.175], "images_per_second": 0.13, "prompt_tokens": 39, "response_tokens_est": 43, "n_tiles": 16, "output_text": "wedding cake: 3 tiers, chairs: multiple, tables: multiple, decorations: multiple, flowers: multiple, candles: multiple, guests: multiple, venue: large room", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21085.2, "ram_available_mb": 41755.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21086.2, "ram_available_mb": 41754.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.308}, "power_stats": {"power_gpu_soc_mean_watts": 21.431, "power_cpu_cv_mean_watts": 1.576, "power_sys_5v0_mean_watts": 8.568, "gpu_utilization_percent_mean": 73.308, "power_watts_avg": 21.431, "energy_joules_est": 164.46, "duration_seconds": 7.674, "sample_count": 65}, "timestamp": "2026-01-26T16:55:34.418511"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10160.901, "latencies_ms": [10160.901], "images_per_second": 0.098, "prompt_tokens": 44, "response_tokens_est": 66, "n_tiles": 16, "output_text": "The wedding cake is the central object in the foreground, placed on a table covered with a blue cloth. In the background, there are multiple tables set up for a banquet, with chairs arranged around them. A chandelier hangs from the ceiling, providing light to the entire scene.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 21086.2, "ram_available_mb": 41754.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21086.5, "ram_available_mb": 41754.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.977}, "power_stats": {"power_gpu_soc_mean_watts": 19.922, "power_cpu_cv_mean_watts": 1.792, "power_sys_5v0_mean_watts": 8.638, "gpu_utilization_percent_mean": 70.977, "power_watts_avg": 19.922, "energy_joules_est": 202.44, "duration_seconds": 10.162, "sample_count": 86}, "timestamp": "2026-01-26T16:55:46.608064"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9203.301, "latencies_ms": [9203.301], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The image showcases a beautifully decorated wedding cake placed on a table covered with a blue cloth. The cake is adorned with white and blue frosting, and is surrounded by elegant chairs and tables set up for a wedding reception.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 21086.5, "ram_available_mb": 41754.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 21086.8, "ram_available_mb": 41754.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.658}, "power_stats": {"power_gpu_soc_mean_watts": 20.299, "power_cpu_cv_mean_watts": 1.717, "power_sys_5v0_mean_watts": 8.587, "gpu_utilization_percent_mean": 72.658, "power_watts_avg": 20.299, "energy_joules_est": 186.83, "duration_seconds": 9.204, "sample_count": 79}, "timestamp": "2026-01-26T16:55:57.852033"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9541.543, "latencies_ms": [9541.543], "images_per_second": 0.105, "prompt_tokens": 36, "response_tokens_est": 61, "n_tiles": 16, "output_text": "The wedding cake is adorned with white and blue frosting, featuring intricate designs and a large white floral decoration on top. The room is elegantly decorated with a chandelier hanging from the ceiling, providing a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21086.8, "ram_available_mb": 41754.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21087.4, "ram_available_mb": 41753.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.561}, "power_stats": {"power_gpu_soc_mean_watts": 20.307, "power_cpu_cv_mean_watts": 1.771, "power_sys_5v0_mean_watts": 8.668, "gpu_utilization_percent_mean": 70.561, "power_watts_avg": 20.307, "energy_joules_est": 193.77, "duration_seconds": 9.542, "sample_count": 82}, "timestamp": "2026-01-26T16:56:09.409067"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11567.004, "latencies_ms": [11567.004], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing in a kitchen, holding a plate with a piece of food on it. She is wearing a blue and red sweater and is smiling as she poses for the camera. The kitchen is equipped with a stove, and there are several bottles placed on the countertop. The woman appears to be enjoying her meal and is likely", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 21087.4, "ram_available_mb": 41753.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21088.2, "ram_available_mb": 41752.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.63}, "power_stats": {"power_gpu_soc_mean_watts": 19.085, "power_cpu_cv_mean_watts": 1.905, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 70.63, "power_watts_avg": 19.085, "energy_joules_est": 220.77, "duration_seconds": 11.568, "sample_count": 100}, "timestamp": "2026-01-26T16:56:23.009603"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7815.665, "latencies_ms": [7815.665], "images_per_second": 0.128, "prompt_tokens": 39, "response_tokens_est": 44, "n_tiles": 16, "output_text": "pot: 2, plate: 1, spoon: 1, knife: 0, bowl: 0, bottle: 0, can: 0, jar: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21088.2, "ram_available_mb": 41752.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21108.9, "ram_available_mb": 41732.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.545}, "power_stats": {"power_gpu_soc_mean_watts": 21.342, "power_cpu_cv_mean_watts": 1.564, "power_sys_5v0_mean_watts": 8.553, "gpu_utilization_percent_mean": 73.545, "power_watts_avg": 21.342, "energy_joules_est": 166.81, "duration_seconds": 7.816, "sample_count": 66}, "timestamp": "2026-01-26T16:56:32.843968"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10698.252, "latencies_ms": [10698.252], "images_per_second": 0.093, "prompt_tokens": 44, "response_tokens_est": 71, "n_tiles": 16, "output_text": "In the foreground, a person is standing in front of a stove, holding a plate with food. The stove is on the left side of the image, and there are two pots on it. In the background, there is a wall with various items on it, including a poster and shelves with bottles and containers.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 21108.9, "ram_available_mb": 41732.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21053.8, "ram_available_mb": 41787.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.418}, "power_stats": {"power_gpu_soc_mean_watts": 19.83, "power_cpu_cv_mean_watts": 1.843, "power_sys_5v0_mean_watts": 8.681, "gpu_utilization_percent_mean": 70.418, "power_watts_avg": 19.83, "energy_joules_est": 212.16, "duration_seconds": 10.699, "sample_count": 91}, "timestamp": "2026-01-26T16:56:45.554732"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8936.429, "latencies_ms": [8936.429], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "A person is standing in a kitchen, wearing a blue and red sweater, and is holding a plate with food on it. There are various items on the wall, including a poster with a cartoon character and a shelf with jars and containers.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 21053.8, "ram_available_mb": 41787.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21088.6, "ram_available_mb": 41752.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.519}, "power_stats": {"power_gpu_soc_mean_watts": 20.451, "power_cpu_cv_mean_watts": 1.689, "power_sys_5v0_mean_watts": 8.602, "gpu_utilization_percent_mean": 73.519, "power_watts_avg": 20.451, "energy_joules_est": 182.77, "duration_seconds": 8.937, "sample_count": 77}, "timestamp": "2026-01-26T16:56:56.521271"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6608.009, "latencies_ms": [6608.009], "images_per_second": 0.151, "prompt_tokens": 36, "response_tokens_est": 35, "n_tiles": 16, "output_text": "The image shows a person in a blue and red sweater standing in a kitchen. The kitchen has a white stove and a shelf with various items on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21088.6, "ram_available_mb": 41752.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21041.9, "ram_available_mb": 41799.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.25}, "power_stats": {"power_gpu_soc_mean_watts": 21.914, "power_cpu_cv_mean_watts": 1.479, "power_sys_5v0_mean_watts": 8.6, "gpu_utilization_percent_mean": 75.25, "power_watts_avg": 21.914, "energy_joules_est": 144.82, "duration_seconds": 6.609, "sample_count": 56}, "timestamp": "2026-01-26T16:57:05.159894"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11585.064, "latencies_ms": [11585.064], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a woman is standing in a dirt field, holding a rope that is attached to a white horse. The woman is wearing a pink shirt and black pants, and she is also wearing brown boots. The horse is positioned behind the woman, and it appears to be looking at her. The scene takes place in a fenced-in area", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 21041.9, "ram_available_mb": 41799.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21043.2, "ram_available_mb": 41797.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.444}, "power_stats": {"power_gpu_soc_mean_watts": 19.157, "power_cpu_cv_mean_watts": 1.9, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 70.444, "power_watts_avg": 19.157, "energy_joules_est": 221.95, "duration_seconds": 11.586, "sample_count": 99}, "timestamp": "2026-01-26T16:57:18.785220"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9989.058, "latencies_ms": [9989.058], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Horse: 1\n2. Woman: 1\n3. Rope: 2\n4. Ground: 1\n5. Trees: 2\n6. Fence: 2\n7. Boot: 2\n8. Pole: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21043.2, "ram_available_mb": 41797.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21086.4, "ram_available_mb": 41754.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.151}, "power_stats": {"power_gpu_soc_mean_watts": 20.122, "power_cpu_cv_mean_watts": 1.768, "power_sys_5v0_mean_watts": 8.597, "gpu_utilization_percent_mean": 72.151, "power_watts_avg": 20.122, "energy_joules_est": 201.01, "duration_seconds": 9.99, "sample_count": 86}, "timestamp": "2026-01-26T16:57:30.793186"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11636.351, "latencies_ms": [11636.351], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, a woman is standing on the left side of the image, holding a rope that extends towards the horse in the background. The horse is positioned on the right side of the image, with its head turned towards the woman, creating a sense of interaction between the two. The background features a wooden fence, which is further away from the viewer, adding depth to", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21086.4, "ram_available_mb": 41754.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21086.6, "ram_available_mb": 41754.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.364}, "power_stats": {"power_gpu_soc_mean_watts": 19.554, "power_cpu_cv_mean_watts": 1.904, "power_sys_5v0_mean_watts": 8.688, "gpu_utilization_percent_mean": 70.364, "power_watts_avg": 19.554, "energy_joules_est": 227.55, "duration_seconds": 11.637, "sample_count": 99}, "timestamp": "2026-01-26T16:57:44.487908"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7925.314, "latencies_ms": [7925.314], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A woman in a pink shirt and black pants is holding a white rope in an outdoor setting with a white horse nearby. The woman appears to be leading the horse or preparing to do so.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21086.6, "ram_available_mb": 41754.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21087.1, "ram_available_mb": 41753.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.194}, "power_stats": {"power_gpu_soc_mean_watts": 21.046, "power_cpu_cv_mean_watts": 1.589, "power_sys_5v0_mean_watts": 8.584, "gpu_utilization_percent_mean": 73.194, "power_watts_avg": 21.046, "energy_joules_est": 166.81, "duration_seconds": 7.926, "sample_count": 67}, "timestamp": "2026-01-26T16:57:54.447008"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8435.153, "latencies_ms": [8435.153], "images_per_second": 0.119, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image features a woman in a pink shirt and black pants standing in a dirt field with a white horse nearby. The lighting is natural and bright, suggesting that the photo was taken outdoors on a sunny day.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21087.1, "ram_available_mb": 41753.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21088.1, "ram_available_mb": 41752.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.732}, "power_stats": {"power_gpu_soc_mean_watts": 20.702, "power_cpu_cv_mean_watts": 1.651, "power_sys_5v0_mean_watts": 8.621, "gpu_utilization_percent_mean": 72.732, "power_watts_avg": 20.702, "energy_joules_est": 174.64, "duration_seconds": 8.436, "sample_count": 71}, "timestamp": "2026-01-26T16:58:04.904194"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11517.313, "latencies_ms": [11517.313], "images_per_second": 0.087, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a busy city street with multiple cars and a bus driving down the road. There are at least 11 cars visible, with some closer to the foreground and others further away. The bus is located towards the right side of the scene, and there are two traffic lights visible, one on the left side and another on the right side of the street.\n\nA person", "error": null, "sys_before": {"cpu_percent": 13.6, "ram_used_mb": 21088.1, "ram_available_mb": 41752.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 21041.9, "ram_available_mb": 41799.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.959}, "power_stats": {"power_gpu_soc_mean_watts": 19.296, "power_cpu_cv_mean_watts": 2.066, "power_sys_5v0_mean_watts": 8.71, "gpu_utilization_percent_mean": 69.959, "power_watts_avg": 19.296, "energy_joules_est": 222.25, "duration_seconds": 11.518, "sample_count": 98}, "timestamp": "2026-01-26T16:58:18.445813"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9772.848, "latencies_ms": [9772.848], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 61, "n_tiles": 16, "output_text": "- Cars: 10\n- Buildings: 5\n- Traffic lights: 4\n- Signs: 3\n- Trees: 2\n- Pedestrians: 1\n- Buses: 1\n- Bicycles: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21041.9, "ram_available_mb": 41799.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21093.9, "ram_available_mb": 41747.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.614}, "power_stats": {"power_gpu_soc_mean_watts": 20.167, "power_cpu_cv_mean_watts": 1.75, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 71.614, "power_watts_avg": 20.167, "energy_joules_est": 197.1, "duration_seconds": 9.773, "sample_count": 83}, "timestamp": "2026-01-26T16:58:30.258503"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10441.92, "latencies_ms": [10441.92], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 68, "n_tiles": 16, "output_text": "In the foreground, there is a busy street with multiple cars and a bus, indicating a high-traffic area. The traffic light is positioned in the middle ground, showing a red light for the vehicles. In the background, there are tall buildings that appear to be residential or commercial structures, suggesting an urban setting.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21093.9, "ram_available_mb": 41747.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21094.3, "ram_available_mb": 41746.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.202}, "power_stats": {"power_gpu_soc_mean_watts": 19.744, "power_cpu_cv_mean_watts": 1.803, "power_sys_5v0_mean_watts": 8.629, "gpu_utilization_percent_mean": 70.202, "power_watts_avg": 19.744, "energy_joules_est": 206.18, "duration_seconds": 10.443, "sample_count": 89}, "timestamp": "2026-01-26T16:58:42.742877"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8730.858, "latencies_ms": [8730.858], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "The image depicts a busy city street with multiple vehicles, including cars and a bus, driving in both directions. There are traffic lights and a pedestrian crossing, indicating an urban environment with a focus on transportation and pedestrian safety.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21094.3, "ram_available_mb": 41746.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21106.6, "ram_available_mb": 41734.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.573}, "power_stats": {"power_gpu_soc_mean_watts": 20.532, "power_cpu_cv_mean_watts": 1.659, "power_sys_5v0_mean_watts": 8.579, "gpu_utilization_percent_mean": 73.573, "power_watts_avg": 20.532, "energy_joules_est": 179.28, "duration_seconds": 8.732, "sample_count": 75}, "timestamp": "2026-01-26T16:58:53.530469"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 10262.83, "latencies_ms": [10262.83], "images_per_second": 0.097, "prompt_tokens": 36, "response_tokens_est": 67, "n_tiles": 16, "output_text": "The image depicts a city street scene with a mix of vehicles, including cars and a bus, traveling on a wet road, suggesting recent rain. The lighting is overcast, with no direct sunlight visible, and the overall color palette is muted with grays and blues dominating the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21044.8, "ram_available_mb": 41796.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21075.7, "ram_available_mb": 41765.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.102}, "power_stats": {"power_gpu_soc_mean_watts": 19.917, "power_cpu_cv_mean_watts": 1.823, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 71.102, "power_watts_avg": 19.917, "energy_joules_est": 204.42, "duration_seconds": 10.263, "sample_count": 88}, "timestamp": "2026-01-26T16:59:05.838790"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11584.804, "latencies_ms": [11584.804], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a person is standing in a bathroom, facing a stainless steel toilet. The toilet is positioned in the center of the image, with the person's legs visible in the foreground. The bathroom walls are tiled, and there is a blue toilet brush hanging on the wall to the left of the toilet", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21075.7, "ram_available_mb": 41765.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21045.8, "ram_available_mb": 41795.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.253}, "power_stats": {"power_gpu_soc_mean_watts": 19.37, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.651, "gpu_utilization_percent_mean": 69.253, "power_watts_avg": 19.37, "energy_joules_est": 224.41, "duration_seconds": 11.585, "sample_count": 99}, "timestamp": "2026-01-26T16:59:19.489509"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 11794.649, "latencies_ms": [11794.649], "images_per_second": 0.085, "prompt_tokens": 39, "response_tokens_est": 81, "n_tiles": 16, "output_text": "object: 1 toilet, count: 1\nobject: 1 toilet brush, count: 1\nobject: 1 toilet paper, count: 1\nobject: 1 toilet brush holder, count: 1\nobject: 1 toilet paper holder, count: 1\nobject: 1 toilet", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21045.8, "ram_available_mb": 41795.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21084.0, "ram_available_mb": 41756.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.109}, "power_stats": {"power_gpu_soc_mean_watts": 19.415, "power_cpu_cv_mean_watts": 1.858, "power_sys_5v0_mean_watts": 8.643, "gpu_utilization_percent_mean": 70.109, "power_watts_avg": 19.415, "energy_joules_est": 229.01, "duration_seconds": 11.795, "sample_count": 101}, "timestamp": "2026-01-26T16:59:33.305617"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10008.088, "latencies_ms": [10008.088], "images_per_second": 0.1, "prompt_tokens": 44, "response_tokens_est": 65, "n_tiles": 16, "output_text": "The toilet is located in the foreground of the image, with the person standing in front of it. The toilet brush is positioned to the left of the toilet, near the top. The pipes are in the background, running vertically along the right side of the image.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21084.0, "ram_available_mb": 41756.9, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21084.5, "ram_available_mb": 41756.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.2}, "power_stats": {"power_gpu_soc_mean_watts": 19.807, "power_cpu_cv_mean_watts": 1.775, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 71.2, "power_watts_avg": 19.807, "energy_joules_est": 198.24, "duration_seconds": 10.009, "sample_count": 85}, "timestamp": "2026-01-26T16:59:45.329989"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7442.756, "latencies_ms": [7442.756], "images_per_second": 0.134, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A person is standing in front of a stainless steel toilet in a public restroom. The toilet has a blue plunger hanging on the wall next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21084.5, "ram_available_mb": 41756.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21111.7, "ram_available_mb": 41729.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.444}, "power_stats": {"power_gpu_soc_mean_watts": 21.526, "power_cpu_cv_mean_watts": 1.531, "power_sys_5v0_mean_watts": 8.593, "gpu_utilization_percent_mean": 74.444, "power_watts_avg": 21.526, "energy_joules_est": 160.23, "duration_seconds": 7.443, "sample_count": 63}, "timestamp": "2026-01-26T16:59:54.800913"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7882.581, "latencies_ms": [7882.581], "images_per_second": 0.127, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The image shows a metallic toilet with a shiny, reflective surface, set against a backdrop of beige tiles. A blue plunger is visible to the left of the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21111.7, "ram_available_mb": 41729.2, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21112.6, "ram_available_mb": 41728.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.955}, "power_stats": {"power_gpu_soc_mean_watts": 20.878, "power_cpu_cv_mean_watts": 1.66, "power_sys_5v0_mean_watts": 8.62, "gpu_utilization_percent_mean": 71.955, "power_watts_avg": 20.878, "energy_joules_est": 164.59, "duration_seconds": 7.883, "sample_count": 67}, "timestamp": "2026-01-26T17:00:04.735065"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11576.351, "latencies_ms": [11576.351], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a room with a wooden floor, where a pink bicycle is prominently displayed. The bicycle is parked in the middle of the room, surrounded by other bicycles of various sizes and colors. There are at least ten bicycles in the room, with some placed close to the pink bicycle and others scattered around the room.\n", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 21112.6, "ram_available_mb": 41728.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21061.5, "ram_available_mb": 41779.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.765}, "power_stats": {"power_gpu_soc_mean_watts": 19.224, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.686, "gpu_utilization_percent_mean": 69.765, "power_watts_avg": 19.224, "energy_joules_est": 222.56, "duration_seconds": 11.577, "sample_count": 98}, "timestamp": "2026-01-26T17:00:18.350144"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8857.097, "latencies_ms": [8857.097], "images_per_second": 0.113, "prompt_tokens": 39, "response_tokens_est": 53, "n_tiles": 16, "output_text": "bicycle: 10, wall: 1, floor: 1, flower: 1, seat: 1, handlebar: 1, pedal: 1, bike rack: 1, woman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21061.5, "ram_available_mb": 41779.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21079.2, "ram_available_mb": 41761.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.693}, "power_stats": {"power_gpu_soc_mean_watts": 20.545, "power_cpu_cv_mean_watts": 1.659, "power_sys_5v0_mean_watts": 8.594, "gpu_utilization_percent_mean": 72.693, "power_watts_avg": 20.545, "energy_joules_est": 181.98, "duration_seconds": 8.858, "sample_count": 75}, "timestamp": "2026-01-26T17:00:29.248825"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11647.738, "latencies_ms": [11647.738], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a pink bicycle with a decorative flower basket on the handlebars, positioned in the center of the image. To the right, there is a row of other bicycles, including a black one and a blue one, which are parked against the wall in the background. The floor is made of light-colored wood, and the", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21079.2, "ram_available_mb": 41761.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21080.7, "ram_available_mb": 41760.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.515}, "power_stats": {"power_gpu_soc_mean_watts": 19.411, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.678, "gpu_utilization_percent_mean": 70.515, "power_watts_avg": 19.411, "energy_joules_est": 226.11, "duration_seconds": 11.649, "sample_count": 99}, "timestamp": "2026-01-26T17:00:42.940887"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7918.103, "latencies_ms": [7918.103], "images_per_second": 0.126, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "The image shows a bicycle shop with a variety of bicycles on display. A pink bicycle with a flower basket is the focal point of the image, surrounded by other bicycles.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21080.7, "ram_available_mb": 41760.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21080.9, "ram_available_mb": 41760.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.746}, "power_stats": {"power_gpu_soc_mean_watts": 21.297, "power_cpu_cv_mean_watts": 1.571, "power_sys_5v0_mean_watts": 8.569, "gpu_utilization_percent_mean": 73.746, "power_watts_avg": 21.297, "energy_joules_est": 168.64, "duration_seconds": 7.919, "sample_count": 67}, "timestamp": "2026-01-26T17:00:52.919779"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8234.201, "latencies_ms": [8234.201], "images_per_second": 0.121, "prompt_tokens": 36, "response_tokens_est": 49, "n_tiles": 16, "output_text": "The image features a collection of bicycles, predominantly in shades of pink and black, displayed indoors on a wooden floor. The lighting is bright and natural, suggesting the photo was taken during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21080.9, "ram_available_mb": 41760.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21080.9, "ram_available_mb": 41760.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.522}, "power_stats": {"power_gpu_soc_mean_watts": 20.756, "power_cpu_cv_mean_watts": 1.635, "power_sys_5v0_mean_watts": 8.609, "gpu_utilization_percent_mean": 73.522, "power_watts_avg": 20.756, "energy_joules_est": 170.92, "duration_seconds": 8.235, "sample_count": 69}, "timestamp": "2026-01-26T17:01:03.170570"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11586.549, "latencies_ms": [11586.549], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the heart of a dry savanna, a majestic giraffe stands tall and proud. Its body, a beautiful canvas of brown and white spots, contrasts with the surrounding landscape. The giraffe's long neck stretches upwards, reaching for the sky, while its legs are firmly planted on the ground. It faces the right side of the image,", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21080.9, "ram_available_mb": 41760.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21081.9, "ram_available_mb": 41759.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.765}, "power_stats": {"power_gpu_soc_mean_watts": 19.372, "power_cpu_cv_mean_watts": 1.874, "power_sys_5v0_mean_watts": 8.67, "gpu_utilization_percent_mean": 70.765, "power_watts_avg": 19.372, "energy_joules_est": 224.47, "duration_seconds": 11.587, "sample_count": 98}, "timestamp": "2026-01-26T17:01:16.789415"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7460.582, "latencies_ms": [7460.582], "images_per_second": 0.134, "prompt_tokens": 39, "response_tokens_est": 41, "n_tiles": 16, "output_text": "giraffe: 1, grass: numerous, trees: 3, bushes: 2, sky: 1, clouds: 1, ground: dry, water: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21081.9, "ram_available_mb": 41759.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21113.3, "ram_available_mb": 41727.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.143}, "power_stats": {"power_gpu_soc_mean_watts": 21.419, "power_cpu_cv_mean_watts": 1.537, "power_sys_5v0_mean_watts": 8.602, "gpu_utilization_percent_mean": 74.143, "power_watts_avg": 21.419, "energy_joules_est": 159.82, "duration_seconds": 7.462, "sample_count": 63}, "timestamp": "2026-01-26T17:01:26.300359"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11589.078, "latencies_ms": [11589.078], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The giraffe is standing in the foreground of the image, with its body facing towards the right side of the frame. In the background, there are trees and shrubs scattered across the landscape, with one prominent tree standing taller than the rest. The giraffe appears to be in a natural habitat, possibly a savannah or grassland, with no other animals or people in", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21113.3, "ram_available_mb": 41727.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21113.6, "ram_available_mb": 41727.3, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.04}, "power_stats": {"power_gpu_soc_mean_watts": 19.296, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 70.04, "power_watts_avg": 19.296, "energy_joules_est": 223.63, "duration_seconds": 11.59, "sample_count": 99}, "timestamp": "2026-01-26T17:01:39.910675"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7217.088, "latencies_ms": [7217.088], "images_per_second": 0.139, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "A giraffe is standing in a dry grassland with sparse trees in the background. The sky is overcast, and the giraffe appears to be looking off into the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21113.6, "ram_available_mb": 41727.3, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21120.3, "ram_available_mb": 41720.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.607}, "power_stats": {"power_gpu_soc_mean_watts": 21.714, "power_cpu_cv_mean_watts": 1.515, "power_sys_5v0_mean_watts": 8.571, "gpu_utilization_percent_mean": 74.607, "power_watts_avg": 21.714, "energy_joules_est": 156.72, "duration_seconds": 7.218, "sample_count": 61}, "timestamp": "2026-01-26T17:01:49.159700"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7906.048, "latencies_ms": [7906.048], "images_per_second": 0.126, "prompt_tokens": 36, "response_tokens_est": 47, "n_tiles": 16, "output_text": "The giraffe has a pattern of brown spots on its body, and it is standing in a dry grassland with a cloudy sky above. The lighting is soft, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 13.3, "ram_used_mb": 21120.3, "ram_available_mb": 41720.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21078.1, "ram_available_mb": 41762.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.761}, "power_stats": {"power_gpu_soc_mean_watts": 20.724, "power_cpu_cv_mean_watts": 1.642, "power_sys_5v0_mean_watts": 8.605, "gpu_utilization_percent_mean": 71.761, "power_watts_avg": 20.724, "energy_joules_est": 163.86, "duration_seconds": 7.907, "sample_count": 67}, "timestamp": "2026-01-26T17:01:59.107939"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11610.174, "latencies_ms": [11610.174], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 80, "n_tiles": 16, "output_text": "The image features a young girl sitting on a luggage cart, holding a child in her lap. The girl appears to be smiling and enjoying the moment. The luggage cart is surrounded by several cars, indicating that they are in a parking lot or a busy area. The girl and the child seem to be traveling together, possibly on a trip or vacation.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 21078.1, "ram_available_mb": 41762.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 21081.9, "ram_available_mb": 41759.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.939}, "power_stats": {"power_gpu_soc_mean_watts": 19.299, "power_cpu_cv_mean_watts": 1.908, "power_sys_5v0_mean_watts": 8.654, "gpu_utilization_percent_mean": 69.939, "power_watts_avg": 19.299, "energy_joules_est": 224.08, "duration_seconds": 11.611, "sample_count": 99}, "timestamp": "2026-01-26T17:02:12.783327"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9144.056, "latencies_ms": [9144.056], "images_per_second": 0.109, "prompt_tokens": 39, "response_tokens_est": 56, "n_tiles": 16, "output_text": "- Cars: 10\n- Cart: 1\n- Luggage: 2\n- Children: 2\n- Parking lot: 1\n- Store: 1\n- Sign: 1\n- Umbrella: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21081.9, "ram_available_mb": 41759.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21082.8, "ram_available_mb": 41758.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.333}, "power_stats": {"power_gpu_soc_mean_watts": 20.425, "power_cpu_cv_mean_watts": 1.744, "power_sys_5v0_mean_watts": 8.634, "gpu_utilization_percent_mean": 72.333, "power_watts_avg": 20.425, "energy_joules_est": 186.78, "duration_seconds": 9.145, "sample_count": 78}, "timestamp": "2026-01-26T17:02:23.942563"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11599.193, "latencies_ms": [11599.193], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a black car with a child sitting on a suitcase, positioned near the front of the image. Behind this car, there are several other vehicles, including a yellow car and a white van, which are parked in a line. The background features a blue sign with the text 'Budget BAY 17-18' indicating", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.8, "ram_available_mb": 41758.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21083.3, "ram_available_mb": 41757.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.429}, "power_stats": {"power_gpu_soc_mean_watts": 19.336, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.655, "gpu_utilization_percent_mean": 70.429, "power_watts_avg": 19.336, "energy_joules_est": 224.29, "duration_seconds": 11.6, "sample_count": 98}, "timestamp": "2026-01-26T17:02:37.581133"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9138.436, "latencies_ms": [9138.436], "images_per_second": 0.109, "prompt_tokens": 37, "response_tokens_est": 56, "n_tiles": 16, "output_text": "A young girl is sitting on a suitcase in a parking lot, holding a paper with the text \"Budget BAY 17-18\". The parking lot is filled with various cars and a yellow taxi is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21083.3, "ram_available_mb": 41757.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21083.8, "ram_available_mb": 41757.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.935}, "power_stats": {"power_gpu_soc_mean_watts": 20.529, "power_cpu_cv_mean_watts": 1.694, "power_sys_5v0_mean_watts": 8.583, "gpu_utilization_percent_mean": 72.935, "power_watts_avg": 20.529, "energy_joules_est": 187.62, "duration_seconds": 9.139, "sample_count": 77}, "timestamp": "2026-01-26T17:02:48.744622"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6617.456, "latencies_ms": [6617.456], "images_per_second": 0.151, "prompt_tokens": 36, "response_tokens_est": 36, "n_tiles": 16, "output_text": "The image shows a wet parking lot with various cars and a yellow taxi. The sky is overcast, and the ground appears to be wet, suggesting recent rain.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 21083.8, "ram_available_mb": 41757.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21084.3, "ram_available_mb": 41756.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.75}, "power_stats": {"power_gpu_soc_mean_watts": 21.778, "power_cpu_cv_mean_watts": 1.529, "power_sys_5v0_mean_watts": 8.649, "gpu_utilization_percent_mean": 73.75, "power_watts_avg": 21.778, "energy_joules_est": 144.13, "duration_seconds": 6.618, "sample_count": 56}, "timestamp": "2026-01-26T17:02:57.421055"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12394.305, "latencies_ms": [12394.305], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a city bus driving down a street in a small town. The bus is yellow and blue, and it is positioned in the middle of the street, with other vehicles nearby. There are several people walking on the sidewalk, some of them carrying handbags. \n\nIn the background, there are multiple cars parked or driving along the street, and a few more", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21084.3, "ram_available_mb": 41756.6, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21040.0, "ram_available_mb": 41800.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11182.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.453}, "power_stats": {"power_gpu_soc_mean_watts": 21.313, "power_cpu_cv_mean_watts": 1.805, "power_sys_5v0_mean_watts": 8.827, "gpu_utilization_percent_mean": 73.453, "power_watts_avg": 21.313, "energy_joules_est": 264.17, "duration_seconds": 12.395, "sample_count": 106}, "timestamp": "2026-01-26T17:03:11.845378"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 7905.204, "latencies_ms": [7905.204], "images_per_second": 0.126, "prompt_tokens": 39, "response_tokens_est": 38, "n_tiles": 16, "output_text": "bus: 1, van: 1, building: 3, tree: many, flower: many, grass: many, bus stop: 1, car: 1", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21040.0, "ram_available_mb": 41800.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 21052.5, "ram_available_mb": 41788.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11211.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.627}, "power_stats": {"power_gpu_soc_mean_watts": 23.307, "power_cpu_cv_mean_watts": 1.415, "power_sys_5v0_mean_watts": 8.73, "gpu_utilization_percent_mean": 78.627, "power_watts_avg": 23.307, "energy_joules_est": 184.26, "duration_seconds": 7.906, "sample_count": 67}, "timestamp": "2026-01-26T17:03:21.797936"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12180.318, "latencies_ms": [12180.318], "images_per_second": 0.082, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the foreground, there is a yellow bus with the word 'citylink' on it, driving on the right side of the road. Behind the bus, there is a white van with a yellow and red pattern on the side, also on the right side of the road. In the background, there are buildings on the left side of the road and a hill with trees on the right", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21052.5, "ram_available_mb": 41788.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21099.9, "ram_available_mb": 41741.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11221.3, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.223}, "power_stats": {"power_gpu_soc_mean_watts": 21.371, "power_cpu_cv_mean_watts": 1.834, "power_sys_5v0_mean_watts": 8.904, "gpu_utilization_percent_mean": 72.223, "power_watts_avg": 21.371, "energy_joules_est": 260.32, "duration_seconds": 12.181, "sample_count": 103}, "timestamp": "2026-01-26T17:03:36.034461"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8718.79, "latencies_ms": [8718.79], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 45, "n_tiles": 16, "output_text": "A blue and yellow bus with the word \"citylink\" on the front is driving down a street in a small town. There are houses on the left side of the street and a hill with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21099.9, "ram_available_mb": 41741.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21053.2, "ram_available_mb": 41787.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11207.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.243}, "power_stats": {"power_gpu_soc_mean_watts": 22.886, "power_cpu_cv_mean_watts": 1.465, "power_sys_5v0_mean_watts": 8.725, "gpu_utilization_percent_mean": 76.243, "power_watts_avg": 22.886, "energy_joules_est": 199.55, "duration_seconds": 8.719, "sample_count": 74}, "timestamp": "2026-01-26T17:03:46.803801"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 8150.117, "latencies_ms": [8150.117], "images_per_second": 0.123, "prompt_tokens": 36, "response_tokens_est": 44, "n_tiles": 16, "output_text": "A blue and yellow bus with the word \"citylink\" on the front is driving down the road. The sky is overcast and the trees in the background are bare, indicating it might be fall or winter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21053.2, "ram_available_mb": 41787.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21042.5, "ram_available_mb": 41798.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11205.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.609}, "power_stats": {"power_gpu_soc_mean_watts": 22.846, "power_cpu_cv_mean_watts": 1.554, "power_sys_5v0_mean_watts": 8.846, "gpu_utilization_percent_mean": 75.609, "power_watts_avg": 22.846, "energy_joules_est": 186.21, "duration_seconds": 8.151, "sample_count": 69}, "timestamp": "2026-01-26T17:03:56.987020"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11616.084, "latencies_ms": [11616.084], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a large bird, possibly a pelican, is standing on a rock near the ocean. The bird is positioned on the right side of the rock, with its head turned to the left, possibly observing the surroundings. The ocean is visible in the background, stretching across the scene.\n\nThere are several chairs scattered around the beach area, with some", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 21042.5, "ram_available_mb": 41798.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21077.9, "ram_available_mb": 41763.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.293}, "power_stats": {"power_gpu_soc_mean_watts": 19.327, "power_cpu_cv_mean_watts": 1.884, "power_sys_5v0_mean_watts": 8.641, "gpu_utilization_percent_mean": 70.293, "power_watts_avg": 19.327, "energy_joules_est": 224.52, "duration_seconds": 11.617, "sample_count": 99}, "timestamp": "2026-01-26T17:04:10.665352"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9952.174, "latencies_ms": [9952.174], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "1. Bird: 1\n2. Water: 1\n3. Sand: 1\n4. Rocks: 1\n5. Trees: 1\n6. Mountains: 1\n7. Umbrellas: 1\n8. Chairs: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21077.9, "ram_available_mb": 41763.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21087.4, "ram_available_mb": 41753.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.417}, "power_stats": {"power_gpu_soc_mean_watts": 20.086, "power_cpu_cv_mean_watts": 1.753, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 71.417, "power_watts_avg": 20.086, "energy_joules_est": 199.91, "duration_seconds": 9.953, "sample_count": 84}, "timestamp": "2026-01-26T17:04:22.630519"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10440.896, "latencies_ms": [10440.896], "images_per_second": 0.096, "prompt_tokens": 44, "response_tokens_est": 69, "n_tiles": 16, "output_text": "In the foreground, there is a rocky outcrop with a bird perched on top, positioned near the center of the image. The beach stretches out in the middle ground, leading to the calm sea in the background. The hills are situated far in the distance, creating a sense of depth in the landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21087.4, "ram_available_mb": 41753.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21087.4, "ram_available_mb": 41753.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.966}, "power_stats": {"power_gpu_soc_mean_watts": 19.695, "power_cpu_cv_mean_watts": 1.824, "power_sys_5v0_mean_watts": 8.628, "gpu_utilization_percent_mean": 70.966, "power_watts_avg": 19.695, "energy_joules_est": 205.65, "duration_seconds": 10.442, "sample_count": 88}, "timestamp": "2026-01-26T17:04:35.080842"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 11783.717, "latencies_ms": [11783.717], "images_per_second": 0.085, "prompt_tokens": 37, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image depicts a serene beach scene with a bird perched on a rock in the foreground. The bird appears to be a pelican, and it is looking out towards the ocean. The beach is lined with thatched umbrellas and chairs, and there are mountains in the background. The sky is cloudy, and the water is a beautiful shade of", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 21087.4, "ram_available_mb": 41753.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21087.4, "ram_available_mb": 41753.5, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.376}, "power_stats": {"power_gpu_soc_mean_watts": 19.405, "power_cpu_cv_mean_watts": 1.866, "power_sys_5v0_mean_watts": 8.626, "gpu_utilization_percent_mean": 70.376, "power_watts_avg": 19.405, "energy_joules_est": 228.68, "duration_seconds": 11.784, "sample_count": 101}, "timestamp": "2026-01-26T17:04:48.906257"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11555.527, "latencies_ms": [11555.527], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a seagull perched on a rocky outcrop with a backdrop of a cloudy sky and a calm sea. The colors in the image are predominantly blue and grey, with the blue representing the sea and sky, and the grey representing the clouds. The lighting is natural and soft, with the sun shining through the clouds, creating a peaceful", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21087.4, "ram_available_mb": 41753.5, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 21106.3, "ram_available_mb": 41734.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.05}, "power_stats": {"power_gpu_soc_mean_watts": 19.222, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 71.05, "power_watts_avg": 19.222, "energy_joules_est": 222.13, "duration_seconds": 11.556, "sample_count": 100}, "timestamp": "2026-01-26T17:05:02.486867"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11600.59, "latencies_ms": [11600.59], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a young man is comfortably seated in a gray recliner chair. He is wearing a blue plaid shirt and glasses, and he's holding a brown paper bag in his left hand. His right hand is holding a black remote control. The chair is positioned against a white wall, and there's a window in the background. The man", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 21044.0, "ram_available_mb": 41796.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21061.3, "ram_available_mb": 41779.6, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.384}, "power_stats": {"power_gpu_soc_mean_watts": 19.295, "power_cpu_cv_mean_watts": 1.892, "power_sys_5v0_mean_watts": 8.675, "gpu_utilization_percent_mean": 70.384, "power_watts_avg": 19.295, "energy_joules_est": 223.85, "duration_seconds": 11.601, "sample_count": 99}, "timestamp": "2026-01-26T17:05:16.143291"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9953.834, "latencies_ms": [9953.834], "images_per_second": 0.1, "prompt_tokens": 39, "response_tokens_est": 63, "n_tiles": 16, "output_text": "- Chair: 1\n\n- Remote control: 2\n\n- Paper bag: 1\n\n- Paper: 1\n\n- Table: 1\n\n- Couch: 1\n\n- Cable: 1\n\n- Wire: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21061.3, "ram_available_mb": 41779.6, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21110.8, "ram_available_mb": 41730.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.452}, "power_stats": {"power_gpu_soc_mean_watts": 20.119, "power_cpu_cv_mean_watts": 1.763, "power_sys_5v0_mean_watts": 8.625, "gpu_utilization_percent_mean": 71.452, "power_watts_avg": 20.119, "energy_joules_est": 200.27, "duration_seconds": 9.954, "sample_count": 84}, "timestamp": "2026-01-26T17:05:28.111303"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 8649.288, "latencies_ms": [8649.288], "images_per_second": 0.116, "prompt_tokens": 44, "response_tokens_est": 53, "n_tiles": 16, "output_text": "The person is seated in a chair that is positioned in the foreground of the image. The chair is placed against a wall in the background, and there is a remote control on the right side of the image, near the person's lap.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21110.8, "ram_available_mb": 41730.1, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21111.3, "ram_available_mb": 41729.6, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.986}, "power_stats": {"power_gpu_soc_mean_watts": 20.256, "power_cpu_cv_mean_watts": 1.689, "power_sys_5v0_mean_watts": 8.648, "gpu_utilization_percent_mean": 71.986, "power_watts_avg": 20.256, "energy_joules_est": 175.21, "duration_seconds": 8.65, "sample_count": 73}, "timestamp": "2026-01-26T17:05:38.808569"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 7401.351, "latencies_ms": [7401.351], "images_per_second": 0.135, "prompt_tokens": 37, "response_tokens_est": 41, "n_tiles": 16, "output_text": "A person is sitting in a gray recliner chair, holding a brown paper bag in one hand and a remote control in the other, possibly preparing to watch television or enjoy a snack.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21041.1, "ram_available_mb": 41799.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21111.2, "ram_available_mb": 41729.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.548}, "power_stats": {"power_gpu_soc_mean_watts": 21.68, "power_cpu_cv_mean_watts": 1.581, "power_sys_5v0_mean_watts": 8.592, "gpu_utilization_percent_mean": 74.548, "power_watts_avg": 21.68, "energy_joules_est": 160.47, "duration_seconds": 7.402, "sample_count": 62}, "timestamp": "2026-01-26T17:05:48.232079"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6865.485, "latencies_ms": [6865.485], "images_per_second": 0.146, "prompt_tokens": 36, "response_tokens_est": 38, "n_tiles": 16, "output_text": "The person is wearing a blue and white checkered shirt and is seated in a gray recliner. There is a remote control on the armrest of the chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21032.8, "ram_available_mb": 41808.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21113.8, "ram_available_mb": 41727.1, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.22}, "power_stats": {"power_gpu_soc_mean_watts": 21.485, "power_cpu_cv_mean_watts": 1.54, "power_sys_5v0_mean_watts": 8.618, "gpu_utilization_percent_mean": 74.22, "power_watts_avg": 21.485, "energy_joules_est": 147.52, "duration_seconds": 6.866, "sample_count": 59}, "timestamp": "2026-01-26T17:05:57.120361"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11565.561, "latencies_ms": [11565.561], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a man is captured in the midst of a tennis match. He is dressed in an orange shirt and black shorts, with a matching orange cap. His right hand grips a blue tennis racket, poised to strike the yellow tennis ball that hovers near the net. The green tennis court beneath him is marked with crisp white lines, indicating the boundaries of", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 21034.6, "ram_available_mb": 41806.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21078.8, "ram_available_mb": 41762.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 68.848}, "power_stats": {"power_gpu_soc_mean_watts": 19.174, "power_cpu_cv_mean_watts": 1.896, "power_sys_5v0_mean_watts": 8.631, "gpu_utilization_percent_mean": 68.848, "power_watts_avg": 19.174, "energy_joules_est": 221.77, "duration_seconds": 11.566, "sample_count": 99}, "timestamp": "2026-01-26T17:06:10.714889"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10435.452, "latencies_ms": [10435.452], "images_per_second": 0.096, "prompt_tokens": 39, "response_tokens_est": 67, "n_tiles": 16, "output_text": "- tennis ball: 1\n\n- tennis racket: 1\n\n- tennis court: 1\n\n- tennis net: 1\n\n- tennis player: 1\n\n- orange shirt: 1\n\n- black shorts: 1\n\n- white socks: 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21078.8, "ram_available_mb": 41762.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 8.4, "ram_used_mb": 21097.1, "ram_available_mb": 41743.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10437.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.42}, "power_stats": {"power_gpu_soc_mean_watts": 19.833, "power_cpu_cv_mean_watts": 2.079, "power_sys_5v0_mean_watts": 8.652, "gpu_utilization_percent_mean": 71.42, "power_watts_avg": 19.833, "energy_joules_est": 206.98, "duration_seconds": 10.436, "sample_count": 88}, "timestamp": "2026-01-26T17:06:23.211243"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 7529.443, "latencies_ms": [7529.443], "images_per_second": 0.133, "prompt_tokens": 44, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The tennis player is positioned in the foreground on the left side of the image, near the net which is in the middle ground. The background shows the tennis court extending to the edges of the image.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21097.1, "ram_available_mb": 41743.8, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21077.2, "ram_available_mb": 41763.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10445.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.952}, "power_stats": {"power_gpu_soc_mean_watts": 21.274, "power_cpu_cv_mean_watts": 1.569, "power_sys_5v0_mean_watts": 8.611, "gpu_utilization_percent_mean": 73.952, "power_watts_avg": 21.274, "energy_joules_est": 160.19, "duration_seconds": 7.53, "sample_count": 63}, "timestamp": "2026-01-26T17:06:32.753379"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8716.991, "latencies_ms": [8716.991], "images_per_second": 0.115, "prompt_tokens": 37, "response_tokens_est": 52, "n_tiles": 16, "output_text": "A tennis player in an orange shirt and black shorts is in the middle of a forehand swing, hitting a yellow tennis ball with a blue and white racket. The player is on a green tennis court with white lines marking the boundaries.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21077.2, "ram_available_mb": 41763.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21030.9, "ram_available_mb": 41810.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10433.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.703}, "power_stats": {"power_gpu_soc_mean_watts": 20.512, "power_cpu_cv_mean_watts": 1.666, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 73.703, "power_watts_avg": 20.512, "energy_joules_est": 178.82, "duration_seconds": 8.718, "sample_count": 74}, "timestamp": "2026-01-26T17:06:43.492687"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 7546.742, "latencies_ms": [7546.742], "images_per_second": 0.133, "prompt_tokens": 36, "response_tokens_est": 43, "n_tiles": 16, "output_text": "The tennis player is wearing an orange shirt and a matching cap, and the court appears to be a vibrant green. The lighting seems natural, suggesting the photo was taken during the day.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21030.9, "ram_available_mb": 41810.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21107.0, "ram_available_mb": 41733.9, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10431.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.8}, "power_stats": {"power_gpu_soc_mean_watts": 20.969, "power_cpu_cv_mean_watts": 1.588, "power_sys_5v0_mean_watts": 8.607, "gpu_utilization_percent_mean": 73.8, "power_watts_avg": 20.969, "energy_joules_est": 158.26, "duration_seconds": 7.547, "sample_count": 65}, "timestamp": "2026-01-26T17:06:53.070473"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12409.801, "latencies_ms": [12409.801], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a cozy, compact kitchen bathed in warm yellow light. The walls, painted in a cheerful yellow, contrast with the white ceiling, creating a vibrant atmosphere. A black floor adds a touch of elegance to the space.\n\nOn the left, a white countertop houses a sink, a stove, and a refrigerator, all", "error": null, "sys_before": {"cpu_percent": 9.4, "ram_used_mb": 21033.6, "ram_available_mb": 41807.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21079.8, "ram_available_mb": 41761.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.283}, "power_stats": {"power_gpu_soc_mean_watts": 21.539, "power_cpu_cv_mean_watts": 1.808, "power_sys_5v0_mean_watts": 8.928, "gpu_utilization_percent_mean": 73.283, "power_watts_avg": 21.539, "energy_joules_est": 267.31, "duration_seconds": 12.41, "sample_count": 106}, "timestamp": "2026-01-26T17:07:07.558081"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 10643.962, "latencies_ms": [10643.962], "images_per_second": 0.094, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Shelf: 2\n- Canister: 4\n- Stove: 1\n- Oven: 1\n- Countertop: 1\n- Chair: 1\n- Fire extinguisher: 1\n- Pipes: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21079.8, "ram_available_mb": 41761.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 21080.8, "ram_available_mb": 41760.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.311}, "power_stats": {"power_gpu_soc_mean_watts": 22.421, "power_cpu_cv_mean_watts": 1.605, "power_sys_5v0_mean_watts": 8.773, "gpu_utilization_percent_mean": 76.311, "power_watts_avg": 22.421, "energy_joules_est": 238.66, "duration_seconds": 10.645, "sample_count": 90}, "timestamp": "2026-01-26T17:07:20.230047"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12369.36, "latencies_ms": [12369.36], "images_per_second": 0.081, "prompt_tokens": 44, "response_tokens_est": 76, "n_tiles": 16, "output_text": "The kitchen area is in the foreground of the image, with the sink and stove positioned near the center. The background features a counter with various items on it, and there is a red fire extinguisher mounted on the wall to the left of the sink. The ceiling has a skylight above the sink, providing natural light to the space.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21080.8, "ram_available_mb": 41760.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21082.0, "ram_available_mb": 41758.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.057}, "power_stats": {"power_gpu_soc_mean_watts": 21.742, "power_cpu_cv_mean_watts": 1.757, "power_sys_5v0_mean_watts": 8.868, "gpu_utilization_percent_mean": 72.057, "power_watts_avg": 21.742, "energy_joules_est": 268.95, "duration_seconds": 12.37, "sample_count": 105}, "timestamp": "2026-01-26T17:07:34.634969"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8314.446, "latencies_ms": [8314.446], "images_per_second": 0.12, "prompt_tokens": 37, "response_tokens_est": 39, "n_tiles": 16, "output_text": "The image shows a small, compact kitchen with yellow walls and wooden cabinets. There is a sink, stove, and a window above the counter, providing natural light to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.0, "ram_available_mb": 41758.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 21082.2, "ram_available_mb": 41758.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.714}, "power_stats": {"power_gpu_soc_mean_watts": 23.6, "power_cpu_cv_mean_watts": 1.378, "power_sys_5v0_mean_watts": 8.738, "gpu_utilization_percent_mean": 78.714, "power_watts_avg": 23.6, "energy_joules_est": 196.24, "duration_seconds": 8.315, "sample_count": 70}, "timestamp": "2026-01-26T17:07:44.969485"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9421.188, "latencies_ms": [9421.188], "images_per_second": 0.106, "prompt_tokens": 36, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The kitchen is predominantly yellow with wooden cabinets and a red fire extinguisher on the wall. The lighting is bright, coming from multiple ceiling lights, and the floor is covered with a black and white checkered mat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.2, "ram_available_mb": 41758.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21083.2, "ram_available_mb": 41757.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_gpu_soc_mean_watts": 22.756, "power_cpu_cv_mean_watts": 1.556, "power_sys_5v0_mean_watts": 8.856, "gpu_utilization_percent_mean": 77.0, "power_watts_avg": 22.756, "energy_joules_est": 214.4, "duration_seconds": 9.422, "sample_count": 80}, "timestamp": "2026-01-26T17:07:56.405131"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11583.525, "latencies_ms": [11583.525], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the center of the image, a delicious sandwich takes the spotlight. It's a classic sub sandwich, generously filled with layers of sliced turkey, ham, and roast beef. The sandwich is nestled between two slices of white bread, each layer of meat adding to the mouth-watering appeal. \n\nThe sandwich is", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 21083.2, "ram_available_mb": 41757.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21034.8, "ram_available_mb": 41806.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.63}, "power_stats": {"power_gpu_soc_mean_watts": 19.33, "power_cpu_cv_mean_watts": 1.897, "power_sys_5v0_mean_watts": 8.657, "gpu_utilization_percent_mean": 70.63, "power_watts_avg": 19.33, "energy_joules_est": 223.92, "duration_seconds": 11.584, "sample_count": 100}, "timestamp": "2026-01-26T17:08:10.036941"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9490.571, "latencies_ms": [9490.571], "images_per_second": 0.105, "prompt_tokens": 39, "response_tokens_est": 59, "n_tiles": 16, "output_text": "- Bread: 2 slices\n- Lettuce: 2 leaves\n- Tomato: 1 slice\n- Ham: 2 slices\n- Pickles: 2 pickles\n- Pepper: 1 pepper\n- Computer: 1 computer", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21034.8, "ram_available_mb": 41806.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21074.0, "ram_available_mb": 41766.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.617}, "power_stats": {"power_gpu_soc_mean_watts": 20.29, "power_cpu_cv_mean_watts": 1.744, "power_sys_5v0_mean_watts": 8.603, "gpu_utilization_percent_mean": 71.617, "power_watts_avg": 20.29, "energy_joules_est": 192.58, "duration_seconds": 9.491, "sample_count": 81}, "timestamp": "2026-01-26T17:08:21.551976"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 10909.373, "latencies_ms": [10909.373], "images_per_second": 0.092, "prompt_tokens": 44, "response_tokens_est": 73, "n_tiles": 16, "output_text": "The sandwich is in the foreground, placed on a white paper plate which is on a wooden table. In the background, there is a computer monitor and a keyboard, suggesting that the setting is an office or a home workspace. The sandwich is positioned to the left of the monitor, and the keyboard is to the right of the monitor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21074.0, "ram_available_mb": 41766.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 21082.7, "ram_available_mb": 41758.2, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.413}, "power_stats": {"power_gpu_soc_mean_watts": 19.353, "power_cpu_cv_mean_watts": 1.849, "power_sys_5v0_mean_watts": 8.64, "gpu_utilization_percent_mean": 70.413, "power_watts_avg": 19.353, "energy_joules_est": 211.14, "duration_seconds": 10.91, "sample_count": 92}, "timestamp": "2026-01-26T17:08:34.503039"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8920.165, "latencies_ms": [8920.165], "images_per_second": 0.112, "prompt_tokens": 37, "response_tokens_est": 54, "n_tiles": 16, "output_text": "A sandwich is placed on a white paper plate on a wooden table, with a computer monitor and keyboard in the background. The sandwich appears to be a multi-layered sub with various ingredients, including lettuce, tomato, and meat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21082.7, "ram_available_mb": 41758.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21083.2, "ram_available_mb": 41757.7, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.724}, "power_stats": {"power_gpu_soc_mean_watts": 20.535, "power_cpu_cv_mean_watts": 1.701, "power_sys_5v0_mean_watts": 8.613, "gpu_utilization_percent_mean": 72.724, "power_watts_avg": 20.535, "energy_joules_est": 183.19, "duration_seconds": 8.921, "sample_count": 76}, "timestamp": "2026-01-26T17:08:45.452101"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 6422.135, "latencies_ms": [6422.135], "images_per_second": 0.156, "prompt_tokens": 36, "response_tokens_est": 34, "n_tiles": 16, "output_text": "The sandwich is placed on a white paper plate, which is on a wooden table. The lighting in the room is dim, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 21083.2, "ram_available_mb": 41757.7, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 21096.9, "ram_available_mb": 41744.0, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 74.093}, "power_stats": {"power_gpu_soc_mean_watts": 21.883, "power_cpu_cv_mean_watts": 1.489, "power_sys_5v0_mean_watts": 8.601, "gpu_utilization_percent_mean": 74.093, "power_watts_avg": 21.883, "energy_joules_est": 140.55, "duration_seconds": 6.423, "sample_count": 54}, "timestamp": "2026-01-26T17:08:53.930288"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 11596.825, "latencies_ms": [11596.825], "images_per_second": 0.086, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image features a desk with a computer setup, including two monitors, a keyboard, and a mouse. The monitors are positioned side by side, with one slightly larger than the other. The keyboard is placed in front of the monitors, and the mouse is located to the right of the keyboard. \n\nThere are also two cell phones on the desk, one", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 21096.9, "ram_available_mb": 41744.0, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21097.7, "ram_available_mb": 41743.2, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10391.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 69.867}, "power_stats": {"power_gpu_soc_mean_watts": 19.308, "power_cpu_cv_mean_watts": 1.895, "power_sys_5v0_mean_watts": 8.662, "gpu_utilization_percent_mean": 69.867, "power_watts_avg": 19.308, "energy_joules_est": 223.92, "duration_seconds": 11.597, "sample_count": 98}, "timestamp": "2026-01-26T17:09:07.575009"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9055.482, "latencies_ms": [9055.482], "images_per_second": 0.11, "prompt_tokens": 39, "response_tokens_est": 55, "n_tiles": 16, "output_text": "- Computer monitor: 2\n- Keyboard: 1\n- Computer mouse: 1\n- Cell phone: 2\n- Tablet: 1\n- Wire: 5\n- Screen: 2\n- Monitor: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21035.7, "ram_available_mb": 41805.2, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 21035.5, "ram_available_mb": 41805.4, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10419.7, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.156}, "power_stats": {"power_gpu_soc_mean_watts": 20.451, "power_cpu_cv_mean_watts": 1.705, "power_sys_5v0_mean_watts": 8.582, "gpu_utilization_percent_mean": 72.156, "power_watts_avg": 20.451, "energy_joules_est": 185.21, "duration_seconds": 9.056, "sample_count": 77}, "timestamp": "2026-01-26T17:09:18.643957"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 11592.875, "latencies_ms": [11592.875], "images_per_second": 0.086, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image shows a workspace with two computer monitors positioned closely together, forming an arc. In the foreground, there is a white keyboard and a white mouse, both in front of the monitors. To the left of the monitors, there are various electronic devices, including a smartphone and a tablet, which are placed near the monitors but slightly in the background. The", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21035.5, "ram_available_mb": 41805.4, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21076.2, "ram_available_mb": 41764.7, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10425.9, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.276}, "power_stats": {"power_gpu_soc_mean_watts": 19.312, "power_cpu_cv_mean_watts": 1.886, "power_sys_5v0_mean_watts": 8.65, "gpu_utilization_percent_mean": 70.276, "power_watts_avg": 19.312, "energy_joules_est": 223.89, "duration_seconds": 11.594, "sample_count": 98}, "timestamp": "2026-01-26T17:09:32.263559"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 8575.244, "latencies_ms": [8575.244], "images_per_second": 0.117, "prompt_tokens": 37, "response_tokens_est": 51, "n_tiles": 16, "output_text": "The image depicts a workspace with a dual monitor computer setup, a keyboard, a mouse, and a smartphone. The monitors are turned on and displaying various web pages, suggesting that the user is working or browsing the internet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21076.2, "ram_available_mb": 41764.7, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 21093.1, "ram_available_mb": 41747.8, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10412.8, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.192}, "power_stats": {"power_gpu_soc_mean_watts": 20.76, "power_cpu_cv_mean_watts": 1.65, "power_sys_5v0_mean_watts": 8.566, "gpu_utilization_percent_mean": 73.192, "power_watts_avg": 20.76, "energy_joules_est": 178.04, "duration_seconds": 8.576, "sample_count": 73}, "timestamp": "2026-01-26T17:09:42.870580"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 11462.735, "latencies_ms": [11462.735], "images_per_second": 0.087, "prompt_tokens": 36, "response_tokens_est": 78, "n_tiles": 16, "output_text": "The image shows a desk with two computer monitors, one of which is turned on displaying a webpage with a red and white color scheme. The other monitor is off, and there is a white keyboard and a white mouse in front of it. The desk is illuminated by ambient light, and there are various cables and a small smartphone on the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21029.8, "ram_available_mb": 41811.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 21068.0, "ram_available_mb": 41772.9, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 10411.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 70.52}, "power_stats": {"power_gpu_soc_mean_watts": 19.28, "power_cpu_cv_mean_watts": 1.891, "power_sys_5v0_mean_watts": 8.666, "gpu_utilization_percent_mean": 70.52, "power_watts_avg": 19.28, "energy_joules_est": 221.01, "duration_seconds": 11.463, "sample_count": 98}, "timestamp": "2026-01-26T17:09:56.347548"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12430.676, "latencies_ms": [12430.676], "images_per_second": 0.08, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The image captures a group of people lying on the floor, with their heads and upper bodies visible. They are all looking upwards, possibly at something above them. The scene takes place in a bathroom, as evidenced by the presence of a toilet in the background. The people are spread out across the floor, with some closer to the camera and others further away. The", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 21068.0, "ram_available_mb": 41772.9, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21030.9, "ram_available_mb": 41810.0, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.085}, "power_stats": {"power_gpu_soc_mean_watts": 21.503, "power_cpu_cv_mean_watts": 1.816, "power_sys_5v0_mean_watts": 8.92, "gpu_utilization_percent_mean": 73.085, "power_watts_avg": 21.503, "energy_joules_est": 267.31, "duration_seconds": 12.431, "sample_count": 106}, "timestamp": "2026-01-26T17:10:10.814305"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 9816.013, "latencies_ms": [9816.013], "images_per_second": 0.102, "prompt_tokens": 39, "response_tokens_est": 52, "n_tiles": 16, "output_text": "person: 10, hand: 10, tile: 100, floor: 100, wall: 100, reflection: 100, camera: 1, toilet: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 21030.9, "ram_available_mb": 41810.0, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 21062.4, "ram_available_mb": 41778.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.675}, "power_stats": {"power_gpu_soc_mean_watts": 22.744, "power_cpu_cv_mean_watts": 1.548, "power_sys_5v0_mean_watts": 8.769, "gpu_utilization_percent_mean": 75.675, "power_watts_avg": 22.744, "energy_joules_est": 223.27, "duration_seconds": 9.817, "sample_count": 83}, "timestamp": "2026-01-26T17:10:22.642776"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12757.516, "latencies_ms": [12757.516], "images_per_second": 0.078, "prompt_tokens": 44, "response_tokens_est": 79, "n_tiles": 16, "output_text": "The image shows a group of people lying on the ground in the foreground, with their heads and upper bodies visible. They are positioned in a way that creates a mosaic effect with the tiles of the floor. In the background, there is a toilet bowl, which is part of the wall and appears to be at a higher elevation than the people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21062.4, "ram_available_mb": 41778.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 21068.4, "ram_available_mb": 41772.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.798}, "power_stats": {"power_gpu_soc_mean_watts": 21.598, "power_cpu_cv_mean_watts": 2.031, "power_sys_5v0_mean_watts": 8.889, "gpu_utilization_percent_mean": 72.798, "power_watts_avg": 21.598, "energy_joules_est": 275.55, "duration_seconds": 12.758, "sample_count": 109}, "timestamp": "2026-01-26T17:10:37.439200"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 10518.21, "latencies_ms": [10518.21], "images_per_second": 0.095, "prompt_tokens": 37, "response_tokens_est": 58, "n_tiles": 16, "output_text": "The image shows a group of people lying on the floor in a public restroom, with their heads and upper bodies visible through the gaps between the tiles. They appear to be playfully posing for a photo, with some of them making funny faces and gestures.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 21068.4, "ram_available_mb": 41772.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 21068.6, "ram_available_mb": 41772.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 75.618}, "power_stats": {"power_gpu_soc_mean_watts": 22.512, "power_cpu_cv_mean_watts": 1.587, "power_sys_5v0_mean_watts": 8.811, "gpu_utilization_percent_mean": 75.618, "power_watts_avg": 22.512, "energy_joules_est": 236.8, "duration_seconds": 10.519, "sample_count": 89}, "timestamp": "2026-01-26T17:10:50.015109"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 12119.786, "latencies_ms": [12119.786], "images_per_second": 0.083, "prompt_tokens": 36, "response_tokens_est": 74, "n_tiles": 16, "output_text": "The image shows a group of people lying on the floor with their heads down, creating a pattern that resembles a mosaic. The lighting is bright and artificial, illuminating the scene with a clear and even distribution of light. The floor is tiled, and the people are wearing casual clothing, suggesting an indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21068.6, "ram_available_mb": 41772.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21069.4, "ram_available_mb": 41771.5, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 73.65}, "power_stats": {"power_gpu_soc_mean_watts": 21.743, "power_cpu_cv_mean_watts": 1.744, "power_sys_5v0_mean_watts": 8.884, "gpu_utilization_percent_mean": 73.65, "power_watts_avg": 21.743, "energy_joules_est": 263.54, "duration_seconds": 12.12, "sample_count": 103}, "timestamp": "2026-01-26T17:11:04.175942"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Describe this image in detail.", "latency_ms": 12398.23, "latencies_ms": [12398.23], "images_per_second": 0.081, "prompt_tokens": 24, "response_tokens_est": 81, "n_tiles": 16, "output_text": "In the image, a small gray bird is perched on a tree branch. The bird is facing to the left, and its head is turned slightly towards the camera. The branch it's perched on is brown and has a few small leaves on it. The background is a blurred green, suggesting a lush, leafy environment. The bird's feathers are a mix", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 21069.4, "ram_available_mb": 41771.5, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 21078.1, "ram_available_mb": 41762.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11491.2, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 71.667}, "power_stats": {"power_gpu_soc_mean_watts": 21.558, "power_cpu_cv_mean_watts": 1.807, "power_sys_5v0_mean_watts": 8.928, "gpu_utilization_percent_mean": 71.667, "power_watts_avg": 21.558, "energy_joules_est": 267.3, "duration_seconds": 12.399, "sample_count": 105}, "timestamp": "2026-01-26T17:11:18.626982"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 8193.663, "latencies_ms": [8193.663], "images_per_second": 0.122, "prompt_tokens": 39, "response_tokens_est": 38, "n_tiles": 16, "output_text": "bird: 1, branch: 2, leaves: multiple, background: multiple, light: multiple, bokeh: multiple, sky: 1, tree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21078.1, "ram_available_mb": 41762.8, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 21078.8, "ram_available_mb": 41762.1, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11522.1, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 78.696}, "power_stats": {"power_gpu_soc_mean_watts": 23.592, "power_cpu_cv_mean_watts": 1.351, "power_sys_5v0_mean_watts": 8.736, "gpu_utilization_percent_mean": 78.696, "power_watts_avg": 23.592, "energy_joules_est": 193.33, "duration_seconds": 8.195, "sample_count": 69}, "timestamp": "2026-01-26T17:11:28.875211"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 12709.684, "latencies_ms": [12709.684], "images_per_second": 0.079, "prompt_tokens": 44, "response_tokens_est": 81, "n_tiles": 16, "output_text": "The bird is perched on a branch in the foreground of the image, with a blurred green background that suggests a dense foliage environment. The bird is positioned slightly to the left of the center of the image, and there is a clear space between it and the background, indicating it is the main subject of the photo. The branch it is perched on is in the", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21078.8, "ram_available_mb": 41762.1, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 21078.6, "ram_available_mb": 41762.3, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11531.6, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 72.306}, "power_stats": {"power_gpu_soc_mean_watts": 21.635, "power_cpu_cv_mean_watts": 1.779, "power_sys_5v0_mean_watts": 8.886, "gpu_utilization_percent_mean": 72.306, "power_watts_avg": 21.635, "energy_joules_est": 274.99, "duration_seconds": 12.71, "sample_count": 108}, "timestamp": "2026-01-26T17:11:43.622795"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 9121.912, "latencies_ms": [9121.912], "images_per_second": 0.11, "prompt_tokens": 37, "response_tokens_est": 46, "n_tiles": 16, "output_text": "A small bird is perched on a branch with a blurred green background, suggesting a natural, outdoor setting. The bird appears to be looking around, possibly observing its surroundings or searching for food.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 21078.6, "ram_available_mb": 41762.3, "ram_percent": 33.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 21083.5, "ram_available_mb": 41757.4, "ram_percent": 33.6}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11517.5, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 77.403}, "power_stats": {"power_gpu_soc_mean_watts": 23.095, "power_cpu_cv_mean_watts": 1.465, "power_sys_5v0_mean_watts": 8.771, "gpu_utilization_percent_mean": 77.403, "power_watts_avg": 23.095, "energy_joules_est": 210.69, "duration_seconds": 9.123, "sample_count": 77}, "timestamp": "2026-01-26T17:11:54.761065"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/waggle/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 9991.622, "latencies_ms": [9991.622], "images_per_second": 0.1, "prompt_tokens": 36, "response_tokens_est": 56, "n_tiles": 16, "output_text": "The bird is perched on a branch with a backdrop of green foliage, suggesting a natural, outdoor setting. The lighting is soft and diffused, indicating that the photo may have been taken on an overcast day or in a shaded area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 21083.5, "ram_available_mb": 41757.4, "ram_percent": 33.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 21080.1, "ram_available_mb": 41760.8, "ram_percent": 33.5}, "cuda_stats": {"cuda": true, "gpu_name": "Orin", "gpu_mem_alloc_mb": 7918.4, "gpu_mem_reserved_mb": 14128.0, "gpu_max_mem_alloc_mb": 11515.4, "gpu_max_mem_reserved_mb": 14128.0, "gpu_utilization_percent": 76.294}, "power_stats": {"power_gpu_soc_mean_watts": 22.505, "power_cpu_cv_mean_watts": 1.606, "power_sys_5v0_mean_watts": 8.855, "gpu_utilization_percent_mean": 76.294, "power_watts_avg": 22.505, "energy_joules_est": 224.88, "duration_seconds": 9.992, "sample_count": 85}, "timestamp": "2026-01-26T17:12:06.772242"}
